{
  "nodes": [
    {
      "id": "I Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Overview of supervised learning techniques and algorithms."
    },
    {
      "id": "1 Linear regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to linear regression models and their applications."
    },
    {
      "id": "1.1 LMS algorithm",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Least Mean Squares (LMS) algorithm for updating model parameters iteratively."
    },
    {
      "id": "1.2 The normal equations",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Derivation and application of the normal equation method in linear regression."
    },
    {
      "id": "1.2.1 Matrix derivatives",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Calculation of matrix derivatives used in deriving the normal equations."
    },
    {
      "id": "1.2.2 Least squares revisited",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Revisiting least squares method with a focus on optimization techniques."
    },
    {
      "id": "1.3 Probabilistic interpretation",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Probabilistic view of linear regression and its assumptions."
    },
    {
      "id": "1.4 Locally weighted linear regression (optional reading)",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Advanced topic: locally weighted linear regression for non-stationary data."
    },
    {
      "id": "2 Classification and logistic regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to classification problems and logistic regression models."
    },
    {
      "id": "2.1 Logistic regression",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Logistic regression model for binary classification tasks."
    },
    {
      "id": "2.2 Digression: the perceptron learning algorithm",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Overview of the Perceptron Learning Algorithm as a precursor to neural networks."
    },
    {
      "id": "2.3 Multi-class classification",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Classification problem where response variable can take on more than two values."
    },
    {
      "id": "2.4 Another algorithm for maximizing λ(θ)",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Alternative methods for optimizing the likelihood function in classification tasks."
    },
    {
      "id": "3 Generalized linear models",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generalized linear models (GLMs) and their applications."
    },
    {
      "id": "3.1 The exponential family",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Overview of the exponential family distributions used in GLMs."
    },
    {
      "id": "3.2 Constructing GLMs",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Steps and methods for constructing generalized linear models."
    },
    {
      "id": "3.2.1 Ordinary least squares",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Ordinary Least Squares (OLS) as a special case of GLMs."
    },
    {
      "id": "3.2.2 Logistic regression",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Logistic regression model within the framework of generalized linear models."
    },
    {
      "id": "4 Generative learning algorithms",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generative learning approaches in classification tasks."
    },
    {
      "id": "4.1 Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Gaussian Discriminant Analysis (GDA) for binary and multi-class classification."
    },
    {
      "id": "4.1.1 The multivariate normal distribution",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Properties and applications of the multivariate normal distribution in GDA."
    },
    {
      "id": "4.1.2 The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Formulation and derivation of the GDA model for classification tasks."
    },
    {
      "id": "4.1.3 Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Comparison between GDA and logistic regression in terms of assumptions and performance."
    },
    {
      "id": "4.2 Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Naive Bayes classifier for text classification tasks with Laplace smoothing."
    },
    {
      "id": "4.2.1 Laplace smoothing",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Technique to handle zero probabilities in the naive Bayes model."
    },
    {
      "id": "4.2.2 Event models for text classification",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Application of event models in text classification using naive Bayes."
    },
    {
      "id": "5 Kernel methods",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to kernel methods for non-linear data transformation and analysis."
    },
    {
      "id": "5.1 Feature maps",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Concept of feature mapping in the context of kernel methods."
    },
    {
      "id": "5.2 LMS (least mean squares) with features",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Application of LMS algorithm with transformed features using kernels."
    },
    {
      "id": "5.3 LMS with the kernel trick",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Use of the kernel trick to extend LMS for non-linear feature spaces."
    },
    {
      "id": "5.4 Properties of kernels",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Mathematical properties and requirements for valid kernel functions."
    },
    {
      "id": "6 Support vector machines",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to support vector machines (SVMs) for classification tasks."
    },
    {
      "id": "II Deep learning",
      "type": "major",
      "parent": null,
      "description": "Overview of deep learning techniques and neural network architectures."
    },
    {
      "id": "7 Deep learning",
      "type": "subnode",
      "parent": "II Deep learning",
      "description": "Introduction to deep learning concepts and applications in machine learning."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of advanced neural network concepts and techniques."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Discussion on the components that make up modern neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Explanation of backpropagation technique for training neural networks."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts required for understanding backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the general approach used in backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Explanation of backward propagation for fundamental network components."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Detailed explanation of backpropagation in multi-layer perceptrons."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques for efficient computation using vector operations."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Discussion on how models generalize to unseen data and methods to prevent overfitting."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Overview of concepts related to model generalization."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Explanation of the balance between bias and variance in machine learning models."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical breakdown for understanding bias and variance in regression tasks."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Discussion on the unexpected performance improvement with increased model complexity."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Bounds on sample size required for learning tasks, including finite and infinite hypothesis spaces."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting and methods for selecting the best model."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Technique used to prevent overfitting by adding a regularizer term to the loss function."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Discussion on how certain algorithms inherently regularize models."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Use of cross-validation for selecting the best performing model."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian methods in regularizing models."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to clustering methods with a focus on the k-means algorithm."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Explanation of Expectation-Maximization algorithms and their applications."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to Gaussian Mixture Models."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in the derivation and understanding of EM algorithms."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Broad discussion on the general theory and applications of EM algorithms."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspectives on Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Revisit and deepen understanding of mixture models using Gaussian distributions."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topics in variational inference including VAEs."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique for data preprocessing."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique for separating mixed signals into independent sources."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent challenges and limitations of ICA."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Mathematical treatment of densities in the context of linear transformations."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis procedure."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to self-supervised learning techniques and large foundational models."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pre-training methods and their role in model adaptation."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on various pre-training approaches used in visual tasks."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Exploration of the architecture and applications of large-scale language models."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Detailed examination of Transformer architectures in NLP tasks."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Discussion on the ability to perform tasks without prior training data."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning optimal actions in an environment through trial and error."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to reinforcement learning concepts and algorithms."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Mathematical framework for modeling decision-making in uncertain environments."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms for finding optimal policies in MDPs through iterative methods."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Techniques for estimating the transition and reward functions of an MDP."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Discussion on handling continuous state spaces in reinforcement learning problems."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Process of converting continuous-valued attributes into discrete categories for Naive Bayes application."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Techniques for approximating value functions in large or continuous state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Theoretical connections between policy iteration and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Optimal control techniques for linear systems with quadratic costs."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Discussion on Markov decision processes with a finite time horizon."
    },
    {
      "id": "Learning a Model for an MDP",
      "type": "major",
      "parent": null,
      "description": "Overview of learning models in Markov Decision Processes"
    },
    {
      "id": "Continuous State MDPs",
      "type": "subnode",
      "parent": "Learning a Model for an MDP",
      "description": "MDPs with continuous state spaces"
    },
    {
      "id": "Value Function Approximation",
      "type": "subnode",
      "parent": "Continuous State MDPs",
      "description": "Estimates the value function using a linear or non-linear model of states."
    },
    {
      "id": "Connections between Policy and Value Iteration",
      "type": "subnode",
      "parent": "Learning a Model for an MDP",
      "description": "Optional: Connections between policy iteration and value iteration methods"
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Special case of finite-horizon setting where exact solutions are tractable and widely used in robotics."
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approaches to apply LQR to nonlinear systems"
    },
    {
      "id": "Linearization of Dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Process of approximating nonlinear dynamics with linear functions for easier analysis."
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization method for nonlinear systems that aims to find optimal control policies."
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Combination of LQR with stochastic inputs modeled as Gaussian noise"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Method for optimizing policies in reinforcement learning"
    },
    {
      "id": "Supervised Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning concepts and examples"
    },
    {
      "id": "Supervised Learning Problem",
      "type": "major",
      "parent": null,
      "description": "Goal is to learn a function h(x) that predicts y based on training data."
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Function h representing the learned model."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Learning problem where target variable is continuous."
    },
    {
      "id": "Classification Problem",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Predicting discrete values from input features in machine learning."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": "Supervised Learning",
      "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables)."
    },
    {
      "id": "Housing Example Dataset",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Dataset including living area, number of bedrooms, and price for houses."
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of choosing relevant features to improve model performance."
    },
    {
      "id": "MachineLearningBasics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including neural networks and activation functions."
    },
    {
      "id": "FunctionRepresentation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "How functions are represented as hypotheses in ML models."
    },
    {
      "id": "LinearHypothesis",
      "type": "subnode",
      "parent": "FunctionRepresentation",
      "description": "Approximating y with a linear function of x."
    },
    {
      "id": "ParametersWeights",
      "type": "subnode",
      "parent": "LinearHypothesis",
      "description": "Theta parameters representing weights in the model."
    },
    {
      "id": "CostFunction",
      "type": "subnode",
      "parent": "FunctionRepresentation",
      "description": "Measures the performance of a machine learning model in regression tasks."
    },
    {
      "id": "OrdinaryLeastSquares",
      "type": "subnode",
      "parent": "CostFunction",
      "description": "Special case of least-squares cost function in linear regression."
    },
    {
      "id": "LMSAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Learning the parameters theta to minimize J(theta) using a search algorithm."
    },
    {
      "id": "GradientDescentAlgorithm",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Iterative optimization algorithm used in machine learning for minimizing loss functions."
    },
    {
      "id": "LearningRateAlpha",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Hyperparameter controlling the size of steps in gradient descent."
    },
    {
      "id": "CostFunctionJTheta",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Function to be minimized, representing error between predictions and actual values."
    },
    {
      "id": "PartialDerivativeTerm",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Component of gradient descent update rule that determines direction of steepest descent."
    },
    {
      "id": "LMSUpdateRule",
      "type": "major",
      "parent": "GradientAscentRule",
      "description": "Specific update rule for single training example in gradient descent algorithm."
    },
    {
      "id": "WidrowHoffLearningRule",
      "type": "subnode",
      "parent": "LMSUpdateRule",
      "description": "Alternative name for LMS update rule, used in adaptive filters and neural networks."
    },
    {
      "id": "LMS_Update_Rule",
      "type": "major",
      "parent": null,
      "description": "Update rule for adjusting parameters in machine learning models."
    },
    {
      "id": "Widrow-Hoff_Learning_Rule",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Alternative name for the LMS update rule."
    },
    {
      "id": "Error_Term",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Difference between actual and predicted values used to adjust parameters."
    },
    {
      "id": "Single_Training_Example",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Derivation of LMS rule for single training example scenario."
    },
    {
      "id": "Batch_Gradient_Descent",
      "type": "major",
      "parent": "Gradient_Descent",
      "description": "Variant of gradient descent that uses the entire dataset for each update."
    },
    {
      "id": "Gradient_Descent",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Optimization technique used to minimize error in models by iteratively adjusting parameters."
    },
    {
      "id": "Convergence",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Process of reaching an optimal solution in gradient descent methods."
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Formulation of SVM optimization with quadratic objective and linear constraints."
    },
    {
      "id": "Gradient Descent",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Algorithm that iteratively minimizes the cost function by moving in the direction of the steepest descent as defined by the negative gradient."
    },
    {
      "id": "Batch Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Variant of gradient descent where parameters are updated after computing the gradients over the entire dataset."
    },
    {
      "id": "Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Variant that updates model parameters using only a single training example at each iteration."
    },
    {
      "id": "Convex Function",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Function where the line segment between any two points on its graph lies above or on the graph."
    },
    {
      "id": "MachineLearningOptimization",
      "type": "major",
      "parent": null,
      "description": "Techniques for minimizing cost functions in machine learning."
    },
    {
      "id": "StochasticGradientDescent",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Variant of gradient descent that uses a single data point to update the model parameters."
    },
    {
      "id": "BatchGradientDescent",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Updates parameters after considering all training examples once."
    },
    {
      "id": "NormalEquations",
      "type": "major",
      "parent": "LinearRegression",
      "description": "Closed-form solution to linear regression problems without gradient descent."
    },
    {
      "id": "MatrixDerivatives",
      "type": "subnode",
      "parent": "NormalEquations",
      "description": "Notation for calculus with matrices used in normal equations derivation."
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Calculation of the gradient for a specific function with respect to a 2x2 matrix."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares method using tools from matrix derivatives."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Definition of the design matrix in the context of a training set."
    },
    {
      "id": "Target Vector",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Description of the target vector containing all target values from the training set."
    },
    {
      "id": "MachineLearningOverview",
      "type": "major",
      "parent": null,
      "description": "General overview of machine learning concepts and neural networks."
    },
    {
      "id": "LinearRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables X."
    },
    {
      "id": "GradientDescent",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Optimization algorithm for minimizing cost functions."
    },
    {
      "id": "LeastSquaresCostFunction",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Method to find the best fit line by minimizing the sum of squared differences between observed values and those predicted by the model."
    },
    {
      "id": "ProbabilisticInterpretation",
      "type": "major",
      "parent": null,
      "description": "Explanation of linear regression using probabilistic assumptions about data generation process."
    },
    {
      "id": "ErrorTerm",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "Random variable representing the difference between observed values and those predicted by the model."
    },
    {
      "id": "IIDAssumption",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "Error terms are independently and identically distributed according to a Gaussian distribution."
    },
    {
      "id": "GaussianDistribution",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "A continuous probability distribution representing normal distribution, also an example of exponential family."
    },
    {
      "id": "Probabilistic_Modeling",
      "type": "major",
      "parent": null,
      "description": "Framework for modeling data using probability distributions."
    },
    {
      "id": "Conditional_Probability_Distribution",
      "type": "subnode",
      "parent": "Probabilistic_Modeling",
      "description": "Distribution of y given x and parameters θ."
    },
    {
      "id": "Design_Matrix_X",
      "type": "subnode",
      "parent": "Probabilistic_Modeling",
      "description": "Matrix containing all input data vectors x^(i)."
    },
    {
      "id": "Likelihood_Function",
      "type": "subnode",
      "parent": "Probabilistic_Modeling",
      "description": "Function used to calculate likelihood of data given parameters in a training set."
    },
    {
      "id": "Independence_Assumption",
      "type": "subnode",
      "parent": "Probabilistic_Modeling",
      "description": "Assumption that errors ε^(i) are independent."
    },
    {
      "id": "Maximum_Likelihood_Estimation",
      "type": "subnode",
      "parent": "Probabilistic_Modeling",
      "description": "Method for estimating parameters θ by maximizing likelihood function L(θ)."
    },
    {
      "id": "Log_Likelihood_Function",
      "type": "subnode",
      "parent": "Maximum_Likelihood_Estimation",
      "description": "Natural logarithm of the likelihood function, simplifying calculations."
    },
    {
      "id": "LikelihoodFunction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Definition and properties of the likelihood function in ML."
    },
    {
      "id": "MaximumLikelihoodEstimation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Method for estimating parameters by maximizing the likelihood function."
    },
    {
      "id": "LeastSquaresRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique for fitting a linear model to data by minimizing squared errors."
    },
    {
      "id": "LocallyWeightedLinearRegression",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Algorithm that reduces dependence on feature selection with sufficient training data"
    },
    {
      "id": "FeatureSelection",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Importance of choosing appropriate features for model performance"
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "FeatureSelection",
      "description": "Situation where a model is too simple to capture the underlying pattern of the data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "FeatureSelection",
      "description": "Result of using overly complex models leading to poor generalization on unseen data."
    },
    {
      "id": "WeightsCalculation",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Method for calculating weights in locally weighted linear regression using a Gaussian-like function."
    },
    {
      "id": "BandwidthParameter",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Controls the influence of training examples based on their distance from the query point."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and models."
    },
    {
      "id": "Non-parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms that do not have a fixed number of parameters and require the entire dataset for predictions."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Non-parametric Algorithms",
      "description": "Regression technique where weights are assigned based on proximity to query point."
    },
    {
      "id": "Parametric Learning Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithm with a fixed number of parameters that can make predictions without the training data."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "A classification problem with two possible outcomes (0 or 1)."
    },
    {
      "id": "Logistic Regression",
      "type": "major",
      "parent": "Cross Validation",
      "description": "Technique for binary classification using a logistic function to model probabilities."
    },
    {
      "id": "Spam Classifier Example",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Example of applying classification techniques to identify spam emails."
    },
    {
      "id": "MachineLearning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "ClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Task of identifying to which of a set of categories a new observation belongs, based on training data."
    },
    {
      "id": "LogisticRegression",
      "type": "subnode",
      "parent": "ClassificationProblem",
      "description": "Binary classification model derived from Bernoulli distribution in the context of GLM framework."
    },
    {
      "id": "SigmoidFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Non-linear function mapping real numbers to (0, 1) range used in logistic regression."
    },
    {
      "id": "HypothesisFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Function predicting the probability of a binary outcome given input features."
    },
    {
      "id": "DerivativeSigmoid",
      "type": "subnode",
      "parent": "SigmoidFunction",
      "description": "Mathematical expression for the rate of change of the sigmoid function."
    },
    {
      "id": "Machine_Learning_Models",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including nonlinear and linear regression."
    },
    {
      "id": "Classification_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Models that predict categorical outcomes."
    },
    {
      "id": "Probabilistic_Assumptions",
      "type": "subnode",
      "parent": "Classification_Models",
      "description": "Assumptions about the probability of outcomes given input data."
    },
    {
      "id": "Log_Likelihood",
      "type": "subnode",
      "parent": "Likelihood_Function",
      "description": "Natural logarithm of likelihood function for easier computation."
    },
    {
      "id": "Gradient_Ascend",
      "type": "subnode",
      "parent": "Maximum_Likelihood_Estimation",
      "description": "Optimization technique to find maximum likelihood estimates."
    },
    {
      "id": "GradientAscentRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Update rule for parameters in logistic regression."
    },
    {
      "id": "StochasticGradientAscent",
      "type": "subnode",
      "parent": "GradientAscentRule",
      "description": "Variant of gradient ascent using a single data point at each iteration to update parameters."
    },
    {
      "id": "LogisticLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Details on the logistic loss function and its derivative."
    },
    {
      "id": "NegativeLogLikelihood",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Calculation of negative log-likelihood for a single example and training data."
    },
    {
      "id": "ChainRuleApplication",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Derivation using the chain rule to find partial derivatives."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including classification and decision boundaries."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Derivation of logistic regression gradient descent formula."
    },
    {
      "id": "Perceptron Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Historical learning algorithm that outputs binary values using a threshold function."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Classification problems where response variable can take on more than two categories."
    },
    {
      "id": "Response Variable",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Discrete variable that can take k possible values."
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Distribution for response variable with k outcomes."
    },
    {
      "id": "Parameterized Model",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Model that outputs probabilities satisfying the constraint of multinomial distribution."
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Function used to convert scores into a probability vector summing up to 1."
    },
    {
      "id": "SoftmaxFunction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Definition and application of the softmax function in probability vectors."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Output of the linear model before applying softmax function, representing class scores."
    },
    {
      "id": "ProbabilityVector",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Output of the softmax function is a probability vector summing up to 1."
    },
    {
      "id": "ProbabilisticModel",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Using softmax outputs as probabilities in probabilistic models."
    },
    {
      "id": "CrossEntropyLoss",
      "type": "subnode",
      "parent": "NegativeLogLikelihood",
      "description": "Measure of performance for classification models, especially in multi-class scenarios."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including optimization and probabilistic models."
    },
    {
      "id": "Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and properties of cross-entropy loss function in machine learning."
    },
    {
      "id": "Softmax_Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Specific form of cross-entropy loss involving the softmax function."
    },
    {
      "id": "Gradient_Calculation",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Calculating gradients of ELBO with respect to model parameters for optimization."
    },
    {
      "id": "GradientDescentOptimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Algorithm to minimize loss functions by iteratively moving towards the minimum value of a function."
    },
    {
      "id": "NewtonMethod",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Optimization technique for finding roots of a real-valued function, used in logistic regression context."
    },
    {
      "id": "ChainRule",
      "type": "subnode",
      "parent": "GradientDescentOptimization",
      "description": "Mathematical principle for computing derivatives of composite functions, essential for backpropagation in neural networks."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "An iterative method for finding roots of a real-valued function."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "The process of determining values of theta where f(theta) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to find the maximum point of a function by setting its derivative to zero."
    },
    {
      "id": "Multidimensional Generalization",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Extending Newton's method to handle vector-valued theta in logistic regression."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Multidimensional Generalization",
      "description": "A square matrix of second-order partial derivatives used in optimization problems."
    },
    {
      "id": "MachineLearningConcepts",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts including Gaussian distributions and classification models."
    },
    {
      "id": "OptimizationMethods",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques used to optimize the parameters of a model."
    },
    {
      "id": "FisherScoring",
      "type": "subnode",
      "parent": "NewtonMethod",
      "description": "Application of Newton's method to logistic regression likelihood function maximization."
    },
    {
      "id": "GeneralizedLinearModels",
      "type": "major",
      "parent": "MachineLearningModels",
      "description": "A class of models that includes both regression and classification methods as special cases."
    },
    {
      "id": "ExponentialFamilyDistributions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "A family of distributions used in GLMs characterized by their form involving natural parameters, sufficient statistics, etc."
    },
    {
      "id": "NaturalParameter",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The canonical parameter that defines the distribution in an exponential family."
    },
    {
      "id": "SufficientStatistic",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "A function of data sufficient to characterize a probability distribution within a family."
    },
    {
      "id": "LogPartitionFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The log partition function ensuring the distribution sums/integrates to 1."
    },
    {
      "id": "BernoulliDistribution",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Description of Bernoulli distribution as an exponential family member with specific parameters."
    },
    {
      "id": "MachineLearningModels",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including GLMs and their applications."
    },
    {
      "id": "ConstructingGLMs",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Process for constructing GLMs using different types of distributions based on problem requirements."
    },
    {
      "id": "PoissonDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Using Poisson distribution for modeling website visitors based on features like promotions, weather, etc."
    },
    {
      "id": "ConditionalDistributionAssumptions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Three key assumptions about the conditional distribution of y given x and model parameters for constructing GLM models."
    },
    {
      "id": "NaturalParameterLinearity",
      "type": "subnode",
      "parent": "ConditionalDistributionAssumptions",
      "description": "The assumption that natural parameter eta is linearly related to inputs x in GLMs."
    },
    {
      "id": "GeneralizedLinearModelsGLMs",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Class of models derived from assumptions about data distributions."
    },
    {
      "id": "AssumptionsForGLMs",
      "type": "subnode",
      "parent": "GeneralizedLinearModelsGLMs",
      "description": "Three key design choices for GLM formulation."
    },
    {
      "id": "OrdinaryLeastSquaresOLS",
      "type": "subnode",
      "parent": "GeneralizedLinearModelsGLMs",
      "description": "Special case of GLMs where target variable is continuous and modeled as Gaussian distribution."
    },
    {
      "id": "ConditionalDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Techniques to model the conditional distribution p(y|x;θ)."
    },
    {
      "id": "BernoulliDistributions",
      "type": "subnode",
      "parent": "ConditionalDistributionModeling",
      "description": "Binary-valued distributions used for modeling binary outcomes."
    },
    {
      "id": "HypothesisFunctions",
      "type": "subnode",
      "parent": "ConditionalDistributionModeling",
      "description": "Functions derived from the conditional distribution to predict outcomes."
    },
    {
      "id": "LogisticFunction",
      "type": "subnode",
      "parent": "HypothesisFunctions",
      "description": "Sigmoid function used in machine learning models to predict probabilities."
    },
    {
      "id": "CanonicalResponseFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Mean as a function of the natural parameter in exponential family distributions."
    },
    {
      "id": "CanonicalLinkFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Inverse of the canonical response function, relating natural parameters to mean responses."
    },
    {
      "id": "Machine_Learning_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms including reinforcement learning techniques."
    },
    {
      "id": "Discriminative_Algorithms",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithms that learn p(y|x) directly such as logistic regression and perceptron algorithm."
    },
    {
      "id": "Generative_Algorithms",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithms that model p(x|y) and p(y), using Bayes rule to derive p(y|x)."
    },
    {
      "id": "Conditional_Distribution",
      "type": "major",
      "parent": null,
      "description": "The conditional distribution of y given x in machine learning models."
    },
    {
      "id": "Logistic_Regression",
      "type": "subnode",
      "parent": "Discriminative_Algorithms",
      "description": "Model that predicts p(y|x) using a sigmoid function."
    },
    {
      "id": "Perceptron_Algorithm",
      "type": "subnode",
      "parent": "Discriminative_Algorithms",
      "description": "Algorithm for binary classification that learns to separate classes with a linear boundary."
    },
    {
      "id": "Class_Priors",
      "type": "subnode",
      "parent": "Generative_Algorithms",
      "description": "Probability of each class before considering the input features."
    },
    {
      "id": "Decision_Boundary",
      "type": "major",
      "parent": null,
      "description": "Boundary that separates different classes in a classification problem."
    },
    {
      "id": "Elephant_Model",
      "type": "subnode",
      "parent": "Generative_Algorithms",
      "description": "Model representing the distribution of features for elephants."
    },
    {
      "id": "Dog_Model",
      "type": "subnode",
      "parent": "Generative_Algorithms",
      "description": "Model representing the distribution of features for dogs."
    },
    {
      "id": "Bayesian Classification",
      "type": "major",
      "parent": null,
      "description": "Classification using Bayes rule to derive posterior distribution."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability of each class before observing data."
    },
    {
      "id": "Conditional Probability p(x|y)",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Distribution of features given the class label."
    },
    {
      "id": "Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability of class given data, derived using Bayes rule."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "Generative learning algorithm assuming multivariate normal distribution for p(x|y)."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Distribution parameterized by mean vector and covariance matrix."
    },
    {
      "id": "Mean Vector",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Vector representing the average value of each feature in the distribution."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Matrix describing how variables vary together, symmetric and positive semi-definite."
    },
    {
      "id": "RandomVariables",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Discussion on random variables and their properties."
    },
    {
      "id": "MeanCalculation",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "Explanation and calculation of mean for a Gaussian random variable."
    },
    {
      "id": "CovarianceDefinition",
      "type": "subnode",
      "parent": "RandomVariables",
      "description": "Definition and properties of covariance in vector-valued random variables."
    },
    {
      "id": "StandardNormalDist",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "Introduction to the standard normal distribution as a special case of Gaussian distribution."
    },
    {
      "id": "CovarianceExamples",
      "type": "subnode",
      "parent": "RandomVariables",
      "description": "Illustration through examples of covariance matrices and their effects on distributions."
    },
    {
      "id": "GaussianDistributions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of multivariate normal distribution properties and visual representations."
    },
    {
      "id": "CovarianceMatrixEffects",
      "type": "subnode",
      "parent": "GaussianDistributions",
      "description": "Impact of varying covariance matrix elements on density contours."
    },
    {
      "id": "MeanVectorShifts",
      "type": "subnode",
      "parent": "GaussianDistributions",
      "description": "Effect of changing mean vectors while keeping the covariance matrix constant."
    },
    {
      "id": "GDAModel",
      "type": "major",
      "parent": null,
      "description": "Introduction to Gaussian Discriminant Analysis model for classification tasks with continuous features."
    },
    {
      "id": "MultivariateNormalDistributions",
      "type": "subnode",
      "parent": "GDAModel",
      "description": "Conditional distributions of input features given class labels, modeled as multivariate normals."
    },
    {
      "id": "Model Parameters",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Parameters of the GDA model including φ, Σ, μ₀, and μ₁."
    },
    {
      "id": "Log-Likelihood",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Measure of how well a probability distribution fits a set of observations; target for maximization in EM algorithm."
    },
    {
      "id": "Maximum Likelihood Estimation",
      "type": "subnode",
      "parent": "Log-Likelihood",
      "description": "Process of finding parameter estimates by maximizing the likelihood function."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "The line where the probability of classifying an input as y=1 equals 0.5, separating classes."
    },
    {
      "id": "Relationship to Logistic Regression",
      "type": "major",
      "parent": null,
      "description": "Discussion on how GDA and logistic regression are related through their probabilistic forms."
    },
    {
      "id": "GaussianDiscriminantAnalysis",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Model that assumes data is generated from a Gaussian distribution with shared covariance matrix."
    },
    {
      "id": "DecisionBoundaries",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Boundaries that separate different classes in feature space, determined by models like GDA and logistic regression."
    },
    {
      "id": "ModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Theoretical assumptions made by models about the data distribution."
    },
    {
      "id": "AsymptoticEfficiency",
      "type": "subnode",
      "parent": "DecisionBoundaries",
      "description": "Property of GDA that ensures optimal performance with large datasets under correct model assumptions."
    },
    {
      "id": "RobustnessToAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Comparison of how models handle deviations from modeling assumptions."
    },
    {
      "id": "NaiveBayesAlgorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions or decisions based on data."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for classifying text documents into categories."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to classify emails as spam or non-spam."
    },
    {
      "id": "Training_Set",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "A dataset used for training a model with examples (x,y)."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Representation of an email using binary values indicating word presence."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of words used to construct the feature vector for emails."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Vocabulary",
      "description": "High-frequency words often excluded from analysis due to lack of content relevance."
    },
    {
      "id": "Machine_Learning_Topic",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and techniques."
    },
    {
      "id": "Feature_Vector_Selection",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Choosing relevant features from the text to represent as vectors."
    },
    {
      "id": "Stop_Words_Exclusion",
      "type": "subnode",
      "parent": "Feature_Vector_Selection",
      "description": "Removing common words that do not contribute meaningful information."
    },
    {
      "id": "Generative_Modeling",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Building probabilistic models to generate text data based on categories."
    },
    {
      "id": "Naive_Bayes_Assumption",
      "type": "subnode",
      "parent": "Generative_Modeling",
      "description": "Assumption of conditional independence between features given the class label."
    },
    {
      "id": "ConditionalProbability",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "The probability of an event given that another event has occurred."
    },
    {
      "id": "NBAssumption",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Features are conditionally independent given the class label."
    },
    {
      "id": "ParameterEstimation",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Process of estimating parameters for Naive Bayes using maximum likelihood estimation and Laplace smoothing."
    },
    {
      "id": "BinaryFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Features are binary-valued, simplifying the model's assumptions and calculations."
    },
    {
      "id": "MultinomialFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Generalization to features that can take values from a finite set, modeled as multinomial distributions."
    },
    {
      "id": "LaplaceSmoothing",
      "type": "major",
      "parent": "ProbabilityEstimation",
      "description": "Technique to prevent zero probabilities in the Naive Bayes algorithm by adding a small constant to each count."
    },
    {
      "id": "MachineLearningConferences",
      "type": "major",
      "parent": null,
      "description": "Top conferences in the field of machine learning."
    },
    {
      "id": "NeurIPS",
      "type": "subnode",
      "parent": "MachineLearningConferences",
      "description": "One of the top machine learning conferences where researchers submit their work for publication."
    },
    {
      "id": "NaiveBayesFilter",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "SpamDetection",
      "type": "subnode",
      "parent": "NaiveBayesFilter",
      "description": "Application of Naive Bayes filter to classify emails as spam or non-spam."
    },
    {
      "id": "TrainingSetLimitations",
      "type": "subnode",
      "parent": "SpamDetection",
      "description": "Issues arising from the limitations of a finite training set in machine learning models."
    },
    {
      "id": "ProbabilityEstimation",
      "type": "major",
      "parent": null,
      "description": "Overview of estimating probabilities in machine learning."
    },
    {
      "id": "MultinomialRandomVariable",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Introduction to multinomial random variables and their parameters."
    },
    {
      "id": "MaximumLikelihoodEstimates",
      "type": "subnode",
      "parent": "MultinomialRandomVariable",
      "description": "Explanation of maximum likelihood estimates for multinomial distributions."
    },
    {
      "id": "NaiveBayesClassifier",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "EventModelsTextClassification",
      "type": "major",
      "parent": null,
      "description": "Overview of event models used in text classification tasks."
    },
    {
      "id": "EventModelsForTextClassification",
      "type": "major",
      "parent": null,
      "description": "Discusses models specifically for text classification in machine learning."
    },
    {
      "id": "BernoulliModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model used in Naive Bayes for text classification assuming binary word presence."
    },
    {
      "id": "MultinomialModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "An alternative to the Bernoulli model, representing emails as sequences of word identities."
    },
    {
      "id": "NaiveBayes",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A generative learning algorithm that works well for many classification problems."
    },
    {
      "id": "ClassPriors",
      "type": "subnode",
      "parent": "BernoulliModel",
      "description": "Probabilities of class labels before observing any data, used to determine spam or non-spam emails."
    },
    {
      "id": "Multinomial_Event_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "A probabilistic model for generating sequences of events (words) based on categories (spam/non-spam)."
    },
    {
      "id": "Bernoulli_Event_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "An earlier model that uses binary outcomes for each event."
    },
    {
      "id": "Probability_Calculation",
      "type": "subnode",
      "parent": "Multinomial_Event_Model",
      "description": "Calculating the probability of a sequence given category using multinomial distributions."
    },
    {
      "id": "Model_Parameters",
      "type": "subnode",
      "parent": "Multinomial_Event_Model",
      "description": "Parameters defining the model for each category and word generation process."
    },
    {
      "id": "Parameter_Estimation",
      "type": "subnode",
      "parent": "Multinomial_Event_Model",
      "description": "Process for estimating model parameters from observed data using maximum likelihood estimation."
    },
    {
      "id": "KernelMethods",
      "type": "major",
      "parent": null,
      "description": "Techniques to extend linear models to handle non-linear relationships."
    },
    {
      "id": "FeatureMaps",
      "type": "subnode",
      "parent": "KernelMethods",
      "description": "Transformation of input data into a higher-dimensional space for better model fitting."
    },
    {
      "id": "CubicFunctions",
      "type": "subnode",
      "parent": "FeatureMaps",
      "description": "Example of non-linear transformations to fit cubic functions in linear regression context."
    },
    {
      "id": "Feature_Mapping",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Mapping input data to a higher dimensional space for better separability."
    },
    {
      "id": "Linear_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Representation of functions as linear combinations in a higher-dimensional space."
    },
    {
      "id": "Cubic_Function_Example",
      "type": "subnode",
      "parent": "Linear_Functions",
      "description": "Illustration using cubic function mapping to feature variables."
    },
    {
      "id": "LMS_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Least Mean Squares method for fitting linear models with features."
    },
    {
      "id": "Gradient_Descent_Update_Rule",
      "type": "subnode",
      "parent": "LMS_Algorithm",
      "description": "Update rule for parameters in the context of feature variables."
    },
    {
      "id": "FeatureTransformation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Transforming input features to a higher-dimensional space for better model fitting."
    },
    {
      "id": "HighDimensionalFeatures",
      "type": "subnode",
      "parent": "FeatureTransformation",
      "description": "Issues and challenges with high-dimensional feature spaces in machine learning models."
    },
    {
      "id": "KernelTrick",
      "type": "major",
      "parent": null,
      "description": "Technique to avoid explicitly computing the coordinates of data points in a very high-dimensional space."
    },
    {
      "id": "Kernel_Tricks",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques to improve efficiency in machine learning models."
    },
    {
      "id": "Runtime_Efficiency",
      "type": "subnode",
      "parent": "Kernel_Tricks",
      "description": "Methods to reduce computational time and resource usage."
    },
    {
      "id": "Memory_Use",
      "type": "subnode",
      "parent": "Kernel_Tricks",
      "description": "Strategies for managing memory in machine learning algorithms."
    },
    {
      "id": "Phi_Functions",
      "type": "subnode",
      "parent": "Runtime_Efficiency",
      "description": "Functions used to transform input data into higher dimensions."
    },
    {
      "id": "Theta_Vector",
      "type": "subnode",
      "parent": "Memory_Use",
      "description": "High-dimensional vector representing model parameters."
    },
    {
      "id": "Linear_Combinations",
      "type": "subnode",
      "parent": "Phi_Functions",
      "description": "Representation of θ as a sum of transformed inputs."
    },
    {
      "id": "Inductive_Step",
      "type": "subnode",
      "parent": "Linear_Combinations",
      "description": "Proof technique showing θ remains a linear combination over iterations."
    },
    {
      "id": "Beta_Update_Equation",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Equation describing how β is updated in batch gradient descent."
    },
    {
      "id": "Inner_Products",
      "type": "subnode",
      "parent": "Beta_Update_Equation",
      "description": "Computation of inner products between transformed features in the equation update."
    },
    {
      "id": "Pre_Compute_Inner_Products",
      "type": "subnode",
      "parent": "Feature_Mapping",
      "description": "Efficient computation and pre-computation strategy for inner products before iterative updates."
    },
    {
      "id": "Efficient_Calculation_Method",
      "type": "subnode",
      "parent": "Pre_Compute_Inner_Products",
      "description": "Method to calculate φ(x),φ(z) efficiently using lower-dimensional features."
    },
    {
      "id": "Feature_Maps_and_Kernels",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Exploration of feature maps and their corresponding kernel functions."
    },
    {
      "id": "Kernel_Functions",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Functions that measure similarity between inputs without explicit feature mapping."
    },
    {
      "id": "Efficient_Computation",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Methods for efficient computation of kernel values and updates to the model parameters."
    },
    {
      "id": "Kernels_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to kernel functions in the context of machine learning algorithms."
    },
    {
      "id": "Feature_Map_and_Kernel_Functions",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Exploration of how feature maps induce kernel functions and vice versa."
    },
    {
      "id": "Properties_of_Kernels",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Discussion on the intrinsic properties and characteristics of valid kernel functions."
    },
    {
      "id": "Kernel_Function_Characterization",
      "type": "subnode",
      "parent": "Properties_of_Kernels",
      "description": "Criteria for determining if a given function can serve as a valid kernel function."
    },
    {
      "id": "KernelsInML",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to kernel functions used in machine learning for efficient computation."
    },
    {
      "id": "KernelFunctionExample1",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "First example of a kernel function and its corresponding feature mapping."
    },
    {
      "id": "FeatureMapping",
      "type": "subnode",
      "parent": "KernelFunctionExample1",
      "description": "Explanation of the feature mapping associated with the first kernel example."
    },
    {
      "id": "ComputationalEfficiency",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Discussion on computational efficiency benefits of using kernels over direct computation of high-dimensional mappings."
    },
    {
      "id": "KernelFunctionExample2",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Second example of a kernel function with an additional parameter c and its feature mapping."
    },
    {
      "id": "ParameterCExplanation",
      "type": "subnode",
      "parent": "KernelFunctionExample2",
      "description": "Explanation of the role of parameter c in controlling term weighting between first and second order features."
    },
    {
      "id": "GeneralFormOfKernels",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Overview of the general form of kernel functions for arbitrary k-order terms."
    },
    {
      "id": "Machine_Learning_Kernels",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning kernels and their properties."
    },
    {
      "id": "Gaussian_Kernel",
      "type": "subnode",
      "parent": "Kernel_Functions",
      "description": "A specific kernel function with infinite dimensional feature space."
    },
    {
      "id": "Similarity_Metrics",
      "type": "subnode",
      "parent": "Machine_Learning_Kernels",
      "description": "Intuitive understanding of kernels as measures of similarity between data points."
    },
    {
      "id": "Valid Kernel Function Properties",
      "type": "major",
      "parent": null,
      "description": "Properties a function K must satisfy to be a valid kernel."
    },
    {
      "id": "Necessary Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Valid Kernel Function Properties",
      "description": "Conditions that must hold if K is a valid kernel."
    },
    {
      "id": "Kernel Matrix Definition",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Definition and properties of the matrix K based on function values."
    },
    {
      "id": "Symmetry Property",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Kernel matrices must be symmetric."
    },
    {
      "id": "Positive Semi-Definiteness",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Kernel matrices are positive semi-definite."
    },
    {
      "id": "Sufficient Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Valid Kernel Function Properties",
      "description": "Conditions that, if satisfied, guarantee a function is a valid kernel."
    },
    {
      "id": "Kernel Matrix Properties",
      "type": "major",
      "parent": null,
      "description": "Properties of the kernel matrix in machine learning."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Theorem stating conditions for a function to be a valid Mercer kernel."
    },
    {
      "id": "Necessary and Sufficient Condition",
      "type": "subnode",
      "parent": "Mercer's Theorem",
      "description": "Condition that the kernel matrix must be symmetric positive semidefinite."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Method to test if a function is a valid kernel by finding corresponding feature mapping."
    },
    {
      "id": "Digit Recognition Example",
      "type": "major",
      "parent": null,
      "description": "Example of using kernels in digit recognition problem with SVMs."
    },
    {
      "id": "Polynomial Kernel",
      "type": "subnode",
      "parent": "Digit Recognition Example",
      "description": "Simple polynomial kernel used for classification tasks."
    },
    {
      "id": "Gaussian Kernel",
      "type": "subnode",
      "parent": "Digit Recognition Example",
      "description": "Another type of kernel used in SVMs for digit recognition."
    },
    {
      "id": "String Classification Example",
      "type": "major",
      "parent": null,
      "description": "Example involving classification of strings as objects."
    },
    {
      "id": "Feature_Engineering",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Process of selecting and transforming raw data into features for use in machine learning models."
    },
    {
      "id": "String_Features",
      "type": "subnode",
      "parent": "Feature_Engineering",
      "description": "Creating feature vectors from substrings of variable-length strings."
    },
    {
      "id": "Kernel_Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques using kernel functions for efficient computation in high-dimensional spaces."
    },
    {
      "id": "Linear_Regression_Kernels",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Application of kernels to linear regression models."
    },
    {
      "id": "Support_Vector_Machines",
      "type": "major",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction and application of support vector machines in machine learning."
    },
    {
      "id": "Perceptron_Kernel_Trick",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Modification of perceptron algorithm using kernel trick for high-dimensional feature spaces."
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of algorithms used in machine learning including perceptron and SVM."
    },
    {
      "id": "Kernel Perceptron Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Derivation of a kernel-based version of the perceptron algorithm."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Binary classification algorithm that maximizes the margin between classes."
    },
    {
      "id": "Margins in SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Concept of margins and the confidence level of predictions."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Classifier that maximizes the margin between classes, leading to discussion on Lagrange duality."
    },
    {
      "id": "Kernels in SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Techniques allowing efficient application of SVMs in high-dimensional spaces."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient implementation method for solving the optimization problem in SVM training."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Notion used to formalize confident classifications based on feature weights."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concept for measuring confidence in predictions based on distance from decision boundary."
    },
    {
      "id": "Separating Hyperplane",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Alternative term for decision boundary in classification problems."
    },
    {
      "id": "Training Data Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of predicting class labels based on training data features."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Introduction to notation used in discussing SVM classifiers and margins."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Measure of confidence for a classifier's prediction on a training example."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Distance from the separating hyperplane to the closest data point, scaled by the norm of w."
    },
    {
      "id": "Confidence and Correct Prediction",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "A large functional margin indicates correct and confident predictions."
    },
    {
      "id": "Scaling w and b",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Scaling parameters does not change the classifier but increases the functional margin."
    },
    {
      "id": "Normalization Condition",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Imposing a condition like ||w||_2=1 to normalize w and b."
    },
    {
      "id": "Function Margin of Training Set",
      "type": "major",
      "parent": null,
      "description": "Smallest functional margin across all training examples in the set S."
    },
    {
      "id": "DecisionBoundary",
      "type": "major",
      "parent": null,
      "description": "The boundary that separates classes in a classification problem."
    },
    {
      "id": "VectorW",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "A vector orthogonal to the decision boundary."
    },
    {
      "id": "DistanceToBoundary",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The distance from a point to the decision boundary."
    },
    {
      "id": "UnitVectorW",
      "type": "subnode",
      "parent": "DistanceToBoundary",
      "description": "A unit vector in the direction of w."
    },
    {
      "id": "PointBFormula",
      "type": "subnode",
      "parent": "DistanceToBoundary",
      "description": "The formula for point B on the decision boundary."
    },
    {
      "id": "GeometricMargin",
      "type": "major",
      "parent": null,
      "description": "A measure of how far a training example is from the decision boundary in terms of units of w."
    },
    {
      "id": "FunctionalMargin",
      "type": "subnode",
      "parent": "GeometricMargin",
      "description": "The margin defined by the parameters (w,b) without normalization."
    },
    {
      "id": "Parameter_Scaling_Invariance",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the scaling invariance property of parameters w and b."
    },
    {
      "id": "Geometric_Margin_Definition",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and calculation of geometric margin for a training set."
    },
    {
      "id": "Optimal_Margin_Classifier",
      "type": "major",
      "parent": null,
      "description": "Introduction to finding the optimal classifier that maximizes the geometric margin."
    },
    {
      "id": "Linearly_Separable_Data",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Assumption of linear separability in training data for optimization."
    },
    {
      "id": "Maximize_Geometric_Margin_Optimization",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifier",
      "description": "Formulation and explanation of the optimization problem to maximize geometric margin."
    },
    {
      "id": "Support Vector Machine (SVM)",
      "type": "major",
      "parent": null,
      "description": "Binary classification algorithm maximizing margin between classes."
    },
    {
      "id": "Non-Convex Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint that makes optimization problem difficult due to lack of convexity."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint used to simplify the original non-convex problem by setting functional margin to 1."
    },
    {
      "id": "Convex Quadratic Objective",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Objective function to minimize in the SVM problem."
    },
    {
      "id": "Linear Constraints",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraints ensuring data points are correctly classified with a margin of at least 1."
    },
    {
      "id": "Quadratic Programming (QP)",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Method used to solve the SVM optimization problem."
    },
    {
      "id": "Lagrange Duality",
      "type": "major",
      "parent": null,
      "description": "Theory explaining how to transform constrained optimization problems into dual forms."
    },
    {
      "id": "Constrained Optimization Problems",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "General class of problems Lagrange duality addresses."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Combines the objective function with constraints using Lagrange multipliers."
    },
    {
      "id": "Lagrange Multipliers",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Multipliers used in the dual formulation of constrained optimization problems."
    },
    {
      "id": "ConstrainedOptimization",
      "type": "major",
      "parent": null,
      "description": "Generalization of optimization problems with constraints"
    },
    {
      "id": "LagrangeMultipliers",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Method used to maximize u^TΣu subject to u^Tu=1, showing u is eigenvector of Σ."
    },
    {
      "id": "PrimalProblem",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "The original problem formulation to be solved directly."
    },
    {
      "id": "GeneralizedLagrangian",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Combination of objective and constraint functions with Lagrange multipliers"
    },
    {
      "id": "ThetaP",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Function representing the maximum value of the generalized Lagrangian under constraints"
    },
    {
      "id": "Primal Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem with constraints and objective function."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Problem derived from primal by exchanging 'min' and 'max'."
    },
    {
      "id": "Objective Function",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Function to be minimized or maximized."
    },
    {
      "id": "Constraints",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Conditions that solutions must satisfy."
    },
    {
      "id": "Lagrangian Function (L)",
      "type": "subnode",
      "parent": "Objective Function",
      "description": "Combination of objective and constraints with Lagrange multipliers."
    },
    {
      "id": "Value of Primal Problem",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Optimal value of the primal problem's objective function."
    },
    {
      "id": "Value of Dual Problem",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Optimal value of the dual problem's objective function."
    },
    {
      "id": "Relationship Between Primal and Dual Problems",
      "type": "major",
      "parent": null,
      "description": "The relationship between primal and dual problems including inequalities."
    },
    {
      "id": "DualProblem",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "A transformed version of the primal problem that can sometimes be easier to solve."
    },
    {
      "id": "dStar",
      "type": "subnode",
      "parent": "DualProblem",
      "description": "The optimal value obtained from solving the dual problem."
    },
    {
      "id": "pStar",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "The optimal value obtained from solving the primal problem."
    },
    {
      "id": "ConvexityConditions",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Conditions under which a function is considered convex, impacting optimization."
    },
    {
      "id": "FeasibilityConstraints",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Requirements for the constraints to be strictly feasible."
    },
    {
      "id": "KKTConditions",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Necessary conditions for a solution in constrained optimization problems."
    },
    {
      "id": "KarushKuhnTuckerTheorem",
      "type": "subnode",
      "parent": "KKTConditions",
      "description": "The theorem that establishes the KKT conditions as necessary optimality conditions."
    },
    {
      "id": "Optimization_Problems",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Challenges in finding optimal parameters due to non-convexity."
    },
    {
      "id": "KKT_Conditions",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Karush-Kuhn-Tucker conditions for optimality in constrained optimization problems."
    },
    {
      "id": "Dual_Complementarity",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Condition indicating active constraints in KKT."
    },
    {
      "id": "SVM_Support_Vectors",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Identification of support vectors in SVM."
    },
    {
      "id": "SMO_Algorithm",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Sequential Minimal Optimization algorithm for solving SVM problems."
    },
    {
      "id": "Primal_Dual_Equivalence",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Equivalence between primal and dual optimization problems in SVM context."
    },
    {
      "id": "SupportVectors",
      "type": "major",
      "parent": null,
      "description": "Points that lie on the decision boundary and affect the optimal solution."
    },
    {
      "id": "LagrangianFormulation",
      "type": "major",
      "parent": null,
      "description": "Mathematical formulation involving Lagrange multipliers and constraints."
    },
    {
      "id": "DualFormProblem",
      "type": "subnode",
      "parent": "LagrangianFormulation",
      "description": "Transformation of the original problem to a dual form using Lagrangian."
    },
    {
      "id": "InnerProduct",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Expression used in algorithms, crucial for applying the kernel trick."
    },
    {
      "id": "Machine_Learning_Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimizing the Evidence Lower Bound (ELBO) in machine learning models."
    },
    {
      "id": "Lagrangian_Methods",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Use of Lagrangian functions to solve constrained optimization problems."
    },
    {
      "id": "Dual_Problem_Formulation",
      "type": "subnode",
      "parent": "Lagrangian_Methods",
      "description": "Formulating the dual problem from a primal problem using Lagrangian methods."
    },
    {
      "id": "Optimal_Solution_Finding",
      "type": "subnode",
      "parent": "Dual_Problem_Formulation",
      "description": "Finding the optimal solution through solving the dual problem and converting back to primal variables."
    },
    {
      "id": "Optimal_Parameter_Finding",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Finding optimal parameters for a model."
    },
    {
      "id": "Optimal_Intercept_Term",
      "type": "subnode",
      "parent": "Optimal_Parameter_Finding",
      "description": "Finding the best intercept term for a model."
    },
    {
      "id": "Prediction_Mechanism",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Mechanism to predict outcomes using support vectors and inner products."
    },
    {
      "id": "Dual_Form_Optimization",
      "type": "subnode",
      "parent": "Optimal_Parameter_Finding",
      "description": "Using dual form for optimization insights."
    },
    {
      "id": "Non-separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Handling datasets that are not linearly separable."
    },
    {
      "id": "\\(\\ell_{1}\\) Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the sum of absolute values of coefficients to handle outliers."
    },
    {
      "id": "Dual Problem Formulation",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Formulating SVM optimization problem in dual form to handle constraints more efficiently."
    },
    {
      "id": "KKT Conditions",
      "type": "subnode",
      "parent": "Dual Problem Formulation",
      "description": "Conditions that must be satisfied for a solution to be optimal in constrained optimization problems."
    },
    {
      "id": "SMO Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient algorithm for solving the dual problem of SVMs, introduced by John Platt."
    },
    {
      "id": "Support_Vector_Machines_SVMs",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Binary classification model for machine learning."
    },
    {
      "id": "Sequential_Minimal_Optimization_SMO",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Efficient algorithm for solving the dual problem of SVMs."
    },
    {
      "id": "Coordinate_Ascend_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Optimization technique used in machine learning."
    },
    {
      "id": "Unconstrained_Optimization_Problem",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Maximizing a function without constraints."
    },
    {
      "id": "Inner_Loop_Operations",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Holding variables constant except for one to optimize the function."
    },
    {
      "id": "OptimizationTechniques",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Various optimization methods used in machine learning problems."
    },
    {
      "id": "CoordinateAscent",
      "type": "subnode",
      "parent": "OptimizationTechniques",
      "description": "An iterative method for solving constrained optimization problems by optimizing one variable at a time."
    },
    {
      "id": "SupportVectorMachinesSVMs",
      "type": "major",
      "parent": null,
      "description": "A type of supervised learning model used for classification and regression analysis."
    },
    {
      "id": "DualFormulation",
      "type": "subnode",
      "parent": "SupportVectorMachinesSVMs",
      "description": "The dual form of the optimization problem in SVMs that involves Lagrange multipliers."
    },
    {
      "id": "SMOAlgorithm",
      "type": "subnode",
      "parent": "SupportVectorMachinesSVMs",
      "description": "Sequential minimal optimization algorithm for solving the quadratic programming problem in SVM training."
    },
    {
      "id": "Heuristic_Selection",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Process of selecting alpha pairs based on progress towards global maximum."
    },
    {
      "id": "Convergence_Checking",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Checking KKT conditions for convergence with tolerance parameter."
    },
    {
      "id": "Efficient_Update",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Method to compute updates efficiently based on fixed alpha values."
    },
    {
      "id": "Constraints_and_Optimization",
      "type": "subnode",
      "parent": "Efficient_Update",
      "description": "Optimizing W(alpha) with respect to specific alphas under constraints."
    },
    {
      "id": "Optimization_in_ML",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for optimizing functions in machine learning models."
    },
    {
      "id": "Alpha_Parameters",
      "type": "subnode",
      "parent": "Optimization_in_ML",
      "description": "Parameters α used in optimization problems."
    },
    {
      "id": "Constraints_and_Limits",
      "type": "subnode",
      "parent": "Alpha_Parameters",
      "description": "Bounds and constraints on the values of alpha parameters."
    },
    {
      "id": "Quadratic_Functions",
      "type": "subnode",
      "parent": "Optimization_in_ML",
      "description": "Usage of quadratic functions in optimization problems."
    },
    {
      "id": "Alpha Value Update",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO) Algorithm",
      "description": "Process of updating alpha values within constraints."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Study of models non-linear in both parameters and inputs."
    },
    {
      "id": "Nonlinear_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Abstract non-linear model used in machine learning for prediction tasks."
    },
    {
      "id": "Training_Examples",
      "type": "subnode",
      "parent": "Nonlinear_Model",
      "description": "Set of training data pairs \\\\( (x^{(i)}, y^{(i)}) \\\\)."
    },
    {
      "id": "Regression_Problems",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Problems where the output is a real number."
    },
    {
      "id": "Least_Square_Cost_Function",
      "type": "subnode",
      "parent": "Regression_Problems",
      "description": "Cost function for individual training examples in regression problems."
    },
    {
      "id": "Mean_Square_Cost_Function",
      "type": "subnode",
      "parent": "Regression_Problems",
      "description": "Average cost over all training examples for regression models."
    },
    {
      "id": "Binary_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Classification problems where the output is binary (0 or 1)."
    },
    {
      "id": "Logistic_Function",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "Function used to convert logit values into probabilities for classification tasks."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Linear combination of input features and weights before applying the logistic function."
    },
    {
      "id": "ProbabilityPrediction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Output probability of class membership using logistic or softmax functions."
    },
    {
      "id": "NegativeLikelihoodLoss",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Loss function measuring the negative log-likelihood for binary classification."
    },
    {
      "id": "TotalLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Average of individual losses over all training examples."
    },
    {
      "id": "MultiClassClassification",
      "type": "major",
      "parent": null,
      "description": "Extension of binary classification to multiple classes using softmax function."
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": null,
      "description": "Function measuring discrepancy between predicted and actual values."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Specific loss function for probabilistic models."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Simplified notation of negative log-likelihood."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Total loss divided by the number of examples."
    },
    {
      "id": "Exponential Family Models",
      "type": "major",
      "parent": null,
      "description": "Models with exponential distribution for conditional probabilities."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": null,
      "description": "Algorithms used to minimize the loss function."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm for finding minimum of a function by moving against the gradient."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Variant of GD that uses random samples to approximate gradients."
    },
    {
      "id": "MiniBatchSGD",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Variant of SGD that uses a batch of samples to update parameters."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Parameters like learning rate and number of iterations that control the optimization process."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Random initialization of parameters before starting optimization."
    },
    {
      "id": "UpdateRule",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Rule for updating model parameters based on gradients and learning rate."
    },
    {
      "id": "DeepLearningModelTraining",
      "type": "major",
      "parent": null,
      "description": "Steps involved in training a deep learning model using SGD or mini-batch SGD."
    },
    {
      "id": "NeuralNetworkParametrization",
      "type": "subnode",
      "parent": "DeepLearningModelTraining",
      "description": "Definition of the neural network architecture and parameters."
    },
    {
      "id": "BackpropagationAlgorithm",
      "type": "subnode",
      "parent": "DeepLearningModelTraining",
      "description": "Efficient method for computing gradients in deep networks."
    },
    {
      "id": "OptimizationProcess",
      "type": "subnode",
      "parent": "DeepLearningModelTraining",
      "description": "Running SGD or mini-batch SGD to minimize the loss function."
    },
    {
      "id": "RegressionProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Predicting continuous values from input data."
    },
    {
      "id": "NeuralNetworksIntroduction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Building and understanding neural networks step-by-step."
    },
    {
      "id": "SingleNeuronNN",
      "type": "subnode",
      "parent": "NeuralNetworksIntroduction",
      "description": "A simple model using a single neuron for prediction tasks."
    },
    {
      "id": "HousingPricePrediction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Example of predicting house prices with a neural network."
    },
    {
      "id": "ReLUActivationFunction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Introduction to the rectified linear unit activation function."
    },
    {
      "id": "Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Artificial neural networks used for modeling complex patterns in data."
    },
    {
      "id": "Activation_Functions",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Functions that add non-linearity to the output of a neuron, such as ReLU."
    },
    {
      "id": "ReLU_Activation",
      "type": "subnode",
      "parent": "Activation_Functions",
      "description": "Activation function used in hidden units of the neural network model."
    },
    {
      "id": "Single_Neuron_Model",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Model with a single neuron including weight vector and bias term."
    },
    {
      "id": "Stacking_Neurons",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Process of combining multiple neurons to form more complex neural networks."
    },
    {
      "id": "Housing_Price_Prediction",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Example application using features like house size, bedrooms, zip code, and neighborhood wealth."
    },
    {
      "id": "Machine_Learning_Features",
      "type": "major",
      "parent": null,
      "description": "Features used in machine learning models for predicting housing prices."
    },
    {
      "id": "Family_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Derived feature based on house size and number of bedrooms."
    },
    {
      "id": "Walkability",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Derived feature indicating ease of walking to amenities like grocery stores."
    },
    {
      "id": "School_Quality",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Predicted quality based on neighborhood wealth and zip code."
    },
    {
      "id": "Neural_Network_Input",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Set of input features used in a neural network model."
    },
    {
      "id": "Hidden_Units",
      "type": "subnode",
      "parent": "Neural_Network_Input",
      "description": "Intermediate variables representing derived features like family size, walkability, and school quality."
    },
    {
      "id": "Output_Parameterization",
      "type": "subnode",
      "parent": "Neural_Network_Input",
      "description": "Final output parameterized by linear combination of intermediate variables and parameters."
    },
    {
      "id": "NeuralNetworkParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of parameters in a neural network including theta values."
    },
    {
      "id": "BiologicalInspiration",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of the inspiration behind artificial neural networks from biological systems."
    },
    {
      "id": "TwoLayerNetworks",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduction to two-layer fully connected neural networks and their parameterization."
    },
    {
      "id": "FullyConnectedNeuralNetworks",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of fully-connected neural network architecture."
    },
    {
      "id": "IntermediateVariables",
      "type": "subnode",
      "parent": "FullyConnectedNeuralNetworks",
      "description": "Introduction to intermediate variables used in computation for loss function calculation."
    },
    {
      "id": "Parameterization",
      "type": "subnode",
      "parent": "FullyConnectedNeuralNetworks",
      "description": "Discussion on parameterization and its role in defining neural network layers."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduction to vectorization for simplifying expressions in neural networks."
    },
    {
      "id": "Efficiency",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Improves performance by leveraging parallelism in GPUs and optimized libraries."
    },
    {
      "id": "Matrix Algebra",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Use of matrix operations to replace for loops for faster computation."
    },
    {
      "id": "Parallel Computation",
      "type": "subnode",
      "parent": "Efficiency",
      "description": "Leveraging parallelism across examples in neural network implementation."
    },
    {
      "id": "Optimized Libraries",
      "type": "subnode",
      "parent": "Efficiency",
      "description": "Use of highly optimized numerical linear algebra packages like BLAS."
    },
    {
      "id": "Two-Layer Neural Network",
      "type": "major",
      "parent": null,
      "description": "Example neural network structure used to illustrate vectorization principles."
    },
    {
      "id": "Weight Matrix W^[1]",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Matrix representation of weights in a two-layer fully-connected neural network."
    },
    {
      "id": "NeuralNetworkArchitecture",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Structure of a neural network with layers, weights, biases, and activations."
    },
    {
      "id": "WeightMatrices",
      "type": "subnode",
      "parent": "NeuralNetworkArchitecture",
      "description": "Description of weight matrices used in different layers of the network."
    },
    {
      "id": "BiasVectors",
      "type": "subnode",
      "parent": "NeuralNetworkArchitecture",
      "description": "Explanation of bias vectors and their role in neural networks."
    },
    {
      "id": "ActivationFunctions",
      "type": "subnode",
      "parent": "NeuralNetworkArchitecture",
      "description": "Introduction to activation functions like ReLU used for non-linear transformations."
    },
    {
      "id": "LayeredStructure",
      "type": "subnode",
      "parent": "NeuralNetworkArchitecture",
      "description": "Description of the layered structure with hidden layers and output layers."
    },
    {
      "id": "Multi-layer Fully-Connected Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Stacking layers to form a deep neural network."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Description of weight matrices and biases in each layer."
    },
    {
      "id": "ReLU Activation Function",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Activation function used between layers, except for the last one."
    },
    {
      "id": "Total Number of Neurons and Parameters",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Calculation of total neurons and parameters in a neural network."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions to ReLU."
    },
    {
      "id": "TanhFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Hyperbolic tangent function used in neural networks."
    },
    {
      "id": "ReLUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Rectified Linear Unit, a piecewise linear function."
    },
    {
      "id": "LeakyReLU",
      "type": "subnode",
      "parent": "ReLUFunction",
      "description": "Variant of ReLU with small non-zero gradient for negative inputs."
    },
    {
      "id": "GELUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Gaussian Error Linear Unit, used in NLP models like BERT and GPT."
    },
    {
      "id": "SoftplusFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Smoothed version of ReLU with a proper second-order derivative."
    },
    {
      "id": "IdentityFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Function where output is equal to input, not commonly used in neural networks."
    },
    {
      "id": "Deep_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Subfield focusing on neural networks with multiple layers to learn representations from complex data."
    },
    {
      "id": "Feature_Maps",
      "type": "subnode",
      "parent": "Feature_Engineering",
      "description": "Transformations of input data into a form that makes learning easier for machine learning models."
    },
    {
      "id": "Neural_Network_Parameters",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Collection of parameters in neural networks used to transform inputs into features or predictions."
    },
    {
      "id": "Linear_Models",
      "type": "subnode",
      "parent": "Feature_Engineering",
      "description": "Models that make predictions using a linear combination of input features."
    },
    {
      "id": "Machine_Learning_Topics",
      "type": "major",
      "parent": null,
      "description": "Major topics in machine learning."
    },
    {
      "id": "Deep_Learning_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Representation learning in deep neural networks."
    },
    {
      "id": "House_Price_Prediction",
      "type": "subnode",
      "parent": "Deep_Learning_Representations",
      "description": "Example of using a fully-connected network for predicting house prices."
    },
    {
      "id": "Feature_Discovery",
      "type": "subnode",
      "parent": "Deep_Learning_Representations",
      "description": "Automatic discovery of useful features by neural networks."
    },
    {
      "id": "Black_Box_Models",
      "type": "subnode",
      "parent": "Deep_Learning_Representations",
      "description": "Complexity and interpretability issues in deep learning models."
    },
    {
      "id": "Modern_Neural_Network_Modules",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Building blocks of modern neural networks."
    },
    {
      "id": "Matrix_Multiplication_Module",
      "type": "subnode",
      "parent": "Modern_Neural_Network_Modules",
      "description": "Definition and operation of matrix multiplication as a building block."
    },
    {
      "id": "MLP_Composition",
      "type": "subnode",
      "parent": "Modern_Neural_Network_Modules",
      "description": "Composition of MLP using multiple modules including matrix multiplications and activations."
    },
    {
      "id": "MLP_Architecture",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Multi-Layer Perceptron architecture using matrix multiplication and activation functions."
    },
    {
      "id": "Nonlinear_Activation_Module",
      "type": "subnode",
      "parent": "MLP_Architecture",
      "description": "Component of MLP applying nonlinear functions to output from matrix multiplication modules."
    },
    {
      "id": "ResNet_Architecture",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Residual Network architecture using residual blocks and convolution layers."
    },
    {
      "id": "Residual_Block",
      "type": "subnode",
      "parent": "ResNet_Architecture",
      "description": "Building block of ResNet that adds input to output from a series of matrix multiplications and activations."
    },
    {
      "id": "Machine_Learning_Architectures",
      "type": "major",
      "parent": null,
      "description": "Overview of different machine learning architectures."
    },
    {
      "id": "ResNet",
      "type": "subnode",
      "parent": "Machine_Learning_Architectures",
      "description": "Residual network architecture using convolution layers and batch normalization."
    },
    {
      "id": "Convolutional_Layers",
      "type": "subnode",
      "parent": "ResNet",
      "description": "Layer type used in ResNet for feature extraction."
    },
    {
      "id": "Batch_Normalization_Variants",
      "type": "subnode",
      "parent": "ResNet",
      "description": "Different types of batch normalization techniques used with convolution layers."
    },
    {
      "id": "Transformer_Architecture",
      "type": "subnode",
      "parent": "Machine_Learning_Architectures",
      "description": "Architecture widely used in modern large language models."
    },
    {
      "id": "Layer_Normalization",
      "type": "subnode",
      "parent": "Batch_Normalization_Variants",
      "description": "Normalization technique applied after nonlinear activations."
    },
    {
      "id": "LN_S_Module",
      "type": "subnode",
      "parent": "Layer_Normalization",
      "description": "Sub-module of layer normalization that normalizes vector elements."
    },
    {
      "id": "Affine_Transformation_Parameters",
      "type": "subnode",
      "parent": "Layer_Normalization",
      "description": "Learnable parameters used to adjust the mean and standard deviation after LN-S."
    },
    {
      "id": "LayerNormalization",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Technique used to normalize the layer inputs in deep neural networks"
    },
    {
      "id": "LN-S",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Standardized version of LN without affine transformation"
    },
    {
      "id": "AffineTransformation",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Process to adjust the mean and standard deviation using learnable parameters"
    },
    {
      "id": "BetaGammaParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Learnable scalars used in affine transformation for LN"
    },
    {
      "id": "ScalingInvariantProperty",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Property of LN making the model invariant to parameter scaling"
    },
    {
      "id": "NormalizationTechniques",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Various normalization methods used in deep learning models."
    },
    {
      "id": "LayerNorm",
      "type": "subnode",
      "parent": "NormalizationTechniques",
      "description": "Layer normalization technique applied to neural network layers."
    },
    {
      "id": "ScaleInvariantProperty",
      "type": "subnode",
      "parent": "LayerNorm",
      "description": "Property of layer norm making it scale-invariant for weights not at the last layer."
    },
    {
      "id": "OtherNormalizationLayers",
      "type": "subnode",
      "parent": "NormalizationTechniques",
      "description": "Alternative normalization layers such as batch and group normalization."
    },
    {
      "id": "ConvolutionalLayers",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Discussion on convolutional layers in neural networks for efficient parameter sharing."
    },
    {
      "id": "1DConvolution",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Simplified version of 1-dimensional convolution layer used in CNNs."
    },
    {
      "id": "Convolutional_Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Type of neural network commonly used for image and text processing."
    },
    {
      "id": "1D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "One-dimensional convolution layer used in natural language processing."
    },
    {
      "id": "2D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "Two-dimensional convolution layer primarily used for image data."
    },
    {
      "id": "Conv1D-S",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Simplified version of 1-D convolution layer, a matrix multiplication with shared parameters."
    },
    {
      "id": "Filter_Vector_w",
      "type": "subnode",
      "parent": "Conv1D-S",
      "description": "A vector parameter in Conv1D-S denoted by w, representing the kernel size k."
    },
    {
      "id": "Bias_Scalar_b",
      "type": "subnode",
      "parent": "Conv1D-S",
      "description": "An additional scalar bias term b used in the simplified 1-D convolution layer."
    },
    {
      "id": "Matrix_Multiplication_Qz",
      "type": "subnode",
      "parent": "Conv1D-S",
      "description": "Output of Conv1D-S can be viewed as a matrix multiplication Qz with shared parameters."
    },
    {
      "id": "ParameterSharing",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Explanation of how convolutional layers share parameters to reduce complexity."
    },
    {
      "id": "EfficiencyComparison",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Comparison between convolution and generic matrix multiplication in terms of efficiency."
    },
    {
      "id": "Conv1D",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Description of Conv1D operation with multiple channels."
    },
    {
      "id": "Conv1DModule",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "One-dimensional convolutional module in neural networks with multiple channels."
    },
    {
      "id": "Conv2DModule",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two-dimensional convolutional module for image processing tasks."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions that can compute real-valued functions."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of calculating gradients with respect to inputs and parameters."
    },
    {
      "id": "Real-Valued Function",
      "type": "subnode",
      "parent": "Differentiable Circuit",
      "description": "Function mapping from R^l to R computed by a differentiable circuit."
    },
    {
      "id": "Arithmetic Operations",
      "type": "subnode",
      "parent": "Differentiable Circuit",
      "description": "Additions, subtractions, multiplications, and divisions in the circuit."
    },
    {
      "id": "Elementary Functions",
      "type": "subnode",
      "parent": "Differentiable Circuit",
      "description": "Functions like ReLU, exp, log, sin, cos used in the circuit."
    },
    {
      "id": "Loss Function J^(j)(θ)",
      "type": "subnode",
      "parent": "Gradient Computation",
      "description": "Function representing loss for j-th example computed by operations and functions."
    },
    {
      "id": "Loss Function J(θ)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Description and computation of the loss function for a given example."
    },
    {
      "id": "Chain Rule Perspective",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "New perspective on the chain rule for understanding backpropagation."
    },
    {
      "id": "General Backprop Strategy",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to general strategies in implementing backpropagation."
    },
    {
      "id": "Basic Modules Backward Function",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Discussion on computing backward functions for basic neural network modules."
    },
    {
      "id": "Concrete Backprop Algorithm",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Putting together a concrete backprop algorithm for MLPs."
    },
    {
      "id": "Auto-Differentiation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Implementation of auto-differentiation in deep learning packages."
    },
    {
      "id": "Partial Derivatives Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to the concept and notation of partial derivatives."
    },
    {
      "id": "Scalar Variable J",
      "type": "subnode",
      "parent": "Partial Derivatives Overview",
      "description": "Description of scalar variable J depending on variables z with same dimensionality."
    },
    {
      "id": "Derivatives_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Discussion on the computation and application of derivatives in machine learning contexts."
    },
    {
      "id": "Partial_Derivatives_of_Scalar_Functions",
      "type": "subnode",
      "parent": "Derivatives_in_Machine_Learning",
      "description": "Focuses on computing partial derivatives when the function is scalar-valued and dependent variables are vectors, matrices or tensors."
    },
    {
      "id": "Chain_Rule_Application",
      "type": "subnode",
      "parent": "Derivatives_in_Machine_Learning",
      "description": "Application of chain rule in computing derivatives for composite functions in machine learning contexts."
    },
    {
      "id": "Multi_Variate_Functions_Derivatives",
      "type": "subnode",
      "parent": "Derivatives_in_Machine_Learning",
      "description": "Challenges and considerations when dealing with partial derivatives of multi-variate functions in machine learning."
    },
    {
      "id": "BackwardFunction",
      "type": "subnode",
      "parent": "ChainRule",
      "description": "Definition and properties of the backward function in machine learning models."
    },
    {
      "id": "JacobianMatrix",
      "type": "subnode",
      "parent": "ChainRule",
      "description": "Introduction to Jacobian matrices and their role in computing gradients."
    },
    {
      "id": "Chain Rule",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Rule for computing derivatives of composite functions."
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Abstract representation of loss functions as compositions of modules."
    },
    {
      "id": "Modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Building blocks such as MM, σ, Conv2D used in neural networks."
    },
    {
      "id": "BinaryClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A specific problem involving binary classification using a neural network model."
    },
    {
      "id": "MLPModel",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Multi-layer perceptron (MLP) used for solving the binary classification problem."
    },
    {
      "id": "LossFunction",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Mathematical function used to measure the performance of SIMCLR during training."
    },
    {
      "id": "ModulesInModel",
      "type": "subnode",
      "parent": "MLPModel",
      "description": "Description of different modules (M1, M2,...) in the MLP model and their parameters."
    },
    {
      "id": "ForwardPass",
      "type": "subnode",
      "parent": "BackpropagationProcess",
      "description": "Sequential computation of intermediate variables (u[1],...,u[k]) during the forward pass."
    },
    {
      "id": "BackwardPass",
      "type": "subnode",
      "parent": "BackpropagationProcess",
      "description": "Calculation of derivatives with respect to parameters and intermediate variables in backward order."
    },
    {
      "id": "DerivativeComputation",
      "type": "subnode",
      "parent": "BackwardPass",
      "description": "Detailed computation of partial derivatives for parameter updates during backpropagation."
    },
    {
      "id": "Machine_Learning_Backpropagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backpropagation in machine learning"
    },
    {
      "id": "Gradient_Computation",
      "type": "subnode",
      "parent": "Chain_Rule_Application",
      "description": "Computation of \frac{ΔJ}{Δu^{[i-1]}} from \frac{ΔJ}{Δu^{[i]}} and u^{[i-1]}"
    },
    {
      "id": "Efficiency_of_Modules",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Discussion on the efficiency of small modules in backpropagation"
    },
    {
      "id": "Neural Networks Composition",
      "type": "major",
      "parent": null,
      "description": "Composition of neural networks using atomic operations and backpropagation."
    },
    {
      "id": "Backward Functions Overview",
      "type": "subnode",
      "parent": "Neural Networks Composition",
      "description": "Overview of backward functions for basic modules in neural networks."
    },
    {
      "id": "Matrix Multiplication Module (MM)",
      "type": "subnode",
      "parent": "Backward Functions Overview",
      "description": "Detailed explanation and derivation of the backward function for matrix multiplication module."
    },
    {
      "id": "Activation Functions",
      "type": "subnode",
      "parent": "Backward Functions Overview",
      "description": "Discussion on computing backward functions for activation functions in neural networks."
    },
    {
      "id": "Loss Functions",
      "type": "subnode",
      "parent": "Backward Functions Overview",
      "description": "Explanation of the computation of backward functions for loss functions used in training neural networks."
    },
    {
      "id": "Backward Function",
      "type": "major",
      "parent": null,
      "description": "Discussion on the backward function in machine learning."
    },
    {
      "id": "W Variable",
      "type": "subnode",
      "parent": "Backward Function",
      "description": "Explains the use of equation (7.58) for matrix W."
    },
    {
      "id": "b Variable",
      "type": "subnode",
      "parent": "Backward Function",
      "description": "Discusses the backward function for bias variable b."
    },
    {
      "id": "BackwardFunctionEfficiency",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the efficiency of backward functions in neural networks."
    },
    {
      "id": "VectorizedNotation",
      "type": "subnode",
      "parent": "BackwardFunctionEfficiency",
      "description": "Explanation of vectorized notation for backward functions."
    },
    {
      "id": "ActivationFunctionsDerivatives",
      "type": "subnode",
      "parent": "BackwardFunctionEfficiency",
      "description": "Details on derivatives of activation functions in the context of backward propagation."
    },
    {
      "id": "LossFunctionsBackwardPass",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Description of how loss functions behave during the backward pass."
    },
    {
      "id": "MachineLearningLossFunctions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions in machine learning."
    },
    {
      "id": "CrossEntropyLossFunction",
      "type": "subnode",
      "parent": "MachineLearningLossFunctions",
      "description": "Explanation of cross-entropy loss and its gradient calculation."
    },
    {
      "id": "BackpropagationMLPs",
      "type": "major",
      "parent": null,
      "description": "Process of backpropagation in multi-layer perceptrons (MLPs)."
    },
    {
      "id": "ForwardPassMLP",
      "type": "subnode",
      "parent": "BackpropagationMLPs",
      "description": "Sequence of operations for the forward pass in an MLP."
    },
    {
      "id": "BackwardPassMLP",
      "type": "subnode",
      "parent": "BackpropagationMLPs",
      "description": "Description of backpropagation steps to compute gradients in MLPs."
    },
    {
      "id": "Machine Learning Basics",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and equations."
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Computation of activations and outputs before gradient calculation."
    },
    {
      "id": "TrainingExamplesMatrixNotation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Explanation of training examples using matrix notation."
    },
    {
      "id": "LayerActivations",
      "type": "subnode",
      "parent": "TrainingExamplesMatrixNotation",
      "description": "Description of activations for each layer in a neural network."
    },
    {
      "id": "VectorizationTechniques",
      "type": "subnode",
      "parent": "TrainingExamplesMatrixNotation",
      "description": "Discussion on vectorizing operations to improve efficiency."
    },
    {
      "id": "BroadcastingConcept",
      "type": "subnode",
      "parent": "VectorizationTechniques",
      "description": "Explanation of broadcasting in matrix operations for adding bias terms."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Discussion on how to generalize matricization approach in deep learning frameworks."
    },
    {
      "id": "Implementation Details",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Subtleties and complications in the implementation of matricization, including data point representation and matrix operations."
    },
    {
      "id": "Data Representation",
      "type": "subnode",
      "parent": "Implementation Details",
      "description": "Explanation on how data points are represented as rows or columns in matrices depending on deep learning packages."
    },
    {
      "id": "Matrix Conversion",
      "type": "subnode",
      "parent": "Implementation Details",
      "description": "Details on converting between row-major and column-major representations of data points."
    },
    {
      "id": "Generalization and Regularization",
      "type": "major",
      "parent": null,
      "description": "Discussion on tools to analyze and understand the generalization performance of machine learning models."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization and Regularization",
      "description": "Explanation of training loss functions used in supervised learning problems to fit data."
    },
    {
      "id": "Training Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The loss calculated on the training dataset, also known as empirical loss or risk."
    },
    {
      "id": "Test Error",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Evaluation metric for model performance using unseen test examples."
    },
    {
      "id": "Mean Squared Error (MSE)",
      "type": "subnode",
      "parent": "Training Loss",
      "description": "A specific loss function used to measure the average squared difference between predictions and actual values."
    },
    {
      "id": "Test Distribution",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The distribution from which test examples are sampled, distinct from the training data distribution."
    },
    {
      "id": "Empirical Distribution",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Uniform distribution over training set used for theoretical analysis and empirical loss calculation."
    },
    {
      "id": "Population Distribution",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "The true underlying data distribution from which test examples are drawn, relevant to population risk or error."
    },
    {
      "id": "Machine_Learning_Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and challenges."
    },
    {
      "id": "Training_Test_Distributions",
      "type": "subnode",
      "parent": "Machine_Learning_Fundamentals",
      "description": "Discussion on the distributions from which training and test data are drawn."
    },
    {
      "id": "Domain_Shift",
      "type": "subnode",
      "parent": "Training_Test_Distributions",
      "description": "Scenario where training and testing datasets come from different distributions."
    },
    {
      "id": "Overfitting_Underfitting",
      "type": "subnode",
      "parent": "Machine_Learning_Fundamentals",
      "description": "Concepts of overfitting and underfitting in machine learning models."
    },
    {
      "id": "Generalization_Gap",
      "type": "subnode",
      "parent": "Overfitting_Underfitting",
      "description": "Difference between training error and test error, also known as generalization gap."
    },
    {
      "id": "Bias_Variance_Tradeoff",
      "type": "major",
      "parent": "Machine_Learning_Fundamentals",
      "description": "Balance between model complexity and error due to underfitting or overfitting."
    },
    {
      "id": "Test_Error_Influence",
      "type": "subnode",
      "parent": "Bias_Variance_Tradeoff",
      "description": "Factors affecting the test error including model parameterization choices."
    },
    {
      "id": "8.1 Bias-variance Tradeoff",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on the balance between model simplicity and complexity to avoid underfitting or overfitting."
    },
    {
      "id": "Linear Model Example",
      "type": "subnode",
      "parent": "Underfitting",
      "description": "Illustration using linear regression showing high training error due to mismatch with true quadratic relationship."
    },
    {
      "id": "Training Dataset",
      "type": "subnode",
      "parent": "8.1 Bias-variance Tradeoff",
      "description": "Dataset used for model training, including randomly chosen inputs and outputs generated by a quadratic function plus noise."
    },
    {
      "id": "Test Error Analysis",
      "type": "subnode",
      "parent": "8.1 Bias-variance Tradeoff",
      "description": "Analysis of test error when fitting various types of models to the data."
    },
    {
      "id": "Machine_Learning_Bias_Variance_Tradeoff",
      "type": "major",
      "parent": null,
      "description": "Overview of bias and variance tradeoffs in machine learning models."
    },
    {
      "id": "Linear_Models_Impairment",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Discussion on the limitations of linear models despite having large training datasets."
    },
    {
      "id": "Bias_Definition",
      "type": "subnode",
      "parent": "Linear_Models_Impairment",
      "description": "Part of the error due to model's inability to capture true function, independent of data quantity."
    },
    {
      "id": "Underfitting_Linear_Models",
      "type": "subnode",
      "parent": "Linear_Models_Impairment",
      "description": "Explanation of underfitting issues with linear models due to high bias."
    },
    {
      "id": "5th_Degree_Polynomial_Failure",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Discussion on the failure of 5th-degree polynomial models in generalization despite fitting training data well."
    },
    {
      "id": "High_Variance_Issue",
      "type": "subnode",
      "parent": "5th_Degree_Polynomial_Failure",
      "description": "Explanation of high variance issues leading to poor model performance on test data."
    },
    {
      "id": "Polynomial_Bias_Low",
      "type": "subnode",
      "parent": "5th_Degree_Polynomial_Failure",
      "description": "Discussion on the low bias nature of 5th-degree polynomials and their potential for accurate representation with large datasets."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Measure of how much the model changes with different training datasets"
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "major",
      "parent": "Sample Complexity Bounds",
      "description": "Balance between model complexity and error due to underfitting or overfitting"
    },
    {
      "id": "5th Degree Polynomial",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Example of a complex model that can overfit data"
    },
    {
      "id": "Training Error",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Error on the dataset used to train the model"
    },
    {
      "id": "Small Training Set",
      "type": "subnode",
      "parent": "Variance",
      "description": "Dataset size affects variance and likelihood of overfitting"
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Degree of flexibility or capacity of a model to fit data."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Breaking down test error into bias and variance components."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Phenomenon observed in machine learning where test error peaks and then decreases again as model complexity increases."
    },
    {
      "id": "RegressionProblems",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Analysis of regression problems and their solutions."
    },
    {
      "id": "BiasVarianceTradeoff",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Exploration of the trade-off between model bias and variance in regression analysis."
    },
    {
      "id": "TrainingDataset",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Description of a training dataset used for model training."
    },
    {
      "id": "TestExample",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Explanation of how test examples are used to evaluate models."
    },
    {
      "id": "ExpectedTestError",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Definition and calculation of expected test error for a given model."
    },
    {
      "id": "MeanSquaredError",
      "type": "subnode",
      "parent": "ExpectedTestError",
      "description": "Formulation of mean squared error as a measure of prediction accuracy."
    },
    {
      "id": "Claim811",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Mathematical claim used to decompose MSE into bias and variance terms."
    },
    {
      "id": "DecompositionOfMSE",
      "type": "subnode",
      "parent": "ExpectedTestError",
      "description": "Breakdown of mean squared error into bias and variance components."
    },
    {
      "id": "Mean_Squared_Error_MSE",
      "type": "subnode",
      "parent": "Bias_Variance_Tradeoff",
      "description": "Definition and decomposition of MSE in machine learning models."
    },
    {
      "id": "Average_Model_h_avg",
      "type": "subnode",
      "parent": "Bias_Variance_Tradeoff",
      "description": "Theoretical model representing the average prediction across an infinite number of datasets."
    },
    {
      "id": "Hypothetical_Single_Dataset_Model",
      "type": "subnode",
      "parent": "Average_Model_h_avg",
      "description": "Model obtained by training on a single dataset with infinite samples, approximating h_avg."
    },
    {
      "id": "Variance_Definition",
      "type": "subnode",
      "parent": "Mean_Squared_Error_MSE",
      "description": "Part of the error due to fluctuations in model predictions across different training sets."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Captures systematic errors due to oversimplification of true function."
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Measures sensitivity of model to training data randomness."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error as model complexity increases and then decreases again with further overparameterization."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error peak when sample size is approximately equal to feature dimension, suggesting suboptimal training algorithms."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Models with more parameters than necessary, showing a second descent in test errors."
    },
    {
      "id": "Historical Context",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Discovery and popularization of the double descent phenomenon by various researchers."
    },
    {
      "id": "Explanation and Mitigation Strategy",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Understanding and addressing strategies for the sample-wise double descent phenomenon."
    },
    {
      "id": "Optimization Algorithms",
      "type": "subnode",
      "parent": "Sample-wise Double Descent",
      "description": "Training algorithms evaluated are suboptimal when sample size is approximately equal to feature dimension."
    },
    {
      "id": "Regularization Techniques",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Optimally tuned regularization can mitigate the peak in double descent phenomena."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Overparameterized models with gradient descent exhibit better generalization due to implicit regularization effects."
    },
    {
      "id": "GradientDescentOptimizer",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Optimization technique used in training models to minimize loss function."
    },
    {
      "id": "MinimumNormSolution",
      "type": "subnode",
      "parent": "GradientDescentOptimizer",
      "description": "The solution with the smallest Euclidean norm found by gradient descent when data is overparameterized."
    },
    {
      "id": "DoubleDescentPhenomenon",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Phenomenon where model performance initially improves then worsens and improves again as complexity increases."
    },
    {
      "id": "ModelComplexityMeasures",
      "type": "subnode",
      "parent": "DoubleDescentPhenomenon",
      "description": "Various measures to quantify the complexity of a machine learning model, such as number of parameters or norm of models."
    },
    {
      "id": "NormAsComplexityMeasure",
      "type": "subnode",
      "parent": "ModelComplexityMeasures",
      "description": "Using the norm of learned models as an alternative measure for model complexity to avoid double descent phenomenon."
    },
    {
      "id": "Linear Regression Setup",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Setup for linear regression using Fashion-MNIST data."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Discussion on bounds related to sample complexity in machine learning."
    },
    {
      "id": "Model Selection Methods",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Methods for selecting optimal model complexity based on training data."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error analysis related to how well a model generalizes from training to unseen data."
    },
    {
      "id": "Learning Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning algorithms and their performance guarantees."
    },
    {
      "id": "Learning Theory Proofs",
      "type": "major",
      "parent": null,
      "description": "Conditions and lemmas for proving learning algorithms' effectiveness."
    },
    {
      "id": "Union Bound Lemma",
      "type": "subnode",
      "parent": "Learning Theory Proofs",
      "description": "Probability bound on the union of multiple events."
    },
    {
      "id": "Hoeffding Inequality",
      "type": "subnode",
      "parent": "Learning Theory Proofs",
      "description": "Bound for deviation between sample mean and true probability in Bernoulli distribution."
    },
    {
      "id": "Machine_Learning_Basics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "Hypothesis_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Error associated with a hypothesis on the given training set."
    },
    {
      "id": "Training_Error",
      "type": "subnode",
      "parent": "Hypothesis_Error",
      "description": "Fraction of misclassified examples in the training dataset by a hypothesis."
    },
    {
      "id": "Generalization_Error",
      "type": "subnode",
      "parent": "Hypothesis_Error",
      "description": "Probability that a hypothesis will misclassify new data drawn from distribution D."
    },
    {
      "id": "PAC_Assumptions",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Framework and assumptions for learning theory, including same-distribution training and testing."
    },
    {
      "id": "Empirical_Risk_Minimization",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Learning algorithm that minimizes the empirical risk over training data."
    },
    {
      "id": "Hypothesis_Class",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Set of all classifiers considered by a learning algorithm."
    },
    {
      "id": "Finite_Hypothesis_Class",
      "type": "subnode",
      "parent": "Hypothesis_Class",
      "description": "Case where the hypothesis class is finite and consists of k hypotheses."
    },
    {
      "id": "Generalization_Error_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Strategies for ensuring that training performance reflects real-world performance."
    },
    {
      "id": "Bernoulli_Random_Variables",
      "type": "subnode",
      "parent": "Generalization_Error_Guarantees",
      "description": "Random variables used to model misclassification events in hypothesis testing."
    },
    {
      "id": "Hoeffding_Inequality",
      "type": "subnode",
      "parent": "Generalization_Error_Guarantees",
      "description": "Statistical tool for bounding the probability of deviation between training and generalization errors."
    },
    {
      "id": "Uniform_Convergence",
      "type": "major",
      "parent": "Machine_Learning_Theory",
      "description": "Property indicating that the empirical error approximates true error uniformly across all hypotheses in H."
    },
    {
      "id": "Training_Error_Generalization_Error",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Relationship between training and generalization errors in machine learning."
    },
    {
      "id": "Union_Bound",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Technique used to bound the probability of multiple events occurring simultaneously."
    },
    {
      "id": "Probability_Error",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "Measure of how likely it is for training error to differ from generalization error by more than a specified amount."
    },
    {
      "id": "Sample_Size_N",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "Number of samples required to ensure that the difference between training and generalization errors is within a certain threshold with high probability."
    },
    {
      "id": "Gamma_Parameter",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "Threshold parameter determining how close the empirical error must be to the true error."
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including concepts like generalization and sample complexity."
    },
    {
      "id": "Generalization_Error_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Mathematical bound on the difference between training error and true error for a hypothesis set H."
    },
    {
      "id": "Sample_Complexity",
      "type": "subnode",
      "parent": "Generalization_Error_Bound",
      "description": "The number of samples needed to achieve a certain level of performance given a confidence parameter δ."
    },
    {
      "id": "Hypothesis_Class_H",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Set of hypotheses or models considered for learning task."
    },
    {
      "id": "Optimal_Hypothesis_h_star",
      "type": "subnode",
      "parent": "Generalization_Error",
      "description": "The hypothesis with the lowest true error in a given hypothesis class."
    },
    {
      "id": "Theorem_on_Generalization_Error",
      "type": "subnode",
      "parent": "Generalization_Error",
      "description": "Formal statement relating empirical risk, true risk, and uniform convergence bounds."
    },
    {
      "id": "Hypothesis Class",
      "type": "major",
      "parent": null,
      "description": "Set of functions considered for learning a model."
    },
    {
      "id": "Sample Complexity Bound",
      "type": "subnode",
      "parent": "Hypothesis Class",
      "description": "Bound on the number of samples required for a given accuracy and confidence level."
    },
    {
      "id": "Finite Hypothesis Classes",
      "type": "major",
      "parent": null,
      "description": "Analysis of hypothesis classes with finite cardinality."
    },
    {
      "id": "Infinite Hypothesis Classes",
      "type": "subnode",
      "parent": "Hypothesis Class",
      "description": "Consideration of hypothesis classes parameterized by real numbers."
    },
    {
      "id": "Parameterization by Real Numbers",
      "type": "subnode",
      "parent": "Infinite Hypothesis Classes",
      "description": "Analysis of models with infinite number of functions due to real-number parameters."
    },
    {
      "id": "Hypothesis_Class_Size",
      "type": "subnode",
      "parent": "Sample_Complexity",
      "description": "Size of the hypothesis class in terms of model parameters."
    },
    {
      "id": "Floating_Point_Precision",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Impact of floating point precision on machine learning models."
    },
    {
      "id": "Linear_Classifiers",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Examples and formulations of linear classifiers with varying parameters."
    },
    {
      "id": "Hypothesis_Class_Parameterization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Exploration of different parameterizations for the same hypothesis class."
    },
    {
      "id": "Shattering",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "The ability to perfectly classify any possible labeling of a given finite subset of points."
    },
    {
      "id": "VC_Dimension",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to Vapnik-Chervonenkis dimension as a measure of complexity for hypothesis classes."
    },
    {
      "id": "VC Dimension",
      "type": "major",
      "parent": null,
      "description": "A measure of the capacity of a statistical classification algorithm, defined as the cardinality of the largest set of points that the algorithm can shatter."
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "A fundamental theorem in learning theory that provides bounds on the difference between empirical and true errors for hypotheses with finite VC dimension."
    },
    {
      "id": "Empirical Error (ε̂(h))",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "The error rate of a hypothesis h based on training data."
    },
    {
      "id": "True Error (ε(h))",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "The actual error rate of a hypothesis h when applied to the entire population."
    },
    {
      "id": "Uniform Convergence",
      "type": "major",
      "parent": null,
      "description": "A property indicating that empirical errors converge uniformly to true errors as sample size increases."
    },
    {
      "id": "Corollary",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Provides a condition for the number of training examples needed to ensure a given level of accuracy in learning with hypothesis class H."
    },
    {
      "id": "Chapter 9 Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Focuses on techniques to control model complexity."
    },
    {
      "id": "Model complexity",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Measure of the capacity of a model, often related to the number or size of parameters."
    },
    {
      "id": "Training loss/cost function",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Objective function used in training models which is modified by regularization."
    },
    {
      "id": "Regularizer term",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Additional term added to the loss function to penalize complexity and reduce overfitting."
    },
    {
      "id": "Regularized loss",
      "type": "subnode",
      "parent": "Training loss/cost function",
      "description": "Modified loss function including a regularizer term for better model generalization."
    },
    {
      "id": "Regularized Loss",
      "type": "major",
      "parent": null,
      "description": "Combination of original loss and regularizer to balance model fit and complexity."
    },
    {
      "id": "Original Loss J(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Measures how well the model fits the training data."
    },
    {
      "id": "Regularizer R(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Penalizes model complexity to prevent overfitting."
    },
    {
      "id": "Regularization Parameter λ",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Controls the trade-off between fitting data and reducing model complexity."
    },
    {
      "id": "L2 Regularization",
      "type": "major",
      "parent": "Regularization",
      "description": "Common regularization technique that penalizes large weights to encourage simpler models."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "L2 Regularization",
      "description": "In deep learning, L2 regularization is often referred to as weight decay due to its effect on model parameters during training."
    },
    {
      "id": "Inductive Biases and Structures",
      "type": "major",
      "parent": null,
      "description": "Regularization can impose prior knowledge or structural constraints on the model parameters."
    },
    {
      "id": "Sparsity",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Imposing structure on model parameters to reduce the number of non-zero elements."
    },
    {
      "id": "L1 Regularization (LASSO)",
      "type": "subnode",
      "parent": "Sparsity",
      "description": "Penalizes the absolute value of coefficients, promoting sparsity."
    },
    {
      "id": "Gradient Descent Compatibility",
      "type": "subnode",
      "parent": "L1 Regularization (LASSO)",
      "description": "L1 regularization is not directly compatible with gradient descent due to non-differentiability."
    },
    {
      "id": "Relaxation Techniques",
      "type": "subnode",
      "parent": "Gradient Descent Compatibility",
      "description": "Techniques like L2 norm used as a continuous surrogate for L1 regularization."
    },
    {
      "id": "Deep Learning Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Techniques specific to deep learning models, including weight decay and dropout."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and concepts in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Effect of optimizers on model parameters beyond explicit regularized loss."
    },
    {
      "id": "Global Minima Diversity",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "Different optimizers converge to different global minima with varying generalization performance."
    },
    {
      "id": "Optimizer Impact",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "Choice of optimizer affects not only training loss but also model generalization and implicit regularization."
    },
    {
      "id": "Optimizer-Generalization",
      "type": "major",
      "parent": null,
      "description": "Discussion on how optimizers affect model generalization through implicit regularization."
    },
    {
      "id": "LearningRateSchedule",
      "type": "subnode",
      "parent": "Optimizer-Generalization",
      "description": "Impact of learning rate schedules on the performance and generalization of neural networks."
    },
    {
      "id": "InitializationEffect",
      "type": "subnode",
      "parent": "Optimizer-Generalization",
      "description": "How different initializations impact model training and generalization."
    },
    {
      "id": "ModelSelectionCV",
      "type": "major",
      "parent": null,
      "description": "Overview of selecting models via cross-validation techniques."
    },
    {
      "id": "FlatMinimaConjecture",
      "type": "subnode",
      "parent": "Optimizer-Generalization",
      "description": "Theoretical conjectures about flat minima and their relation to generalization performance."
    },
    {
      "id": "Model Selection",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model for a given task."
    },
    {
      "id": "Cross Validation",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Technique for evaluating machine learning models by splitting data into subsets."
    },
    {
      "id": "Polynomial Regression Models",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Models with varying degrees of polynomial terms for regression tasks."
    },
    {
      "id": "SVM (Support Vector Machine)",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Machine learning algorithm for classification and regression tasks."
    },
    {
      "id": "Neural Networks",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Deep learning models with multiple layers of interconnected nodes."
    },
    {
      "id": "EmpiricalRiskMinimization",
      "type": "subnode",
      "parent": "MachineLearningOptimization",
      "description": "Process of minimizing empirical risk for model selection."
    },
    {
      "id": "CrossValidation",
      "type": "major",
      "parent": null,
      "description": "Technique to validate models using a subset of data not seen during training."
    },
    {
      "id": "HoldOutCrossValidation",
      "type": "subnode",
      "parent": "CrossValidation",
      "description": "Method involving splitting dataset into train and validation sets for model selection."
    },
    {
      "id": "TrainingSetSplitting",
      "type": "subnode",
      "parent": "HoldOutCrossValidation",
      "description": "Process of dividing data into training, cross-validation, and test sets."
    },
    {
      "id": "ModelSelection",
      "type": "major",
      "parent": "HoldoutCrossValidation",
      "description": "Process of choosing the best model from a set of candidate models based on validation error."
    },
    {
      "id": "MachineLearningValidationTechniques",
      "type": "major",
      "parent": null,
      "description": "Various methods to validate machine learning models."
    },
    {
      "id": "HoldoutCrossValidation",
      "type": "subnode",
      "parent": "MachineLearningValidationTechniques",
      "description": "A method where a portion of the data is held out for validation purposes."
    },
    {
      "id": "kFoldCrossValidation",
      "type": "subnode",
      "parent": "MachineLearningValidationTechniques",
      "description": "An alternative to holdout cross-validation that uses multiple subsets of the data."
    },
    {
      "id": "RetrainingOnFullDataset",
      "type": "subnode",
      "parent": "HoldoutCrossValidation",
      "description": "Option to retrain the selected model with all available training data."
    },
    {
      "id": "DataSplitting",
      "type": "subnode",
      "parent": "kFoldCrossValidation",
      "description": "Randomly dividing the dataset into k equal parts for validation."
    },
    {
      "id": "KFoldCV",
      "type": "subnode",
      "parent": "CrossValidation",
      "description": "Divides dataset into k subsets and trains models using (k-1) subsets while testing on the remaining subset."
    },
    {
      "id": "LOOCV",
      "type": "subnode",
      "parent": "CrossValidation",
      "description": "Variant of cross validation where one data point is left out for validation, repeated for each sample."
    },
    {
      "id": "DataScarcity",
      "type": "major",
      "parent": null,
      "description": "Challenge in machine learning when the amount of available training data is limited."
    },
    {
      "id": "Leave-One-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Method of cross validation where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Approach to statistics that treats parameters as random variables with prior distributions."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Method for estimating parameters by maximizing the likelihood function under frequentist view."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Distribution expressing prior beliefs about parameter values before observing data."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of Bayesian approaches in machine learning."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Set of training examples used for model fitting."
    },
    {
      "id": "Posterior Distribution on Parameters",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Distribution of parameters given the training set."
    },
    {
      "id": "Model Specification",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Definition of model likelihood function for predictions."
    },
    {
      "id": "Bayesian Logistic Regression",
      "type": "subnode",
      "parent": "Model Specification",
      "description": "Example model using logistic regression with Bayesian interpretation."
    },
    {
      "id": "Posterior Distribution on Class Label",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Distribution of class labels given input and training set."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Prediction method using posterior distribution over parameters."
    },
    {
      "id": "Bayesian_Inference",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Statistical method for applying Bayes' theorem to update probabilities based on evidence."
    },
    {
      "id": "Posterior_Distribution",
      "type": "subnode",
      "parent": "Bayesian_Inference",
      "description": "Conditional probability distribution of latent variables given observed data and model parameters, crucial for setting Q(z) in EM."
    },
    {
      "id": "MAP_Estimate",
      "type": "subnode",
      "parent": "Bayesian_Inference",
      "description": "Point estimate that maximizes the posterior probability in Bayesian inference."
    },
    {
      "id": "MLE_Estimate",
      "type": "subnode",
      "parent": "Bayesian_Inference",
      "description": "Estimation method that finds parameter values maximizing likelihood of observed data."
    },
    {
      "id": "Prior_Distribution",
      "type": "subnode",
      "parent": "Bayesian_Inference",
      "description": "Initial probability distribution representing prior beliefs about parameters before observing data."
    },
    {
      "id": "Unsupervised_Learning",
      "type": "major",
      "parent": null,
      "description": "Type of machine learning where the model learns from unlabeled data to discover hidden structures."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Technique for grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups."
    },
    {
      "id": "K_Means_Algorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "Popular clustering algorithm used to partition data into k clusters based on feature similarity."
    },
    {
      "id": "k-means_algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "distortion_function",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures sum of squared distances between examples and cluster centroids."
    },
    {
      "id": "initialization_step",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Randomly selects k training examples as initial cluster centroids."
    },
    {
      "id": "inner_loop_steps",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Assigns each example to closest centroid and updates centroid positions."
    },
    {
      "id": "coordinate_descent",
      "type": "subnode",
      "parent": "distortion_function",
      "description": "Optimization technique used in k-means for minimizing distortion function."
    },
    {
      "id": "distortion_function_J",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures the quality of clustering; non-convex and can lead to local minima."
    },
    {
      "id": "convergence_of_k_means",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "J decreases monotonically until convergence, but may oscillate in rare cases."
    },
    {
      "id": "EM_algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in probabilistic models."
    },
    {
      "id": "mixture_of_gaussians",
      "type": "subnode",
      "parent": "EM_algorithm",
      "description": "Model where data is generated from multiple Gaussian distributions with unknown parameters."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning setting where data lacks labels."
    },
    {
      "id": "Joint Distribution Model",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Models the joint distribution of observed and latent variables."
    },
    {
      "id": "Mixture of Gaussians",
      "type": "subnode",
      "parent": "Joint Distribution Model",
      "description": "Model where data is generated from a mixture of Gaussian distributions."
    },
    {
      "id": "Latent Variables",
      "type": "subnode",
      "parent": "Joint Distribution Model",
      "description": "Hidden variables that influence the generation of observed data."
    },
    {
      "id": "Likelihood Estimation",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Estimating parameters by maximizing the likelihood function of observed data."
    },
    {
      "id": "Closed-Form Solution",
      "type": "subnode",
      "parent": "Likelihood Estimation",
      "description": "Not possible to find closed-form solutions for parameter estimation in this model."
    },
    {
      "id": "GaussianMixtureModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters."
    },
    {
      "id": "EMAlgorithm",
      "type": "subnode",
      "parent": "GaussianMixtureModel",
      "description": "An iterative method used to find maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."
    },
    {
      "id": "EM-Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "E-step",
      "type": "subnode",
      "parent": "EM-Algorithm",
      "description": "Calculates the expected value of the log-likelihood evaluated using the current estimate for the parameters."
    },
    {
      "id": "M-step",
      "type": "subnode",
      "parent": "EM-Algorithm",
      "description": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "E-step",
      "description": "Probability distribution used for calculating likelihoods of data points given latent variables."
    },
    {
      "id": "Soft Assignments",
      "type": "subnode",
      "parent": "EM-Algorithm",
      "description": "Probabilistic assignments of data points to clusters, contrasted with hard assignments in K-means."
    },
    {
      "id": "K-Means Clustering",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that assigns each data point to a single cluster based on minimum distance from centroid."
    },
    {
      "id": "Expectation_Maximization_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the guarantees and convergence properties of EM algorithm."
    },
    {
      "id": "Jensens_Inequality",
      "type": "major",
      "parent": null,
      "description": "Introduction to Jensen's inequality in machine learning context."
    },
    {
      "id": "Convex_Functions",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Definition and properties of convex functions used in Jensen's inequality."
    },
    {
      "id": "Theorem_Jensen",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Statement and implications of Jensen's inequality theorem."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "Inequality relating expected values of convex functions and function values at expected points."
    },
    {
      "id": "Concave Function",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Negative of a convex function, with reversed inequality in Jensen's Inequality."
    },
    {
      "id": "EM Algorithm",
      "type": "major",
      "parent": "Variational Inference",
      "description": "Iterative method for finding maximum likelihood estimates in models with latent variables."
    },
    {
      "id": "Latent Variable Model",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Statistical model involving observable and unobservable (latent) variables."
    },
    {
      "id": "EM_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Iterative algorithm for finding maximum likelihood or maximum a posteriori estimates in statistical models with latent variables."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Estimation step where posterior distribution of latent variables is computed given observed data and current parameters."
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "Likelihood_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of maximizing likelihood functions for parameter estimation."
    },
    {
      "id": "Latent_Variables",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Hidden variables that complicate direct optimization of the likelihood function."
    },
    {
      "id": "ProbabilityDistribution",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on probability distributions and their roles in ML models."
    },
    {
      "id": "JensensInequality",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of Jensen's inequality and its application in deriving lower bounds."
    },
    {
      "id": "LogLikelihoodBound",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation of a bound on the log-likelihood function using probability distributions Q."
    },
    {
      "id": "ContinuousVariables",
      "type": "subnode",
      "parent": "ProbabilityDistribution",
      "description": "Handling continuous variables in probability distribution functions."
    },
    {
      "id": "Expectation_Maximization_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Statistical method for finding maximum likelihood or maximum a posteriori estimates of parameters in probabilistic models, especially when dealing with incomplete data."
    },
    {
      "id": "Evidence_Lower_Bound_(ELBO)",
      "type": "subnode",
      "parent": "Expectation_Maximization_Algorithm",
      "description": "Objective function used to optimize the EM algorithm by maximizing a lower bound on the log-likelihood of observed data."
    },
    {
      "id": "Jensen's_Inequality",
      "type": "subnode",
      "parent": "Evidence_Lower_Bound_(ELBO)",
      "description": "Mathematical inequality used to derive conditions for making the ELBO tight, ensuring equality holds under specific distribution assumptions."
    },
    {
      "id": "Log_Likelihood_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Optimizing the log-likelihood function for a single example under EM framework."
    },
    {
      "id": "Multiple_Examples",
      "type": "subnode",
      "parent": "Log_Likelihood_Optimization",
      "description": "Extending optimization to multiple training examples by summing over individual ELBOs."
    },
    {
      "id": "ELBO_Formulation",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Formulating the evidence lower bound for a single example and its generalization to multiple examples."
    },
    {
      "id": "Q_Distributions",
      "type": "subnode",
      "parent": "Multiple_Examples",
      "description": "Introducing individual distributions Q_i for each training example x^(i)."
    },
    {
      "id": "Optimal_Q_Choice",
      "type": "subnode",
      "parent": "ELBO_Formulation",
      "description": "Determining the optimal choice of Q as p(z|x;θ) to achieve equality in ELBO formulation."
    },
    {
      "id": "Loglikelihood_Lower_Bound",
      "type": "subnode",
      "parent": "Multiple_Examples",
      "description": "Deriving a lower bound on log-likelihood for multiple examples using summed ELBOs."
    },
    {
      "id": "Convergence_Criterion",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Condition under which the EM algorithm stops iterating based on changes in log-likelihood between iterations."
    },
    {
      "id": "ELBO",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Evidence Lower BOund; a lower bound on log-likelihood used in the E-step and M-step calculations."
    },
    {
      "id": "ELBOExplanation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Detailed explanation and interpretation of Evidence Lower Bound (ELBO)."
    },
    {
      "id": "AlternativeFormulationsOfELBO",
      "type": "subnode",
      "parent": "ELBOExplanation",
      "description": "Different mathematical formulations of ELBO."
    },
    {
      "id": "KL_Divergence",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Measure of difference between two probability distributions."
    },
    {
      "id": "MixtureOfGaussians",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Model for clustering data into multiple Gaussian distributions."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."
    },
    {
      "id": "Parameter Updates",
      "type": "subnode",
      "parent": "M-step",
      "description": "Updates for φ, μ, and Σ based on maximizing expected log-likelihood."
    },
    {
      "id": "Update Rule for μ_j",
      "type": "subnode",
      "parent": "Parameter Updates",
      "description": "Rule to update mean vector μ_j using weighted sum of data points."
    },
    {
      "id": "Update Rule for φ_j",
      "type": "subnode",
      "parent": "Parameter Updates",
      "description": "Rule to update mixture coefficients φ_j based on maximizing expected log-likelihood."
    },
    {
      "id": "ExpectationMaximizationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."
    },
    {
      "id": "MStepUpdateRule",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "Derivation and explanation of the M-step update rule for parameters like \\(\\phi_{j}\\)."
    },
    {
      "id": "LagrangianMethod",
      "type": "subnode",
      "parent": "MStepUpdateRule",
      "description": "Use of Lagrangian to handle constraints in optimization problems."
    },
    {
      "id": "VariationalInference",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Technique for approximating complex probability distributions in Bayesian inference."
    },
    {
      "id": "VariationalAutoEncoder",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Family of algorithms that extend EM to complex models parameterized by neural networks."
    },
    {
      "id": "Variational_Autoencoder",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Family of algorithms extending EM techniques for complex models with neural networks."
    },
    {
      "id": "EM_Algorithms",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Expectation-Maximization algorithm used in probabilistic modeling."
    },
    {
      "id": "Reparametrization_Trick",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Technique allowing sampling from a distribution parameterized by another random variable."
    },
    {
      "id": "High_Dimensional_Latent_Variables",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Non-linear models dealing with high-dimensional continuous latent variables."
    },
    {
      "id": "Gaussian_Mixture_Models",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Statistical model using a mixture of Gaussian distributions to represent subpopulations within an overall population."
    },
    {
      "id": "Variational Inference",
      "type": "major",
      "parent": "Latent Variable Models",
      "description": "Technique for approximating posterior distributions in probabilistic models."
    },
    {
      "id": "ELBO Lower Bound",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Objective function to maximize in variational inference."
    },
    {
      "id": "Optimization Over Q",
      "type": "subnode",
      "parent": "ELBO Lower Bound",
      "description": "Maximizing ELBO over family Q and parameters θ."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption of independent coordinates for discrete latent variables."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Handling continuous latent variables requires additional techniques beyond mean field assumption."
    },
    {
      "id": "Latent Variable Models",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Models involving latent variables for data representation."
    },
    {
      "id": "Encoder-Decoder Architecture",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Architecture used to encode data into latent space and decode back to original space."
    },
    {
      "id": "ELBO Optimization",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Objective function for optimizing variational parameters in inference problems."
    },
    {
      "id": "ELBO_Evaluation",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Evaluating ELBO for fixed Q and θ parameters."
    },
    {
      "id": "Q_i_Formulation",
      "type": "subnode",
      "parent": "ELBO_Evaluation",
      "description": "Formulating Qi as a Gaussian distribution with specific mean and variance functions."
    },
    {
      "id": "Gradient_Ascent_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Optimizing model parameters using gradient ascent over η, φ, ψ, θ."
    },
    {
      "id": "GradientComputation",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Methods for computing gradients in the context of variational inference."
    },
    {
      "id": "ReparameterizationTrick",
      "type": "subnode",
      "parent": "GradientComputation",
      "description": "Technique to compute gradients through stochastic variables by reparameterizing them."
    },
    {
      "id": "Expectation_Maximization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for estimating parameters in statistical models with latent variables."
    },
    {
      "id": "Reparameterization_Trick",
      "type": "subnode",
      "parent": "Expectation_Maximization",
      "description": "Method to compute gradients of expectations involving random variables."
    },
    {
      "id": "Gradient_Ascend_Algorithm",
      "type": "subnode",
      "parent": "Expectation_Maximization",
      "description": "Optimization technique used in machine learning for parameter updates."
    },
    {
      "id": "Principal_Components_Analysis",
      "type": "major",
      "parent": null,
      "description": "Dimensionality reduction method that identifies principal components of data."
    },
    {
      "id": "PCA_Application",
      "type": "subnode",
      "parent": "Principal_Components_Analysis",
      "description": "Application of PCA in analyzing attributes of automobiles such as speed and turn radius."
    },
    {
      "id": "Data Redundancy Detection",
      "type": "major",
      "parent": null,
      "description": "Identifying and removing redundant attributes in data sets."
    },
    {
      "id": "PCA Algorithm Introduction",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Introduction to Principal Component Analysis for detecting intrinsic patterns."
    },
    {
      "id": "Normalization Process",
      "type": "subnode",
      "parent": "PCA Algorithm Introduction",
      "description": "Preprocessing step of normalizing data features before PCA application."
    },
    {
      "id": "Linear Dependence Example",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Example illustrating linearly dependent attributes in car dataset."
    },
    {
      "id": "Pilot Survey Data",
      "type": "subnode",
      "parent": "PCA Algorithm Introduction",
      "description": "Dataset example showing correlation between piloting skill and enjoyment of flying."
    },
    {
      "id": "Data Normalization",
      "type": "major",
      "parent": null,
      "description": "Process of standardizing data features to have zero mean and unit variance."
    },
    {
      "id": "Mean Zeros Out",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Subtracting the mean from each feature centers the data around zero."
    },
    {
      "id": "Standard Deviation Rescaling",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Dividing by standard deviation ensures unit variance for all features."
    },
    {
      "id": "Comparability of Attributes",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Rescaling makes different attributes comparable in scale."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Direction that captures the maximum variance of data projection."
    },
    {
      "id": "Projection Maximizing Variance",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Finding unit vector u to maximize variance in projected data."
    },
    {
      "id": "PrincipalComponentAnalysis",
      "type": "major",
      "parent": null,
      "description": "Technique to reduce dimensionality by finding principal eigenvectors."
    },
    {
      "id": "VarianceMaximization",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Objective is to maximize variance of projections onto unit vector u."
    },
    {
      "id": "ProjectionOntoUnitVector",
      "type": "subnode",
      "parent": "VarianceMaximization",
      "description": "Length of projection given by x^Tu, where x is data point and u is unit vector."
    },
    {
      "id": "EmpiricalCovarianceMatrix",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Matrix representing covariance of dataset with zero mean."
    },
    {
      "id": "TopKEigenvectors",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Choosing top k eigenvectors for k-dimensional subspace approximation."
    },
    {
      "id": "OrthogonalBasis",
      "type": "subnode",
      "parent": "TopKEigenvectors",
      "description": "Eigenvectors form an orthogonal basis for the data in reduced dimensions."
    },
    {
      "id": "PCA",
      "type": "major",
      "parent": "Dimensionality_Reduction",
      "description": "Principal Component Analysis for data visualization and noise reduction."
    },
    {
      "id": "Eigenvectors of Sigma",
      "type": "subnode",
      "parent": "PCA",
      "description": "Top k eigenvectors form an orthogonal basis."
    },
    {
      "id": "Orthogonal Basis",
      "type": "subnode",
      "parent": "Eigenvectors of Sigma",
      "description": "New basis for data representation."
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces high-dimensional data to lower dimensions."
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "Eigenvectors of Sigma",
      "description": "First k eigenvectors are principal components."
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Minimizes error from projecting data onto subspace."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Visualize high-dimensional data in 2D or 3D."
    },
    {
      "id": "Compression",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Reduces dimension for efficient storage and processing."
    },
    {
      "id": "Data Preprocessing",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduce dataset dimensions before further analysis."
    },
    {
      "id": "Machine_Learning_Applications",
      "type": "major",
      "parent": null,
      "description": "Applications of machine learning techniques in data analysis and pattern recognition."
    },
    {
      "id": "Dimensionality_Reduction",
      "type": "subnode",
      "parent": "Machine_Learning_Applications",
      "description": "Techniques to reduce the number of random variables under consideration."
    },
    {
      "id": "Data_Visualization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Visualizing similarities between cars using PCA."
    },
    {
      "id": "Computational_Benefits",
      "type": "subnode",
      "parent": "Dimensionality_Reduction",
      "description": "Reducing computational complexity and avoiding overfitting by dimension reduction."
    },
    {
      "id": "Noise_Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Estimating intrinsic features from noisy data using PCA."
    },
    {
      "id": "Eigenfaces_Method",
      "type": "subnode",
      "parent": "Noise_Reduction",
      "description": "Using PCA to reduce dimensionality of face images for better recognition."
    },
    {
      "id": "ICA",
      "type": "major",
      "parent": null,
      "description": "Independent Component Analysis for finding a new basis in data representation."
    },
    {
      "id": "Cocktail_Party_Problem",
      "type": "subnode",
      "parent": "ICA",
      "description": "Motivating example of ICA with simultaneous speakers and microphones."
    },
    {
      "id": "Independent Component Analysis (ICA)",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Technique for separating a multivariate signal into independent non-Gaussian components."
    },
    {
      "id": "Cocktail Party Problem",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Example problem where ICA is applied to separate overlapping audio signals."
    },
    {
      "id": "Mixing Matrix A",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix representing the mixing process of independent sources into observed data."
    },
    {
      "id": "Unmixing Matrix W",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Inverse matrix used to recover original signals from mixed observations."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discussion on the limitations and uncertainties in recovering unmixing matrix W without prior knowledge."
    },
    {
      "id": "Permutation Matrix P",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Describes permutation matrices and their role in ICA ambiguity."
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Explains the inability to recover correct scaling factors for sources."
    },
    {
      "id": "Volume Change",
      "type": "subnode",
      "parent": "Scaling Ambiguity",
      "description": "Discusses how scaling affects volume but not identity in speech signals."
    },
    {
      "id": "ICA_Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discusses ambiguities in Independent Component Analysis (ICA)"
    },
    {
      "id": "ScalingFactor",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Explains the effect of scaling a signal by a positive factor"
    },
    {
      "id": "SignChangeIrrelevance",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Discusses how sign changes in signals do not affect the outcome"
    },
    {
      "id": "NonGaussianSources",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "States that non-Gaussian sources resolve ambiguities in ICA"
    },
    {
      "id": "GaussianDataExample",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Provides an example with Gaussian data to illustrate ambiguity"
    },
    {
      "id": "MixingMatrix",
      "type": "subnode",
      "parent": "GaussianDataExample",
      "description": "Describes the mixing matrix and its effect on observed signals"
    },
    {
      "id": "RotationAmbiguity",
      "type": "subnode",
      "parent": "GaussianDataExample",
      "description": "Explains how rotation of the mixing matrix does not change the distribution of mixed data"
    },
    {
      "id": "Mixing Matrix",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix used in ICA to mix original sources into observed data."
    },
    {
      "id": "Rotational Symmetry",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Property of multivariate normal distribution affecting ICA on Gaussian data."
    },
    {
      "id": "Non-Gaussian Data Recovery",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Recovery of independent sources from non-Gaussian mixed data in ICA."
    },
    {
      "id": "Linear Transformations and Densities",
      "type": "major",
      "parent": null,
      "description": "Discussion on how linear transformations affect probability densities."
    },
    {
      "id": "Effect of Linear Transformation",
      "type": "subnode",
      "parent": "Linear Transformations and Densities",
      "description": "Impact of transforming random variables through a matrix A on their density functions."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of density functions under linear transformations."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Illustration with a 1-dimensional example involving scaling and uniform distribution."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension to vector-valued distributions using invertible matrices."
    },
    {
      "id": "Volume Calculation",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Explanation of volume calculation for transformed sets in linear algebra."
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation and interpretation of an Independent Component Analysis algorithm."
    },
    {
      "id": "ICAIndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Assumption in Independent Component Analysis that sources are independent."
    },
    {
      "id": "JointDistributionModeling",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Modeling the joint distribution of data as a product of marginals."
    },
    {
      "id": "SourceDensitySpecification",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Specifying density functions for individual sources in ICA."
    },
    {
      "id": "CDFandPDFRelationship",
      "type": "subnode",
      "parent": "SourceDensitySpecification",
      "description": "Relation between cumulative distribution function and probability density function."
    },
    {
      "id": "SigmoidFunctionChoice",
      "type": "subnode",
      "parent": "SourceDensitySpecification",
      "description": "Choosing sigmoid function as a default for source densities in ICA."
    },
    {
      "id": "DataPreprocessing",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of preparing data for analysis by normalizing or standardizing it."
    },
    {
      "id": "ModelParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Variables and matrices that define the structure of a model, such as W."
    },
    {
      "id": "LogLikelihood",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Measure used in statistics to estimate the goodness of fit for a set of data given a model."
    },
    {
      "id": "GradientAscent",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Optimization algorithm that maximizes an objective function iteratively."
    },
    {
      "id": "ConvergenceCriteria",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Conditions under which an iterative algorithm stops improving and reaches an optimal solution."
    },
    {
      "id": "SourceRecovery",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of recovering the original sources from mixed signals using learned parameters."
    },
    {
      "id": "IndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Statistical assumption that training examples are independent, affecting likelihood computation and model performance."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Training models with unlabeled data by predicting parts of the input as labels."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Models that are trained on large datasets to learn general representations."
    },
    {
      "id": "Pretraining and Adaptation",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Process of training a model on large unlabeled datasets followed by adaptation for specific tasks."
    },
    {
      "id": "Pretraining",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Phase where a model learns general representations from unlabeled data."
    },
    {
      "id": "Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Customizing pretrained models for specific downstream tasks with labeled data."
    },
    {
      "id": "Unlabeled_Data",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Data used in pretraining phase without labels."
    },
    {
      "id": "Labeled_Data",
      "type": "subnode",
      "parent": "Adaptation",
      "description": "Data used for fine-tuning models to specific tasks with labels."
    },
    {
      "id": "Representations",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Learned features that capture intrinsic semantic structure of data."
    },
    {
      "id": "Self_Supervised_Loss",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Loss function used during pretraining, supervised by the data itself."
    },
    {
      "id": "SGD_ADAM_Optimizers",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Optimization algorithms used to minimize pretraining loss."
    },
    {
      "id": "Machine Learning Adaptation Methods",
      "type": "major",
      "parent": null,
      "description": "Overview of methods used for adapting machine learning models to new tasks."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Learning scenario where no labeled data is available for the target task."
    },
    {
      "id": "Few-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Scenario with a very small amount of labeled data, typically 1-50 examples."
    },
    {
      "id": "Linear Probe Approach",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Uses a linear head on top of pretrained model representations to predict labels."
    },
    {
      "id": "Finetuning Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Adjusts both the linear layer and the pretrained model parameters for better task performance."
    },
    {
      "id": "Downstream Dataset",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Dataset specific to a particular downstream task, often labeled."
    },
    {
      "id": "Finetuning Pretrained Models",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Process of optimizing both the linear head and pretrained model parameters."
    },
    {
      "id": "Pretraining in Computer Vision",
      "type": "major",
      "parent": null,
      "description": "Introduction to pretraining methods used in computer vision tasks."
    },
    {
      "id": "Supervised Pretraining",
      "type": "subnode",
      "parent": "Pretraining in Computer Vision",
      "description": "Training with a large labeled dataset and removing the last layer of the neural network."
    },
    {
      "id": "Contrastive Learning",
      "type": "subnode",
      "parent": "Pretraining in Computer Vision",
      "description": "Self-supervised method using unlabeled data to learn good representations."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning method using unlabeled data to train models."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Function mapping images to representations based on similarity."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Augmented versions of the same original image, considered semantically similar."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Randomly selected augmented images from different originals, generally not related."
    },
    {
      "id": "Supervised Contrastive Algorithms",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Algorithms using labeled data for better pretraining."
    },
    {
      "id": "Data Augmentation",
      "type": "major",
      "parent": null,
      "description": "Technique to generate variations of images for training purposes."
    },
    {
      "id": "Loss Function Design",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Designing loss functions to ensure positive pairs are close and negative pairs distant."
    },
    {
      "id": "ContrastiveLearning",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Technique where models are trained to distinguish similar and dissimilar pairs of inputs."
    },
    {
      "id": "SIMCLRAlgorithm",
      "type": "subnode",
      "parent": "ContrastiveLearning",
      "description": "Specific implementation of contrastive learning introduced by Chen et al. in 2020."
    },
    {
      "id": "RandomAugmentation",
      "type": "subnode",
      "parent": "SIMCLRAlgorithm",
      "description": "Process of applying random transformations to input data for robust learning."
    },
    {
      "id": "NegativePair",
      "type": "subnode",
      "parent": "ContrastiveLearning",
      "description": "Term used in literature to describe pairs that should be far apart in the learned representation space."
    },
    {
      "id": "Loss_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Functions that measure the performance of a model on training data."
    },
    {
      "id": "Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Models trained on large datasets to capture language patterns."
    },
    {
      "id": "Language_Modeling",
      "type": "subnode",
      "parent": "Pretrained_Models",
      "description": "Probabilistic models predicting the likelihood of sequences of words."
    },
    {
      "id": "ConditionalProbabilityModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Modeling conditional probabilities in sequence prediction tasks."
    },
    {
      "id": "ParameterizedFunction",
      "type": "subnode",
      "parent": "ConditionalProbabilityModeling",
      "description": "Using parameterized functions to model conditional probabilities."
    },
    {
      "id": "EmbeddingsAndRepresentations",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introduction of embeddings for discrete variables in machine learning models."
    },
    {
      "id": "TransformerModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Overview of the Transformer model and its input-output interface."
    },
    {
      "id": "EmbeddingMatrix",
      "type": "subnode",
      "parent": "EmbeddingsAndRepresentations",
      "description": "Description of embedding matrix used for word representations."
    },
    {
      "id": "Transformer_Model",
      "type": "major",
      "parent": null,
      "description": "A model that maps inputs to outputs, denoted as f_θ."
    },
    {
      "id": "Conditional_Probability",
      "type": "subnode",
      "parent": "Transformer_Model",
      "description": "Probability of the next input given previous inputs."
    },
    {
      "id": "Softmax_Function",
      "type": "subnode",
      "parent": "Conditional_Probability",
      "description": "Converts logits to probabilities for each possible output."
    },
    {
      "id": "Training_Transformer",
      "type": "subnode",
      "parent": "Transformer_Model",
      "description": "Process of minimizing negative log-likelihood using cross-entropy loss."
    },
    {
      "id": "Autoregressive_Decoding",
      "type": "major",
      "parent": null,
      "description": "Generating text sequentially based on conditional probabilities from the Transformer model."
    },
    {
      "id": "Machine Learning Techniques",
      "type": "major",
      "parent": null,
      "description": "Overview of various machine learning techniques and concepts."
    },
    {
      "id": "Temperature Parameter in Softmax",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Explains the role of temperature parameter in softmax function for text generation."
    },
    {
      "id": "Text Generation Models",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Models used for generating text based on learned probabilities."
    },
    {
      "id": "Zero-shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Adapting pre-trained models to new tasks without task-specific training data."
    },
    {
      "id": "In-context Learning",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Learning in the context of specific examples or prompts for better adaptation."
    },
    {
      "id": "Machine Learning Adaptation Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for adapting machine learning models to new tasks."
    },
    {
      "id": "Zero-shot Adaptation",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Techniques",
      "description": "Adapting a model without any task-specific data."
    },
    {
      "id": "Language Model Utilization",
      "type": "subnode",
      "parent": "Zero-shot Adaptation",
      "description": "Methods for using language models to solve tasks without task-specific data."
    },
    {
      "id": "Prompt Construction",
      "type": "subnode",
      "parent": "In-context Learning",
      "description": "Creating prompts that include few labeled examples and a test example."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of various machine learning models and their applications."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Learning through interaction with an environment based on rewards and penalties."
    },
    {
      "id": "Supervised Learning",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A type of machine learning where the model learns from labeled data to predict outcomes for new data."
    },
    {
      "id": "Sequential Decision Making",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Making decisions in sequence based on past actions and rewards."
    },
    {
      "id": "Reinforcement_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Type of machine learning where an agent learns to behave in the world by performing actions and receiving rewards."
    },
    {
      "id": "Markov_Decision_Processes",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Formal framework for modeling decision-making situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "A framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Random transition from one state to another based on action taken."
    },
    {
      "id": "Action Selection",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Selects actions for each sampled state and estimates future rewards."
    },
    {
      "id": "Total Payoff",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Cumulative reward over time, discounted by factor gamma."
    },
    {
      "id": "Discount Factor",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Factor that discounts future rewards to reflect their current value."
    },
    {
      "id": "Reinforcement Learning Goal",
      "type": "major",
      "parent": null,
      "description": "Maximizing the expected total discounted reward over time."
    },
    {
      "id": "Policy Function",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Function mapping states to actions to be taken in those states."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Expected total reward starting from a given state and following the policy."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Describes the process of executing a policy in states and actions."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Set of equations used to solve for the value function V^π(s) in a finite-state MDP."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering state s."
    },
    {
      "id": "Future Discounted Rewards",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Expected sum of discounted rewards after the first step in an MDP."
    },
    {
      "id": "V^π(s)",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Expected value of state s under policy π."
    },
    {
      "id": "Optimal Value Function V*(s)",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Best possible expected sum of discounted rewards for any policy."
    },
    {
      "id": "Bellman's Equation",
      "type": "major",
      "parent": null,
      "description": "Equation that defines the value function in terms of itself and future states."
    },
    {
      "id": "Optimal Policy π*(s)",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Policy that maximizes the expected sum of discounted rewards for all states."
    },
    {
      "id": "Bellman's Equation for Optimal Value Function",
      "type": "subnode",
      "parent": "Bellman's Equation",
      "description": "Equation defining V*(s) in terms of immediate reward and future state values."
    },
    {
      "id": "Policy π(s)",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Mapping from states to actions that maximizes the expected sum of discounted rewards."
    },
    {
      "id": "Optimal Policy in MDPs",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Discusses the concept of an optimal policy for all states in a Markov Decision Process (MDP)."
    },
    {
      "id": "Value Iteration and Policy Iteration",
      "type": "major",
      "parent": "Discretization of State Space",
      "description": "Algorithms used to find optimal policies in a discretized MDP."
    },
    {
      "id": "Finite-State MDP Assumptions",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "Details the assumptions made about state and action spaces in MDPs."
    },
    {
      "id": "Value Iteration Algorithm",
      "type": "subnode",
      "parent": "Value Iteration and Policy Iteration",
      "description": "Describes the value iteration algorithm for solving finite-state MDPs."
    },
    {
      "id": "Synchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Explains synchronous updates in the context of value iteration."
    },
    {
      "id": "Asynchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Describes asynchronous updates for value iteration."
    },
    {
      "id": "Convergence to Optimal Value Function",
      "type": "subnode",
      "parent": "Value Iteration Algorithm",
      "description": "Discusses the convergence of value iteration to the optimal value function."
    },
    {
      "id": "MachineLearningAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Standard algorithms for solving MDPs in machine learning."
    },
    {
      "id": "ValueIteration",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithm that updates state values to find optimal policy."
    },
    {
      "id": "PolicyIteration",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithm that alternates between policy evaluation and improvement."
    },
    {
      "id": "BellmanEquations",
      "type": "subnode",
      "parent": "PolicyIteration",
      "description": "Set of equations used for policy evaluation in MDPs."
    },
    {
      "id": "GreedyPolicy",
      "type": "subnode",
      "parent": "PolicyIteration",
      "description": "Policy derived from value function that maximizes immediate reward."
    },
    {
      "id": "Value_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm to find the optimal policy in reinforcement learning through iterative updates of value functions."
    },
    {
      "id": "Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm for finding optimal policies by iteratively improving policy evaluations."
    },
    {
      "id": "Learning_Model_for_MDP",
      "type": "major",
      "parent": null,
      "description": "Process of estimating MDP parameters from observed data in practical scenarios."
    },
    {
      "id": "State_Transition_Probabilities",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Estimating probabilities of transitioning between states given actions."
    },
    {
      "id": "Inverted_Pendulum_Problem",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Example problem demonstrating the need for learning MDP parameters from experience."
    },
    {
      "id": "MDP_Model_Learning",
      "type": "major",
      "parent": null,
      "description": "Learning the model of an MDP with unknown state transitions and rewards."
    },
    {
      "id": "Expected_Immediate_Reward",
      "type": "subnode",
      "parent": "MDP_Model_Learning",
      "description": "Estimating the expected reward for being in a given state."
    },
    {
      "id": "Optimization_Technique",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Technique to speed up value iteration by initializing with previous solution."
    },
    {
      "id": "Continuous_State_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithms for MDPs with infinite state spaces, such as those involving continuous variables."
    },
    {
      "id": "Discretization_Method",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Technique to convert continuous-state MDP problems into discrete-state ones for easier solving."
    },
    {
      "id": "Discretization of State Space",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Process of converting continuous state space into discrete states for computational feasibility."
    },
    {
      "id": "Piecewise Constant Representation",
      "type": "subnode",
      "parent": "Discretization of State Space",
      "description": "A representation where the value function is constant within each discrete state interval."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Problem arising when analyzing data in high-dimensional spaces due to exponentially increasing volume."
    },
    {
      "id": "Continuous-State MDPs",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Markov Decision Processes with continuous state spaces."
    },
    {
      "id": "Discretization Method",
      "type": "subnode",
      "parent": "Continuous-State MDPs",
      "description": "Approach to handle continuous states by dividing the space into discrete bins."
    },
    {
      "id": "Model or Simulator",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Black-box that simulates next-state transitions based on current state and action."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Different ways to create a model for an MDP."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software to simulate system behavior."
    },
    {
      "id": "Open Dynamics Engine (ODE)",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Free/open-source physics simulator used in RL research."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Inferring model parameters through data collected during MDP trials."
    },
    {
      "id": "MDP Trials",
      "type": "subnode",
      "parent": "Learning from Data",
      "description": "Executing actions in an MDP to collect state sequences for learning."
    },
    {
      "id": "LinearModelPrediction",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Predicting future states using a linear model based on current state and action."
    },
    {
      "id": "NonlinearFeatureMappings",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Using non-linear feature mappings for better prediction accuracy."
    },
    {
      "id": "DeterministicModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A model where the next state is exactly determined by current state and action."
    },
    {
      "id": "StochasticModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A probabilistic model incorporating noise for more realistic predictions."
    },
    {
      "id": "LossFunctions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Different loss functions used to estimate parameters in the learning process."
    },
    {
      "id": "Nonlinear_Feature_Mappings",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Feature mappings for states and actions in models."
    },
    {
      "id": "Fitted_Value_Iteration",
      "type": "major",
      "parent": "Expectation_Computation",
      "description": "Algorithm for approximating value function in continuous state MDPs."
    },
    {
      "id": "Continuous_State_Space",
      "type": "subnode",
      "parent": "Fitted_Value_Iteration",
      "description": "Assumption of a large and continuous state space with discrete actions."
    },
    {
      "id": "Value_Iteration_Update",
      "type": "subnode",
      "parent": "Fitted_Value_Iteration",
      "description": "Update rule for value iteration in continuous states using integrals instead of summations."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm that uses supervised learning to approximate value functions in reinforcement learning."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Linear regression used for approximating the relationship between state features and values."
    },
    {
      "id": "Feature Mapping (phi)",
      "type": "subnode",
      "parent": "Supervised Learning Algorithm",
      "description": "Maps state features to a higher-dimensional space for better approximation."
    },
    {
      "id": "State Sampling",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Randomly samples states from the state space to approximate value functions."
    },
    {
      "id": "Reward Estimation",
      "type": "subnode",
      "parent": "Action Selection",
      "description": "Estimates immediate reward plus discounted future value for each action in a state."
    },
    {
      "id": "Linear Regression Update",
      "type": "subnode",
      "parent": "Supervised Learning Algorithm",
      "description": "Updates the model parameters to minimize the error between predicted and actual values."
    },
    {
      "id": "SupervisedLearning",
      "type": "major",
      "parent": null,
      "description": "Machine learning technique using labeled data to predict outcomes."
    },
    {
      "id": "FittedValueIteration",
      "type": "subnode",
      "parent": "SupervisedLearning",
      "description": "Technique using regression algorithms to approximate value iteration in reinforcement learning."
    },
    {
      "id": "ConvergenceIssues",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Discussion on convergence properties and practical considerations of fitted value iteration."
    },
    {
      "id": "DeterministicSimulator",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Simplification when using a deterministic simulator in the algorithm."
    },
    {
      "id": "PolicyDefinition",
      "type": "subnode",
      "parent": "SupervisedLearning",
      "description": "Defining policy based on approximated value function V(s)."
    },
    {
      "id": "Value_Function_Approximation",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Techniques for estimating the value function in large state spaces."
    },
    {
      "id": "Expectation_Computation",
      "type": "subnode",
      "parent": "Value_Function_Approximation",
      "description": "Process of computing or approximating expectations in reinforcement learning."
    },
    {
      "id": "Deterministic_Simulator",
      "type": "subnode",
      "parent": "Expectation_Computation",
      "description": "Case where the simulator does not include random noise."
    },
    {
      "id": "Gaussian_Noise_Model",
      "type": "subnode",
      "parent": "Expectation_Computation",
      "description": "Model with deterministic function and Gaussian noise."
    },
    {
      "id": "Value_Evaluation_Procedure",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Procedure to evaluate the value function in policy iteration."
    },
    {
      "id": "Algorithm Design",
      "type": "major",
      "parent": null,
      "description": "Designing algorithms for reinforcement learning problems."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Algorithm Design",
      "description": "An iterative method to find the optimal policy in a Markov Decision Process (MDP)."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Algorithm Design",
      "description": "A dynamic programming algorithm used for solving MDPs by iteratively computing values of states."
    },
    {
      "id": "Procedure VE",
      "type": "subnode",
      "parent": "Algorithm Design",
      "description": "Evaluates the value function given a policy and number of iterations k."
    },
    {
      "id": "k Parameter",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Hyperparameter controlling the number of iterations in Procedure VE."
    },
    {
      "id": "Option 1 Initialization",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Initializes value function V(s) to zero for all states s."
    },
    {
      "id": "Option 2 Initialization",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Uses the current value function from the main algorithm as initialization."
    },
    {
      "id": "Value Function Update",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Updates V(s) based on Bellman equation for each state s."
    },
    {
      "id": "Policy Improvement",
      "type": "subnode",
      "parent": "Algorithm Design",
      "description": "Improves the policy by choosing actions that maximize expected rewards based on current value function."
    },
    {
      "id": "Chapter_15_Summary",
      "type": "major",
      "parent": null,
      "description": "Summary of Chapter 15 on MDPs and value/policy iteration."
    },
    {
      "id": "Optimal_Balance_Update_Frequencies",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Discussion on optimal update frequencies for iterative procedures."
    },
    {
      "id": "Policy_Iteration_Speeding_Up",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Explanation of how policy iteration can speed up computation using linear system solvers."
    },
    {
      "id": "Value_Iteration_Preferrable_Conditions",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Conditions under which value iteration is more preferable than policy iteration."
    },
    {
      "id": "Chapter_16_LQR_DDP_and_LQG",
      "type": "major",
      "parent": null,
      "description": "Introduction to Chapter 16 on LQR, DDP, and LQG concepts."
    },
    {
      "id": "Finite_Horizon_MDPs",
      "type": "subnode",
      "parent": "Chapter_16_LQR_DDP_and_LQG",
      "description": "Overview of finite-horizon MDPs and their relation to optimal Bellman equation."
    },
    {
      "id": "Optimal_Bellman_Equation",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Definition of the optimal value function via the optimal Bellman equation."
    },
    {
      "id": "Recovering_Optimal_Policy",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Explanation on how to recover the optimal policy from the optimal value function."
    },
    {
      "id": "General_Setting_Equations",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Introduction of equations that apply in both discrete and continuous settings using expectation notation."
    },
    {
      "id": "Expectation Rewriting",
      "type": "major",
      "parent": null,
      "description": "Rewriting expectations in finite and continuous cases."
    },
    {
      "id": "Rewards Dependency",
      "type": "subnode",
      "parent": "Expectation Rewriting",
      "description": "Rewards depend on both states and actions."
    },
    {
      "id": "Optimal Action Computation",
      "type": "subnode",
      "parent": "Rewards Dependency",
      "description": "Computing optimal action considering state-action rewards and expected future value."
    },
    {
      "id": "Finite Horizon MDP",
      "type": "major",
      "parent": null,
      "description": "MDP with a defined time horizon T."
    },
    {
      "id": "Time Horizon Definition",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "Definition of the finite time horizon in an MDP."
    },
    {
      "id": "Payoff Calculation",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "Calculation of payoff without discount factor for a finite number of steps."
    },
    {
      "id": "Discount Factor Relevance",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "Explanation of why the discount factor is not necessary in a finite horizon setting."
    },
    {
      "id": "Non-Stationary Optimal Policy",
      "type": "major",
      "parent": null,
      "description": "Optimal policy changes over time in a finite horizon MDP."
    },
    {
      "id": "Markov_Decision_Processes_Finite_Horizon",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Description and dynamics of finite-horizon MDPs in ML context."
    },
    {
      "id": "Policy_Dynamics",
      "type": "subnode",
      "parent": "Markov_Decision_Processes_Finite_Horizon",
      "description": "Explanation of how policies change over time in a finite horizon setting."
    },
    {
      "id": "Optimal_Policy_Non_Stationarity",
      "type": "subnode",
      "parent": "Policy_Dynamics",
      "description": "Discussion on why optimal policies can be non-stationary due to limited actions and changing environment conditions."
    },
    {
      "id": "Time_Dependent_Dynamics",
      "type": "subnode",
      "parent": "Markov_Decision_Processes_Finite_Horizon",
      "description": "Explanation of how transition probabilities and rewards may change over time in MDPs."
    },
    {
      "id": "Value_Functions",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Function that estimates the long-term reward starting from a given state under a policy."
    },
    {
      "id": "Policy_Pi",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "Strategy defining how an agent selects actions in different states."
    },
    {
      "id": "Bellman_Equation",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Equation that decomposes the value of a state into immediate reward and discounted future rewards."
    },
    {
      "id": "Dynamic_Programming",
      "type": "subnode",
      "parent": "Bellman_Equation",
      "description": "Method for solving complex problems by breaking them down into simpler subproblems."
    },
    {
      "id": "Bellman Operator",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Operator used to update value functions in the context of dynamic programming and reinforcement learning."
    },
    {
      "id": "Geometric Convergence",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Property indicating that the error decreases geometrically with each iteration."
    },
    {
      "id": "Continuous Setting",
      "type": "subnode",
      "parent": "LQR",
      "description": "Model assumes state and action spaces as continuous real vectors."
    },
    {
      "id": "Linear Transitions",
      "type": "subnode",
      "parent": "LQR",
      "description": "Assumption that the next state is a linear function of current state and action with additive Gaussian noise."
    },
    {
      "id": "Quadratic Rewards",
      "type": "subnode",
      "parent": "LQR",
      "description": "Rewards are defined as quadratic functions of states and actions, promoting smooth trajectories towards an optimal state."
    },
    {
      "id": "LQRModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumptions made in the Linear Quadratic Regulator (LQR) model for optimal control problems."
    },
    {
      "id": "LQRAlgorithmSteps",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two steps of the LQR algorithm: estimation and policy derivation."
    },
    {
      "id": "Step1Estimation",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "First step involves estimating model parameters using linear regression and Gaussian Discriminant Analysis."
    },
    {
      "id": "Step2PolicyDerivation",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "Second step derives the optimal policy assuming known or estimated model parameters."
    },
    {
      "id": "DynamicProgrammingApplication",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Use of dynamic programming to compute optimal value functions in LQR problems."
    },
    {
      "id": "Optimal Value Function",
      "type": "major",
      "parent": null,
      "description": "Describes the optimal value function in a quadratic form."
    },
    {
      "id": "Quadratic Assumption",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Assumes that the value functions are quadratic for simplification."
    },
    {
      "id": "Bellman Equation",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Expresses the relationship between current and future values in a dynamic system."
    },
    {
      "id": "Optimal Policy",
      "type": "major",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Policy derived from solving the LQR problem, linear in state variables."
    },
    {
      "id": "Linear Optimal Policy",
      "type": "subnode",
      "parent": "Optimal Policy",
      "description": "Shows that the optimal policy is a linear function of state under certain conditions."
    },
    {
      "id": "Linear Quadratic Regulator (LQR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimal control algorithm for linear systems with quadratic cost functions."
    },
    {
      "id": "Discrete Ricatti Equations",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Set of equations used to solve the LQR problem iteratively."
    },
    {
      "id": "Noise Independence",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Optimal policy does not depend on system noise."
    },
    {
      "id": "Algorithm Steps",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Steps to solve the LQR problem iteratively."
    },
    {
      "id": "Non-linear Dynamics Reduction",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Reduction of non-linear dynamics problems to linear systems for LQR application."
    },
    {
      "id": "Inverted Pendulum Example",
      "type": "subnode",
      "parent": "Dynamics and Control Systems",
      "description": "Example of a dynamic system used to illustrate control theory principles."
    },
    {
      "id": "State Transition Function F",
      "type": "subnode",
      "parent": "Inverted Pendulum Example",
      "description": "Function describing the transition between states in the inverted pendulum model."
    },
    {
      "id": "Taylor Expansion",
      "type": "subnode",
      "parent": "Linearization of Dynamics",
      "description": "Mathematical technique used to approximate functions using polynomial series."
    },
    {
      "id": "Machine_Learning_Methods",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning techniques and methods."
    },
    {
      "id": "Linear_Regression",
      "type": "subnode",
      "parent": "Machine_Learning_Methods",
      "description": "Technique for modeling the relationship between variables using a linear model."
    },
    {
      "id": "Differential_Dynamic_Programming_(DDP)",
      "type": "major",
      "parent": null,
      "description": "Method used to solve trajectory optimization problems in dynamic systems."
    },
    {
      "id": "Nominal_Trajectory",
      "type": "subnode",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Initial approximation of the desired trajectory using a simple controller."
    },
    {
      "id": "Linearization_of_Dynamics",
      "type": "subnode",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Process of approximating system dynamics with linear equations around each point in the trajectory."
    },
    {
      "id": "Rewriting_Dynamics",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Expressing the dynamics using matrices A and B for state and action transitions."
    },
    {
      "id": "Reward_Function_Linearization",
      "type": "subnode",
      "parent": "Differential_Dynamic_Programming_(DDP)",
      "description": "Approximating reward functions with Taylor expansions around trajectory points."
    },
    {
      "id": "Optimization_Frameworks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Frameworks used for optimization in machine learning problems."
    },
    {
      "id": "Linear_Quadratic_Regulator_(LQR)",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "A method to find optimal control policies in linear systems with quadratic cost functions."
    },
    {
      "id": "Hessian_Matrix",
      "type": "subnode",
      "parent": "Linear_Quadratic_Regulator_(LQR)",
      "description": "Matrix of second-order partial derivatives used for optimization problems."
    },
    {
      "id": "State_Estimation",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Techniques to estimate the state of a system when full state information is not available."
    },
    {
      "id": "Linear_Quadratic_Gaussian_(LQG)",
      "type": "major",
      "parent": null,
      "description": "Extension of LQR for systems with stochastic disturbances and partial observations."
    },
    {
      "id": "Partially_Observable_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "An extension of MDPs where the state is not fully observable, only through observations."
    },
    {
      "id": "Observation_Model",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Model describing how observations are generated from states in a POMDP."
    },
    {
      "id": "Belief_State",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Distribution over possible states based on all past and current observations."
    },
    {
      "id": "LQR_Extension",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Extending Linear Quadratic Regulator to handle partially observable systems."
    },
    {
      "id": "Kalman_Filter",
      "type": "subnode",
      "parent": "Belief_State",
      "description": "Algorithm used for estimating the state of a system over time in the presence of noise."
    },
    {
      "id": "Kalman Filter",
      "type": "major",
      "parent": null,
      "description": "Algorithm for efficient computation of mean and variance over time"
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step to establish the distribution of state given observations"
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Compute next state prediction based on current state and previous observations"
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Refine the state estimate after receiving new observation data"
    },
    {
      "id": "LQR Algorithm",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulator for control problems independent of noise"
    },
    {
      "id": "Computational Efficiency",
      "type": "major",
      "parent": null,
      "description": "Efficient computation methods compared to direct matrix inversion"
    },
    {
      "id": "Kalman Filter Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of the Kalman filter process and its steps."
    },
    {
      "id": "Belief State Update",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "Combining predict and update steps to refine belief states over time."
    },
    {
      "id": "Gaussian Distribution in Predict Step",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "Distribution of the next state given current observations is Gaussian."
    },
    {
      "id": "Kalman Gain Calculation",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Calculation of Kalman gain using previous distribution and new observation."
    },
    {
      "id": "Backward Pass (LQR Updates)",
      "type": "subnode",
      "parent": "Kalman Filter Overview",
      "description": "Updating quantities based on previous computations to refine estimates."
    },
    {
      "id": "Chapter 17 Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Model-free algorithm for reinforcement learning without value functions."
    },
    {
      "id": "Finite Horizon Case",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Assumption of a fixed trajectory length T in REINFORCE algorithm."
    },
    {
      "id": "Randomized Policy",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Policy that outputs actions probabilistically based on state and parameters theta."
    },
    {
      "id": "Transition Probabilities Sampling",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Sampling from transition probabilities without needing their analytical form."
    },
    {
      "id": "Reward Function Querying",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Querying rewards based on state and action pairs, no need for function's analytical form."
    },
    {
      "id": "Expected Total Payoff Optimization",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Optimizing the expected total payoff over policy parameters theta."
    },
    {
      "id": "Policy_Gradient_Methods",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for optimizing policies in reinforcement learning."
    },
    {
      "id": "Gradient_Ascend_Optimization",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Optimizing policy parameters using gradient ascent to maximize expected reward."
    },
    {
      "id": "Reward_Function_Estimation",
      "type": "subnode",
      "parent": "Gradient_Ascend_Optimization",
      "description": "Estimating gradients of the expected reward without knowing the exact form of the reward function."
    },
    {
      "id": "Reparametrization_Technique",
      "type": "subnode",
      "parent": "Reward_Function_Estimation",
      "description": "Technique used in VAEs to compute gradients through a stochastic variable."
    },
    {
      "id": "REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Gradient_Ascend_Optimization",
      "description": "Algorithm for estimating policy gradients without direct access to the reward function's gradient."
    },
    {
      "id": "Policy_Gradient_Theorem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation of the policy gradient theorem using expectation and differentiation."
    },
    {
      "id": "Expectation_Equations",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Mathematical formulation of expected values in reinforcement learning contexts."
    },
    {
      "id": "Sample_Based_Estimator",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Introduction to sample-based estimation for policy gradients using empirical samples."
    },
    {
      "id": "Log_Probability_Derivation",
      "type": "subnode",
      "parent": "Sample_Based_Estimator",
      "description": "Deriving the logarithmic probability of trajectories under a given policy."
    },
    {
      "id": "Policy_Gradient_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundation for policy gradient methods in reinforcement learning."
    },
    {
      "id": "Log_Probability_Gradient",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Derivation of the gradient of log probability with respect to parameters θ."
    },
    {
      "id": "Vanilla_REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Basic algorithm for policy optimization using estimated gradients from sample trajectories."
    },
    {
      "id": "Trajectory_Probability_Change",
      "type": "subnode",
      "parent": "Log_Probability_Gradient",
      "description": "Explanation of how changes in θ affect the likelihood of specific trajectories τ."
    },
    {
      "id": "Empirical_Estimation",
      "type": "subnode",
      "parent": "Vanilla_REINFORCE_Algorithm",
      "description": "Method for estimating gradients using empirical sample trajectories without needing exact transition probabilities."
    },
    {
      "id": "Trajectory_Probability",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Probability of a trajectory given a policy and parameters."
    },
    {
      "id": "Reward_Function",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Function that assigns rewards to trajectories based on their outcomes."
    },
    {
      "id": "Expectation_Zero_Property",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Property stating the expectation of a gradient term is zero under certain conditions."
    },
    {
      "id": "Simplified_Formula",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Simplification of the policy gradient formula based on the zero property."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Techniques that optimize policies directly by taking gradients of the performance measure w.r.t. policy parameters."
    },
    {
      "id": "Expected Value Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Estimating expected values under a given policy in reinforcement learning contexts."
    },
    {
      "id": "Law of Total Expectation",
      "type": "subnode",
      "parent": "Expected Value Estimation",
      "description": "Theorem used to simplify expectation calculations by conditioning on intermediate variables."
    },
    {
      "id": "Gradient of Log Policy",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Derivative of the logarithm of a policy with respect to its parameters, crucial in policy gradient methods."
    },
    {
      "id": "State-Action Value Function",
      "type": "subnode",
      "parent": "Expected Value Estimation",
      "description": "Function estimating expected future rewards starting from a state and taking an action under a given policy."
    },
    {
      "id": "Policy Gradient Algorithms",
      "type": "major",
      "parent": null,
      "description": "Algorithms for updating policy parameters in reinforcement learning."
    },
    {
      "id": "Vanilla Policy Gradient with Baseline",
      "type": "subnode",
      "parent": "Policy Gradient Algorithms",
      "description": "Algorithm that uses a baseline to reduce variance of the gradient estimator."
    },
    {
      "id": "Gradient Estimator",
      "type": "subnode",
      "parent": "Vanilla Policy Gradient with Baseline",
      "description": "Estimates the policy gradient using trajectories and a baseline function."
    },
    {
      "id": "Baseline Function",
      "type": "subnode",
      "parent": "Vanilla Policy Gradient with Baseline",
      "description": "Function used to reduce variance in the gradient estimation process."
    },
    {
      "id": "Value Function Estimation",
      "type": "subnode",
      "parent": "Baseline Function",
      "description": "Estimating expected future rewards as a baseline for reducing estimator variance."
    },
    {
      "id": "Machine_Learning_Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of recent papers in machine learning and statistics."
    },
    {
      "id": "Implicit_Bias_Noise_Covariance",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper discussing the implicit bias in noise covariance for deep learning models."
    },
    {
      "id": "High_Dimensional_Ridgeless_Least_Squares_Interpolation",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on unexpected behaviors of ridgeless least squares interpolation in high dimensions."
    },
    {
      "id": "Deep_Residual_Learning_Image_Recognition",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper introducing deep residual learning for improving image recognition tasks."
    },
    {
      "id": "Introduction_to_Statistical_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Book providing an introduction to statistical learning methods and theory."
    },
    {
      "id": "Adam_Optimization_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper presenting Adam, a popular stochastic optimization method for machine learning."
    },
    {
      "id": "Auto-Encoding_Variational_Bayes",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on variational auto-encoders using Bayesian methods."
    },
    {
      "id": "Model-Based_DRL_Theoretical_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Framework for model-based deep reinforcement learning with theoretical guarantees."
    },
    {
      "id": "Random_Features_Regression_Error_Analysis",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analysis of generalization error in random features regression models."
    },
    {
      "id": "Linear_Regression_Sample_Size_Impact",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Study on the effect of sample size on linear regression model performance and double descent phenomenon."
    },
    {
      "id": "Regularization_Optimization_DRL",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on optimal regularization techniques to mitigate the double descent issue in deep learning models."
    },
    {
      "id": "Statistical_Mechanics_of_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Overview of statistical mechanics principles applied to understanding generalization in machine learning models."
    },
    {
      "id": "double_descent",
      "type": "major",
      "parent": null,
      "description": "Concept in machine learning showing a pattern of performance improvement after an initial decline."
    },
    {
      "id": "statistical_mechanics_of_learning",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Application of statistical mechanics principles to understand the generalization ability in machine learning models."
    },
    {
      "id": "generalization",
      "type": "subnode",
      "parent": "statistical_mechanics_of_learning",
      "description": "The process by which a model learns from training data and applies that knowledge to unseen data."
    },
    {
      "id": "learning_to_generalize",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Exploration of how machine learning models can improve their generalization capabilities over time."
    }
  ],
  "edges": [
    {
      "from": "M-step",
      "to": "Parameter Updates",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep_Residual_Learning_Image_Recognition",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "example_of"
    },
    {
      "from": "k-means_algorithm",
      "to": "inner_loop_steps",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining",
      "to": "Self_Supervised_Loss",
      "relationship": "uses"
    },
    {
      "from": "Logits",
      "to": "SoftmaxFunction",
      "relationship": "related_to"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Expected_Immediate_Reward",
      "relationship": "depends_on"
    },
    {
      "from": "SupportVectorMachinesSVMs",
      "to": "DualFormulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "DualProblem",
      "to": "dStar",
      "relationship": "defines"
    },
    {
      "from": "Training_Error_Generalization_Error",
      "to": "Uniform_Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Partial Derivatives Overview",
      "to": "Scalar Variable J",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Continuous_State_MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward Functions Overview",
      "to": "Loss Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Engineering",
      "to": "Feature_Maps",
      "relationship": "subtopic"
    },
    {
      "from": "ContrastiveLearning",
      "to": "SIMCLRAlgorithm",
      "relationship": "example_of"
    },
    {
      "from": "4.2 Naive bayes (Option Reading)",
      "to": "4.2.2 Event models for text classification",
      "relationship": "contains"
    },
    {
      "from": "Empirical Error (ε̂(h))",
      "to": "Vapnik's Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesClassifier",
      "to": "ParameterEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "MixtureOfGaussians",
      "to": "EMAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "LeastSquaresRegression",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "related_to"
    },
    {
      "from": "ModelParameters",
      "to": "LogLikelihood",
      "relationship": "related_to"
    },
    {
      "from": "Union Bound Lemma",
      "to": "Hoeffding Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "BackpropagationMLPs",
      "to": "BackwardPassMLP",
      "relationship": "has_subtopic"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.2 Constructing GLMs",
      "relationship": "contains"
    },
    {
      "from": "Optimization_Problems",
      "to": "Latent_Variables",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Applications",
      "to": "Dimensionality_Reduction",
      "relationship": "contains"
    },
    {
      "from": "Data Redundancy Detection",
      "to": "Linear Dependence Example",
      "relationship": "contains_example"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "contains"
    },
    {
      "from": "Regression_Problems",
      "to": "Least_Square_Cost_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Differential_Dynamic_Programming_(DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "Beta_Update_Equation",
      "to": "Inner_Products",
      "relationship": "requires"
    },
    {
      "from": "CrossEntropyLoss",
      "to": "NegativeLogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "NormalizationTechniques",
      "to": "LayerNorm",
      "relationship": "subtopic"
    },
    {
      "from": "Parameter Updates",
      "to": "Update Rule for φ_j",
      "relationship": "depends_on"
    },
    {
      "from": "E_Step",
      "to": "ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Derivatives_in_Machine_Learning",
      "to": "Multi_Variate_Functions_Derivatives",
      "relationship": "related_to"
    },
    {
      "from": "High_Dimensional_Ridgeless_Least_Squares_Interpolation",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Update Step",
      "to": "Kalman Filter",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Vectorization",
      "relationship": "related_to"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "Likelihood_Function",
      "relationship": "uses"
    },
    {
      "from": "Chapter 9 Regularization and model selection",
      "to": "Regularization",
      "relationship": "contains"
    },
    {
      "from": "KKTConditions",
      "to": "KarushKuhnTuckerTheorem",
      "relationship": "defines"
    },
    {
      "from": "Value_Functions",
      "to": "Policy_Pi",
      "relationship": "related_to"
    },
    {
      "from": "Payoff Calculation",
      "to": "Finite Horizon MDP",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Reward_Function",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation",
      "to": "SVM (Support Vector Machine)",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BinaryClassificationProblem",
      "relationship": "contains"
    },
    {
      "from": "Variance Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Convergence",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "SourceDensitySpecification",
      "to": "SigmoidFunctionChoice",
      "relationship": "subtopic"
    },
    {
      "from": "BackpropagationMLPs",
      "to": "ForwardPassMLP",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Scaling Constraint",
      "relationship": "simplifies"
    },
    {
      "from": "8.1 Bias-variance Tradeoff",
      "to": "Underfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Model Selection",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Nonlinear_Feature_Mappings",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Optimization_Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter Overview",
      "to": "Update Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "related_to"
    },
    {
      "from": "E-step",
      "to": "Gaussian Distribution",
      "relationship": "uses"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Maximum_Likelihood_Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Differentiable Circuit",
      "to": "Real-Valued Function",
      "relationship": "computes"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Uniform_Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Likelihood_Function",
      "to": "Probabilistic_Modeling",
      "relationship": "subtopic"
    },
    {
      "from": "Predict Step",
      "to": "Belief State Update",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "2 Classification and logistic regression",
      "relationship": "contains"
    },
    {
      "from": "LossFunction",
      "to": "IntermediateVariables",
      "relationship": "depends_on"
    },
    {
      "from": "distortion_function",
      "to": "coordinate_descent",
      "relationship": "related_to"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution on Class Label",
      "relationship": "depends_on"
    },
    {
      "from": "Matricization Approach",
      "to": "Implementation Details",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "SMO_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "MiniBatchSGD",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "\\(\\ell_{1}\\) Regularization",
      "relationship": "is_a_type_of"
    },
    {
      "from": "Hoeffding Inequality",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "NegativeLogLikelihood",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "ExpectationMaximizationAlgorithm",
      "to": "MStepUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Dual Problem Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "ConvexityConditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Kernels in SVM",
      "relationship": "subtopic"
    },
    {
      "from": "EmbeddingsAndRepresentations",
      "to": "EmbeddingMatrix",
      "relationship": "subtopic"
    },
    {
      "from": "DataPreprocessing",
      "to": "LogisticFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization_Error_Guarantees",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "BernoulliModel",
      "to": "ClassPriors",
      "relationship": "depends_on"
    },
    {
      "from": "NewtonMethod",
      "to": "FisherScoring",
      "relationship": "related_to"
    },
    {
      "from": "TrainingDataset",
      "to": "BiasVarianceTradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous Setting",
      "to": "LQR",
      "relationship": "subtopic"
    },
    {
      "from": "ParameterEstimation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "BackpropagationProcess",
      "relationship": "related_to"
    },
    {
      "from": "Inverted Pendulum Example",
      "to": "Linearization of Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "VectorizationTechniques",
      "to": "BroadcastingConcept",
      "relationship": "subtopic"
    },
    {
      "from": "Theta_Vector",
      "to": "Memory_Use",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConditionalProbabilityModeling",
      "relationship": "contains"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable Circuit",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "StochasticGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "In-context Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "ModelComplexityMeasures",
      "to": "NormAsComplexityMeasure",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference",
      "to": "Continuous Latent Variables",
      "relationship": "subtopic"
    },
    {
      "from": "UpdateRule",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "PartialDerivativeTerm",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis Class",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Efficiency",
      "to": "Optimized Libraries",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Normalization",
      "to": "Standard Deviation Rescaling",
      "relationship": "depends_on"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Properties_of_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "contains"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Explanation and Mitigation Strategy",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Simplified_Formula",
      "relationship": "follows_from"
    },
    {
      "from": "KernelMethods",
      "to": "FeatureMaps",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureTransformation",
      "to": "GradientDescentAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "BandwidthParameter",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "contains"
    },
    {
      "from": "Generative_Algorithms",
      "to": "Dog_Model",
      "relationship": "contains"
    },
    {
      "from": "VarianceMaximization",
      "to": "ProjectionOntoUnitVector",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistributions",
      "to": "MeanVectorShifts",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "has_subtopic"
    },
    {
      "from": "Finetuning Algorithm",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Likelihood_Optimization",
      "to": "Multiple_Examples",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian_Inference",
      "to": "MAP_Estimate",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Foundation Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "defined_by"
    },
    {
      "from": "Vanilla_REINFORCE_Algorithm",
      "to": "Empirical_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "BernoulliModel",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis_Class",
      "to": "Machine_Learning_Basics",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Dynamics",
      "to": "Optimal_Policy_Non_Stationarity",
      "relationship": "contains"
    },
    {
      "from": "Continuous State MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Non-separable Case",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Feature Mapping (phi)",
      "relationship": "depends_on"
    },
    {
      "from": "Inverted Pendulum Example",
      "to": "State Transition Function F",
      "relationship": "depends_on"
    },
    {
      "from": "Probability_Error",
      "to": "Training_Error_Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Expectation_Maximization",
      "to": "Gradient_Ascend_Algorithm",
      "relationship": "subtopic_of"
    },
    {
      "from": "Loss_Functions",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "SourceDensitySpecification",
      "relationship": "depends_on"
    },
    {
      "from": "Reward_Function_Linearization",
      "to": "Differential_Dynamic_Programming_(DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO_Evaluation",
      "to": "Q_i_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "Hyperparameters",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "LogisticLossFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error_Bound",
      "to": "Sample_Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Shattering",
      "to": "VC Dimension",
      "relationship": "subtopic"
    },
    {
      "from": "Gamma_Parameter",
      "to": "Training_Error_Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "TrainingSetSplitting",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ProbabilityDistribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "ResNet",
      "to": "Convolutional_Layers",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "FeasibilityConstraints",
      "relationship": "has_subtopic"
    },
    {
      "from": "Algorithm Design",
      "to": "Procedure VE",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "ScalingFactor",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimization Algorithms",
      "relationship": "depends_on"
    },
    {
      "from": "8.1 Bias-variance Tradeoff",
      "to": "Linear Model Example",
      "relationship": "related_to"
    },
    {
      "from": "TrainingExamplesMatrixNotation",
      "to": "VectorizationTechniques",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkArchitecture",
      "to": "ActivationFunctions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample_Complexity",
      "to": "Hypothesis_Class_Size",
      "relationship": "depends_on"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.1 Matrix derivatives",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "2D_Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional_Probability",
      "to": "Softmax_Function",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Sequential Decision Making",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ConvolutionalLayers",
      "relationship": "contains"
    },
    {
      "from": "Reward_Function_Estimation",
      "to": "Reparametrization_Technique",
      "relationship": "related_to"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of Dynamics",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal_Parameter_Finding",
      "to": "Optimal_Intercept_Term",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "contains"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ConditionalDistributionAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous State MDPs",
      "to": "Value Function Approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Normalization Condition",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization",
      "to": "Matrix Algebra",
      "relationship": "subtopic_of"
    },
    {
      "from": "Law of Total Expectation",
      "to": "Expected Value Estimation",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Action Selection",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNeuralNetworks",
      "to": "Parameterization",
      "relationship": "depends_on"
    },
    {
      "from": "FullyConnectedNeuralNetworks",
      "to": "TwoLayerNetworks",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Inverted Pendulum Example",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Concrete Backprop Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Procedure VE",
      "to": "Value Function Update",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient of Log Policy",
      "to": "Policy Gradient Methods",
      "relationship": "subtopic"
    },
    {
      "from": "MultinomialRandomVariable",
      "to": "MaximumLikelihoodEstimates",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "EM_Algorithms",
      "relationship": "extends"
    },
    {
      "from": "SupportVectorMachinesSVMs",
      "to": "SMOAlgorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "UnitVectorW",
      "to": "DistanceToBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional_Probability_Distribution",
      "to": "Probabilistic_Modeling",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting",
      "to": "5th Degree Polynomial",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Lagrangian_Methods",
      "relationship": "depends_on"
    },
    {
      "from": "GradientComputation",
      "to": "ReparameterizationTrick",
      "relationship": "has_subtopic"
    },
    {
      "from": "PCA",
      "to": "Data Preprocessing",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "Clustering",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Basic Modules Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Housing Example Dataset",
      "relationship": "subtopic"
    },
    {
      "from": "Generative_Algorithms",
      "to": "Class_Priors",
      "relationship": "depends_on"
    },
    {
      "from": "mixture_of_gaussians",
      "to": "EM_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Tricks",
      "to": "Runtime_Efficiency",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "HypothesisFunction",
      "relationship": "defines"
    },
    {
      "from": "Kernel_Methods",
      "to": "Linear_Regression_Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworksIntroduction",
      "to": "SingleNeuronNN",
      "relationship": "has_subtopic"
    },
    {
      "from": "SoftmaxFunction",
      "to": "MachineLearningOverview",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Reinforcement_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "True Error (ε(h))",
      "to": "Vapnik's Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Clustering",
      "to": "K_Means_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "PrimalProblem",
      "to": "pStar",
      "relationship": "defines"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Walkability",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "6 Support vector machines",
      "relationship": "contains"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "DeepLearningModelTraining",
      "to": "StochasticGradientDescent",
      "relationship": "depends_on"
    },
    {
      "from": "Separating Hyperplane",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "BackpropagationProcess",
      "to": "ForwardPass",
      "relationship": "contains"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Policy_Iteration_Speeding_Up",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "contains"
    },
    {
      "from": "RandomVariables",
      "to": "CovarianceExamples",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines_SVMs",
      "relationship": "contains"
    },
    {
      "from": "kFoldCrossValidation",
      "to": "DataSplitting",
      "relationship": "subtopic_of"
    },
    {
      "from": "NeuralNetworkParameters",
      "to": "BiologicalInspiration",
      "relationship": "related_to"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Fundamentals",
      "relationship": "related_to"
    },
    {
      "from": "HoldoutCrossValidation",
      "to": "RetrainingOnFullDataset",
      "relationship": "subtopic_of"
    },
    {
      "from": "MLP_Architecture",
      "to": "Matrix_Multiplication_Module",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement Learning Goal",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimal Margin Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Quadratic Regulator (LQR)",
      "relationship": "contains"
    },
    {
      "from": "Optimization_Problems",
      "to": "SVM_Support_Vectors",
      "relationship": "related_to"
    },
    {
      "from": "Algorithm Design",
      "to": "Policy Improvement",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDistribution",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm",
      "to": "Kalman Filter",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "CanonicalResponseFunction",
      "relationship": "has_subnode"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Regression_Problems",
      "relationship": "subtopic"
    },
    {
      "from": "Non-separable Case",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "contains"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Unmixing Matrix W",
      "relationship": "depends_on"
    },
    {
      "from": "Training loss/cost function",
      "to": "Regularized loss",
      "relationship": "modified_by"
    },
    {
      "from": "Rewriting_Dynamics",
      "to": "Linearization_of_Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO_Formulation",
      "to": "Optimal_Q_Choice",
      "relationship": "depends_on"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "ModelSelection",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Linear_Models_Impairment",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Curse of Dimensionality",
      "relationship": "depends_on"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "JointDistributionModeling",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Family_Size",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistributions",
      "to": "CovarianceMatrixEffects",
      "relationship": "subtopic_of"
    },
    {
      "from": "Linear Regression",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Test_Error_Influence",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Policy in MDPs",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "RegressionProblems",
      "to": "BiasVarianceTradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Overfitting",
      "to": "Training Error",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Regression Problem",
      "relationship": "related_to"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Heuristic_Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Bernoulli_Event_Model",
      "relationship": "related_to"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "contains"
    },
    {
      "from": "Geometric_Margin_Definition",
      "to": "Optimal_Margin_Classifier",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "Zero-shot Adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.1 LMS algorithm",
      "relationship": "contains"
    },
    {
      "from": "Feature_Mapping",
      "to": "Pre_Compute_Inner_Products",
      "relationship": "enables"
    },
    {
      "from": "Optimization_in_ML",
      "to": "Quadratic_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "BackpropagationAlgorithm",
      "to": "DeepLearningModelTraining",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Feature_Engineering",
      "relationship": "contains"
    },
    {
      "from": "LinearModelPrediction",
      "to": "DeterministicModel",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Noise Independence",
      "relationship": "related_to"
    },
    {
      "from": "LinearModelPrediction",
      "to": "StochasticModel",
      "relationship": "related_to"
    },
    {
      "from": "Total Number of Neurons and Parameters",
      "to": "Multi-layer Fully-Connected Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Likelihood_Function",
      "to": "Maximum_Likelihood_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "ProbabilityPrediction",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Global Minima Diversity",
      "relationship": "has_subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "VariationalAutoEncoder",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "V^π(s)",
      "to": "Bellman's Equation",
      "relationship": "related_to"
    },
    {
      "from": "Inductive Biases and Structures",
      "to": "Regularizer R(θ)",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Gaussian_Mixture_Models",
      "relationship": "includes"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Discriminative_Algorithms",
      "relationship": "contains"
    },
    {
      "from": "Underfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Hoeffding_Inequality",
      "to": "Generalization_Error_Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "FlatMinimaConjecture",
      "to": "Optimizer-Generalization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Expectation_Maximization",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Ascend_Optimization",
      "to": "Reward_Function_Estimation",
      "relationship": "related_to"
    },
    {
      "from": "convergence_of_k_means",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent",
      "to": "Batch Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "contains"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Kalman Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Synchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "SingleNeuronNN",
      "to": "ReLUActivationFunction",
      "relationship": "introduces"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Non-parametric Algorithms",
      "relationship": "contains"
    },
    {
      "from": "1 Linear regression",
      "to": "1.2 The normal equations",
      "relationship": "contains"
    },
    {
      "from": "Learning a Model for an MDP",
      "to": "Continuous State MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "contains"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Widrow-Hoff_Learning_Rule",
      "relationship": "related_to"
    },
    {
      "from": "Non-parametric Algorithms",
      "to": "Locally Weighted Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Vocabulary",
      "to": "Stop_Words",
      "relationship": "includes"
    },
    {
      "from": "Sequential Minimal Optimization (SMO) Algorithm",
      "to": "Support Vector Machines (SVM)",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "ELBOExplanation",
      "to": "AlternativeFormulationsOfELBO",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVM)",
      "relationship": "contains"
    },
    {
      "from": "DoubleDescentPhenomenon",
      "to": "ModelComplexityMeasures",
      "relationship": "depends_on"
    },
    {
      "from": "Efficient_Update",
      "to": "Constraints_and_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Descent",
      "to": "Batch_Gradient_Descent",
      "relationship": "subtopic_of"
    },
    {
      "from": "PCA",
      "to": "Dimensionality Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Distribution on Parameters",
      "to": "Fully Bayesian Prediction",
      "relationship": "related_to"
    },
    {
      "from": "LogPartitionFunction",
      "to": "ExponentialFamilyDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "DecisionBoundaries",
      "relationship": "depends_on"
    },
    {
      "from": "Procedure VE",
      "to": "k Parameter",
      "relationship": "depends_on"
    },
    {
      "from": "Regression_Problems",
      "to": "Mean_Square_Cost_Function",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FullyConnectedNeuralNetworks",
      "relationship": "contains"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.2 Digression: the perceptron learning algorithm",
      "relationship": "contains"
    },
    {
      "from": "SIMCLRAlgorithm",
      "to": "RandomAugmentation",
      "relationship": "uses"
    },
    {
      "from": "Conv1D-S",
      "to": "Matrix_Multiplication_Qz",
      "relationship": "outputs"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Markov_Decision_Processes",
      "relationship": "defines"
    },
    {
      "from": "Volume Calculation",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Layer_Normalization",
      "to": "Affine_Transformation_Parameters",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Deep Learning Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization_Error",
      "to": "Optimal_Hypothesis_h_star",
      "relationship": "defines"
    },
    {
      "from": "Zero-shot Adaptation",
      "to": "Language Model Utilization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Finding Roots",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted_Value_Iteration",
      "to": "Value_Iteration_Update",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Unconstrained_Optimization_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.3 Multi-class classification",
      "relationship": "contains"
    },
    {
      "from": "Valid Kernel Function Properties",
      "to": "Sufficient Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Linear_Quadratic_Regulator_(LQR)",
      "relationship": "subtopic"
    },
    {
      "from": "Finite_Hypothesis_Class",
      "to": "Hypothesis_Class",
      "relationship": "subtopic"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Regression",
      "to": "Machine_Learning_Methods",
      "relationship": "related_to"
    },
    {
      "from": "Convergence to Optimal Value Function",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm Design",
      "to": "Policy Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Likelihood_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Policy π*(s)",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Primal Problem",
      "to": "Value of Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Covariance Matrix",
      "relationship": "has_part"
    },
    {
      "from": "Efficiency",
      "to": "Parallel Computation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Gradient_Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Tricks",
      "to": "Memory_Use",
      "relationship": "related_to"
    },
    {
      "from": "LMS_Algorithm",
      "to": "Gradient_Descent_Update_Rule",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "PrimalProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "GradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning_Representations",
      "to": "Black_Box_Models",
      "relationship": "subtopic_of"
    },
    {
      "from": "Linear_Combinations",
      "to": "Inductive_Step",
      "relationship": "subtopic"
    },
    {
      "from": "ErrorTerm",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "SpamDetection",
      "to": "TrainingSetLimitations",
      "relationship": "causes"
    },
    {
      "from": "EM-Algorithm",
      "to": "M-step",
      "relationship": "depends_on"
    },
    {
      "from": "Differentiable Circuit",
      "to": "Arithmetic Operations",
      "relationship": "includes"
    },
    {
      "from": "Implicit_Bias_Noise_Covariance",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Time Horizon Definition",
      "to": "Finite Horizon MDP",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Housing_Price_Prediction",
      "relationship": "related_to"
    },
    {
      "from": "LogisticLossFunction",
      "to": "ChainRuleApplication",
      "relationship": "uses"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linear_Quadratic_Gaussian_(LQG)",
      "relationship": "next_topic"
    },
    {
      "from": "PCA",
      "to": "Noise_Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "CostFunction",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "LossFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Methods",
      "to": "Finetuning Pretrained Models",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Sparsity",
      "to": "L1 Regularization (LASSO)",
      "relationship": "depends_on"
    },
    {
      "from": "Differential_Dynamic_Programming_(DDP)",
      "to": "Machine_Learning_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference",
      "to": "Mean Field Assumption",
      "relationship": "related_to"
    },
    {
      "from": "Vectorization",
      "to": "Two-Layer Neural Network",
      "relationship": "related_to"
    },
    {
      "from": "Learning a Model for an MDP",
      "to": "Connections between Policy and Value Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization Parameter λ",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDataExample",
      "to": "RotationAmbiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConferences",
      "to": "NeurIPS",
      "relationship": "contains"
    },
    {
      "from": "Joint Distribution Model",
      "to": "Mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution in Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Loss Function Design",
      "relationship": "subtopic"
    },
    {
      "from": "distortion_function_J",
      "to": "k-means_algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Learning Theory",
      "to": "Model Selection Methods",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Shattering",
      "relationship": "related_to"
    },
    {
      "from": "ConditionalDistributionAssumptions",
      "to": "NaturalParameterLinearity",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalProbability",
      "to": "NBAssumption",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Gradient_Theorem",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "Step1Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Layer_Normalization",
      "to": "LN_S_Module",
      "relationship": "subtopic_of"
    },
    {
      "from": "V^π(s)",
      "to": "Optimal Value Function V*(s)",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningBasics",
      "to": "FunctionRepresentation",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Convergence_Criterion",
      "relationship": "has"
    },
    {
      "from": "Backward Functions Overview",
      "to": "Matrix Multiplication Module (MM)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Gradient_Ascent_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "LinearHypothesis",
      "to": "ParametersWeights",
      "relationship": "has_subtopic"
    },
    {
      "from": "NaturalParameter",
      "to": "ExponentialFamilyDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "LearningRateAlpha",
      "relationship": "depends_on"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "BernoulliDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "State Transition",
      "to": "Action Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Auto-Encoding_Variational_Bayes",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Networks",
      "to": "Stacking_Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "Pre_Compute_Inner_Products",
      "to": "Efficient_Calculation_Method",
      "relationship": "uses"
    },
    {
      "from": "Optimization Problem",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "PCA",
      "to": "Approximation Error Minimization",
      "relationship": "related_to"
    },
    {
      "from": "DecisionBoundaries",
      "to": "AsymptoticEfficiency",
      "relationship": "related_to"
    },
    {
      "from": "Hypothesis_Error",
      "to": "Training_Error",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Problem",
      "to": "Relationship Between Primal and Dual Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Bayesian_Inference",
      "to": "MLE_Estimate",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearning",
      "to": "LayerNormalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "contains"
    },
    {
      "from": "Total Payoff",
      "to": "Discount Factor",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "contains"
    },
    {
      "from": "Feature_Vector_Selection",
      "to": "Stop_Words_Exclusion",
      "relationship": "related_to"
    },
    {
      "from": "ResNet_Architecture",
      "to": "Residual_Block",
      "relationship": "subtopic"
    },
    {
      "from": "Data Normalization",
      "to": "Comparability of Attributes",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Sequential_Minimal_Optimization_SMO",
      "relationship": "depends_on"
    },
    {
      "from": "Action Selection",
      "to": "Reward Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Data Normalization",
      "to": "Mean Zeros Out",
      "relationship": "depends_on"
    },
    {
      "from": "KernelsInML",
      "to": "KernelFunctionExample2",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "Optimal_Solution_Finding",
      "relationship": "depends_on"
    },
    {
      "from": "EM-Algorithm",
      "to": "E-step",
      "relationship": "depends_on"
    },
    {
      "from": "Perceptron Algorithm",
      "to": "Multi-class Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Quadratic Programming (QP)",
      "relationship": "can_be_solved_by"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Beta_Update_Equation",
      "relationship": "defines"
    },
    {
      "from": "Machine_Learning",
      "to": "EM_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Downstream Dataset",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Action Computation",
      "to": "Rewards Dependency",
      "relationship": "subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "GELUFunction",
      "to": "ReLUFunction",
      "relationship": "variant_of"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "SoftmaxFunction",
      "relationship": "related_to"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Optimal_Balance_Update_Frequencies",
      "relationship": "subtopic"
    },
    {
      "from": "Union_Bound",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.1 Ordinary least squares",
      "relationship": "contains"
    },
    {
      "from": "Parameter Updates",
      "to": "Update Rule for μ_j",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Bellman Operator",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Physics Simulation",
      "to": "Open Dynamics Engine (ODE)",
      "relationship": "related_to"
    },
    {
      "from": "Design_Matrix_X",
      "to": "Probabilistic_Modeling",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "VarianceMaximization",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "PoissonDistributionModeling",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Partially_Observable_MDPs",
      "relationship": "contains"
    },
    {
      "from": "Sample_Based_Estimator",
      "to": "Log_Probability_Derivation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LOOCV",
      "to": "CrossValidation",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Transitions",
      "to": "LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Multidimensional Generalization",
      "relationship": "subtopic"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.1 The exponential family",
      "relationship": "contains"
    },
    {
      "from": "LogisticRegression",
      "to": "Logit",
      "relationship": "depends_on"
    },
    {
      "from": "Overfitting",
      "to": "Test Error",
      "relationship": "subtopic"
    },
    {
      "from": "ExpectedTestError",
      "to": "BiasVarianceTradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Bellman_Equation",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctionExample2",
      "to": "ParameterCExplanation",
      "relationship": "explains"
    },
    {
      "from": "Backpropagation",
      "to": "General Backprop Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Functions",
      "to": "Gaussian_Kernel",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Optimal Policy",
      "relationship": "results_in"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "WeightsCalculation",
      "relationship": "depends_on"
    },
    {
      "from": "b Variable",
      "to": "Backward Function",
      "relationship": "related_to"
    },
    {
      "from": "Algorithm Design",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Sample_Size_N",
      "to": "Training_Error_Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization of State Space",
      "to": "Piecewise Constant Representation",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "ExponentialFamilyDistributions",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Prediction_Mechanism",
      "relationship": "subtopic"
    },
    {
      "from": "Conv1D-S",
      "to": "Filter_Vector_w",
      "relationship": "has_parameter"
    },
    {
      "from": "1 Linear regression",
      "to": "1.3 Probabilistic interpretation",
      "relationship": "contains"
    },
    {
      "from": "Value Iteration Algorithm",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "CrossValidation",
      "to": "HoldOutCrossValidation",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization",
      "to": "Parallel Computation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "1 Linear regression",
      "relationship": "contains"
    },
    {
      "from": "NormalizationTechniques",
      "to": "OtherNormalizationLayers",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Initialization",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Sample_Based_Estimator",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "DoubleDescentPhenomenon",
      "relationship": "related_to"
    },
    {
      "from": "PrimalProblem",
      "to": "ThetaP",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "ICA",
      "to": "Cocktail_Party_Problem",
      "relationship": "example"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "Activation Functions",
      "to": "Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "GaussianDataExample",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "ParameterSharing",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "contains"
    },
    {
      "from": "Gradient Descent Compatibility",
      "to": "Relaxation Techniques",
      "relationship": "related_to"
    },
    {
      "from": "SupportVectors",
      "to": "LagrangianFormulation",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Discretization of State Space",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "RegressionProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretraining",
      "to": "Unlabeled_Data",
      "relationship": "depends_on"
    },
    {
      "from": "Multiple_Examples",
      "to": "Loglikelihood_Lower_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Vectorization",
      "relationship": "contains"
    },
    {
      "from": "Optimization Problem",
      "to": "Linear Constraints",
      "relationship": "has_subtopic"
    },
    {
      "from": "IIDAssumption",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Projection Maximizing Variance",
      "relationship": "subtopic"
    },
    {
      "from": "Noise_Reduction",
      "to": "Eigenfaces_Method",
      "relationship": "contains"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Hypothesis",
      "relationship": "depends_on"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "includes_concept"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Taylor Expansion",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Differentiable Circuit",
      "to": "Elementary Functions",
      "relationship": "includes"
    },
    {
      "from": "ContrastiveLearning",
      "to": "NegativePair",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkArchitecture",
      "to": "LayeredStructure",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Generalization and Regularization",
      "relationship": "related_to"
    },
    {
      "from": "SupportVectors",
      "to": "InnerProduct",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machine (SVM)",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "8.1 Bias-variance Tradeoff",
      "to": "Test Error Analysis",
      "relationship": "depends_on"
    },
    {
      "from": "Vanilla Policy Gradient with Baseline",
      "to": "Baseline Function",
      "relationship": "depends_on"
    },
    {
      "from": "GradientAscentRule",
      "to": "LMSUpdateRule",
      "relationship": "compares_to"
    },
    {
      "from": "Conv1D-S",
      "to": "Bias_Scalar_b",
      "relationship": "has_parameter"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Fundamentals",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningValidationTechniques",
      "to": "HoldoutCrossValidation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Discretization of State Space",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Kernels",
      "to": "Similarity_Metrics",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Ascent_Optimization",
      "to": "Gradient_Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Nominal_Trajectory",
      "to": "Differential_Dynamic_Programming_(DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientAscent",
      "to": "ConvergenceCriteria",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Bayesian_Inference",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "LogisticRegression",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Concave Function",
      "relationship": "related_to"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "Evidence_Lower_Bound_(ELBO)",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Architectures",
      "to": "ResNet",
      "relationship": "contains"
    },
    {
      "from": "MStepUpdateRule",
      "to": "LagrangianMethod",
      "relationship": "depends_on"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.2 LMS (least mean squares) with features",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "5th_Degree_Polynomial_Failure",
      "relationship": "related_to"
    },
    {
      "from": "Foundation Models",
      "to": "Pretraining and Adaptation",
      "relationship": "subtopic_of"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "subtopic"
    },
    {
      "from": "DataScarcity",
      "to": "KFoldCV",
      "relationship": "related_to"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Optimal_Bellman_Equation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "BatchGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM-Algorithm",
      "to": "Soft Assignments",
      "relationship": "has"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ClassificationProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Vanilla Policy Gradient with Baseline",
      "to": "Gradient Estimator",
      "relationship": "depends_on"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.4 Another algorithm for maximizing λ(θ)",
      "relationship": "contains"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "Gradient_Ascend",
      "relationship": "uses"
    },
    {
      "from": "Training_Test_Distributions",
      "to": "Machine_Learning_Fundamentals",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "contains"
    },
    {
      "from": "KernelTrick",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "MLPModel",
      "relationship": "depends_on"
    },
    {
      "from": "ProbabilityVector",
      "to": "SoftmaxFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Polynomial Regression Models",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "EmpiricalCovarianceMatrix",
      "relationship": "related_to"
    },
    {
      "from": "Rewards Dependency",
      "to": "Expectation Rewriting",
      "relationship": "depends_on"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworksIntroduction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hypothesis Class",
      "to": "Infinite Hypothesis Classes",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "ValueIteration",
      "relationship": "contains"
    },
    {
      "from": "Representation Function",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "Mean_Squared_Error_MSE",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Hidden_Units",
      "to": "ReLU_Activation",
      "relationship": "related_to"
    },
    {
      "from": "Linear_Functions",
      "to": "Cubic_Function_Example",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "PAC_Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Exponential Family Models",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Classification_Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Probe Approach",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Supervised Contrastive Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "NaiveBayesClassifier",
      "relationship": "depends_on"
    },
    {
      "from": "Overfitting_Underfitting",
      "to": "Machine_Learning_Fundamentals",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "InnerProduct",
      "relationship": "subtopic"
    },
    {
      "from": "KKT_Conditions",
      "to": "Dual_Complementarity",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Expectation_Zero_Property",
      "relationship": "implies"
    },
    {
      "from": "Optimization Problem",
      "to": "Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "NaiveBayesAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Loss Functions",
      "to": "Training Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Variance_Definition",
      "to": "Mean_Squared_Error_MSE",
      "relationship": "subtopic"
    },
    {
      "from": "5th_Degree_Polynomial_Failure",
      "to": "Polynomial_Bias_Low",
      "relationship": "subtopic"
    },
    {
      "from": "Deep_Learning",
      "to": "Neural_Network_Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Spam_Filtering",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "Primal_Dual_Equivalence",
      "to": "Optimization_Problems",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GeneralizedLinearModels",
      "relationship": "depends_on"
    },
    {
      "from": "Introduction_to_Statistical_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.2 The Gaussian discriminant analysis model",
      "relationship": "contains"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ConstructingGLMs",
      "relationship": "subtopic"
    },
    {
      "from": "SourceRecovery",
      "to": "ModelParameters",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Kernel_Methods",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "VC_Dimension",
      "relationship": "subtopic"
    },
    {
      "from": "Discount Factor Relevance",
      "to": "Finite Horizon MDP",
      "relationship": "related_to"
    },
    {
      "from": "Hypothesis_Error",
      "to": "Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Inner_Products",
      "relationship": "subtopic"
    },
    {
      "from": "RandomVariables",
      "to": "CovarianceDefinition",
      "relationship": "subtopic_of"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Expectation_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Linearly_Separable_Data",
      "to": "Maximize_Geometric_Margin_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Generative_Modeling",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "Latent Variable Model",
      "relationship": "subtopic"
    },
    {
      "from": "Latent Variable Models",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Model-Based_DRL_Theoretical_Guarantees",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "GaussianDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous-State MDPs",
      "to": "Discretization Method",
      "relationship": "depends_on"
    },
    {
      "from": "Weight Matrices and Biases",
      "to": "Multi-layer Fully-Connected Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Eigenvectors of Sigma",
      "to": "Orthogonal Basis",
      "relationship": "depends_on"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "TopKEigenvectors",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Applications",
      "to": "ICA",
      "relationship": "follows"
    },
    {
      "from": "In-context Learning",
      "to": "Machine Learning Techniques",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Generalization_Error",
      "relationship": "depends_on"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Convergence_Checking",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Non-Gaussian Data Recovery",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "Probabilistic_Modeling",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "L2 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Training Loss",
      "to": "Mean Squared Error (MSE)",
      "relationship": "depends_on"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Mean Vector",
      "relationship": "has_part"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Gradient_Theorem",
      "relationship": "contains"
    },
    {
      "from": "4.2 Naive bayes (Option Reading)",
      "to": "4.2.1 Laplace smoothing",
      "relationship": "contains"
    },
    {
      "from": "Optimization Problem",
      "to": "Convex Quadratic Objective",
      "relationship": "has_subtopic"
    },
    {
      "from": "HoldoutCrossValidation",
      "to": "ModelSelection",
      "relationship": "subtopic_of"
    },
    {
      "from": "Functional Margin",
      "to": "Confidence and Correct Prediction",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Non-linear Dynamics Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Transformations and Densities",
      "to": "Effect of Linear Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Statistical_Mechanics_of_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Foundation Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "LossFunctionsBackwardPass",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Response Variable",
      "relationship": "depends_on"
    },
    {
      "from": "SigmoidFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "4 Generative learning algorithms",
      "relationship": "contains"
    },
    {
      "from": "GradientDescentOptimization",
      "to": "ChainRule",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GDAModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Spam Classifier Example",
      "relationship": "example_of"
    },
    {
      "from": "JensensInequality",
      "to": "LogLikelihoodBound",
      "relationship": "depends_on"
    },
    {
      "from": "Temperature Parameter in Softmax",
      "to": "Text Generation Models",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ICAIndependenceAssumption",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Conv2DModule",
      "relationship": "contains"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Reinforcement Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "Logits",
      "relationship": "depends_on"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "General_Setting_Equations",
      "relationship": "subtopic"
    },
    {
      "from": "MeanSquaredError",
      "to": "ExpectedTestError",
      "relationship": "subtopic"
    },
    {
      "from": "double_descent",
      "to": "statistical_mechanics_of_learning",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "High_Dimensional_Latent_Variables",
      "relationship": "applies_to"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "LossFunction",
      "relationship": "related_to"
    },
    {
      "from": "ChainRule",
      "to": "JacobianMatrix",
      "relationship": "relates_to"
    },
    {
      "from": "Generative_Modeling",
      "to": "Naive_Bayes_Assumption",
      "relationship": "depends_on"
    },
    {
      "from": "Decision Boundary",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Dimensionality_Reduction",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Engineering",
      "to": "String_Features",
      "relationship": "depends_on"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "MultinomialRandomVariable",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningValidationTechniques",
      "to": "kFoldCrossValidation",
      "relationship": "has_subtopic"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "ModelSelection",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Binary_Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Expected Value Estimation",
      "to": "Policy Gradient Methods",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "EventModelsTextClassification",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNeuralNetworks",
      "to": "IntermediateVariables",
      "relationship": "depends_on"
    },
    {
      "from": "ELBO Lower Bound",
      "to": "Optimization Over Q",
      "relationship": "depends_on"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Optimal Margin Classifier",
      "relationship": "results_in"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Policy Function",
      "relationship": "subtopic"
    },
    {
      "from": "MLPModel",
      "to": "ModulesInModel",
      "relationship": "contains"
    },
    {
      "from": "Value Function Approximation",
      "to": "Model or Simulator",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningLossFunctions",
      "to": "LogisticLossFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Mercer's Theorem",
      "to": "Necessary and Sufficient Condition",
      "relationship": "subtopic"
    },
    {
      "from": "Eigenvectors of Sigma",
      "to": "Principal Components",
      "relationship": "defines"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "1DConvolution",
      "relationship": "subtopic"
    },
    {
      "from": "Regularizer R(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KernelsInML",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Matricization Approach",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "M-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "MultinomialFeatures",
      "to": "Discretization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Bias_Definition",
      "to": "Mean_Squared_Error_MSE",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LogLikelihoodBound",
      "relationship": "has_subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Function Margin of Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter 17 Policy Gradient (REINFORCE)",
      "to": "Transition Probabilities Sampling",
      "relationship": "related_to"
    },
    {
      "from": "Derivatives_in_Machine_Learning",
      "to": "Chain_Rule_Application",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Log_Likelihood_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Algorithm Steps",
      "relationship": "describes"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimal_Parameter_Finding",
      "relationship": "depends_on"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Multinomial Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "contains"
    },
    {
      "from": "Model_Parameters",
      "to": "Likelihood_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Chain_Rule_Application",
      "to": "Gradient_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ConvolutionalLayers",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Chapter 17 Policy Gradient (REINFORCE)",
      "to": "Finite Horizon Case",
      "relationship": "depends_on"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.1 The multivariate normal distribution",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Backpropagation",
      "relationship": "follows"
    },
    {
      "from": "Binary Classification",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "II Deep learning",
      "to": "7 Deep learning",
      "relationship": "contains"
    },
    {
      "from": "TestExample",
      "to": "BiasVarianceTradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "State_Transition_Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworkParametrization",
      "to": "DeepLearningModelTraining",
      "relationship": "subtopic"
    },
    {
      "from": "Phi_Functions",
      "to": "Linear_Combinations",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization and Regularization",
      "to": "Training Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegression",
      "to": "FittedValueIteration",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Mapping",
      "to": "Cubic_Function_Example",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Models_Impairment",
      "to": "Bias_Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "contains"
    },
    {
      "from": "Primal Problem",
      "to": "Objective Function",
      "relationship": "depends_on"
    },
    {
      "from": "Transformer_Model",
      "to": "Training_Transformer",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrange Multipliers",
      "relationship": "uses"
    },
    {
      "from": "Bayesian Classification",
      "to": "Class Priors",
      "relationship": "depends_on"
    },
    {
      "from": "Pretraining",
      "to": "Representations",
      "relationship": "produces"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "Model Specification",
      "to": "Bayesian Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule Perspective",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryFeatures",
      "to": "NaiveBayesAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "subtopic"
    },
    {
      "from": "L2 Regularization",
      "to": "Regularized Loss",
      "relationship": "related_to"
    },
    {
      "from": "KernelsInML",
      "to": "ComputationalEfficiency",
      "relationship": "discusses"
    },
    {
      "from": "Model Complexity",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "CoordinateAscent",
      "to": "OptimizationTechniques",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Supervised Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Rotational Symmetry",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining in Computer Vision",
      "to": "Supervised Pretraining",
      "relationship": "subtopic"
    },
    {
      "from": "Decision_Boundary",
      "to": "Logistic_Regression",
      "relationship": "related_to"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.3 LMS with the kernel trick",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Notation for SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "1D_Convolution",
      "to": "Conv1D-S",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Empirical Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "contains"
    },
    {
      "from": "Nonlinear_Model",
      "to": "Training_Examples",
      "relationship": "subtopic"
    },
    {
      "from": "Convex_Functions",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Functions Overview",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Feature_Maps_and_Kernels",
      "relationship": "contains"
    },
    {
      "from": "Gradient Descent",
      "to": "Stochastic Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Markov_Decision_Processes_Finite_Horizon",
      "to": "Time_Dependent_Dynamics",
      "relationship": "contains"
    },
    {
      "from": "FunctionRepresentation",
      "to": "CostFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "SigmoidFunction",
      "to": "DerivativeSigmoid",
      "relationship": "related_to"
    },
    {
      "from": "Domain_Shift",
      "to": "Training_Test_Distributions",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "contains"
    },
    {
      "from": "Loss Functions",
      "to": "Test Error",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Map_and_Kernel_Functions",
      "to": "Properties_of_Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Support_Vector_Machines",
      "relationship": "related_to"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "contains"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.1 Gaussian discriminant analysis",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningLossFunctions",
      "to": "CrossEntropyLossFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "ICA Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Training Loss",
      "to": "Empirical Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter Overview",
      "to": "Backward Pass (LQR Updates)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "uses"
    },
    {
      "from": "Update Step",
      "to": "Belief State Update",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization_Optimization_DRL",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "CostFunction",
      "to": "OrdinaryLeastSquares",
      "relationship": "is_special_case_of"
    },
    {
      "from": "Principal_Components_Analysis",
      "to": "PCA_Application",
      "relationship": "contains"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Kernel_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_in_ML",
      "to": "Alpha_Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Gap",
      "to": "Overfitting_Underfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Mixture of Gaussians",
      "to": "Model Parameters",
      "relationship": "has_part"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Supervised Learning",
      "relationship": "related_to"
    },
    {
      "from": "Parameter_Scaling_Invariance",
      "to": "Geometric_Margin_Definition",
      "relationship": "related_to"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "contains"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Single_Training_Example",
      "relationship": "subtopic"
    },
    {
      "from": "Model Selection",
      "to": "Regularization Techniques",
      "relationship": "depends_on"
    },
    {
      "from": "Dual Problem Formulation",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Lagrange Duality",
      "to": "Constrained Optimization Problems",
      "relationship": "applies_to"
    },
    {
      "from": "Asynchronous Updates",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "1D_Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Value_Evaluation_Procedure",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Value_Iteration_Preferrable_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Problem Formulation",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "EmpiricalRiskMinimization",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "SourceDensitySpecification",
      "to": "CDFandPDFRelationship",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Parametric Learning Algorithm",
      "relationship": "contains"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Target Vector",
      "relationship": "depends_on"
    },
    {
      "from": "RandomVariables",
      "to": "GaussianDistribution",
      "relationship": "subtopic_of"
    },
    {
      "from": "Functional Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Finite-State MDP Assumptions",
      "to": "Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Properties_of_Kernels",
      "to": "Kernel_Function_Characterization",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "GradientAscentRule",
      "relationship": "subtopic"
    },
    {
      "from": "Theorem_Jensen",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Implementation Details",
      "to": "Matrix Conversion",
      "relationship": "subtopic"
    },
    {
      "from": "Alpha_Parameters",
      "to": "Constraints_and_Limits",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "Fitted_Value_Iteration",
      "to": "Continuous_State_Space",
      "relationship": "subtopic"
    },
    {
      "from": "Multinomial_Event_Model",
      "to": "Model_Parameters",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "NonGaussianSources",
      "relationship": "has_subtopic"
    },
    {
      "from": "Primal Problem",
      "to": "Dual Problem",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "LinearModelPrediction",
      "relationship": "subtopic"
    },
    {
      "from": "Random_Features_Regression_Error_Analysis",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "contains"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Log_Probability_Gradient",
      "relationship": "depends_on"
    },
    {
      "from": "Pretraining",
      "to": "SGD_ADAM_Optimizers",
      "relationship": "uses"
    },
    {
      "from": "Policy π(s)",
      "to": "Bellman's Equation for Optimal Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayesFilter",
      "to": "SpamDetection",
      "relationship": "applied_to"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "GaussianDistribution",
      "relationship": "contains"
    },
    {
      "from": "TopKEigenvectors",
      "to": "OrthogonalBasis",
      "relationship": "subtopic"
    },
    {
      "from": "DistanceToBoundary",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linearization of Dynamics",
      "relationship": "related_to"
    },
    {
      "from": "VectorizedNotation",
      "to": "BackwardFunctionEfficiency",
      "relationship": "related_to"
    },
    {
      "from": "Chapter_16_LQR_DDP_and_LQG",
      "to": "Finite_Horizon_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogLikelihood",
      "to": "GradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "VarianceMaximization",
      "to": "LagrangeMultipliers",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Network_Input",
      "to": "Hidden_Units",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization",
      "to": "Efficiency",
      "relationship": "depends_on"
    },
    {
      "from": "Adam_Optimization_Method",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Hypothesis_Class_H",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Feature_Engineering",
      "relationship": "depends_on"
    },
    {
      "from": "FunctionalMargin",
      "to": "GeometricMargin",
      "relationship": "related_to"
    },
    {
      "from": "ReLUFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "ModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Positive Semi-Definiteness",
      "relationship": "related_to"
    },
    {
      "from": "LikelihoodFunction",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Joint Distribution Model",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Empirical_Risk_Minimization",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "Conv1DModule",
      "relationship": "contains"
    },
    {
      "from": "HypothesisFunctions",
      "to": "LogisticFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Quadratic_Regulator_(LQR)",
      "to": "Hessian_Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Feature Mapping",
      "relationship": "related_to"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Compression",
      "relationship": "subtopic"
    },
    {
      "from": "KFoldCV",
      "to": "CrossValidation",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Theory",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Markov_Decision_Processes_Finite_Horizon",
      "to": "Policy_Dynamics",
      "relationship": "contains"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Gradient Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "AssumptionsForGLMs",
      "relationship": "depends_on"
    },
    {
      "from": "VectorW",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "GDAModel",
      "to": "MultivariateNormalDistributions",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearning",
      "to": "ClassificationProblem",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "BernoulliDistribution",
      "relationship": "contains"
    },
    {
      "from": "Procedure VE",
      "to": "Option 1 Initialization",
      "relationship": "related_to"
    },
    {
      "from": "Modern_Neural_Network_Modules",
      "to": "Matrix_Multiplication_Module",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "NormalizationTechniques",
      "relationship": "contains"
    },
    {
      "from": "Zero-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "double_descent",
      "to": "learning_to_generalize",
      "relationship": "subtopic"
    },
    {
      "from": "Infinite Hypothesis Classes",
      "to": "Parameterization by Real Numbers",
      "relationship": "depends_on"
    },
    {
      "from": "BackwardPass",
      "to": "DerivativeComputation",
      "relationship": "depends_on"
    },
    {
      "from": "statistical_mechanics_of_learning",
      "to": "generalization",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Derivatives_in_Machine_Learning",
      "relationship": "contains"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-S",
      "relationship": "subtopic_of"
    },
    {
      "from": "Generative_Algorithms",
      "to": "Elephant_Model",
      "relationship": "contains"
    },
    {
      "from": "ConditionalProbabilityModeling",
      "to": "ParameterizedFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Floating_Point_Precision",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Reparametrization_Trick",
      "relationship": "uses"
    },
    {
      "from": "GaussianDistribution",
      "to": "MeanCalculation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Generalization_Error_Bound",
      "relationship": "contains"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrangian Function",
      "relationship": "defines"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "JensensInequality",
      "relationship": "has_subtopic"
    },
    {
      "from": "Scaling Ambiguity",
      "to": "Volume Change",
      "relationship": "related_to"
    },
    {
      "from": "BackpropagationProcess",
      "to": "BackwardPass",
      "relationship": "contains"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "Observation_Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Stochastic Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "FeatureSelection",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Explicit Regularization Techniques",
      "relationship": "has_subtopic"
    },
    {
      "from": "DecompositionOfMSE",
      "to": "BiasVarianceTradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Objective Function",
      "to": "Lagrangian Function (L)",
      "relationship": "related_to"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "State_Estimation",
      "relationship": "related_to"
    },
    {
      "from": "SufficientStatistic",
      "to": "ExponentialFamilyDistributions",
      "relationship": "related_to"
    },
    {
      "from": "OptimizationProcess",
      "to": "DeepLearningModelTraining",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Regression_Sample_Size_Impact",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KL_Divergence",
      "relationship": "related_to"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "Belief_State",
      "relationship": "has_subtopic"
    },
    {
      "from": "IndependenceAssumption",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "5 Kernel methods",
      "relationship": "contains"
    },
    {
      "from": "SoftplusFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation_Computation",
      "to": "Deterministic_Simulator",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "RobustnessToAssumptions",
      "relationship": "illustrates"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Discretization_Method",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "School_Quality",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Nonlinear_Model",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Learning Theory",
      "to": "Generalization Error",
      "relationship": "related_to"
    },
    {
      "from": "k-means_algorithm",
      "to": "initialization_step",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining in Computer Vision",
      "to": "Contrastive Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Original Loss J(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Dual Problem",
      "to": "Value of Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Step 1",
      "to": "Kalman Filter",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Margins in SVM",
      "relationship": "subtopic"
    },
    {
      "from": "DualFormProblem",
      "to": "LagrangianFormulation",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Margins",
      "to": "Decision Boundary",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional_Distribution",
      "to": "Logistic_Regression",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkArchitecture",
      "to": "WeightMatrices",
      "relationship": "has_subtopic"
    },
    {
      "from": "Claim811",
      "to": "DecompositionOfMSE",
      "relationship": "depends_on"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticLossFunction",
      "to": "NegativeLogLikelihood",
      "relationship": "derives_from"
    },
    {
      "from": "K-Means Clustering",
      "to": "EM-Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning_Representations",
      "to": "House_Price_Prediction",
      "relationship": "subtopic_of"
    },
    {
      "from": "LogisticRegression",
      "to": "TotalLossFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Functional Margin",
      "to": "Scaling w and b",
      "relationship": "related_to"
    },
    {
      "from": "Gradient_Ascend_Optimization",
      "to": "REINFORCE_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "PCA",
      "to": "Data_Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Runtime_Efficiency",
      "to": "Phi_Functions",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "SignChangeIrrelevance",
      "relationship": "has_subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.4 Properties of kernels",
      "relationship": "contains"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Recovering_Optimal_Policy",
      "relationship": "subtopic"
    },
    {
      "from": "HighDimensionalFeatures",
      "to": "FeatureTransformation",
      "relationship": "related_to"
    },
    {
      "from": "FeatureSelection",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Mapping",
      "to": "Kernel_Functions",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "ELBO_Evaluation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Pretraining",
      "relationship": "has_subtopic"
    },
    {
      "from": "Procedure VE",
      "to": "Option 2 Initialization",
      "relationship": "related_to"
    },
    {
      "from": "Quadratic Assumption",
      "to": "Bellman Equation",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworkArchitecture",
      "to": "BiasVectors",
      "relationship": "has_subtopic"
    },
    {
      "from": "Multidimensional Generalization",
      "to": "Hessian Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Machine_Learning_Basics",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "NeuralNetworkArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Transformer_Model",
      "to": "Conditional_Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Cross_Entropy_Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "k-means_algorithm",
      "to": "distortion_function",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "StochasticModel",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Architectures",
      "to": "Transformer_Architecture",
      "relationship": "contains"
    },
    {
      "from": "Chapter 17 Policy Gradient (REINFORCE)",
      "to": "Randomized Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Class",
      "to": "Sample Complexity Bound",
      "relationship": "related_to"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "contains"
    },
    {
      "from": "Union Bound Lemma",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "contains"
    },
    {
      "from": "Feature_Engineering",
      "to": "Linear_Models",
      "relationship": "related_to"
    },
    {
      "from": "Optimal_Parameter_Finding",
      "to": "Dual_Form_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "contains"
    },
    {
      "from": "DynamicProgrammingApplication",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Multinomial_Event_Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Linear Regression Update",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "ELBO",
      "relationship": "proves_validity_of"
    },
    {
      "from": "Batch_Normalization_Variants",
      "to": "Layer_Normalization",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningBasics",
      "to": "RandomVariables",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Data Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian_Inference",
      "to": "Posterior_Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Language_Modeling",
      "to": "Pretrained_Models",
      "relationship": "subtopic"
    },
    {
      "from": "GradientAscentRule",
      "to": "StochasticGradientAscent",
      "relationship": "contains"
    },
    {
      "from": "GradientDescentOptimizer",
      "to": "MinimumNormSolution",
      "relationship": "subtopic"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "State_Transition_Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Discrete Ricatti Equations",
      "relationship": "depends_on"
    },
    {
      "from": "KernelsInML",
      "to": "KernelFunctionExample1",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Historical Context",
      "relationship": "related_to"
    },
    {
      "from": "SingleNeuronNN",
      "to": "HousingPricePrediction",
      "relationship": "example_of"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVM)",
      "relationship": "depends_on"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "CanonicalLinkFunction",
      "relationship": "has_subnode"
    },
    {
      "from": "IdentityFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Efficiency_of_Modules",
      "relationship": "contains"
    },
    {
      "from": "OptimizationMethods",
      "to": "NewtonMethod",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "contains"
    },
    {
      "from": "SigmoidFunction",
      "to": "TanhFunction",
      "relationship": "related_to"
    },
    {
      "from": "LayerNorm",
      "to": "ScaleInvariantProperty",
      "relationship": "has_property"
    },
    {
      "from": "Model Selection",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Corollary",
      "to": "Vapnik's Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Methods",
      "to": "Perceptron_Kernel_Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous-State MDPs",
      "to": "Value Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "LMSUpdateRule",
      "to": "WidrowHoffLearningRule",
      "relationship": "related_to"
    },
    {
      "from": "In-context Learning",
      "to": "Prompt Construction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Computational Efficiency",
      "to": "Kalman Filter",
      "relationship": "subtopic"
    },
    {
      "from": "PoissonDistributionModeling",
      "to": "GeneralizedLinearModels",
      "relationship": "subtopic"
    },
    {
      "from": "Discriminative_Algorithms",
      "to": "Perceptron_Algorithm",
      "relationship": "contains"
    },
    {
      "from": "GeometricMargin",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Mercer's Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "Variance",
      "to": "Small Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation_Maximization_Guarantees",
      "to": "Machine_Learning_Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "CrossEntropyLoss",
      "relationship": "depends_on"
    },
    {
      "from": "Derivatives_in_Machine_Learning",
      "to": "Partial_Derivatives_of_Scalar_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "contains"
    },
    {
      "from": "Chapter 17 Policy Gradient (REINFORCE)",
      "to": "Expected Total Payoff Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Sparsity",
      "relationship": "subtopic"
    },
    {
      "from": "Latent Variable Models",
      "to": "Variational Inference",
      "relationship": "related_to"
    },
    {
      "from": "Bellman's Equation for Optimal Value Function",
      "to": "Optimal Value Function V*(s)",
      "relationship": "defines"
    },
    {
      "from": "VariationalInference",
      "to": "GradientComputation",
      "relationship": "related_to"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Efficient_Update",
      "relationship": "has_subtopic"
    },
    {
      "from": "Joint Distribution Model",
      "to": "Latent Variables",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "ValueIteration",
      "to": "Convergence",
      "relationship": "has_subtopic"
    },
    {
      "from": "E_Step",
      "to": "M_Step",
      "relationship": "followed_by"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.3 Discussion: GDA and logistic regression",
      "relationship": "contains"
    },
    {
      "from": "Multinomial_Event_Model",
      "to": "Parameter_Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Cross Validation",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Alpha Value Update",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "ReLUFunction",
      "to": "LeakyReLU",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "ELBO_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "Classification_Models",
      "to": "Probabilistic_Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "LogisticRegression",
      "relationship": "compared_to"
    },
    {
      "from": "ELBO_Evaluation",
      "to": "Efficient_Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "contains"
    },
    {
      "from": "Posterior_Distribution",
      "to": "Expectation_Maximization_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "3 Generalized linear models",
      "relationship": "contains"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "NaiveBayes",
      "relationship": "related_to"
    },
    {
      "from": "Predict Step",
      "to": "Kalman Filter",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Maximizing Functions",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "TransformerModel",
      "relationship": "contains"
    },
    {
      "from": "LayerNormalization",
      "to": "ScalingInvariantProperty",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "8.1 Bias-variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Kernel Matrix Definition",
      "relationship": "depends_on"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Probability_Gradient",
      "to": "Trajectory_Probability_Change",
      "relationship": "subtopic"
    },
    {
      "from": "ReLU Activation Function",
      "to": "Multi-layer Fully-Connected Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "PolicyIteration",
      "to": "BellmanEquations",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "FunctionRepresentation",
      "to": "LinearHypothesis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning with Non-Linear Models",
      "to": "Deep Learning Introduction",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Feature_Map_and_Kernel_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Modern_Neural_Network_Modules",
      "to": "MLP_Composition",
      "relationship": "has_subtopic"
    },
    {
      "from": "PointBFormula",
      "to": "DistanceToBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "ChainRule",
      "to": "BackwardFunction",
      "relationship": "defines"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Expectation_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "PCA Algorithm Introduction",
      "to": "Normalization Process",
      "relationship": "depends_on"
    },
    {
      "from": "Variational Inference",
      "to": "ELBO Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Trajectory_Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Data Redundancy Detection",
      "to": "PCA Algorithm Introduction",
      "relationship": "leads_to"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "Inverted_Pendulum_Problem",
      "relationship": "example_of"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "E-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution on Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Belief_State",
      "to": "Kalman_Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Model Parameters",
      "to": "Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "M_Step",
      "to": "Log-Likelihood",
      "relationship": "maximizes"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix P",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "NegativeLikelihoodLoss",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModelsGLMs",
      "to": "OrdinaryLeastSquaresOLS",
      "relationship": "subtopic"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Parameterized Model",
      "relationship": "subtopic"
    },
    {
      "from": "DataScarcity",
      "to": "LOOCV",
      "relationship": "related_to"
    },
    {
      "from": "Expectation_Computation",
      "to": "Gaussian_Noise_Model",
      "relationship": "related_to"
    },
    {
      "from": "Likelihood Estimation",
      "to": "Closed-Form Solution",
      "relationship": "related_to"
    },
    {
      "from": "Value_Iteration",
      "to": "Optimization_Technique",
      "relationship": "subtopic"
    },
    {
      "from": "Other Activation Functions",
      "to": "Multi-layer Fully-Connected Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Implicit Regularization Effect",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GeneralizedLinearModels",
      "relationship": "contains"
    },
    {
      "from": "Transformer_Model",
      "to": "Autoregressive_Decoding",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Bias-variance tradeoff",
      "relationship": "relates_to"
    },
    {
      "from": "Unsupervised learning",
      "to": "Self-supervised learning and foundation models",
      "relationship": "contains"
    },
    {
      "from": "GaussianDistribution",
      "to": "StandardNormalDist",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearning",
      "to": "ContrastiveLearning",
      "relationship": "has_subtopic"
    },
    {
      "from": "GDAModel",
      "to": "BernoulliDistribution",
      "relationship": "subtopic_of"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Mixing Matrix A",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMaps",
      "to": "CubicFunctions",
      "relationship": "example_of"
    },
    {
      "from": "Training Data Classification",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "SIMCLRAlgorithm",
      "to": "LossFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Unmixing Matrix W",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "MultinomialModel",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "GradientDescentAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix A",
      "relationship": "related_to"
    },
    {
      "from": "Learning from Data",
      "to": "MDP Trials",
      "relationship": "depends_on"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "contains"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.1 Logistic regression",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Modern_Neural_Network_Modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "GELUFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Error_Term",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Network_Input",
      "to": "Output_Parameterization",
      "relationship": "subtopic"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.2 Naive bayes (Option Reading)",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "PolicyIteration",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Deep_Learning_Representations",
      "to": "Feature_Discovery",
      "relationship": "subtopic_of"
    },
    {
      "from": "Gradient Computation",
      "to": "Loss Function J^(j)(θ)",
      "relationship": "applies_to"
    },
    {
      "from": "KernelsInML",
      "to": "GeneralFormOfKernels",
      "relationship": "discusses"
    },
    {
      "from": "Log-Likelihood",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference",
      "to": "Encoder-Decoder Architecture",
      "relationship": "subtopic"
    },
    {
      "from": "Linear_Functions",
      "to": "LMS_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Cross-Entropy Loss",
      "relationship": "related_to"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.2 Logistic regression",
      "relationship": "contains"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Softmax Function",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error",
      "to": "Theorem_on_Generalization_Error",
      "relationship": "defines"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "contains"
    },
    {
      "from": "ResNet",
      "to": "Batch_Normalization_Variants",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Networks",
      "to": "Activation_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "VC Dimension",
      "to": "Uniform Convergence",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "NewtonMethod",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "RobustnessToAssumptions",
      "relationship": "contains"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "BinaryFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Efficient_Computation",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianMixtureModel",
      "to": "EMAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "TrainingExamplesMatrixNotation",
      "to": "LayerActivations",
      "relationship": "depends_on"
    },
    {
      "from": "LinearRegression",
      "to": "NormalEquations",
      "relationship": "has_subtopic"
    },
    {
      "from": "ProbabilityDistribution",
      "to": "ContinuousVariables",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "contains"
    },
    {
      "from": "PolicyDefinition",
      "to": "SupervisedLearning",
      "relationship": "depends_on"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "EfficiencyComparison",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctionExample1",
      "to": "FeatureMapping",
      "relationship": "explains"
    },
    {
      "from": "Neural Networks Composition",
      "to": "Backward Functions Overview",
      "relationship": "depends_on"
    },
    {
      "from": "Adaptation",
      "to": "Labeled_Data",
      "relationship": "depends_on"
    },
    {
      "from": "W Variable",
      "to": "Backward Function",
      "relationship": "depends_on"
    },
    {
      "from": "Bayesian Classification",
      "to": "Posterior Distribution",
      "relationship": "derives_from"
    },
    {
      "from": "Linear_Models_Impairment",
      "to": "Underfitting_Linear_Models",
      "relationship": "subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "PrimalProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Value_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "DualProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "contrasts_with"
    },
    {
      "from": "Digit Recognition Example",
      "to": "Gaussian Kernel",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "related_to"
    },
    {
      "from": "8.1 Bias-variance Tradeoff",
      "to": "Training Dataset",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistribution",
      "to": "ExponentialFamilyDistributions",
      "relationship": "example_of"
    },
    {
      "from": "Regularization",
      "to": "Overfitting",
      "relationship": "addresses"
    },
    {
      "from": "Dimensionality_Reduction",
      "to": "Computational_Benefits",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "MultinomialFeatures",
      "relationship": "generalizes_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Non-Convex Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Expectation_Maximization_Algorithm",
      "to": "Evidence_Lower_Bound_(ELBO)",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "KKTConditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Test Error",
      "to": "Population Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Chain_Rule_Application",
      "relationship": "contains"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "contains"
    },
    {
      "from": "Dynamic_Programming",
      "to": "Value_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "LearningRateSchedule",
      "to": "Optimizer-Generalization",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Feature_Mapping",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "DualFormulation",
      "to": "SMOAlgorithm",
      "relationship": "leads_to"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "DeterministicModel",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter Overview",
      "to": "Forward Pass",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Regularization Techniques",
      "relationship": "related_to"
    },
    {
      "from": "OptimizationTechniques",
      "to": "CoordinateAscent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Convex Function",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDataExample",
      "to": "MixingMatrix",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Bernoulli_Random_Variables",
      "to": "Generalization_Error_Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Supervised Learning Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Optimizer Impact",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Overparameterized Models",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimization_Frameworks",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Deep_Learning_Representations",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureSelection",
      "relationship": "depends_on"
    },
    {
      "from": "Multiple_Examples",
      "to": "Q_Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter 17 Policy Gradient (REINFORCE)",
      "to": "Reward Function Querying",
      "relationship": "related_to"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "subtopic"
    },
    {
      "from": "Independence_Assumption",
      "to": "Probabilistic_Modeling",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression Setup",
      "relationship": "depends_on"
    },
    {
      "from": "Primal Problem",
      "to": "Constraints",
      "relationship": "depends_on"
    },
    {
      "from": "Implementation Details",
      "to": "Data Representation",
      "relationship": "subtopic"
    },
    {
      "from": "InitializationEffect",
      "to": "Optimizer-Generalization",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LMSAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "State-Action Value Function",
      "to": "Expected Value Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Average_Model_h_avg",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter Overview",
      "to": "Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "ELBO Lower Bound",
      "relationship": "related_to"
    },
    {
      "from": "Text_Classification",
      "to": "Feature_Vector_Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization Method",
      "to": "Curse of Dimensionality",
      "relationship": "related_to"
    },
    {
      "from": "Training_Set",
      "to": "Hypothesis_Error",
      "relationship": "depends_on"
    },
    {
      "from": "NormalEquations",
      "to": "MatrixDerivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hypothesis_Class_Parameterization",
      "to": "Linear_Classifiers",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothetical_Single_Dataset_Model",
      "to": "Average_Model_h_avg",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Markov_Decision_Processes_Finite_Horizon",
      "relationship": "contains"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GaussianDistributions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Quadratic Rewards",
      "to": "LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Baseline Function",
      "to": "Value Function Estimation",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Deep_Learning",
      "relationship": "related_to"
    },
    {
      "from": "Relationship to Logistic Regression",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "related_to"
    },
    {
      "from": "Future Discounted Rewards",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Few-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Generative_Algorithms",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Gradient_Ascend_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "Conv1D",
      "relationship": "subtopic"
    },
    {
      "from": "Activation_Functions",
      "to": "ReLU_Activation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Linear Optimal Policy",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.4 Locally weighted linear regression (optional reading)",
      "relationship": "contains"
    },
    {
      "from": "Bayesian Classification",
      "to": "Conditional Probability p(x|y)",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization",
      "to": "Model complexity",
      "relationship": "controls"
    },
    {
      "from": "1D Example",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Softmax_Cross_Entropy_Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Binary_Classification",
      "to": "Logistic_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Design Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "General Case",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman_Equation",
      "to": "Dynamic_Programming",
      "relationship": "depends_on"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Gradient_Descent",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "NonlinearFeatureMappings",
      "relationship": "subtopic"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.2 Least squares revisited",
      "relationship": "contains"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "CostFunctionJTheta",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Kernel Perceptron Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LeastSquaresRegression",
      "relationship": "subtopic"
    },
    {
      "from": "LeastSquaresCostFunction",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Principal_Components_Analysis",
      "relationship": "related_to"
    },
    {
      "from": "PCA Algorithm Introduction",
      "to": "Pilot Survey Data",
      "relationship": "contains_example"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Feature_Mapping",
      "relationship": "depends_on"
    },
    {
      "from": "Lagrangian_Methods",
      "to": "Dual_Problem_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "5th_Degree_Polynomial_Failure",
      "to": "High_Variance_Issue",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianMixtureModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GeneralizedLinearModelsGLMs",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "EmbeddingsAndRepresentations",
      "relationship": "contains"
    },
    {
      "from": "Bayesian_Inference",
      "to": "Prior_Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "MLP_Architecture",
      "to": "Nonlinear_Activation_Module",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliDistribution",
      "to": "ExponentialFamilyDistributions",
      "relationship": "example_of"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Linear_Classifiers",
      "relationship": "depends_on"
    },
    {
      "from": "BackwardFunctionEfficiency",
      "to": "ActivationFunctionsDerivatives",
      "relationship": "depends_on"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "LQR_Extension",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Functional Margin",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Vanilla_REINFORCE_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Digit Recognition Example",
      "to": "Polynomial Kernel",
      "relationship": "depends_on"
    },
    {
      "from": "Value_Function_Approximation",
      "to": "Fitted_Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "SigmoidFunction",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "Model Parameters",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticModel",
      "to": "MachineLearningOverview",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "BetaGammaParameters",
      "relationship": "subtopic_of"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ChainRule",
      "relationship": "contains"
    },
    {
      "from": "Multinomial_Event_Model",
      "to": "Probability_Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Inner_Loop_Operations",
      "relationship": "contains"
    },
    {
      "from": "Likelihood_Function",
      "to": "Log_Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GradientDescentOptimizer",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Gradient_Descent",
      "relationship": "contains"
    },
    {
      "from": "DeterministicSimulator",
      "to": "FittedValueIteration",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Networks",
      "to": "Single_Neuron_Model",
      "relationship": "subtopic"
    },
    {
      "from": "ConvergenceIssues",
      "to": "FittedValueIteration",
      "relationship": "subtopic"
    },
    {
      "from": "Non-Stationary Optimal Policy",
      "to": "Finite Horizon MDP",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Lagrangian Function",
      "relationship": "leads_to"
    },
    {
      "from": "Pretrained_Models",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Population Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Zero-shot Learning",
      "to": "Machine Learning Techniques",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation_Maximization",
      "to": "Reparameterization_Trick",
      "relationship": "subtopic_of"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.1 Feature maps",
      "relationship": "contains"
    },
    {
      "from": "PrimalProblem",
      "to": "GeneralizedLagrangian",
      "relationship": "subtopic_of"
    },
    {
      "from": "Weight Decay",
      "to": "L2 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function J(θ)",
      "relationship": "depends_on"
    },
    {
      "from": "LeakyReLU",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "has_subtopic"
    },
    {
      "from": "TanhFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Step1Estimation",
      "to": "Step2PolicyDerivation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "BiologicalInspiration",
      "to": "TwoLayerNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "Test Error Decomposition",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Convolutional_Neural_Networks",
      "relationship": "contains"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Symmetry Property",
      "relationship": "related_to"
    },
    {
      "from": "PolicyIteration",
      "to": "GreedyPolicy",
      "relationship": "results_in"
    }
  ]
}