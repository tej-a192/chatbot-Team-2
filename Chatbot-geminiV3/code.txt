`client/public/index.html`

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>

```

`client/public/robots.txt`

```
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

```

`client/src/App.css`

```css
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background-color: #f0f2f5; /* Consistent background */
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}

.App {
  text-align: center;
}

/* Add any other global styles you need */

```

`client/src/App.js`

```javascript
// client/src/App.js
import React, { useState, useEffect, Suspense } from 'react';
import { BrowserRouter as Router, Route, Routes, Navigate } from 'react-router-dom';
import { ThemeProvider, createTheme, CircularProgress } from '@mui/material';
import Notebook from './components/Notebook';

// Lazy load components to reduce initial bundle size
const AuthPage = React.lazy(() => import('./components/AuthPage'));
const ChatPage = React.lazy(() => import('./components/ChatPage'));
const NotebookPage = React.lazy(() => import('./components/Notebook'));

const darkTheme = createTheme({
  palette: {
    mode: 'dark',
    background: {
      default: '#1a1a1a',
      paper: '#2d2d2d',
    },
    primary: {
      main: '#90caf9',
    },
    text: {
      primary: '#ffffff',
      secondary: '#b3b3b3',
    }
  },
  components: {
    MuiCard: {
      styleOverrides: {
        root: {
          backgroundColor: '#2d2d2d',
          color: '#ffffff',
        }
      }
    },
    MuiPaper: {
      styleOverrides: {
        root: {
          backgroundColor: '#2d2d2d',
          color: '#ffffff',
        }
      }
    }
  }
});

const LoadingFallback = () => (
    <div style={{ 
        display: 'flex', 
        justifyContent: 'center', 
        alignItems: 'center', 
        height: '100vh',
        backgroundColor: '#121212'
    }}>
        <CircularProgress />
    </div>
);

function App() {
    const [isAuthenticated, setIsAuthenticated] = useState(!!localStorage.getItem('userId'));

    useEffect(() => {
        const handleStorageChange = (event) => {
            if (event.key === 'userId') {
                const hasUserId = !!event.newValue;
                console.log("App Storage Listener: userId changed, setting isAuthenticated to", hasUserId);
                setIsAuthenticated(hasUserId);
            }
        };

        window.addEventListener('storage', handleStorageChange);

        return () => {
            window.removeEventListener('storage', handleStorageChange);
        };
    }, []);

    return (
        <ThemeProvider theme={darkTheme}>
            <Router>
                <div style={{ 
                    minHeight: '100vh',
                    backgroundColor: '#1a1a1a',
                    color: '#ffffff',
                    padding: '20px'
                }}>
                    <Suspense fallback={<LoadingFallback />}>
                        <Routes>
                            <Route
                                path="/login"
                                element={
                                    !isAuthenticated ? (
                                        <AuthPage setIsAuthenticated={setIsAuthenticated} />
                                    ) : (
                                        <Navigate to="/chat" replace />
                                    )
                                }
                            />

                            <Route 
                                path="/notebook" 
                                element={
                                    isAuthenticated ? (
                                        <NotebookPage setIsAuthenticated={setIsAuthenticated} />
                                    ) : (
                                        <Navigate to="/login" replace />
                                    )
                                }
                            /> 

                            <Route
                                path="/chat"
                                element={
                                    isAuthenticated ? (
                                        <ChatPage setIsAuthenticated={setIsAuthenticated} />
                                    ) : (
                                        <Navigate to="/login" replace />
                                    )
                                }
                            />

                            <Route
                                path="/"
                                element={
                                    isAuthenticated ? (
                                        <Navigate to="/chat" replace />
                                    ) : (
                                        <Navigate to="/login" replace />
                                    )
                                }
                            />

                            <Route path="*" element={<Navigate to="/" replace />} />
                        </Routes>
                    </Suspense>
                </div>
            </Router>
        </ThemeProvider>
    );
}

export default App;

```

`client/src/App.test.js`

```javascript
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});

```

`client/src/components/AuthPage.js`

```javascript
// client/src/components/AuthPage.js
import React, { useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { signinUser, signupUser } from '../services/api';

const AuthPage = ({ setIsAuthenticated }) => {
    const [isLogin, setIsLogin] = useState(true);
    const [username, setUsername] = useState('');
    const [password, setPassword] = useState('');
    const [error, setError] = useState('');
    const [loading, setLoading] = useState(false);
    const navigate = useNavigate();

    const handleAuth = async (e) => {
        e.preventDefault();
        setError(''); setLoading(true);

        if (!username.trim() || !password.trim()) {
            setError('Username and password cannot be empty.');
            setLoading(false); return;
        }

        try {
            let response;
            const userData = { username, password };
            if (isLogin) {
                response = await signinUser(userData);
            } else {
                 if (password.length < 6) {
                     setError('Password must be at least 6 characters long.');
                     setLoading(false); return;
                 }
                response = await signupUser(userData);
            }

            const { sessionId, username: loggedInUsername, _id: userId } = response.data;

            if (!userId || !sessionId || !loggedInUsername) {
                 throw new Error("Incomplete authentication data received from server.");
            }

            // Store user info in localStorage
            localStorage.setItem('sessionId', sessionId);
            localStorage.setItem('username', loggedInUsername);
            localStorage.setItem('userId', userId); // Store userId

            setIsAuthenticated(true); // Update App state
            navigate('/chat', { replace: true }); // Redirect to chat page

        } catch (err) {
            const errorMessage = err.response?.data?.message || err.message || `An error occurred during ${isLogin ? 'sign in' : 'sign up'}.`;
            setError(errorMessage);
            console.error("Auth Error:", err.response || err);
            // Clear potentially invalid items on auth error
            localStorage.removeItem('sessionId');
            localStorage.removeItem('username');
            localStorage.removeItem('userId');
            setIsAuthenticated(false);
        } finally {
            setLoading(false);
        }
    };

    const toggleMode = () => {
        setIsLogin(!isLogin);
        setUsername(''); setPassword(''); setError('');
    };

    return (
        <div className="auth-container">
            <div className="auth-box">
                <h2>{isLogin ? 'Sign In' : 'Sign Up'}</h2>
                <form onSubmit={handleAuth}>
                    <div className="input-group">
                        <label htmlFor="username">Username</label>
                        <input
                            type="text" id="username" value={username}
                            onChange={(e) => setUsername(e.target.value)}
                            required autoComplete="username"
                            disabled={loading}
                        />
                    </div>
                    <div className="input-group">
                        <label htmlFor="password">Password</label>
                        <input
                            type="password" id="password" value={password}
                            onChange={(e) => setPassword(e.target.value)}
                            required autoComplete={isLogin ? "current-password" : "new-password"}
                            disabled={loading}
                        />
                    </div>
                    {error && <p className="error-message">{error}</p>}
                    <button type="submit" disabled={loading} className="auth-button">
                        {loading ? 'Processing...' : (isLogin ? 'Sign In' : 'Sign Up')}
                    </button>
                </form>
                <button onClick={toggleMode} className="toggle-button" disabled={loading}>
                    {isLogin ? 'Need an account? Sign Up' : 'Have an account? Sign In'}
                </button>
            </div>
        </div>
    );
};

// --- CSS for AuthPage (included directly) ---
const AuthPageCSS = `
.auth-container { display: flex; justify-content: center; align-items: center; min-height: 100vh; background-color: #f0f2f5; }
.auth-box { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); width: 100%; max-width: 400px; text-align: center; }
.auth-box h2 { margin-bottom: 25px; color: #333; }
.input-group { margin-bottom: 20px; text-align: left; }
.input-group label { display: block; margin-bottom: 8px; color: #555; font-weight: bold; }
.input-group input { width: 100%; padding: 12px 15px; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; font-size: 1rem; }
.input-group input:focus { outline: none; border-color: #007bff; box-shadow: 0 0 0 2px rgba(0, 123, 255, 0.25); }
.input-group input:disabled { background-color: #e9ecef; cursor: not-allowed; }
.auth-button { width: 100%; padding: 12px; background-color: #007bff; color: white; border: none; border-radius: 4px; font-size: 1rem; cursor: pointer; transition: background-color 0.3s ease; margin-top: 10px; }
.auth-button:hover:not(:disabled) { background-color: #0056b3; }
.auth-button:disabled { background-color: #cccccc; cursor: not-allowed; }
.toggle-button { background: none; border: none; color: #007bff; cursor: pointer; margin-top: 20px; font-size: 0.9rem; }
.toggle-button:hover:not(:disabled) { text-decoration: underline; }
.toggle-button:disabled { color: #999; cursor: not-allowed; }
.error-message { color: #dc3545; margin-top: 15px; margin-bottom: 0; font-size: 0.9rem; }
`;
// --- Inject CSS ---
const styleTagAuthId = 'auth-page-styles';
if (!document.getElementById(styleTagAuthId)) {
    const styleTag = document.createElement("style");
    styleTag.id = styleTagAuthId;
    styleTag.type = "text/css";
    styleTag.innerText = AuthPageCSS;
    document.head.appendChild(styleTag);
}
// --- End CSS Injection ---

export default AuthPage;

```

`client/src/components/ChatPage.css`

```css
/* client/src/components/ChatPage.css */

/* === Dark Theme Variables === */
:root {
  --bg-main: #121212; /* Very dark grey, almost black */
  --bg-sidebar: #1e1e1e; /* Slightly lighter dark grey */
  --bg-header: #1e1e1e;
  --bg-messages: #121212;
  --bg-input: #2a2a2a; /* Dark grey for inputs */
  --bg-widget: #252526; /* VSCode-like dark grey */

  --text-primary: #e0e0e0; /* Light grey for primary text */
  --text-secondary: #a0a0a0; /* Medium grey for secondary text */
  --text-link: #58a6ff; /* Light blue for links/accents */

  --border-color: #3a3a3a; /* Dark border */
  --scrollbar-thumb: #4a4a50;
  --scrollbar-track: transparent;

  --user-message-bg: #005c9d; /* Darker blue for user messages */
  --user-message-text: #ffffff;
  --model-message-bg: #333333; /* Dark grey for model messages */
  --model-message-text: #e0e0e0;

  --accent-blue: #007acc; /* Standard blue accent */
  --accent-blue-light: #3b9cff;
  --error-color: #f44747;
  --error-bg: rgba(244, 71, 71, 0.1);
  --success-color: #4caf50;
  --success-bg: rgba(76, 175, 80, 0.1);

  --code-bg: #1e1e1e;
  --code-text: #d4d4d4;
  --code-border: #3a3a3a;
}

/* === Global Styles === */
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background-color: var(--bg-main);
  color: var(--text-primary);
  height: 100vh;
  overflow: hidden; /* Prevent body scroll */
}

/* === Main Layout === */
.chat-page-container {
  display: flex;
  height: 100vh;
  background-color: var(--bg-main);
}

/* === Sidebar Area === */
.sidebar-area {
  width: 280px; /* Slightly wider */
  flex-shrink: 0;
  background-color: var(--bg-sidebar);
  border-right: 1px solid var(--border-color);
  display: flex;
  flex-direction: column;
  overflow: hidden; /* Prevent sidebar itself from scrolling */
  height: 100vh; /* Ensure full height */
}

/* Widgets within Sidebar */
.system-prompt-widget,
.file-upload-widget,
.file-manager-widget {
  background-color: var(--bg-widget); /* Use widget background */
  border-bottom: 1px solid var(--border-color);
  flex-shrink: 0; /* Prevent shrinking */
}
.system-prompt-widget { padding: 15px; }
.file-upload-widget { padding: 15px; }
.file-manager-widget {
  flex-grow: 1; /* Allow file manager to take remaining space */
  overflow: hidden; /* Contains the scrolling within its list */
  border-bottom: none; /* No border at the very bottom */
  display: flex; /* Ensure flex properties apply */
  flex-direction: column;
}


/* === Chat Container === */
.chat-container {
  display: flex;
  flex-direction: column;
  flex-grow: 1;
  background-color: var(--bg-messages);
  overflow: hidden; /* Prevent chat container scroll, manage internally */
  height: 100vh;
}

/* Chat Header */
.chat-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 12px 25px;
  background-color: var(--bg-header);
  border-bottom: 1px solid var(--border-color);
  flex-shrink: 0;
}
.chat-header h1 {
  margin: 0;
  font-size: 1.25rem;
  font-weight: 600;
  color: var(--text-primary);
}
.header-controls {
  display: flex;
  align-items: center;
  gap: 12px;
}
.username-display {
  font-size: 0.9rem;
  color: var(--text-secondary);
  white-space: nowrap;
}
.header-button {
  padding: 6px 14px;
  font-size: 0.85rem;
  font-weight: 500;
  border-radius: 5px;
  cursor: pointer;
  transition: background-color 0.2s, color 0.2s, border-color 0.2s;
  background-color: #3a3a40;
  color: var(--text-primary);
  border: 1px solid var(--border-color);
}
.header-button:hover:not(:disabled) {
  background-color: #4a4a50;
  border-color: #5a5a60;
}
.header-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}
.logout-button:hover:not(:disabled) {
  background-color: var(--error-bg);
  border-color: var(--error-color);
  color: var(--error-color);
}

/* Messages Area */
.messages-area {
  flex-grow: 1;
  overflow-y: auto; /* Enable scrolling ONLY for messages */
  padding: 25px;
  display: flex;
  flex-direction: column;
  gap: 20px;
  scrollbar-width: thin;
  scrollbar-color: var(--scrollbar-thumb) var(--scrollbar-track);
}
.messages-area::-webkit-scrollbar { width: 8px; }
.messages-area::-webkit-scrollbar-track { background: var(--scrollbar-track); }
.messages-area::-webkit-scrollbar-thumb { background-color: var(--scrollbar-thumb); border-radius: 4px; }

/* Individual Message Styling */
.message {
  display: flex;
  max-width: 80%; /* Max width of a message bubble */
  position: relative;
  word-wrap: break-word;
  flex-direction: column; /* Stack content and timestamp */
}
.message.user {
  align-self: flex-end;
  align-items: flex-end; /* Align timestamp to the right */
}
.message.model {
  align-self: flex-start;
  align-items: flex-start; /* Align timestamp to the left */
}
.message-content {
  padding: 12px 18px;
  border-radius: 18px;
  font-size: 0.95rem;
  line-height: 1.6;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
  text-align: left; /* Ensure text aligns left within bubble */
}
.message.user .message-content {
  background-color: var(--user-message-bg);
  color: var(--user-message-text);
  border-bottom-right-radius: 5px; /* Speech bubble tail */
}
.message.model .message-content {
  background-color: var(--model-message-bg);
  color: var(--model-message-text);
  border-bottom-left-radius: 5px; /* Speech bubble tail */
}
/* Markdown Styles within messages */
.message-content p { margin: 0 0 0.6em 0; }
.message-content p:last-child { margin-bottom: 0; }
.message-content strong { font-weight: 600; }
.message-content em { font-style: italic; }
.message-content ul, .message-content ol { padding-left: 25px; margin: 0.5em 0; }
.message-content li { margin-bottom: 0.3em; }
.message-content a { color: var(--text-link); text-decoration: none; }
.message-content a:hover { text-decoration: underline; }
.message-content pre {
  background-color: var(--code-bg);
  border: 1px solid var(--code-border);
  border-radius: 6px;
  padding: 12px 15px;
  overflow-x: auto;
  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
  font-size: 0.9rem;
  margin: 1em 0;
  white-space: pre;
  color: var(--code-text);
  scrollbar-width: thin;
  scrollbar-color: var(--scrollbar-thumb) var(--code-bg);
}
.message-content pre::-webkit-scrollbar { height: 6px; }
.message-content pre::-webkit-scrollbar-track { background: var(--code-bg); }
.message-content pre::-webkit-scrollbar-thumb { background-color: var(--scrollbar-thumb); border-radius: 3px; }
.message-content code { /* Inline code */
  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
  font-size: 0.88rem;
  background-color: rgba(255, 255, 255, 0.08);
  padding: 0.2em 0.5em;
  border-radius: 4px;
  border: 1px solid var(--border-color);
  color: var(--text-secondary);
}
.message-content pre code { /* Code within pre block */
  background-color: transparent;
  padding: 0;
  border: none;
  font-size: inherit;
  color: inherit;
}
/* Citation Style */
.message-content em:has(> span.citation-ref) {
   font-style: normal;
   display: block;
   margin-top: 10px;
   font-size: 0.8rem;
   color: var(--text-secondary);
}
.citation-ref {
   /* Add specific styles if needed */
}


.message-timestamp {
  font-size: 0.7rem;
  color: var(--text-secondary);
  margin-top: 6px;
  padding: 0 5px; /* Add slight padding */
}

/* Loading/Error Indicators */
.loading-indicator, .error-indicator {
  text-align: center;
  padding: 10px 20px;
  font-size: 0.85rem;
  font-style: italic;
  color: var(--text-secondary);
  flex-shrink: 0; /* Prevent shrinking */
  margin: 5px 20px; /* Add margin */
}
.error-indicator {
  color: var(--error-color);
  background-color: var(--error-bg);
  border: 1px solid var(--error-color);
  border-radius: 4px;
  font-style: normal;
}
.message-error { /* Style for message rendering errors */
  color: var(--error-color);
  font-style: italic;
  text-align: center;
  padding: 5px;
  font-size: 0.8rem;
}

/* Input Area */
.input-area {
  display: flex;
  align-items: center; /* Align items vertically */
  padding: 12px 20px; /* Reduced padding slightly */
  border-top: 1px solid var(--border-color);
  background-color: var(--bg-header); /* Match header background */
  flex-shrink: 0;
  gap: 10px; /* Space between elements */
}

.input-area textarea {
  flex-grow: 1;
  padding: 10px 18px;
  border: 1px solid var(--border-color);
  border-radius: 20px; /* Pill shape */
  resize: none;
  font-size: 0.95rem;
  line-height: 1.5;
  max-height: 120px; /* Limit height */
  overflow-y: auto; /* Scroll if needed */
  box-sizing: border-box;
  font-family: inherit;
  background-color: var(--bg-input);
  color: var(--text-primary);
  scrollbar-width: thin;
  scrollbar-color: var(--scrollbar-thumb) var(--scrollbar-track);
}
.input-area textarea::-webkit-scrollbar { width: 6px; }
.input-area textarea::-webkit-scrollbar-track { background: var(--scrollbar-track); }
.input-area textarea::-webkit-scrollbar-thumb { background-color: var(--scrollbar-thumb); border-radius: 3px; }
.input-area textarea::placeholder { color: var(--text-secondary); opacity: 0.7; }
.input-area textarea:focus {
  outline: none;
  border-color: var(--accent-blue);
  background-color: var(--bg-input); /* Keep bg same on focus */
  box-shadow: 0 0 0 2px rgba(0, 123, 255, 0.3);
}
.input-area textarea:disabled {
  background-color: #2a2a30;
  opacity: 0.6;
  cursor: not-allowed;
}

/* RAG Toggle Styles */
.rag-toggle-container {
  display: flex;
  align-items: center;
  gap: 6px; /* Space between checkbox and label */
  color: var(--text-secondary);
  cursor: pointer;
  padding: 5px 8px;
  border-radius: 4px;
  white-space: nowrap; /* Prevent "RAG" label wrapping */
  flex-shrink: 0; /* Prevent toggle shrinking */
  transition: background-color 0.2s ease;
  user-select: none;
}
.rag-toggle-container:has(input:not(:disabled)):hover {
   background-color: rgba(255, 255, 255, 0.05);
}
.rag-toggle-container input[type="checkbox"] {
  cursor: pointer;
  width: 16px;
  height: 16px;
  accent-color: var(--accent-blue); /* Use theme color for checkmark */
  margin: 0; /* Remove default margins */
  vertical-align: middle; /* Align checkbox better */
}
.rag-toggle-container label {
  font-size: 0.85rem;
  cursor: pointer;
  line-height: 1; /* Ensure label aligns well */
}
/* Styles for disabled state */
.rag-toggle-container input[type="checkbox"]:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}
.rag-toggle-container input[type="checkbox"]:disabled + label {
  cursor: not-allowed;
  opacity: 0.6;
  color: #666; /* Muted color when disabled */
}

/* Send Button */
.input-area button {
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 8px;
  width: 40px; /* Fixed size */
  height: 40px; /* Fixed size */
  background-color: var(--accent-blue);
  color: white;
  border: none;
  border-radius: 50%; /* Circle */
  cursor: pointer;
  transition: background-color 0.2s ease;
  flex-shrink: 0;
}
.input-area button:hover:not(:disabled) {
  background-color: var(--accent-blue-light);
}
.input-area button:disabled {
  background-color: #3a3a40; /* Use a dark disabled color */
  cursor: not-allowed;
  opacity: 0.7;
}
.input-area button svg {
  width: 20px;
  height: 20px;
}


/* --- Responsive Adjustments --- */
@media (max-width: 900px) {
    .sidebar-area { display: none; } /* Hide sidebar */
    .chat-container { border-left: none; }
}
@media (max-width: 600px) {
     .chat-header { padding: 10px 15px; }
     .chat-header h1 { font-size: 1.1rem; }
     .header-controls { gap: 8px; }
     .header-button { padding: 5px 10px; font-size: 0.8rem; }
     .username-display { display: none; } /* Hide username on small screens */

     .messages-area { padding: 15px; gap: 15px; }
     .message { max-width: 90%; }
     .message-content { padding: 10px 15px; font-size: 0.9rem; }

     .input-area { padding: 10px 12px; gap: 8px; }
     .input-area textarea { font-size: 0.9rem; padding: 8px 15px; }

     /* Adjust toggle for mobile */
     .rag-toggle-container { padding: 4px 6px; gap: 4px;}
     .rag-toggle-container label { font-size: 0.8rem; }
     .rag-toggle-container input[type="checkbox"] { width: 14px; height: 14px; }

     .input-area button { width: 36px; height: 36px; padding: 6px; }
     .input-area button svg { width: 18px; height: 18px; }

}

```

`client/src/components/ChatPage.js`

```javascript
// client/src/components/ChatPage.js
// Changes made lines - 303(Notebook button)
import React, { useState, useEffect, useRef, useCallback } from 'react';
import { useNavigate } from 'react-router-dom';
import { sendMessage, saveChatHistory, getUserFiles, queryRagService } from '../services/api';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import { v4 as uuidv4 } from 'uuid';

import SystemPromptWidget, { availablePrompts, getPromptTextById } from './SystemPromptWidget';
import HistoryModal from './HistoryModal';
import FileUploadWidget from './FileUploadWidget';
import FileManagerWidget from './FileManagerWidget';

import './ChatPage.css';

const ChatPage = ({ setIsAuthenticated }) => {
    const [messages, setMessages] = useState([]);
    const [inputText, setInputText] = useState('');
    const [isLoading, setIsLoading] = useState(false);
    const [isRagLoading, setIsRagLoading] = useState(false);
    const [error, setError] = useState('');
    const [sessionId, setSessionId] = useState(''); // Initialize empty, set from localStorage
    const [userId, setUserId] = useState(''); // Initialize empty, set from localStorage
    const [username, setUsername] = useState(''); // Initialize empty, set from localStorage
    const [currentSystemPromptId, setCurrentSystemPromptId] = useState('friendly');
    const [editableSystemPromptText, setEditableSystemPromptText] = useState(() => getPromptTextById('friendly'));
    const [isHistoryModalOpen, setIsHistoryModalOpen] = useState(false);
    const [fileRefreshTrigger, setFileRefreshTrigger] = useState(0);
    const [hasFiles, setHasFiles] = useState(false);
    const [isRagEnabled, setIsRagEnabled] = useState(false);

    const messagesEndRef = useRef(null);
    const navigate = useNavigate();

    // --- Effects ---
    useEffect(() => { messagesEndRef.current?.scrollIntoView({ behavior: "smooth" }); }, [messages]);

    // Validate auth info on mount
    useEffect(() => {
        const storedSessionId = localStorage.getItem('sessionId');
        const storedUserId = localStorage.getItem('userId');
        const storedUsername = localStorage.getItem('username');

        if (!storedUserId || !storedSessionId || !storedUsername) {
            console.warn("ChatPage Mount: Missing auth info in localStorage. Redirecting to login.");
            handleLogout(true); // Use logout function for cleanup
        } else {
            console.log("ChatPage Mount: Auth info found. Setting state.");
            setSessionId(storedSessionId);
            setUserId(storedUserId);
            setUsername(storedUsername);
        }
    // eslint-disable-next-line react-hooks/exhaustive-deps
    }, []); // Run ONCE on mount

    // Check for user files on mount and refresh, only if userId is set
    useEffect(() => {
        const checkUserFiles = async () => {
            // Ensure userId is available before fetching
            const currentUserId = localStorage.getItem('userId');
            if (!currentUserId) {
                console.log("User files check skipped: No userId available.");
                setHasFiles(false);
                setIsRagEnabled(false);
                return;
            }
            console.log("Checking user files for userId:", currentUserId);
            try {
                const response = await getUserFiles(); // API interceptor adds the header
                const filesExist = response.data && response.data.length > 0;
                setHasFiles(filesExist);
                setIsRagEnabled(filesExist); // Enable RAG by default if files exist
                console.log("User files check successful:", filesExist ? `${response.data.length} files found. RAG default: ${filesExist}` : "No files found. RAG default: false");
            } catch (err) {
                console.error("Error checking user files:", err);
                if (err.response?.status === 401 && !window.location.pathname.includes('/login')) {
                     console.warn("Received 401 checking files, logging out.");
                     handleLogout(true);
                } else {
                    setError("Could not check user files.");
                    setHasFiles(false);
                    setIsRagEnabled(false);
                }
            }
        };

        // Only run if userId is set (from the initial mount effect)
        if (userId) {
            checkUserFiles();
        }
    // eslint-disable-next-line react-hooks/exhaustive-deps
    }, [userId, fileRefreshTrigger]); // Re-check when userId is set or file list might change

    // --- Callback Definitions ---
    const triggerFileRefresh = useCallback(() => setFileRefreshTrigger(prev => prev + 1), []);
    const handlePromptSelectChange = useCallback((newId) => {
        setCurrentSystemPromptId(newId); setEditableSystemPromptText(getPromptTextById(newId));
        setError(prev => prev && (prev.includes("Session invalid") || prev.includes("Critical Error")) ? prev : `Assistant mode changed.`);
        setTimeout(() => { setError(prev => prev === `Assistant mode changed.` ? '' : prev); }, 3000);
    }, []);
    const handlePromptTextChange = useCallback((newText) => {
        setEditableSystemPromptText(newText);
        const matchingPreset = availablePrompts.find(p => p.id !== 'custom' && p.prompt === newText);
        setCurrentSystemPromptId(matchingPreset ? matchingPreset.id : 'custom');
    }, []);
    const handleHistory = useCallback(() => setIsHistoryModalOpen(true), []);
    const closeHistoryModal = useCallback(() => setIsHistoryModalOpen(false), []);

    const saveAndReset = useCallback(async (isLoggingOut = false, onCompleteCallback = null) => {
        const currentSessionId = localStorage.getItem('sessionId'); // Get fresh ID
        const currentUserId = localStorage.getItem('userId'); // Get fresh ID
        const messagesToSave = [...messages];

        if (!currentSessionId || !currentUserId) {
             console.error("Save Error: Session ID or User ID missing.");
             setError("Critical Error: Session info missing.");
             if (onCompleteCallback) onCompleteCallback();
             return;
        }
        if (isLoading || isRagLoading || messagesToSave.length === 0) {
             if (onCompleteCallback) onCompleteCallback();
             return;
        }

        let newSessionId = null;
        setIsLoading(true);
        setError(prev => prev && (prev.includes("Session invalid") || prev.includes("Critical Error")) ? prev : '');

        try {
            console.log(`Saving history for session: ${currentSessionId} (User: ${currentUserId})`);
            // API interceptor will add the x-user-id header
            const response = await saveChatHistory({ sessionId: currentSessionId, messages: messagesToSave });
            newSessionId = response.data.newSessionId;
            if (!newSessionId) throw new Error("Backend failed to provide new session ID.");

            localStorage.setItem('sessionId', newSessionId);
            setSessionId(newSessionId);
            setMessages([]);
            if (!isLoggingOut) {
                handlePromptSelectChange('friendly');
                setError('');
            }
            console.log(`History saved. New session ID: ${newSessionId}`);

        } catch (err) {
            const failErrorMsg = err.response?.data?.message || err.message || 'Failed to save/reset session.';
            console.error("Save/Reset Error:", err.response || err);
            setError(`Session Error: ${failErrorMsg}`);
            if (err.response?.status === 401 && !isLoggingOut) {
                 console.warn("Received 401 saving history, logging out.");
                 handleLogout(true); // Force logout if save fails due to auth
                 // Don't proceed with client-side session generation if auth failed
            } else if (!newSessionId && !isLoggingOut) {
                 newSessionId = uuidv4();
                 localStorage.setItem('sessionId', newSessionId);
                 setSessionId(newSessionId);
                 setMessages([]);
                 handlePromptSelectChange('friendly');
                 console.warn("Save failed, generated new client-side session ID:", newSessionId);
            } else if (isLoggingOut && !newSessionId) {
                 console.error("Save failed during logout.");
            }
        } finally {
            setIsLoading(false);
            if (onCompleteCallback) onCompleteCallback();
        }
    }, [messages, isLoading, isRagLoading, handlePromptSelectChange]); // Removed userId/sessionId state deps, use localStorage directly

    const handleLogout = useCallback((skipSave = false) => {
        const performCleanup = () => {
            console.log("Performing logout cleanup...");
            localStorage.clear(); // Clear everything
            setIsAuthenticated(false);
            setMessages([]); setSessionId(''); setUserId(''); setUsername(''); // Clear state
            setCurrentSystemPromptId('friendly');
            setEditableSystemPromptText(getPromptTextById('friendly')); setError('');
            setHasFiles(false); setIsRagEnabled(false);
            // Ensure navigation happens after state updates
            requestAnimationFrame(() => {
                 if (window.location.pathname !== '/login') {
                     navigate('/login', { replace: true });
                 }
            });
        };
        if (!skipSave && messages.length > 0) {
             saveAndReset(true, performCleanup);
        } else {
             performCleanup();
        }
    // eslint-disable-next-line react-hooks/exhaustive-deps
    }, [navigate, setIsAuthenticated, saveAndReset, messages.length]);

    const handleNewChat = useCallback(() => {
        if (!isLoading && !isRagLoading) {
             saveAndReset(false);
        }
     }, [isLoading, isRagLoading, saveAndReset]);

    const handleSendMessage = useCallback(async (e) => {
    if (e) e.preventDefault();
    const textToSend = inputText.trim();
    const currentSessionId = localStorage.getItem('sessionId');
    const currentUserId = localStorage.getItem('userId');

    // ... (existing validation) ...

    const newUserMessage = { role: 'user', parts: [{ text: textToSend }], timestamp: new Date() };
    const previousMessages = messages;
    setMessages(prev => [...prev, newUserMessage]);
    setInputText('');
    setError('');

    let ragDataForMessage = {
        relevantDocs: [],
        knowledge_graphs: null // Initialize knowledge_graphs for the message payload
    };

    if (isRagEnabled) {
        setIsRagLoading(true);
        try {
            console.log("RAG Enabled: Querying backend /api/chat/rag endpoint...");
            // Fetch RAG data (Qdrant docs + KGs)
            // The filter for RAG query can be constructed based on current context if needed.
            // For simplicity, let's assume a general query or a filter you might already have.
            const ragApiResponse = await queryRagService({ message: textToSend /*, filter: yourOptionalFilter */ });
            
            ragDataForMessage.relevantDocs = ragApiResponse.data.relevantDocs || [];
            ragDataForMessage.knowledge_graphs = ragApiResponse.data.knowledge_graphs || null; // Store the fetched KGs
            
            console.log(`RAG Query returned ${ragDataForMessage.relevantDocs.length} text documents and KGs for ${Object.keys(ragDataForMessage.knowledge_graphs || {}).length} docs.`);
            // You might want to set currentKnowledgeGraphs state here if needed for UI display
            // setCurrentKnowledgeGraphs(ragDataForMessage.knowledge_graphs);

        } catch (err) {
            // ... (existing RAG error handling) ...
            console.error("RAG Query Error (ChatPage):", err.response || err);
            const ragErrorMessage = err.response?.data?.message || "Failed to retrieve documents/KGs for RAG.";
            setError(prev => prev ? `${prev} | RAG Error: ${ragErrorMessage}` : `RAG Error: ${ragErrorMessage}`);
            if (err.response?.status === 401) {
                 handleLogout(true);
                 setIsRagLoading(false);
                 return;
            }
        } finally {
            setIsRagLoading(false);
        }
    } else {
        console.log("RAG Disabled: Skipping RAG query.");
    }

    setIsLoading(true);
    const historyForAPI = previousMessages;
    const systemPromptToSend = editableSystemPromptText;

    try {
        // If there was a RAG error that didn't lead to a return, it might still be in the error state
        // You might want to clear non-critical RAG errors or show them differently

        console.log(`Sending message to backend /api/chat/message. RAG Enabled: ${isRagEnabled}, Docs Count: ${ragDataForMessage.relevantDocs.length}`);
        
        const sendMessagePayload = {
            message: textToSend,
            history: historyForAPI,
            sessionId: currentSessionId,
            systemPrompt: systemPromptToSend,
            isRagEnabled: isRagEnabled,
            relevantDocs: ragDataForMessage.relevantDocs, // Text documents
            knowledge_graphs: isRagEnabled ? ragDataForMessage.knowledge_graphs : null // <<< SEND KG DATA
        };

        const sendMessageResponse = await sendMessage(sendMessagePayload); // sendMessage calls /api/chat/message

        // ... (rest of your existing success/error handling for sendMessageResponse) ...
        const modelReply = sendMessageResponse.data.reply;
        if (modelReply?.role && modelReply?.parts?.length > 0) {
            setMessages(prev => [...prev, modelReply]);
            // Clear RAG-specific errors if the main chat message was successful
            setError(prev => (prev && (prev.includes("Session invalid") || prev.includes("Critical Error"))) ? prev : '');
        } else {
            throw new Error("Invalid reply structure received from backend.");
        }

    } catch (err) {
        // ... (existing error handling for send message) ...
        const errorMessage = err.response?.data?.message || err.message || 'Failed to get response.';
        setError(prev => prev ? `${prev} | Chat Error: ${errorMessage}` : `Chat Error: ${errorMessage}`);
        console.error("Send Message Error (ChatPage):", err.response || err);
        setMessages(previousMessages); // Rollback UI
        if (err.response?.status === 401 && !window.location.pathname.includes('/login')) {
             handleLogout(true);
        }
    } finally {
        setIsLoading(false);
    }
// eslint-disable-next-line react-hooks/exhaustive-deps
}, [inputText, isLoading, isRagLoading, messages, editableSystemPromptText, isRagEnabled, handleLogout, /* currentKnowledgeGraphs - if used directly */]);
     // Removed sessionId/userId state deps

    // const handleSendMessage = useCallback(async (e) => {
    //     if (e) e.preventDefault();
    //     const textToSend = inputText.trim();
    //     const currentSessionId = localStorage.getItem('sessionId'); // Get fresh ID
    //     const currentUserId = localStorage.getItem('userId'); // Get fresh ID

    //     if (!textToSend || isLoading || isRagLoading || !currentSessionId || !currentUserId) {
    //         if (!currentSessionId || !currentUserId) {
    //              setError("Session invalid. Please refresh or log in again.");
    //              // Optionally trigger logout if auth info is missing
    //              if (!currentUserId) handleLogout(true);
    //         }
    //         return;
    //     }

    //     const newUserMessage = { role: 'user', parts: [{ text: textToSend }], timestamp: new Date() };
    //     const previousMessages = messages;
    //     setMessages(prev => [...prev, newUserMessage]);
    //     setInputText('');
    //     setError('');

    //     let relevantDocs = [];
    //     let ragError = null;

    //     if (isRagEnabled) {
    //         setIsRagLoading(true);
    //         try {
    //             console.log("RAG Enabled: Querying backend /rag endpoint...");
    //             // Interceptor adds user ID header
    //             const ragResponse = await queryRagService({ message: textToSend });
    //             relevantDocs = ragResponse.data.relevantDocs || [];
    //             console.log(`RAG Query returned ${relevantDocs.length} documents.`);
    //         } catch (err) {
    //             console.error("RAG Query Error:", err.response || err);
    //             ragError = err.response?.data?.message || "Failed to retrieve documents for RAG.";
    //             if (err.response?.status === 401) {
    //                  console.warn("Received 401 during RAG query, logging out.");
    //                  handleLogout(true);
    //                  setIsRagLoading(false); // Stop loading before returning
    //                  return; // Stop processing if auth failed
    //             }
    //         } finally {
    //             setIsRagLoading(false);
    //         }
    //     } else {
    //         console.log("RAG Disabled: Skipping RAG query.");
    //     }

    //     setIsLoading(true);
    //     const historyForAPI = previousMessages;
    //     const systemPromptToSend = editableSystemPromptText;

    //     try {
    //         if (ragError) {
    //              setError(prev => prev ? `${prev} | RAG Error: ${ragError}` : `RAG Error: ${ragError}`);
    //         }

    //         console.log(`Sending message to backend /message. RAG Enabled: ${isRagEnabled}, Docs Found: ${relevantDocs.length}`);
    //         // Interceptor adds user ID header
    //         const sendMessageResponse = await sendMessage({
    //             message: textToSend,
    //             history: historyForAPI,
    //             sessionId: currentSessionId,
    //             systemPrompt: systemPromptToSend,
    //             isRagEnabled: isRagEnabled,
    //             relevantDocs: relevantDocs
    //         });

    //         const modelReply = sendMessageResponse.data.reply;
    //         if (modelReply?.role && modelReply?.parts?.length > 0) {
    //             setMessages(prev => [...prev, modelReply]);
    //         } else {
    //             throw new Error("Invalid reply structure received from backend.");
    //         }
    //         setError(prev => prev && (prev.includes("Session invalid") || prev.includes("Critical Error")) ? prev : '');

    //     } catch (err) {
    //         const errorMessage = err.response?.data?.message || err.message || 'Failed to get response.';
    //         setError(prev => prev ? `${prev} | Chat Error: ${errorMessage}` : `Chat Error: ${errorMessage}`);
    //         console.error("Send Message Error:", err.response || err);
    //         setMessages(previousMessages); // Rollback UI
    //         if (err.response?.status === 401 && !window.location.pathname.includes('/login')) {
    //              console.warn("Received 401 sending message, logging out.");
    //              handleLogout(true);
    //         }
    //     } finally {
    //         setIsLoading(false);
    //     }
    // }, [inputText, isLoading, isRagLoading, messages, editableSystemPromptText, isRagEnabled, handleLogout]); // Removed sessionId/userId state deps

    const handleEnterKey = useCallback((e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSendMessage();
        }
    }, [handleSendMessage]);

    const handleRagToggle = (event) => {
        setIsRagEnabled(event.target.checked);
    };

    const isProcessing = isLoading || isRagLoading;

    // Render loading or null if userId isn't set yet
    if (!userId) {
        return <div className="loading-indicator"><span>Initializing...</span></div>; // Or some other placeholder
    }

    return (
        <div className="chat-page-container">
            <div className="sidebar-area">
                 <SystemPromptWidget
                    selectedPromptId={currentSystemPromptId} promptText={editableSystemPromptText}
                    onSelectChange={handlePromptSelectChange} onTextChange={handlePromptTextChange}
                 />
                <FileUploadWidget onUploadSuccess={triggerFileRefresh} />
                <FileManagerWidget refreshTrigger={fileRefreshTrigger} />
            </div>

            <div className="chat-container">
                 <header className="chat-header">
                    <h1>Engineering Tutor</h1>
                    <div className="header-controls">
                        <span className="username-display">Hi, {username}!</span>
                        <button onClick={() => navigate('/notebook')} className="header-button notebook-button" disabled={isProcessing}>Open Notebook</button> {/* Added to navigate to Notebook */}
                        <button onClick={handleHistory} className="header-button history-button" disabled={isProcessing}>History</button>
                        <button onClick={handleNewChat} className="header-button newchat-button" disabled={isProcessing}>New Chat</button>
                        <button onClick={() => handleLogout(false)} className="header-button logout-button" disabled={isProcessing}>Logout</button>
                    </div>
                </header>

                 <div className="messages-area">
                    {messages.map((msg, index) => {
                         if (!msg?.role || !msg?.parts?.length || !msg.timestamp) {
                            console.warn("Rendering invalid message structure at index", index, msg);
                            return <div key={`error-${index}`} className="message-error">Msg Error</div>;
                         }
                         const messageText = msg.parts[0]?.text || '';
                         return (
                            <div key={`${sessionId}-${index}`} className={`message ${msg.role}`}>
                                <div className="message-content">
                                    <ReactMarkdown remarkPlugins={[remarkGfm]}>
                                        {messageText}
                                    </ReactMarkdown>
                                </div>
                                <span className="message-timestamp">
                                    {new Date(msg.timestamp).toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}
                                </span>
                            </div>
                         );
                    })}
                    <div ref={messagesEndRef} />
                 </div>

                {isProcessing && <div className="loading-indicator"><span>{isRagLoading ? 'Searching documents...' : 'Thinking...'}</span></div>}
                {!isProcessing && error && <div className="error-indicator">{error}</div>}

                <footer className="input-area">
                    <textarea
                        value={inputText} onChange={(e) => setInputText(e.target.value)} onKeyDown={handleEnterKey}
                        placeholder="Ask your tutor..." rows="1" disabled={isProcessing} aria-label="Chat input"
                    />
                    <div className="rag-toggle-container" title={!hasFiles ? "Upload files to enable RAG" : (isRagEnabled ? "Disable RAG (Retrieval-Augmented Generation)" : "Enable RAG (Retrieval-Augmented Generation)")}>
                        <input type="checkbox" id="rag-toggle" checked={isRagEnabled} onChange={handleRagToggle}
                               disabled={!hasFiles || isProcessing} aria-label="Enable RAG" />
                        <label htmlFor="rag-toggle">RAG</label>
                    </div>
                    <button onClick={handleSendMessage} disabled={isProcessing || !inputText.trim()} title="Send Message" aria-label="Send message">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" width="20" height="20">
                            <path d="M3.478 2.405a.75.75 0 00-.926.94l2.432 7.905H13.5a.75.75 0 010 1.5H4.984l-2.432 7.905a.75.75 0 00.926.94 60.519 60.519 0 0018.445-8.986.75.75 0 000-1.218A60.517 60.517 0 003.478 2.405z" />
                        </svg>
                    </button>
                </footer>
            </div>

            <HistoryModal isOpen={isHistoryModalOpen} onClose={closeHistoryModal} />

        </div>
    );
};

export default ChatPage;

```

`client/src/components/FileManagerWidget.js`

```javascript
// client/src/components/FileManagerWidget.js
import React, { useState, useEffect, useCallback } from 'react';
import { getUserFiles, renameUserFile, deleteUserFile } from '../services/api';

const getFileIcon = (type) => {
  switch (type) {
    case 'docs': return '';
    case 'images': return '';
    case 'code': return '';
    default: return '';
  }
};

const formatFileSize = (bytes) => {
  if (bytes === 0) return '0 Bytes';
  const k = 1024;
  const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];
  if (typeof bytes !== 'number' || bytes < 0) return 'N/A';
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  const index = Math.max(0, Math.min(i, sizes.length - 1));
  return parseFloat((bytes / Math.pow(k, index)).toFixed(1)) + ' ' + sizes[index];
};


const FileManagerWidget = ({ refreshTrigger }) => {
  const [userFiles, setUserFiles] = useState([]);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState('');
  const [renamingFile, setRenamingFile] = useState(null);
  const [newName, setNewName] = useState('');
  const [statusMessage, setStatusMessage] = useState(''); // <<< ADDED: For success/general status

  const fetchUserFiles = useCallback(async () => {
    const currentUserId = localStorage.getItem('userId');
    if (!currentUserId) {
        console.log("FileManager: Skipping fetch, no userId.");
        setUserFiles([]);
        return;
    }
    setIsLoading(true);
    setError('');
    setStatusMessage(''); // Clear status message on fetch
    try {
      const response = await getUserFiles();
      setUserFiles(response.data || []);
    } catch (err) {
      console.error("Error fetching user files:", err);
      setError(err.response?.data?.message || 'Failed to load files.');
      setUserFiles([]);
      if (err.response?.status === 401) {
          console.warn("FileManager: Received 401, potential logout needed.");
      }
    } finally {
      setIsLoading(false);
    }
  }, []);

  useEffect(() => {
    fetchUserFiles();
  }, [refreshTrigger, fetchUserFiles]);

  const handleRenameClick = (file) => {
    setRenamingFile(file.serverFilename);
    setNewName(file.originalName);
    setError('');
    setStatusMessage(''); // Clear status message
  };

  const handleRenameCancel = () => {
    setRenamingFile(null);
    setNewName('');
    setError('');
    setStatusMessage(''); // Clear status message
  };

  const handleRenameSave = async () => {
    if (!renamingFile || !newName.trim()) {
         setError('New name cannot be empty.');
         return;
    }
    if (newName.includes('/') || newName.includes('\\')) {
        setError('New name cannot contain slashes.');
        return;
    }
    setIsLoading(true);
    setError('');
    setStatusMessage(''); // Clear status message
    try {
      await renameUserFile(renamingFile, newName.trim());
      setRenamingFile(null);
      setNewName('');
      fetchUserFiles(); // Refreshes and clears messages
      setStatusMessage('File renamed successfully!'); // <<< ADDED: Set success message
      setTimeout(() => setStatusMessage(''), 5000); // Clear after 5 seconds
    } catch (err) {
      console.error("Error renaming file:", err);
      setError(err.response?.data?.message || 'Failed to rename file.');
       if (err.response?.status === 401) {
          console.warn("FileManager: Received 401 during rename.");
      }
    } finally {
       setIsLoading(false);
    }
  };

  const handleRenameInputKeyDown = (e) => {
      if (e.key === 'Enter') {
          handleRenameSave();
      } else if (e.key === 'Escape') {
          handleRenameCancel();
      }
  };

  const handleDeleteFile = async (serverFilename, originalName) => {
    // <<< MODIFIED: Updated confirmation message
    if (!window.confirm(`Are you sure you want to delete "${originalName}"? This will remove the file and all associated data (analysis, graph, vector embeddings). This cannot be undone.`)) {
      return;
    }

    setIsLoading(true);
    setError('');
    setStatusMessage(''); // <<< ADDED: Clear previous messages

    try {
      // <<< MODIFIED: deleteUserFile should return the full Axios response
      const response = await deleteUserFile(serverFilename);

      // <<< MODIFIED: Handle more informative backend responses
      if (response.status === 200 || response.status === 207) {
        setStatusMessage(response.data.message || `Deletion process for '${originalName}' completed.`);
        console.log("Deletion API Response:", response.data);
        fetchUserFiles(); // Refresh the file list
      } else {
        // This case might not be hit if axios throws for non-2xx, but good for robustness
        setError(response.data.message || 'File deletion failed with an unexpected status.');
      }
    } catch (err) {
      console.error("Error deleting file:", err.response || err);
      // <<< MODIFIED: Extract error message from backend response
      const apiErrorMessage = err.response?.data?.message || 'Failed to delete file. Please try again.';
      setError(apiErrorMessage);

      if (err.response?.data?.details) { // Log backend details if available
          console.log("Deletion attempt details from backend:", err.response.data.details);
      }

       if (err.response?.status === 401) {
          console.warn("FileManager: Received 401 during delete.");
          // Consider triggering logout or displaying specific auth error
      }
    } finally {
       setIsLoading(false);
       // <<< MODIFIED: Clear status message after a delay only if it was a success/status message
       if (statusMessage && !error) {
           setTimeout(() => setStatusMessage(''), 7000);
       }
    }
  };

  return (
    <div className="file-manager-widget">
      <div className="fm-header">
        <h4>Your Uploaded Files</h4>
        <button
            onClick={fetchUserFiles}
            disabled={isLoading}
            className="fm-refresh-btn"
            title="Refresh File List"
        >
            
        </button>
      </div>

      {/* <<< MODIFIED: Display status or error messages */}
      {statusMessage && !error && <div className="fm-status-message">{statusMessage}</div>}
      {error && <div className="fm-error">{error}</div>}


      <div className="fm-file-list-container">
        {/* <<< MODIFIED: Added !statusMessage to the condition for "No files uploaded yet" */}
        {isLoading && userFiles.length === 0 ? (
          <p className="fm-loading">Loading files...</p>
        ) : userFiles.length === 0 && !isLoading && !error && !statusMessage ? (
          <p className="fm-empty">No files uploaded yet.</p>
        ) : (
          <ul className="fm-file-list">
            {userFiles.map((file) => (
              <li key={file.serverFilename} className="fm-file-item">
                <span className="fm-file-icon">{getFileIcon(file.type)}</span>
                <div className="fm-file-details">
                  {renamingFile === file.serverFilename ? (
                    <div className="fm-rename-section">
                      <input
                        type="text"
                        value={newName}
                        onChange={(e) => setNewName(e.target.value)}
                        onKeyDown={handleRenameInputKeyDown}
                        autoFocus
                        className="fm-rename-input"
                        aria-label={`New name for ${file.originalName}`}
                      />
                      <button onClick={handleRenameSave} disabled={isLoading || !newName.trim()} className="fm-action-btn fm-save-btn" title="Save Name"></button>
                      <button onClick={handleRenameCancel} disabled={isLoading} className="fm-action-btn fm-cancel-btn" title="Cancel Rename"></button>
                    </div>
                  ) : (
                    <>
                      <span className="fm-file-name" title={file.originalName}>{file.originalName}</span>
                      <span className="fm-file-size">{formatFileSize(file.size)}</span>
                    </>
                  )}
                </div>
                {renamingFile !== file.serverFilename && (
                  <div className="fm-file-actions">
                    <button
                        onClick={() => handleRenameClick(file)}
                        disabled={isLoading || !!renamingFile}
                        className="fm-action-btn fm-rename-btn"
                        title="Rename"
                    >
                       
                    </button>
                    <button
                        onClick={() => handleDeleteFile(file.serverFilename, file.originalName)}
                        disabled={isLoading || !!renamingFile}
                        className="fm-action-btn fm-delete-btn"
                        title="Delete"
                    >
                        
                    </button>
                  </div>
                )}
              </li>
            ))}
          </ul>
        )}
         {isLoading && userFiles.length > 0 && <p className="fm-loading fm-loading-bottom">Processing...</p>}
      </div>
    </div>
  );
};

// --- CSS for FileManagerWidget ---
const FileManagerWidgetCSS = `
/* client/src/components/FileManagerWidget.css */
.file-manager-widget { display: flex; flex-direction: column; gap: 10px; padding: 15px 0px 15px 20px; box-sizing: border-box; height: 100%; overflow: hidden; }
.fm-header { display: flex; justify-content: space-between; align-items: center; padding-right: 20px; flex-shrink: 0; }
.file-manager-widget h4 { margin: 0; color: var(--text-primary); font-size: 0.95rem; font-weight: 600; }
.fm-refresh-btn { background: none; border: 1px solid var(--border-color); color: var(--text-secondary); padding: 3px 6px; border-radius: 4px; cursor: pointer; font-size: 0.9rem; line-height: 1; transition: color 0.2s, border-color 0.2s, background-color 0.2s; }
.fm-refresh-btn:hover:not(:disabled) { color: var(--text-primary); border-color: #555; background-color: #3a3a40; }
.fm-refresh-btn:disabled { cursor: not-allowed; opacity: 0.5; }

/* <<< MODIFIED: Consolidated message styling and added .fm-status-message */
.fm-error, .fm-loading, .fm-empty, .fm-status-message { 
  font-size: 0.85rem; 
  padding: 10px 15px; 
  border-radius: 4px; 
  text-align: center; 
  margin: 5px 20px 5px 0; 
  flex-shrink: 0; 
}
.fm-error { 
  color: var(--error-color, #f44336); /* Fallback to a default red */
  border: 1px solid var(--error-color, #f44336); 
  background-color: var(--error-bg, rgba(244, 67, 54, 0.1)); 
}
.fm-status-message { /* Styling for general status/success messages */
  color: var(--success-color, #4CAF50); /* Fallback to a default green */
  background-color: var(--success-bg, rgba(76, 175, 80, 0.1));
  border: 1px solid var(--success-color-border, #c8e6c9);
  font-style: normal; /* Not italic like loading/empty */
}
.fm-loading, .fm-empty { color: var(--text-secondary); font-style: italic; }
/* End of modified message styling */

.fm-loading-bottom { margin-top: auto; padding: 5px; }
.fm-file-list-container { flex-grow: 1; overflow-y: auto; padding-right: 10px; margin-right: 10px; position: relative; }
.fm-file-list-container::-webkit-scrollbar { width: 8px; }
.fm-file-list-container::-webkit-scrollbar-track { background: transparent; }
.fm-file-list-container::-webkit-scrollbar-thumb { background-color: #4a4a50; border-radius: 10px; }
.fm-file-list-container { scrollbar-width: thin; scrollbar-color: #4a4a50 transparent; }
.fm-file-list { list-style: none; padding: 0; margin: 0; }
.fm-file-item { display: flex; align-items: center; padding: 8px 5px; margin-bottom: 5px; border-radius: 4px; background-color: #2f2f34; transition: background-color 0.2s ease; gap: 10px; }
.fm-file-item:hover { background-color: #3a3a40; }
.fm-file-icon { flex-shrink: 0; font-size: 1.1rem; line-height: 1; }
.fm-file-details { flex-grow: 1; overflow: hidden; display: flex; flex-direction: column; justify-content: center; min-height: 30px; }
.fm-file-name { font-size: 0.85rem; color: var(--text-primary); white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
.fm-file-size { font-size: 0.7rem; color: var(--text-secondary); margin-top: 2px; }
.fm-file-actions { display: flex; gap: 5px; flex-shrink: 0; margin-left: auto; }
.fm-action-btn { background: none; border: none; color: var(--text-secondary); cursor: pointer; padding: 3px; font-size: 1rem; line-height: 1; border-radius: 3px; transition: color 0.2s ease, background-color 0.2s ease; }
.fm-action-btn:hover:not(:disabled) { color: var(--text-primary); background-color: #4a4a50; }
.fm-action-btn:disabled { opacity: 0.5; cursor: not-allowed; }
.fm-delete-btn:hover:not(:disabled) { color: var(--error-color); }
.fm-rename-btn:hover:not(:disabled) { color: var(--accent-blue-light); }
.fm-save-btn:hover:not(:disabled) { color: #52c41a; } /* Green */
.fm-cancel-btn:hover:not(:disabled) { color: #ffc107; } /* Orange/Yellow */
.fm-rename-section { display: flex; align-items: center; gap: 5px; width: 100%; }
.fm-rename-input { flex-grow: 1; padding: 4px 8px; background-color: var(--bg-input); color: var(--text-primary); border: 1px solid var(--border-color); border-radius: 4px; font-size: 0.85rem; outline: none; min-width: 50px; }
.fm-rename-input:focus { border-color: var(--accent-blue); }
`;
// --- Inject CSS ---
const styleTagFileManagerId = 'file-manager-widget-styles';
if (!document.getElementById(styleTagFileManagerId)) {
    const styleTag = document.createElement("style");
    styleTag.id = styleTagFileManagerId;
    styleTag.type = "text/css";
    styleTag.innerText = FileManagerWidgetCSS;
    document.head.appendChild(styleTag);
}
// --- End CSS Injection ---

export default FileManagerWidget;
```

`client/src/components/FileUploadWidget.js`

```javascript
// client/src/components/FileUploadWidget.js
import React, { useState, useRef } from 'react';
import { uploadFile } from '../services/api';

const FileUploadWidget = ({ onUploadSuccess }) => {
  const [selectedFile, setSelectedFile] = useState(null);
  const [uploadStatus, setUploadStatus] = useState(''); // 'uploading', 'success', 'error', ''
  const [statusMessage, setStatusMessage] = useState('');
  const fileInputRef = useRef(null);

  const allowedFileTypesString = ".pdf,.txt,.docx,.doc,.pptx,.ppt,.py,.js,.bmp,.png,.jpg,.jpeg";

  const handleFileChange = (event) => {
    const file = event.target.files[0];
    if (file) {
      const fileExt = "." + file.name.split('.').pop().toLowerCase();
      if (!allowedFileTypesString.includes(fileExt)) {
           setStatusMessage(`Error: File type (${fileExt}) not allowed.`);
           setUploadStatus('error');
           setSelectedFile(null);
           if (fileInputRef.current) fileInputRef.current.value = '';
           return;
      }

      const MAX_SIZE_MB = 20;
      const MAX_SIZE_BYTES = MAX_SIZE_MB * 1024 * 1024;
      if (file.size > MAX_SIZE_BYTES) {
          setStatusMessage(`Error: File exceeds ${MAX_SIZE_MB}MB limit.`);
          setUploadStatus('error');
          setSelectedFile(null);
          if (fileInputRef.current) fileInputRef.current.value = '';
          return;
      }

      setSelectedFile(file);
      setStatusMessage(`Selected: ${file.name}`);
      setUploadStatus('');

    } else {
        // Handle user cancelling file selection if needed
    }
  };

  const handleUpload = async () => {
    if (!selectedFile) {
      setStatusMessage('Please select a file first.');
      setUploadStatus('error');
      return;
    }
    // Ensure userId exists before uploading
     const currentUserId = localStorage.getItem('userId');
     if (!currentUserId) {
         setStatusMessage('Error: Not logged in. Cannot upload file.');
         setUploadStatus('error');
         return;
     }

    setUploadStatus('uploading');
    setStatusMessage(`Uploading ${selectedFile.name}...`);

    const formData = new FormData();
    formData.append('file', selectedFile);

    try {
      // Interceptor adds user ID header
      const response = await uploadFile(formData);

      setUploadStatus('success');
      setStatusMessage(response.data.message || 'Upload successful!');
      console.log('Upload successful:', response.data);

      setSelectedFile(null);
      if (fileInputRef.current) {
          fileInputRef.current.value = '';
      }

      if (onUploadSuccess && typeof onUploadSuccess === 'function') {
          onUploadSuccess();
      }

      setTimeout(() => {
          // Check if status is still success before clearing
          setUploadStatus(prevStatus => prevStatus === 'success' ? '' : prevStatus);
          setStatusMessage(prevMsg => prevMsg === (response.data.message || 'Upload successful!') ? '' : prevMsg);
      }, 4000);


    } catch (err) {
      console.error("Upload Error:", err.response || err);
      setUploadStatus('error');
      setStatusMessage(err.response?.data?.message || 'Upload failed. Please check the file or try again.');
      if (err.response?.status === 401) {
          console.warn("FileUpload: Received 401 during upload.");
          // Consider calling a logout function passed via props or context
      }
    }
  };

  const triggerFileInput = () => {
    fileInputRef.current?.click();
  };

  return (
    <div className="file-upload-widget">
      <h4>Upload File</h4>
      <input
        type="file"
        ref={fileInputRef}
        onChange={handleFileChange}
        accept={allowedFileTypesString}
        style={{ display: 'none' }}
        aria-hidden="true"
      />
      <button
        type="button"
        className="select-file-btn"
        onClick={triggerFileInput}
        disabled={uploadStatus === 'uploading'}
      >
        Choose File
      </button>
      <div className={`status-message ${uploadStatus}`}>
        {statusMessage || 'No file selected.'}
      </div>
      <button
        type="button"
        className="upload-btn"
        onClick={handleUpload}
        disabled={!selectedFile || uploadStatus === 'uploading'}
      >
        {uploadStatus === 'uploading' ? 'Uploading...' : 'Upload'}
      </button>
    </div>
  );
};

// --- CSS for FileUploadWidget ---
const FileUploadWidgetCSS = `
/* client/src/components/FileUploadWidget.css */
.file-upload-widget { display: flex; flex-direction: column; gap: 12px; padding: 20px; box-sizing: border-box; }
.file-upload-widget h4 { margin-top: 0; margin-bottom: 10px; color: var(--text-primary); font-size: 0.95rem; font-weight: 600; }
.select-file-btn, .upload-btn { width: 100%; padding: 9px 15px; border: 1px solid var(--border-color); border-radius: 6px; cursor: pointer; font-size: 0.9rem; font-weight: 500; transition: background-color 0.2s ease, border-color 0.2s ease, color 0.2s ease, opacity 0.2s ease; background-color: #2a2a30; color: var(--text-primary); text-align: center; box-sizing: border-box; }
.select-file-btn:hover:not(:disabled), .upload-btn:hover:not(:disabled) { background-color: #3a3a40; border-color: #4a4a50; }
.select-file-btn:disabled, .upload-btn:disabled { opacity: 0.6; cursor: not-allowed; }
.upload-btn { background-color: var(--accent-blue); border-color: var(--accent-blue); color: var(--user-message-text); }
.upload-btn:hover:not(:disabled) { background-color: var(--accent-blue-light); border-color: var(--accent-blue-light); }
.upload-btn:disabled { background-color: #3a3a40; border-color: var(--border-color); color: var(--text-secondary); opacity: 0.7; }
.status-message { font-size: 0.8rem; color: var(--text-secondary); padding: 8px 10px; background-color: var(--bg-input); border: 1px solid var(--border-color); border-radius: 4px; text-align: center; min-height: 1.6em; line-height: 1.4; word-break: break-word; transition: color 0.2s ease, border-color 0.2s ease, background-color 0.2s ease; }
.status-message.uploading { color: var(--accent-blue-light); border-color: var(--accent-blue); }
.status-message.success { color: #52c41a; border-color: #52c41a; background-color: rgba(82, 196, 26, 0.1); }
.status-message.error { color: var(--error-color); border-color: var(--error-color); background-color: var(--error-bg); }
`;
// --- Inject CSS ---
const styleTagUploadId = 'file-upload-widget-styles';
if (!document.getElementById(styleTagUploadId)) {
    const styleTag = document.createElement("style");
    styleTag.id = styleTagUploadId;
    styleTag.type = "text/css";
    styleTag.innerText = FileUploadWidgetCSS;
    document.head.appendChild(styleTag);
}
// --- End CSS Injection ---

export default FileUploadWidget;

```

`client/src/components/HistoryModal.js`

```javascript
// client/src/components/HistoryModal.js
import React, { useState, useEffect } from 'react';
import ReactMarkdown from 'react-markdown';
import { getChatSessions, getSessionDetails } from '../services/api';

const HistoryModal = ({ isOpen, onClose }) => {
  const [sessions, setSessions] = useState([]);
  const [selectedSession, setSelectedSession] = useState(null);
  const [isLoadingSessions, setIsLoadingSessions] = useState(false);
  const [isLoadingDetails, setIsLoadingDetails] = useState(false);
  const [error, setError] = useState('');

  useEffect(() => {
    if (isOpen) {
      const fetchSessions = async () => {
        // Ensure userId exists before fetching
        const currentUserId = localStorage.getItem('userId');
        if (!currentUserId) {
            setError('Cannot load history: User not logged in.');
            setSessions([]);
            return;
        }

        setIsLoadingSessions(true);
        setError('');
        setSelectedSession(null);
        setSessions([]);
        try {
          // Interceptor adds user ID
          const response = await getChatSessions();
          setSessions(response.data || []);
        } catch (err) {
          console.error("Error fetching sessions:", err);
          setError(err.response?.data?.message || 'Failed to load chat history sessions.');
          setSessions([]);
          if (err.response?.status === 401) {
              console.warn("HistoryModal: Received 401 fetching sessions.");
              // Optionally close modal or trigger logout
              onClose();
          }
        } finally {
          setIsLoadingSessions(false);
        }
      };
      fetchSessions();
    } else {
      setSessions([]);
      setSelectedSession(null);
      setError('');
      setIsLoadingSessions(false);
      setIsLoadingDetails(false);
    }
  }, [isOpen, onClose]); // Added onClose to dependency array

  const handleSelectSession = async (sessionId) => {
     // Ensure userId exists before fetching
    const currentUserId = localStorage.getItem('userId');
    if (!currentUserId) {
        setError('Cannot load session details: User not logged in.');
        return;
    }
    if (!sessionId || isLoadingDetails || selectedSession?.sessionId === sessionId) return;

    setIsLoadingDetails(true);
    setError('');
    try {
      // Interceptor adds user ID
      const response = await getSessionDetails(sessionId);
      setSelectedSession(response.data);
    } catch (err) {
      console.error(`Error fetching session ${sessionId}:`, err);
      setError(err.response?.data?.message || `Failed to load details for session ${sessionId}.`);
      setSelectedSession(null);
      if (err.response?.status === 401) {
          console.warn("HistoryModal: Received 401 fetching session details.");
          // Optionally close modal or trigger logout
          onClose();
      }
    } finally {
      setIsLoadingDetails(false);
    }
  };

  const formatDate = (dateString) => {
    if (!dateString) return 'N/A';
    try {
      const date = new Date(dateString);
      if (isNaN(date.getTime())) {
          throw new Error("Invalid date string");
      }
      return date.toLocaleString(undefined, { dateStyle: 'short', timeStyle: 'short' });
    } catch (e) {
      console.warn("Error formatting date:", dateString, e);
      return dateString;
    }
  };

  if (!isOpen) return null;

  return (
    <div className="history-modal-overlay" onClick={onClose}>
      <div className="history-modal-content" onClick={(e) => e.stopPropagation()}>
        <button className="history-modal-close-btn" onClick={onClose} aria-label="Close history"></button>
        <h2>Chat History</h2>

        {error && !isLoadingDetails && !isLoadingSessions && <div className="history-error">{error}</div>}

        <div className="history-layout">
          <div className="history-session-list">
            <h3>Sessions</h3>
            {isLoadingSessions ? (
              <p className="history-loading">Loading sessions...</p>
            ) : sessions.length === 0 && !error ? (
              <p className="history-empty">No past sessions found.</p>
            ) : (
              <ul>
                {sessions.map((session) => (
                  <li
                    key={session.sessionId}
                    className={selectedSession?.sessionId === session.sessionId ? 'active' : ''}
                    onClick={() => handleSelectSession(session.sessionId)}
                    tabIndex={0}
                    onKeyDown={(e) => (e.key === 'Enter' || e.key === ' ') && handleSelectSession(session.sessionId)}
                    role="button"
                  >
                    <div className="session-preview" title={session.preview || 'Chat session'}>
                      {session.preview || 'Chat session'}
                    </div>
                    <div className="session-date">
                      {formatDate(session.updatedAt || session.createdAt)} ({session.messageCount || 0} msgs)
                    </div>
                  </li>
                ))}
              </ul>
            )}
          </div>

          <div className="history-session-details">
            <h3>Session Details</h3>
            {isLoadingDetails ? (
              <p className="history-loading">Loading details...</p>
            ) : !selectedSession ? (
              <p className="history-empty">Select a session from the left to view its messages.</p>
            ) : (
              <div className="history-messages-area">
                {selectedSession.messages && selectedSession.messages.length > 0 ? (
                  selectedSession.messages.map((msg, index) => (
                    !msg?.role || !msg?.parts?.length ?
                      <div key={`${selectedSession.sessionId}-err-${index}`} className="history-message-error">Invalid message data</div>
                    :
                    <div key={`${selectedSession.sessionId}-${index}`} className={`history-message ${msg.role}`}>
                      <div className="history-message-content">
                        <ReactMarkdown children={msg.parts[0]?.text || ''} />
                      </div>
                      <span className="history-message-timestamp">
                        {formatDate(msg.timestamp)}
                      </span>
                    </div>
                  ))
                ) : (
                  <p className="history-empty">This session appears to have no messages.</p>
                )}
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};


// --- CSS for HistoryModal ---
const HistoryModalCSS = `
/* client/src/components/HistoryModal.css */
.history-modal-overlay { position: fixed; inset: 0; background-color: rgba(0, 0, 0, 0.75); display: flex; justify-content: center; align-items: center; z-index: 1000; backdrop-filter: blur(4px); padding: 20px; box-sizing: border-box; }
.history-modal-content { background-color: var(--bg-header); color: var(--text-primary); padding: 20px 25px; border-radius: 10px; width: 90%; max-width: 1200px; height: 85vh; max-height: 800px; box-shadow: 0 8px 30px rgba(0, 0, 0, 0.5); position: relative; display: flex; flex-direction: column; overflow: hidden; }
.history-modal-close-btn { position: absolute; top: 10px; right: 10px; background: transparent; border: none; font-size: 2.2rem; font-weight: bold; color: var(--text-secondary); cursor: pointer; line-height: 1; padding: 5px; transition: color 0.2s ease; }
.history-modal-close-btn:hover { color: var(--text-primary); }
.history-modal-content h2 { margin: 0 0 15px 0; padding-bottom: 10px; text-align: center; font-weight: 600; color: var(--text-primary); border-bottom: 1px solid var(--border-color); flex-shrink: 0; }
.history-error, .history-loading, .history-empty { color: var(--text-secondary); padding: 12px 15px; border-radius: 6px; margin-bottom: 15px; text-align: center; font-size: 0.9rem; font-style: italic; }
.history-error { color: var(--error-color); background-color: var(--error-bg); border: 1px solid var(--error-color); font-style: normal; }
.history-layout { display: flex; flex-grow: 1; gap: 20px; overflow: hidden; }
.history-session-list { width: 320px; flex-shrink: 0; display: flex; flex-direction: column; overflow-y: auto; padding-right: 10px; border-right: 1px solid var(--border-color); }
.history-session-list h3 { margin: 0 0 12px 0; padding: 0 5px; font-size: 1rem; font-weight: 600; color: var(--text-secondary); flex-shrink: 0; }
.history-session-list ul { list-style: none; padding: 0; margin: 0; flex-grow: 1; overflow-y: auto; }
.history-session-list li { padding: 10px 12px; margin-bottom: 8px; border: 1px solid transparent; border-radius: 6px; cursor: pointer; transition: background-color 0.15s ease, border-color 0.15s ease; background-color: #2f2f34; }
.history-session-list li:hover { background-color: #3a3a40; border-color: #4a4a50; }
.history-session-list li.active { background-color: var(--accent-blue); border-color: var(--accent-blue); color: var(--user-message-text); }
.history-session-list li.active .session-date { color: rgba(255, 255, 255, 0.85); }
.session-preview { font-size: 0.9rem; font-weight: 500; margin-bottom: 4px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
.session-date { font-size: 0.75rem; color: var(--text-secondary); }
.history-session-details { flex-grow: 1; display: flex; flex-direction: column; overflow: hidden; background-color: var(--bg-messages); border-radius: 8px; }
.history-session-details h3 { margin: 0; padding: 12px 20px; font-size: 1rem; font-weight: 600; color: var(--text-secondary); border-bottom: 1px solid var(--border-color); background-color: var(--bg-header); border-top-left-radius: 8px; border-top-right-radius: 8px; flex-shrink: 0; }
.history-session-details > .history-empty, .history-session-details > .history-loading { padding: 30px; text-align: center; flex-grow: 1; display: flex; align-items: center; justify-content: center; }
.history-messages-area { flex-grow: 1; overflow-y: auto; padding: 20px; display: flex; flex-direction: column; gap: 18px; }
.history-message { display: flex; max-width: 85%; position: relative; word-wrap: break-word; flex-direction: column; }
.history-message.user { align-self: flex-end; align-items: flex-end; }
.history-message.model { align-self: flex-start; align-items: flex-start; }
.history-message-content { padding: 10px 15px; border-radius: 16px; font-size: 0.9rem; line-height: 1.6; box-shadow: 0 1px 2px rgba(0, 0, 0, 0.15); text-align: left; }
.history-message.user .history-message-content { background-color: var(--user-message-bg); color: var(--user-message-text); border-bottom-right-radius: 5px; }
.history-message.model .history-message-content { background-color: var(--model-message-bg); color: var(--model-message-text); border-bottom-left-radius: 5px; }
.history-message-content p { margin: 0 0 0.5em 0; }
.history-message-content p:last-child { margin-bottom: 0; }
.history-message-content pre { background-color: var(--code-bg); border: 1px solid var(--code-border); border-radius: 6px; padding: 12px; overflow-x: auto; font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 0.88rem; margin: 0.8em 0; white-space: pre; color: var(--code-text); }
.history-message-content code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 0.88rem; background-color: rgba(255, 255, 255, 0.08); padding: 0.2em 0.4em; border-radius: 4px; border: 1px solid var(--border-color); color: var(--text-secondary); }
.history-message-content pre code { background-color: transparent; padding: 0; border: none; font-size: inherit; color: inherit; }
.history-message-timestamp { font-size: 0.7rem; color: var(--text-secondary); margin-top: 5px; }
.history-message-error { color: var(--error-color); font-style: italic; padding: 5px 0; text-align: center; font-size: 0.8rem; }
.history-session-list::-webkit-scrollbar, .history-session-list ul::-webkit-scrollbar, .history-messages-area::-webkit-scrollbar { width: 8px; }
.history-session-list::-webkit-scrollbar-track, .history-session-list ul::-webkit-scrollbar-track, .history-messages-area::-webkit-scrollbar-track { background: transparent; }
.history-session-list::-webkit-scrollbar-thumb, .history-session-list ul::-webkit-scrollbar-thumb, .history-messages-area::-webkit-scrollbar-thumb { background-color: #4a4a50; border-radius: 10px; }
.history-session-list, .history-session-list ul, .history-messages-area { scrollbar-width: thin; scrollbar-color: #4a4a50 transparent; }
`;
// --- Inject CSS ---
const styleTagHistoryId = 'history-modal-styles';
if (!document.getElementById(styleTagHistoryId)) {
    const styleTag = document.createElement("style");
    styleTag.id = styleTagHistoryId;
    styleTag.type = "text/css";
    styleTag.innerText = HistoryModalCSS;
    document.head.appendChild(styleTag);
}
// --- End CSS Injection ---

export default HistoryModal;

```

`client/src/components/Notebook.js`

```javascript
// import React, { useEffect, useRef } from 'react';

// const Notebook = () => {
//   const iframeRef = useRef(null);

//   useEffect(() => {
//     const username = localStorage.getItem("username");
//     if (!username) {
//       console.error("Username not found in localStorage.");
//       return;
//     }

//     const sendUsername = () => {
//       iframeRef.current?.contentWindow?.postMessage(
//         { type: "SET_USERNAME", username },
//         "http://127.0.0.1:5000"
//       );
//     };

//     // Give time for iframe to load
//     const timer = setTimeout(sendUsername, 1000);
//     return () => clearTimeout(timer);
//   }, []);

//   return (
//     <div style={{
//       position: 'fixed',
//       top: 0,
//       left: 0,
//       width: '100vw',
//       height: '100vh',
//       margin: 0,
//       padding: 0,
//       zIndex: 1000,
//       background: '#1a1a1a'
//     }}>
//       <iframe
//         ref={iframeRef}
//         src="http://127.0.0.1:5000"
//         title="Notebook App"
//         sandbox="allow-scripts allow-same-origin"
//         style={{
//           width: '100vw',
//           height: '100vh',
//           border: 'none',
//           margin: 0,
//           padding: 0,
//           background: '#1a1a1a'
//         }}
//         allowFullScreen
//       />
//     </div>
//   );
// };

// export default Notebook;


import React, { useEffect, useRef } from 'react';

const Notebook = () => {
  const iframeRef = useRef(null);

  useEffect(() => {
    const username = localStorage.getItem('username');
    const sessionId = localStorage.getItem('sessionId');
    const sendMessageToIframe = () => {
      if (iframeRef.current) {
        iframeRef.current.contentWindow.postMessage(
          { type: 'SET_USER_AND_SESSION', username, sessionId },
          'http://localhost:5000' // target origin
        );
      }
    };

    // Wait for iframe to load
    const iframe = iframeRef.current;
    iframe?.addEventListener('load', sendMessageToIframe);

    return () => {
      iframe?.removeEventListener('load', sendMessageToIframe);
    };
  }, []);

  return (
    <div style={{
      position: 'fixed',
      top: 0,
      left: 0,
      width: '100vw',
      height: '100vh',
      margin: 0,
      padding: 0,
      zIndex: 1000,
      background: '#1a1a1a'
    }}>
      <iframe
        ref={iframeRef}
        src="http://localhost:5000"
        title="Notebook App"
        sandbox="allow-scripts allow-same-origin"
        style={{
          width: '100vw',
          height: '100vh',
          border: 'none',
          margin: 0,
          padding: 0,
          background: '#1a1a1a'
        }}
        allowFullScreen
      />
    </div>
  );
};

export default Notebook;


```

`client/src/components/SystemPromptWidget.js`

```javascript
// client/src/components/SystemPromptWidget.js
import React from 'react';
// Removed CSS import

// Define the THREE required system prompts + a Custom option
export const availablePrompts = [
  {
    id: 'friendly',
    title: 'Friendly Tutor',
    prompt: 'You are a friendly, patient, and encouraging tutor specializing in engineering and scientific topics for PhD students. Explain concepts clearly, break down complex ideas, use analogies, and offer positive reinforcement. Ask follow-up questions to ensure understanding.',
  },
  {
    id: 'explorer', // Changed ID slightly for clarity
    title: 'Concept Explorer',
    prompt: 'You are an expert academic lecturer introducing a new, complex engineering or scientific concept. Your goal is to provide a deep, structured explanation. Define terms rigorously, outline the theory, provide relevant mathematical formulations (using Markdown), illustrative examples, and discuss applications or limitations pertinent to PhD-level research.',
  },
  {
    id: 'knowledge_check',
    title: 'Knowledge Check',
    prompt: 'You are assessing understanding of engineering/scientific topics. Ask targeted questions to test knowledge, identify misconceptions, and provide feedback on the answers. Start by asking the user what topic they want to be quizzed on.',
  },
   {
    id: 'custom', // Represents user-edited state
    title: 'Custom Prompt',
    prompt: '', // Placeholder, actual text comes from textarea
  },
];

// Helper to find prompt text by ID - Export if needed elsewhere
export const getPromptTextById = (id) => {
  const prompt = availablePrompts.find(p => p.id === id);
  return prompt ? prompt.prompt : ''; // Return empty string if not found
};


/**
 * Renders a sidebar widget with a dropdown for preset prompts
 * and an editable textarea for the current system prompt.
 * @param {object} props - Component props.
 * @param {string} props.selectedPromptId - The ID of the currently active preset (or 'custom').
 * @param {string} props.promptText - The current text of the system prompt (potentially edited).
 * @param {function} props.onSelectChange - Callback when dropdown selection changes. Passes the new ID.
 * @param {function} props.onTextChange - Callback when the textarea content changes. Passes the new text.
 */
const SystemPromptWidget = ({ selectedPromptId, promptText, onSelectChange, onTextChange }) => {

  const handleDropdownChange = (event) => {
    const newId = event.target.value;
    onSelectChange(newId); // Notify parent of the ID change
  };

  const handleTextareaChange = (event) => {
    onTextChange(event.target.value); // Notify parent of the text change
  };

  return (
    <div className="system-prompt-widget">
      <h3>Assistant Mode</h3>

      {/* Dropdown for selecting presets */}
      <select
        className="prompt-select"
        value={selectedPromptId} // Control the selected option via state
        onChange={handleDropdownChange}
        aria-label="Select Assistant Mode"
      >
        {/* Filter out 'custom' from being a selectable option initially */}
        {availablePrompts.filter(p => p.id !== 'custom').map((p) => (
          <option key={p.id} value={p.id}>
            {p.title}
          </option>
        ))}
        {/* Add Custom option dynamically if the current ID is 'custom' */}
        {/* This ensures "Custom Prompt" appears in the dropdown only when it's actually active */}
        {selectedPromptId === 'custom' && (
            <option key="custom" value="custom">
                Custom Prompt
            </option>
        )}
      </select>

      {/* Editable Textarea for the actual prompt */}
      <label htmlFor="system-prompt-text" className="prompt-label">
        System Prompt (Editable)
      </label>
      <textarea
        id="system-prompt-text"
        className="prompt-textarea"
        value={promptText} // Display the current prompt text (could be preset or edited)
        onChange={handleTextareaChange}
        rows="5" // Suggests initial height, CSS controls actual fixed height
        maxLength="2000" // Optional: Limit character count if desired
        placeholder="The current system prompt will appear here. You can edit it directly."
        aria-label="Editable System Prompt Text"
      />
       {/* Optional: Character count indicator
       <div className="char-count">{promptText?.length || 0} / 2000</div> */}
    </div>
  );
};

// --- CSS for SystemPromptWidget ---
const SystemPromptWidgetCSS = `
/* client/src/components/SystemPromptWidget.css */
.system-prompt-widget { padding: 20px; background-color: var(--bg-header); box-sizing: border-box; display: flex; flex-direction: column; flex-shrink: 0; }
.system-prompt-widget h3 { margin-top: 0; margin-bottom: 15px; color: var(--text-primary); font-size: 1rem; font-weight: 600; padding-bottom: 10px; }
.prompt-select { width: 100%; padding: 10px 12px; margin-bottom: 15px; background-color: #2a2a30; color: var(--text-primary); border: 1px solid var(--border-color); border-radius: 6px; font-size: 0.9rem; cursor: pointer; appearance: none; -webkit-appearance: none; -moz-appearance: none; background-image: url('data:image/svg+xml;utf8,<svg fill="%23b0b3b8" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M7 10l5 5 5-5z"/><path d="M0 0h24v24H0z" fill="none"/></svg>'); background-repeat: no-repeat; background-position: right 10px center; background-size: 18px; }
.prompt-select:focus { outline: none; border-color: var(--accent-blue); box-shadow: 0 0 0 2px rgba(0, 132, 255, 0.3); }
.prompt-label { display: block; margin-bottom: 8px; color: var(--text-secondary); font-size: 0.85rem; font-weight: 500; }
.prompt-textarea { width: 100%; background-color: var(--bg-input); color: var(--text-primary); border: 1px solid var(--border-color); border-radius: 6px; padding: 10px 12px; font-size: 0.85rem; line-height: 1.5; box-sizing: border-box; font-family: inherit; resize: none; height: 100px; overflow-y: auto; }
.prompt-textarea:focus { outline: none; border-color: var(--accent-blue); box-shadow: 0 0 0 2px rgba(0, 132, 255, 0.3); }
.prompt-textarea::placeholder { color: var(--text-secondary); opacity: 0.7; }
.char-count { text-align: right; font-size: 0.75rem; color: var(--text-secondary); margin-top: 5px; }
`;
// --- Inject CSS ---
const styleTagPromptId = 'system-prompt-widget-styles';
if (!document.getElementById(styleTagPromptId)) {
    const styleTag = document.createElement("style");
    styleTag.id = styleTagPromptId;
    styleTag.type = "text/css";
    styleTag.innerText = SystemPromptWidgetCSS;
    document.head.appendChild(styleTag);
}
// --- End CSS Injection ---


export default SystemPromptWidget;

```

`client/src/index.css`

```css
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}

```

`client/src/index.html`

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

```

`client/src/index.js`

```javascript
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

```

`client/src/reportWebVitals.js`

```javascript
const reportWebVitals = onPerfEntry => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;

```

`client/src/services/api.js`

```javascript
// client/src/services/api.js
import axios from 'axios';

// Dynamically determine API Base URL
const getApiBaseUrl = () => {
    // Use REACT_APP_BACKEND_PORT environment variable if set during build, otherwise default
    // This allows overriding the port via build environment if needed.
    const backendPort = process.env.REACT_APP_BACKEND_PORT || 5001;
    const hostname = window.location.hostname; // Get hostname browser is accessing

    // Use http protocol by default for local development
    const protocol = window.location.protocol === 'https:' ? 'https:' : 'http:';

    // If hostname is localhost or 127.0.0.1, construct URL with localhost
    // Otherwise, use the hostname the frontend was accessed with (e.g., LAN IP)
    const backendHost = (hostname === 'localhost' || hostname === '127.0.0.1')
        ? 'localhost'
        : hostname;

    return `${protocol}//${backendHost}:${backendPort}/api`;
};

const API_BASE_URL = getApiBaseUrl();
console.log("API Base URL:", API_BASE_URL); // Log the dynamically determined URL

// Create Axios instance
const api = axios.create({
    baseURL: API_BASE_URL,
});

// --- Interceptor to add User ID header (TEMP AUTH) ---
api.interceptors.request.use(
    (config) => {
        const userId = localStorage.getItem('userId');
        // Add header only if userId exists
        if (userId) {
            config.headers['x-user-id'] = userId;
        } else if (!config.url.includes('/auth/')) {
             // Only warn if it's NOT an authentication request
             console.warn("API Interceptor: userId not found in localStorage for non-auth request to", config.url);
             // Consider rejecting the request if userId is absolutely mandatory for the endpoint
             // return Promise.reject(new Error("User ID not found. Please log in."));
        }

        // Handle FormData content type specifically
        if (config.data instanceof FormData) {
            // Let Axios set the correct 'multipart/form-data' header with boundary
            // Deleting it ensures Axios handles it automatically.
            delete config.headers['Content-Type'];
        } else if (!config.headers['Content-Type']) {
             // Set default Content-Type for other requests (like JSON) if not already set
             config.headers['Content-Type'] = 'application/json';
        }
        // console.log("API Request Config:", config); // Debug: Log outgoing request config
        return config;
    },
    (error) => {
        console.error("API Request Interceptor Error:", error);
        return Promise.reject(error);
    }
);

// --- Interceptor to handle 401 Unauthorized responses ---
api.interceptors.response.use(
    (response) => {
        // Any status code within the range of 2xx cause this function to trigger
        return response;
    },
    (error) => {
        // Any status codes outside the range of 2xx cause this function to trigger
        if (error.response && error.response.status === 401) {
            console.warn("API Response Interceptor: Received 401 Unauthorized. Clearing auth data and redirecting to login.");
            // Clear potentially invalid auth tokens/user info
            localStorage.removeItem('sessionId');
            localStorage.removeItem('username');
            localStorage.removeItem('userId');

            // Use window.location to redirect outside of React Router context if needed
            // Check if already on login page to prevent loop
            if (!window.location.pathname.includes('/login')) {
                 window.location.href = '/login?sessionExpired=true'; // Redirect to login page
            }
        }
        // Return the error so that the calling code can handle it (e.g., display message)
        return Promise.reject(error);
    }
);
// --- End Interceptors ---


// --- NAMED EXPORTS for API functions ---

// Authentication
export const signupUser = (userData) => api.post('/auth/signup', userData);
export const signinUser = (userData) => api.post('/auth/signin', userData);

// Chat Interaction
// messageData includes { message, history, sessionId, systemPrompt, isRagEnabled, relevantDocs }
export const sendMessage = (messageData) => api.post('/chat/message', messageData);
export const saveChatHistory = (historyData) => api.post('/chat/history', historyData);

// RAG Query
// queryData includes { message }
export const queryRagService = (queryData) => api.post('/chat/rag', queryData);

// Chat History Retrieval
export const getChatSessions = () => api.get('/chat/sessions');
export const getSessionDetails = (sessionId) => api.get(`/chat/session/${sessionId}`);

// File Upload
// Pass FormData directly
export const uploadFile = (formData) => api.post('/upload', formData);

// File Management
export const getUserFiles = () => api.get('/files');
export const renameUserFile = (serverFilename, newOriginalName) => api.patch(`/files/${serverFilename}`, { newOriginalName });
export const deleteUserFile = async (serverFilename) => {
    const response = await api.delete(`/files/${serverFilename}`);
    return response; // Return the full response object
};


// --- DEFAULT EXPORT ---
// Export the configured Axios instance if needed for direct use elsewhere
export default api;

```

`client/src/setupTests.js`

```javascript
// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';

```

`code.txt`

```

```

`o.txt`

```
.
|-- client
|   |-- .gitignore
|   |-- package-lock.json
|   |-- package.json
|   |-- public
|   |   |-- favicon.ico
|   |   |-- index.html
|   |   |-- logo192.png
|   |   |-- logo512.png
|   |   |-- manifest.json
|   |   |-- robots.txt
|   |-- README.md
|   |-- src
|   |   |-- App.css
|   |   |-- App.js
|   |   |-- App.test.js
|   |   |-- components
|   |   |   |-- AuthPage.js
|   |   |   |-- ChatPage.css
|   |   |   |-- ChatPage.js
|   |   |   |-- FileManagerWidget.js
|   |   |   |-- FileUploadWidget.js
|   |   |   |-- HistoryModal.js
|   |   |   |-- Notebook.js
|   |   |   |-- SystemPromptWidget.js
|   |   |-- index.css
|   |   |-- index.html
|   |   |-- index.js
|   |   |-- logo.svg
|   |   |-- package-lock.json
|   |   |-- reportWebVitals.js
|   |   |-- services
|   |   |   |-- api.js
|   |   |-- setupTests.js
|-- code.txt
|-- files_md.sh
|-- o.txt
|-- README.md
|-- server
|   |-- .env
|   |-- .gitignore
|   |-- config
|   |   |-- db.js
|   |   |-- promptTemplates.js
|   |-- faiss_indices
|   |   |-- sample.txt
|   |   |-- user_682477567efd11b5da11da01
|   |   |   |-- index.faiss
|   |   |   |-- index.pkl
|   |   |-- user_682f041fae7dff294014a9ee
|   |   |   |-- index.faiss
|   |   |   |-- index.pkl
|   |   |-- user___DEFAULT__
|   |   |   |-- index.faiss
|   |   |   |-- index.pkl
|   |-- install.sh
|   |-- middleware
|   |   |-- authMiddleware.js
|   |-- models
|   |   |-- ChatHistory.js
|   |   |-- User.js
|   |-- package-lock.json
|   |-- package.json
|   |-- rag_service
|   |   |-- ai_core.py
|   |   |-- app.py
|   |   |-- code.txt
|   |   |-- config.py
|   |   |-- default.py
|   |   |-- faiss_handler.py
|   |   |-- file_parser.py
|   |   |-- generate_code.sh
|   |   |-- indexes
|   |   |   |-- default.faiss
|   |   |   |-- default_metadata.json
|   |   |-- Readne.txt
|   |   |-- requirements.txt
|   |   |-- vector_db_service.py
|   |   |-- __init__.py
|   |-- routes
|   |   |-- analysis.js
|   |   |-- auth.js
|   |   |-- chat.js
|   |   |-- files.js
|   |   |-- network.js
|   |   |-- syllabus.js
|   |   |-- upload.js
|   |-- server.js
|   |-- services
|   |   |-- geminiService.js
|   |   |-- kgService.js
|   |-- syllabi
|   |   |-- embedded_systems.md
|   |   |-- machine_learning.md
|   |-- utils
|   |   |-- assetCleanup.js
|   |   |-- networkUtils.js
|   |-- workers
|   |   |-- kgWorker.js

```

`server/.env`

```
PORT=5001 # Port for the backend (make sure it's free)
MONGO_URI = "mongodb://localhost:27017/chatbot_gemini" # Your MongoDB connection string
JWT_SECRET = "your_super_strong_and_secret_jwt_key_12345" # A strong, random secret key for JWT
GEMINI_API_KEY = "AIzaSyBiCYeCICXAuEEqVvwwaiOpO9gTg6mJLzw" # Your actual Gemini API Key
DEFAULT_PYTHON_RAG_URL = "http://localhost:5000"
```

`server/config/db.js`

```javascript
const mongoose = require('mongoose');
// const dotenv = require('dotenv'); // Removed dotenv

// dotenv.config(); // Removed dotenv

// Modified connectDB to accept the URI as an argument
const connectDB = async (mongoUri) => {
  if (!mongoUri) {
      console.error('MongoDB Connection Error: URI is missing.');
      process.exit(1);
  }
  try {
    // console.log(`Attempting MongoDB connection to: ${mongoUri}`); // Debug: Careful logging URI
    const conn = await mongoose.connect(mongoUri, {
      // Mongoose 6+ uses these defaults, so they are not needed
      // useNewUrlParser: true,
      // useUnifiedTopology: true,
      // serverSelectionTimeoutMS: 5000 // Example: Optional: Timeout faster
    });

    console.log(` MongoDB Connected Successfully`); // Simpler success message
    return conn; // Return connection object if needed elsewhere
  } catch (error) {
    console.error('MongoDB Connection Error:', error.message);
    // Exit process with failure
    process.exit(1);
  }
};

module.exports = connectDB;

```

`server/config/promptTemplates.js`

```javascript
// server/config/promptTemplates.js

const ANALYSIS_THINKING_PREFIX_TEMPLATE = `**STEP 1: THINKING PROCESS (Recommended):**
*   Before generating the analysis, briefly outline your plan in \`<thinking>\` tags. Example: \`<thinking>Analyzing for FAQs. Will scan for key questions and answers presented in the text.</thinking>\`
*   If you include thinking, place the final analysis *after* the \`</thinking>\` tag.

**STEP 2: ANALYSIS OUTPUT:**
*   Generate the requested analysis based **strictly** on the text provided below.
*   Follow the specific OUTPUT FORMAT instructions carefully.

--- START DOCUMENT TEXT ---
{doc_text_for_llm}
--- END DOCUMENT TEXT ---
`; // Note: Escaped backticks if your template string itself uses them inside.

const ANALYSIS_PROMPTS = {
    faq: {
        // No need for PromptTemplate class here, just the string parts
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate 5-7 Frequently Asked Questions (FAQs) with concise answers based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first FAQ (after thinking, if used). Do **NOT** include preamble.
*   Format each FAQ as:
    Q: [Question derived ONLY from the text]
    A: [Answer derived ONLY from the text, concise]
*   If the text doesn't support an answer, don't invent one. Use Markdown for formatting if appropriate (e.g., lists within an answer).

**BEGIN OUTPUT (Start with 'Q:' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    topics: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Identify the 5-8 most important topics discussed. Provide a 1-2 sentence explanation per topic based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first topic (after thinking, if used). Do **NOT** include preamble.
*   Format as a Markdown bulleted list:
    *   **Topic Name:** Brief explanation derived ONLY from the text content (1-2 sentences max).

**BEGIN OUTPUT (Start with '*   **' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    mindmap: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate a mind map outline in Markdown list format representing key concepts and hierarchy ONLY from the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the main topic as the top-level item (using '-') (after thinking, if used). Do **NOT** include preamble.
*   Use nested Markdown lists ('-' or '*') with indentation (2 or 4 spaces) for hierarchy.
*   Focus **strictly** on concepts and relationships mentioned in the text. Be concise.

**BEGIN OUTPUT (Start with e.g., '- Main Topic' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    }
};

module.exports = { ANALYSIS_PROMPTS };
```

`server/faiss_indices/sample.txt`

```

```

`server/middleware/authMiddleware.js`

```javascript
// server/middleware/authMiddleware.js
const User = require('../models/User');

// TEMPORARY Authentication Middleware (INSECURE - for debugging only)
// Checks for 'X-User-ID' header and attaches user to req.user
const tempAuth = async (req, res, next) => {
    const userId = req.headers['x-user-id']; // Read custom header (lowercase)

    // console.log("TempAuth Middleware: Checking for X-User-ID:", userId); // Debug log

    if (!userId) {
        console.warn("TempAuth Middleware: Missing X-User-ID header.");
        // Send 401 immediately if header is missing
        return res.status(401).json({ message: 'Unauthorized: Missing User ID header' });
    }

    try {
        // Find user by the ID provided in the header
        // Ensure Mongoose is connected before this runs (handled by server.js)
        const user = await User.findById(userId).select('-password'); // Exclude password

        if (!user) {
            console.warn(`TempAuth Middleware: User not found for ID: ${userId}`);
            // Send 401 if user ID is provided but not found in DB
            return res.status(401).json({ message: 'Unauthorized: User not found' });
        }

        // Attach user object to the request
        req.user = user;
        // console.log("TempAuth Middleware: User attached:", req.user.username); // Debug log
        next(); // Proceed to the next middleware or route handler

    } catch (error) {
        console.error('TempAuth Middleware: Error fetching user:', error);
        // Handle potential invalid ObjectId format errors
        if (error.name === 'CastError' && error.kind === 'ObjectId') {
             return res.status(400).json({ message: 'Bad Request: Invalid User ID format' });
        }
        // Send 500 for other unexpected errors during auth check
        res.status(500).json({ message: 'Server error during temporary authentication' });
    }
};

// Export the temporary middleware
module.exports = { tempAuth };

```

`server/models/ChatHistory.js`

```javascript
const mongoose = require('mongoose');

const MessageSchema = new mongoose.Schema({
    role: {
        type: String,
        enum: ['user', 'model'], // Gemini roles
        required: true
    },
    parts: [{
        text: {
            type: String,
            required: true
        }
        // _id: false // Mongoose adds _id by default, can disable if truly not needed per part
    }],
    timestamp: {
        type: Date,
        default: Date.now
    }
}, { _id: false }); // Don't create separate _id for each message object in the array

const ChatHistorySchema = new mongoose.Schema({
    userId: {
        type: mongoose.Schema.Types.ObjectId,
        ref: 'User',
        required: true,
        index: true,
    },
    sessionId: {
        type: String,
        required: true,
        unique: true,
        index: true,
    },
    messages: [MessageSchema], // Array of message objects
    createdAt: {
        type: Date,
        default: Date.now,
    },
    updatedAt: {
        type: Date,
        default: Date.now,
    }
});

// Update `updatedAt` timestamp before saving any changes
ChatHistorySchema.pre('save', function (next) {
    if (this.isModified()) { // Only update if document changed
      this.updatedAt = Date.now();
    }
    next();
});

// Also update `updatedAt` on findOneAndUpdate operations if messages are modified
ChatHistorySchema.pre('findOneAndUpdate', function(next) {
  this.set({ updatedAt: new Date() });
  next();
});


const ChatHistory = mongoose.model('ChatHistory', ChatHistorySchema);

module.exports = ChatHistory;

```

`server/models/User.js`

```javascript
const mongoose = require('mongoose');
const bcrypt = require('bcryptjs');

const UserSchema = new mongoose.Schema({
  username: {
    type: String,
    required: [true, 'Please provide a username'],
    unique: true,
    trim: true,
  },
  password: {
    type: String,
    required: [true, 'Please provide a password'],
    minlength: 6,
    select: false, // Explicitly prevent password from being returned by default
  },
  uploadedDocuments: [
    {
      filename: {
        type: String,
      },
      text: {
        type: String,
        default: "",
      },
      analysis: {
        faq: {
          type: String,
          default: "",
        },
        topics: {
          type: String,
          default: "",
        },
        mindmap: {
          type: String,
          default: "",
        },
      },
    },
  ],
  createdAt: {
    type: Date,
    default: Date.now,
  },
});

// Password hashing middleware before saving
UserSchema.pre('save', async function (next) {
  // Only hash the password if it has been modified (or is new)
  if (!this.isModified('password')) {
    return next();
  }
  try {
    const salt = await bcrypt.genSalt(10);
    this.password = await bcrypt.hash(this.password, salt);
    next();
  } catch (err) {
    next(err);
  }
});

// Method to compare entered password with hashed password
// Ensure we fetch the password field when needed for comparison
UserSchema.methods.comparePassword = async function (candidatePassword) {
  // 'this.password' might be undefined due to 'select: false'
  // Fetch the user again including the password if needed, or ensure the calling context selects it
  // However, bcrypt.compare handles the comparison securely.
  // We assume 'this.password' is available in the context where comparePassword is called.
  if (!this.password) {
      // This scenario should be handled by the calling code (e.g., findOne().select('+password'))
      // Or by using a static method like findByCredentials
      console.error("Attempted to compare password, but password field was not loaded on the User object."); // Added more specific log
      throw new Error("Password field not available for comparison.");
  }
  // Use bcryptjs's compare function
  return await bcrypt.compare(candidatePassword, this.password);
};

// Ensure password is selected when finding user for login comparison
UserSchema.statics.findByCredentials = async function(username, password) {
    // Find user by username AND explicitly select the password field
    const user = await this.findOne({ username }).select('+password');
    if (!user) {
        console.log(`findByCredentials: User not found for username: ${username}`); // Debug log
        return null; // User not found
    }
    // Now 'user' object has the password field, safe to call comparePassword
    const isMatch = await user.comparePassword(password);
    if (!isMatch) {
        console.log(`findByCredentials: Password mismatch for username: ${username}`); // Debug log
        return null; // Password doesn't match
    }
    console.log(`findByCredentials: Credentials match for username: ${username}`); // Debug log
    // Return user object (password will still be selected here, but won't be sent in JSON response usually)
    return user;
};


const User = mongoose.model('User', UserSchema);

module.exports = User;

```

`server/rag_service/ai_core.py`

```python
# ./ai_core.py

# Standard Library Imports
import logging
import os
import io
import re
import copy
import uuid
from typing import Any, Callable, Dict, List, Optional


# --- Global Initializations ---
logger = logging.getLogger(__name__)

# --- Configuration Import ---
# Assumes 'server/config.py' is the actual config file.
# `import config` will work if 'server/' directory is in sys.path.
try:
    import config # This should import server/config.py
except ImportError as e:
    logger.info(f"CRITICAL: Failed to import 'config' (expected server/config.py): {e}. ")



# Local aliases for config flags, models, constants, and classes

# Availability Flags
PYPDF_AVAILABLE = config.PYPDF_AVAILABLE
PDFPLUMBER_AVAILABLE = config.PDFPLUMBER_AVAILABLE
PANDAS_AVAILABLE = config.PANDAS_AVAILABLE
DOCX_AVAILABLE = config.DOCX_AVAILABLE
PIL_AVAILABLE = config.PIL_AVAILABLE
FITZ_AVAILABLE = config.FITZ_AVAILABLE
PYTESSERACT_AVAILABLE = config.PYTESSERACT_AVAILABLE
SPACY_MODEL_LOADED = config.SPACY_MODEL_LOADED
PYPDF2_AVAILABLE = config.PYPDF2_AVAILABLE
EMBEDDING_MODEL_LOADED = config.EMBEDDING_MODEL_LOADED
MAX_TEXT_LENGTH_FOR_NER  = config.MAX_TEXT_LENGTH_FOR_NER
LANGCHAIN_SPLITTER_AVAILABLE = config.LANGCHAIN_SPLITTER_AVAILABLE

# Error Strings
PYPDF_PDFREADERROR = config.PYPDF_PDFREADERROR
TESSERACT_ERROR = config.TESSERACT_ERROR

# Libraries and Models
pypdf = config.pypdf
PyPDF2 = config.PyPDF2
pdfplumber = config.pdfplumber
pd = config.pd
DocxDocument = config.DocxDocument
Image = config.Image
fitz = config.fitz
pytesseract = config.pytesseract
nlp_spacy_core = config.nlp_spacy_core
document_embedding_model = config.document_embedding_model
RecursiveCharacterTextSplitter = config.RecursiveCharacterTextSplitter

# Constants
AI_CORE_CHUNK_SIZE = config.AI_CORE_CHUNK_SIZE
AI_CORE_CHUNK_OVERLAP = config.AI_CORE_CHUNK_OVERLAP
DOCUMENT_EMBEDDING_MODEL_NAME = config.DOCUMENT_EMBEDDING_MODEL_NAME


# --- Stage 1: File Parsing and Raw Content Extraction --- (Functions as previously corrected)
def _parse_pdf_content(file_path: str) -> Optional[str]:
    if not PYPDF_AVAILABLE or not pypdf:
        logger.error("pypdf library not available. PDF parsing with pypdf will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text: text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"pypdf: Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pypdf: File not found: {file_path}"); return None
    except PYPDF_PDFREADERROR as pdf_err: 
        logger.error(f"pypdf: Error reading PDF {os.path.basename(file_path)}: {pdf_err}"); return None
    except Exception as e:
        logger.error(f"pypdf: Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_docx_content(file_path: str) -> Optional[str]:
    if not DOCX_AVAILABLE or not DocxDocument:
        logger.error("python-docx library not available. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"docx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"docx: Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_txt_content(file_path: str) -> Optional[str]:
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: text_content = f.read()
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"txt: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"txt: Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_pptx_content(file_path: str) -> Optional[str]:
    if not PPTX_AVAILABLE or not Presentation:
        logger.warning(f"python-pptx not available. PPTX parsing for {os.path.basename(file_path)} skipped.")
        return None
    text_content = ""
    try:
        prs = Presentation(file_path)
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip(): text_content += shape.text.strip() + "\n\n"
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pptx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"pptx: Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _get_parser_for_file(file_path: str) -> Optional[Callable]:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf': return _parse_pdf_content
    if ext == '.docx': return _parse_docx_content
    if ext == '.pptx': return _parse_pptx_content
    if ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: return _parse_txt_content
    logger.warning(f"Unsupported file extension for basic parsing: {ext} ({os.path.basename(file_path)})")
    return None

def extract_raw_content_from_file(file_path: str) -> Dict[str, Any]:
    file_base_name = os.path.basename(file_path)
    logger.info(f"Starting raw content extraction for: {file_base_name}")
    text_content, tables, images, is_scanned = "", [], [], False
    file_extension = os.path.splitext(file_path)[1].lower()

    parser_func = _get_parser_for_file(file_path)
    if parser_func:
        initial_text = parser_func(file_path)
        if initial_text: text_content = initial_text

    if file_extension == '.pdf':
        if PDFPLUMBER_AVAILABLE and pdfplumber:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_parts = [p.extract_text(x_tolerance=1, y_tolerance=1) or "" for p in pdf.pages]
                    pdfplumber_text = "\n".join(pdfplumber_text_parts)
                    clean_pdfplumber_text_len = len(pdfplumber_text.replace("\n", ""))

                    if len(pdf.pages) > 0 and (clean_pdfplumber_text_len < 50 * len(pdf.pages) and clean_pdfplumber_text_len < 200):
                        is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (low text from pdfplumber).")
                    
                    if len(pdfplumber_text.strip()) > len(text_content.strip()): text_content = pdfplumber_text.strip()
                    elif not text_content.strip() and pdfplumber_text.strip(): text_content = pdfplumber_text.strip()
                    
                    for page_num, page in enumerate(pdf.pages): 
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data_list in page_tables_data:
                                if table_data_list and PANDAS_AVAILABLE and pd:
                                    try:
                                        if len(table_data_list) > 1 and all(isinstance(c, str) or c is None for c in table_data_list[0]):
                                            tables.append(pd.DataFrame(table_data_list[1:], columns=table_data_list[0]))
                                        else: tables.append(table_data_list)
                                    except Exception as df_err: logger.warning(f"pdfplumber: DataFrame conversion error for table on page {page_num} of {file_base_name}: {df_err}"); tables.append(table_data_list)
                                elif table_data_list: 
                                    tables.append(table_data_list)
                    if tables: logger.info(f"pdfplumber: Extracted {len(tables)} tables from {file_base_name}.")
            except Exception as e: logger.warning(f"pdfplumber: Error during rich PDF processing for {file_base_name}: {e}", exc_info=True)
        
        elif not text_content.strip() and PYPDF_AVAILABLE and pypdf: 
            try:
                if len(pypdf.PdfReader(file_path).pages) > 0: is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (no text from pypdf and pdfplumber not used/failed).")
            except: pass

        if FITZ_AVAILABLE and fitz and PIL_AVAILABLE and Image: 
            try:
                doc = fitz.open(file_path)
                for page_idx in range(len(doc)):
                    for img_info in doc.get_page_images(page_idx):
                        try: images.append(Image.open(io.BytesIO(doc.extract_image(img_info[0])["image"])))
                        except Exception as img_err: logger.warning(f"fitz: Could not open image xref {img_info[0]} from page {page_idx} of {file_base_name}: {img_err}")
                if images: logger.info(f"fitz: Extracted {len(images)} images from {file_base_name}.")
                doc.close()
            except Exception as e: logger.warning(f"fitz: Error extracting images from PDF {file_base_name}: {e}", exc_info=True)

    if not text_content.strip() and file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif']:
        is_scanned = True
        if PIL_AVAILABLE and Image:
            try: images.append(Image.open(file_path))
            except Exception as e: logger.error(f"Could not open image file {file_base_name}: {e}", exc_info=True)

    logger.info(f"Raw content extraction complete for {file_base_name}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {'text_content': text_content.strip(), 'tables': tables, 'images': images, 'is_scanned': is_scanned, 'file_type': file_extension}

# --- Stage 2: OCR --- (Function as previously corrected)
def perform_ocr_on_images(image_objects: List[Any]) -> str:
    if not image_objects: return ""
    if not PYTESSERACT_AVAILABLE or not pytesseract:
        logger.error("Pytesseract is not available. OCR cannot be performed.")
        return ""

    logger.info(f"Performing OCR on {len(image_objects)} image(s).")
    ocr_text_parts = []
    images_ocrd = 0
    for i, img in enumerate(image_objects):
        try:
            if not (PIL_AVAILABLE and Image and isinstance(img, Image.Image)):
                logger.warning(f"Skipping non-PIL Image object at index {i} for OCR.")
                continue
            text = pytesseract.image_to_string(img.convert('L'))
            if text and text.strip(): 
                ocr_text_parts.append(text.strip())
                images_ocrd += 1
        except Exception as e:
            if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
                logger.critical("Tesseract executable not found in PATH. OCR will fail for subsequent images too.")
                raise 
            logger.error(f"Error during OCR for image {i+1}/{len(image_objects)}: {e}", exc_info=True)
    
    full_ocr_text = "\n\n--- OCR Text from Image ---\n\n".join(ocr_text_parts)
    logger.info(f"OCR: Extracted {len(full_ocr_text)} chars from {images_ocrd} image(s) (out of {len(image_objects)} provided).")
    return full_ocr_text

# --- Stage 3: Text Cleaning and Normalization --- (Function as previously corrected)
def clean_and_normalize_text_content(text: str) -> str:
    if not text or not text.strip(): return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")
    text = re.sub(r'<script[^>]*>.*?</script>|<style[^>]*>.*?</style>|<[^>]+>', ' ', text, flags=re.I | re.S)
    text = re.sub(r'http\S+|www\S+|https\S+|\S*@\S*\s?', '', text, flags=re.MULTILINE)
    text = re.sub(r'[\n\t\r]+', ' ', text) 
    text = re.sub(r'\s+', ' ', text).strip() 
    text = re.sub(r'[^\w\s.,!?-]', '', text) 
    text_lower = text.lower()

    if not SPACY_MODEL_LOADED or not nlp_spacy_core:
        logger.warning("SpaCy model not loaded. Skipping tokenization/lemmatization. Returning regex-cleaned text.")
        return text_lower
    try:
        doc = nlp_spacy_core(text_lower, disable=['parser', 'ner']) 
        lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 1]
        final_cleaned_text = " ".join(lemmatized_tokens)
        logger.info(f"SpaCy-based text cleaning and normalization complete. Final length: {len(final_cleaned_text)}")
        if final_cleaned_text: logger.info(f"Final cleaned text (first 200 chars): {final_cleaned_text[:200]}...")
        return final_cleaned_text
    except Exception as e:
        logger.error(f"SpaCy processing failed: {e}. Returning pre-SpaCy cleaned text.", exc_info=True)
        return text_lower

# --- Stage 4: Layout Reconstruction & Table Integration --- (Function as previously corrected)
def reconstruct_document_layout(text_content: str, tables_data: List[Any], file_type: str) -> str:
    if not text_content and not tables_data: return ""
    logger.info(f"Starting layout reconstruction for file type '{file_type}'. Initial text length: {len(text_content)}, Tables: {len(tables_data)}.")
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text_content) 
    processed_text = re.sub(r'(\w+)-(\w+)', r'\1\2', processed_text)       

    if tables_data:
        table_md_parts = []
        for i, table_obj in enumerate(tables_data):
            header_md = f"\n\n[START OF TABLE {i+1}]\n"
            footer_md = f"\n[END OF TABLE {i+1}]\n"
            md_content = ""
            try:
                if PANDAS_AVAILABLE and pd and isinstance(table_obj, pd.DataFrame): 
                    md_content = table_obj.to_markdown(index=False)
                elif isinstance(table_obj, list) and table_obj and all(isinstance(r, list) for r in table_obj):
                    if table_obj and table_obj[0]: 
                        md_content = "| " + " | ".join(map(str, table_obj[0])) + " |\n"
                        md_content += "| " + " | ".join(["---"] * len(table_obj[0])) + " |\n"
                        for row_idx, row_data in enumerate(table_obj[1:]):
                            if len(row_data) == len(table_obj[0]): 
                                md_content += "| " + " | ".join(map(str, row_data)) + " |\n"
                            else:
                                logger.warning(f"Table {i+1}, row {row_idx+1} length mismatch with header. Skipping row.")
                    else: md_content = "[Empty Table Data]"
                else: md_content = str(table_obj)
            except Exception as e: 
                logger.warning(f"Table {i+1} to markdown conversion error: {e}. Using string representation."); 
                md_content = str(table_obj)
            if md_content: table_md_parts.append(header_md + md_content + footer_md)
        if table_md_parts: processed_text += "\n\n" + "\n\n".join(table_md_parts)
    
    processed_text = re.sub(r'\s+', ' ', processed_text).strip()
    logger.info(f"Layout reconstruction complete. Final text length: {len(processed_text)}")
    return processed_text

# --- Stage 5: Metadata Extraction --- (Function as previously corrected, uses original_file_name for logging)
def extract_document_metadata_info(file_path: str, processed_text: str, file_type_from_extraction: str, original_file_name: str, user_id: str) -> Dict[str, Any]:
    logger.info(f"Starting metadata extraction for: {original_file_name} (User: {user_id})")
    doc_meta = {'file_name': original_file_name, 'file_path_on_server': file_path, 'original_file_type': file_type_from_extraction,
                'processing_user': user_id, 'title': original_file_name, 'author': "Unknown", 'creation_date': None,
                'modification_date': None, 'page_count': 0, 'char_count_processed_text': len(processed_text),
                'named_entities': {}, 'structural_elements': "Paragraphs, Tables (inferred)", 'is_scanned_pdf': False}
    try:
        doc_meta['file_size_bytes'] = os.path.getsize(file_path)
        if PANDAS_AVAILABLE and pd:
            doc_meta['creation_date_os'] = pd.Timestamp(os.path.getctime(file_path), unit='s').isoformat()
            doc_meta['modification_date_os'] = pd.Timestamp(os.path.getmtime(file_path), unit='s').isoformat()
    except Exception as e: logger.warning(f"Metadata: OS metadata error for {original_file_name}: {e}")

    if file_type_from_extraction == '.pdf' and PYPDF2_AVAILABLE and PyPDF2:
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                info = reader.metadata
                if info:
                    if hasattr(info, 'title') and info.title: doc_meta['title'] = str(info.title).strip()
                    if hasattr(info, 'author') and info.author: doc_meta['author'] = str(info.author).strip()
                    if hasattr(info, 'creation_date') and info.creation_date and PANDAS_AVAILABLE and pd: 
                        doc_meta['creation_date'] = pd.Timestamp(info.creation_date).isoformat()
                doc_meta['page_count'] = len(reader.pages)
        except Exception as e: logger.warning(f"Metadata: PyPDF2 error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.docx' and DOCX_AVAILABLE and DocxDocument:
        try:
            doc = DocxDocument(file_path)
            props = doc.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = sum(1 for p in doc.paragraphs if p.text.strip()) or 1
        except Exception as e: logger.warning(f"Metadata: DOCX error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.pptx' and PPTX_AVAILABLE and Presentation:
        try:
            prs = Presentation(file_path)
            props = prs.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = len(prs.slides)
        except Exception as e: logger.warning(f"Metadata: PPTX error for {original_file_name}: {e}", exc_info=True)

    if doc_meta['page_count'] == 0 and processed_text: doc_meta['page_count'] = processed_text.count('\n\n') + 1

    if processed_text and SPACY_MODEL_LOADED and nlp_spacy_core:
        logger.info(f"Extracting named entities using SpaCy for {original_file_name}...")
        try:
            max_len = getattr(config, 'MAX_TEXT_LENGTH_FOR_NER', 500000) 
            text_for_ner = processed_text[:max_len]
            spacy_doc = nlp_spacy_core(text_for_ner) 
            ner_labels = nlp_spacy_core.pipe_labels.get("ner", [])
            entities = {label: [] for label in ner_labels}
            for ent in spacy_doc.ents:
                if ent.label_ in entities: entities[ent.label_].append(ent.text)
            doc_meta['named_entities'] = {k: list(set(v)) for k, v in entities.items() if v} 
            num_entities = sum(len(v_list) for v_list in doc_meta['named_entities'].values())
            logger.info(f"Extracted {num_entities} named entities for {original_file_name}.")
        except Exception as e: logger.error(f"Metadata: NER error for {original_file_name}: {e}", exc_info=True); doc_meta['named_entities'] = {}
    else: logger.info(f"Skipping NER for {original_file_name} (no text or SpaCy model not available/loaded)."); doc_meta['named_entities'] = {}
    
    logger.info(f"Metadata extraction complete for {original_file_name}.")
    return doc_meta

# --- Stage 6: Text Chunking ---
def chunk_document_into_segments(
    text_to_chunk: str,
    document_level_metadata: Dict[str, Any]
) -> List[Dict[str, Any]]:
    if not text_to_chunk or not text_to_chunk.strip():
        logger.warning(f"Chunking: No text for {document_level_metadata.get('file_name', 'unknown')}.")
        return []

    if not LANGCHAIN_SPLITTER_AVAILABLE or not RecursiveCharacterTextSplitter:
        logger.error("RecursiveCharacterTextSplitter not available. Cannot chunk text.")
        return []
        
    # CORRECTED: Use AI_CORE_CHUNK_SIZE and AI_CORE_CHUNK_OVERLAP from server/config.py
    chunk_s = AI_CORE_CHUNK_SIZE
    chunk_o = AI_CORE_CHUNK_OVERLAP

    original_doc_name_for_log = document_level_metadata.get('file_name', 'unknown')
    logger.info(f"Starting text chunking for {original_doc_name_for_log}. "
                f"Using config: CHUNK_SIZE={chunk_s}, CHUNK_OVERLAP={chunk_o}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_s,
        chunk_overlap=chunk_o,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], 
        keep_separator=True 
    )

    try: raw_text_segments: List[str] = text_splitter.split_text(text_to_chunk)
    except Exception as e: 
        logger.error(f"Chunking: Error splitting text for {original_doc_name_for_log}: {e}", exc_info=True)
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    base_file_name_for_ref = os.path.splitext(original_doc_name_for_log)[0] 

    for i, segment_content in enumerate(raw_text_segments):
        if not segment_content.strip(): 
            logger.debug(f"Skipping empty chunk at index {i} for {original_doc_name_for_log}.")
            continue

        chunk_specific_metadata = copy.deepcopy(document_level_metadata)
        
        qdrant_point_id = str(uuid.uuid4()) # Generate UUID for Qdrant

        chunk_specific_metadata['chunk_id'] = qdrant_point_id 
        chunk_specific_metadata['chunk_reference_name'] = f"{base_file_name_for_ref}_chunk_{i:04d}"
        chunk_specific_metadata['chunk_index'] = i
        chunk_specific_metadata['chunk_char_count'] = len(segment_content)
        
        output_chunks.append({
            'id': qdrant_point_id, 
            'text_content': segment_content,
            'metadata': chunk_specific_metadata 
        })
    
    logger.info(f"Chunking: Split '{original_doc_name_for_log}' into {len(output_chunks)} non-empty chunks with UUID IDs.")
    return output_chunks

# --- Stage 7: Embedding Generation --- (Function as previously corrected)
def generate_segment_embeddings(document_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not document_chunks: return []
    if not EMBEDDING_MODEL_LOADED or not document_embedding_model: # Check if model is loaded
        logger.error("Embedding model not loaded. Cannot generate embeddings.")
        for chunk_dict in document_chunks: chunk_dict['embedding'] = None # Ensure key exists
        return document_chunks

    # CORRECTED: Get model name from DOCUMENT_EMBEDDING_MODEL_NAME
    model_name_for_logging = getattr(config, 'DOCUMENT_EMBEDDING_MODEL_NAME', "Unknown Model")
    logger.info(f"Embedding: Starting embedding generation for {len(document_chunks)} chunks "
                f"using model: {model_name_for_logging}.")
    
    texts_to_embed: List[str] = []
    valid_chunk_indices: List[int] = []

    for i, chunk_dict in enumerate(document_chunks):
        text_content = chunk_dict.get('text_content')
        if text_content and text_content.strip():
            texts_to_embed.append(text_content)
            valid_chunk_indices.append(i)
        else:
            chunk_dict['embedding'] = None
            logger.debug(f"Embedding: Chunk {chunk_dict.get('id', i)} has no text content, skipping embedding.")

    if not texts_to_embed:
        logger.warning("Embedding: No actual text content found in any provided chunks to generate embeddings.")
        return document_chunks

    try:
        logger.info(f"Embedding: Generating embeddings for {len(texts_to_embed)} non-empty text segments...")
        embeddings_np_array = document_embedding_model.encode(texts_to_embed, show_progress_bar=False)
        
        for i, original_chunk_index in enumerate(valid_chunk_indices):
            if i < len(embeddings_np_array):
                document_chunks[original_chunk_index]['embedding'] = embeddings_np_array[i].tolist()
            else:
                logger.error(f"Embedding: Mismatch in embedding count for chunk at original index {original_chunk_index}. Assigning None.")
                document_chunks[original_chunk_index]['embedding'] = None
        
        logger.info(f"Embedding: Embeddings generated and assigned to {len(valid_chunk_indices)} chunks.")
        
    except Exception as e:
        logger.error(f"Embedding: Error during embedding generation with model {model_name_for_logging}: {e}", exc_info=True)
        for original_chunk_index in valid_chunk_indices:
            document_chunks[original_chunk_index]['embedding'] = None
        
    return document_chunks


# --- Main Orchestration Function (to be called by app.py) ---
def process_document_for_qdrant(file_path: str, original_name: str, user_id: str) -> List[Dict[str, Any]]:
    logger.info(f"ai_core: Orchestrating document processing for {original_name}, user {user_id}")
    if not os.path.exists(file_path): 
        logger.error(f"File not found at ai_core entry point: {file_path}")
        raise FileNotFoundError(f"File not found: {file_path}")

    try:

        # Step 1: Extract Raw content
        raw_content = extract_raw_content_from_file(file_path)
        # Example of how to access: raw_content['text_content'], raw_content['images'], etc.
        # file_type will be important: raw_content['file_type']
        # is_scanned will be important: raw_content['is_scanned']
        initial_extracted_text = raw_content.get('text_content', "") # THIS IS WHAT YOU WANT TO RETURN for Node.js analysis


        # Step 2: Perform OCR if needed
        ocr_text_output = ""
        if raw_content.get('is_scanned') and raw_content.get('images'):
            if PYTESSERACT_AVAILABLE and pytesseract:
                 ocr_text_output = perform_ocr_on_images(raw_content['images'])
            else:
                logger.warning(f"OCR requested for {original_name} but Pytesseract is not available/configured. Skipping OCR.")

        combined_text = raw_content.get('text_content', "")
        if ocr_text_output:
            if raw_content.get('is_scanned') and len(ocr_text_output) > len(combined_text) / 2:
                 combined_text = ocr_text_output + "\n\n" + combined_text
            else:
                 combined_text += "\n\n" + ocr_text_output
        
        if not combined_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No text content for {original_name} after raw extraction/OCR, and no tables. Returning empty.")
            return []

        cleaned_text = clean_and_normalize_text_content(combined_text)
        if not cleaned_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No meaningful text for {original_name} after cleaning, and no tables. Returning empty.")
            return []

        text_for_metadata_and_chunking = reconstruct_document_layout(
            cleaned_text,
            raw_content.get('tables', []),
            raw_content.get('file_type', '')
        )

        doc_metadata = extract_document_metadata_info(
            file_path,
            text_for_metadata_and_chunking, 
            raw_content.get('file_type', ''),
            original_name,
            user_id
        )
        doc_metadata['is_scanned_pdf'] = raw_content.get('is_scanned', False)

        # This now uses corrected config variable names and UUIDs for IDs
        chunks_with_metadata = chunk_document_into_segments( 
            text_for_metadata_and_chunking,
            doc_metadata
        )
        if not chunks_with_metadata:
            logger.warning(f"No chunks produced for {original_name}. Returning empty list.")
            return []

        # This now uses corrected config variable name for logging
        final_chunks_with_embeddings = generate_segment_embeddings(chunks_with_metadata)
        
        logger.info(f"ai_core: Successfully processed {original_name}. Generated {len(final_chunks_with_embeddings)} chunks.")
        return final_chunks_with_embeddings, initial_extracted_text, chunks_with_metadata

    except Exception as e: 
        if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
            logger.critical(f"ai_core: Tesseract (OCR engine) was not found during processing of {original_name}. OCR could not be performed.", exc_info=False)
            raise 
        
        logger.error(f"ai_core: Critical error during processing of {original_name}: {e}", exc_info=True)
        raise

```

`server/rag_service/app.py`

```python
# server/rag_service/app.py

import os
import sys
import traceback
from flask import Flask, request, jsonify, current_app
import logging
import atexit # For graceful shutdown

# --- Add server directory to sys.path ---
SERVER_DIR = os.path.dirname(os.path.abspath(__file__))
if SERVER_DIR not in sys.path:
    sys.path.insert(0, SERVER_DIR)

import config
config.setup_logging() # Initialize logging as per your config

# --- Import configurations and services ---
try:
    from vector_db_service import VectorDBService
    import ai_core
    import neo4j_handler 
    from neo4j import exceptions as neo4j_exceptions # For specific error handling
except ImportError as e:
    print(f"CRITICAL IMPORT ERROR: {e}. Ensure all modules are correctly placed and server directory is in PYTHONPATH.")
    print("PYTHONPATH:", sys.path)
    sys.exit(1)

logger = logging.getLogger(__name__)
app = Flask(__name__)

# --- Initialize VectorDBService (Qdrant) ---
vector_service = None
try:
    logger.info("Initializing VectorDBService for Qdrant...")
    vector_service = VectorDBService()
    vector_service.setup_collection()
    app.vector_service = vector_service
    logger.info("VectorDBService initialized and Qdrant collection setup successfully.")
except Exception as e:
    logger.critical(f"Failed to initialize VectorDBService or setup Qdrant collection: {e}", exc_info=True)
    app.vector_service = None

# --- Initialize Neo4j Driver (via handler) ---
try:
    neo4j_handler.init_driver() # Initialize Neo4j driver on app start
except Exception as e:
    logger.critical(f"Neo4j driver failed to initialize on startup: {e}. KG endpoints will likely fail.")
    # Depending on how critical Neo4j is, you might sys.exit(1) here.

# Register Neo4j driver close function for app exit
atexit.register(neo4j_handler.close_driver)


# --- Helper for Error Responses ---
def create_error_response(message, status_code=500, details=None):
    log_message = f"API Error ({status_code}): {message}"
    if details:
        log_message += f" | Details: {details}"
    current_app.logger.error(log_message)
    response_payload = {"error": message}
    if details and status_code != 500:
        response_payload["details"] = details
    return jsonify(response_payload), status_code

# === API Endpoints ===

@app.route('/health', methods=['GET'])
def health_check():
    current_app.logger.info("--- Health Check Request ---")
    # ... (Qdrant health check part from your existing code) ...
    status_details = {
        "status": "error",
        "qdrant_service": "not_initialized",
        "qdrant_collection_name": config.QDRANT_COLLECTION_NAME,
        "qdrant_collection_status": "unknown",
        "document_embedding_model": config.DOCUMENT_EMBEDDING_MODEL_NAME,
        "query_embedding_model": config.QUERY_EMBEDDING_MODEL_NAME,
        "neo4j_service": "not_initialized_via_handler", # Updated message
        "neo4j_connection": "unknown"
    }
    http_status_code = 503

    # Qdrant Check
    if not vector_service:
        status_details["qdrant_service"] = "failed_to_initialize"
    else:
        status_details["qdrant_service"] = "initialized"
        try:
            vector_service.client.get_collection(collection_name=vector_service.collection_name)
            status_details["qdrant_collection_status"] = "exists_and_accessible"
        except Exception as e:
            status_details["qdrant_collection_status"] = f"error_accessing_collection: {str(e)}"
            current_app.logger.error(f"Health check: Error accessing Qdrant collection: {e}", exc_info=False)
    
    # Neo4j Check
    neo4j_ok, neo4j_conn_status = neo4j_handler.check_neo4j_connectivity()
    if neo4j_ok:
        status_details["neo4j_service"] = "initialized_via_handler"
        status_details["neo4j_connection"] = "connected"
    else:
        status_details["neo4j_service"] = "initialization_failed_or_handler_error"
        status_details["neo4j_connection"] = neo4j_conn_status


    if status_details["qdrant_service"] == "initialized" and \
       status_details["qdrant_collection_status"] == "exists_and_accessible" and \
       neo4j_ok: # Check the boolean return from neo4j_handler
        status_details["status"] = "ok"
        http_status_code = 200
        current_app.logger.info("Health check successful (Qdrant & Neo4j).")
    else:
        current_app.logger.warning(f"Health check issues found: Qdrant service: {status_details['qdrant_service']}, Qdrant collection: {status_details['qdrant_collection_status']}, Neo4j service: {status_details['neo4j_service']}, Neo4j connection: {status_details['neo4j_connection']}")
        
    return jsonify(status_details), http_status_code

@app.route('/add_document', methods=['POST'])
def add_document_qdrant():
    # ... (your existing /add_document endpoint logic)
    # This remains unchanged as it deals with Qdrant.
    current_app.logger.info("--- /add_document Request (Qdrant) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    user_id = data.get('user_id')
    file_path = data.get('file_path')
    original_name = data.get('original_name')

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    current_app.logger.info(f"Processing file: '{original_name}' (Path: '{file_path}') for user: '{user_id}' for Qdrant")

    if not os.path.exists(file_path):
        current_app.logger.error(f"File not found at server path: {file_path}")
        return create_error_response(f"File not found at server path: {file_path}", 404)

    try:
        current_app.logger.info(f"Calling ai_core to process document: '{original_name}' for Qdrant")
        # ai_core.process_document_for_qdrant returns: processed_chunks_with_embeddings, raw_text_for_node_analysis, chunks_with_metadata
        processed_chunks_with_embeddings, raw_text_for_node_analysis, chunks_with_metadata_for_kg = ai_core.process_document_for_qdrant(
            file_path=file_path,
            original_name=original_name,
            user_id=user_id
        )
        
        num_chunks_added_to_qdrant = 0
        processing_status = "processed_no_content_for_qdrant"

        if processed_chunks_with_embeddings:
            current_app.logger.info(f"ai_core generated {len(processed_chunks_with_embeddings)} chunks for '{original_name}'. Adding to Qdrant.")
            num_chunks_added_to_qdrant = app.vector_service.add_processed_chunks(processed_chunks_with_embeddings)
            if num_chunks_added_to_qdrant > 0:
                processing_status = "added_to_qdrant"
            else:
                processing_status = "processed_qdrant_chunks_not_added"
        elif raw_text_for_node_analysis:
             current_app.logger.info(f"ai_core produced no processable Qdrant chunks for '{original_name}', but raw text was extracted.")
             processing_status = "processed_for_analysis_only_no_qdrant"
        else:
            current_app.logger.warning(f"ai_core produced no Qdrant chunks and no raw text for '{original_name}'.")
            return jsonify({
                "message": f"Processed '{original_name}' but no content was extracted for Qdrant or analysis.",
                "status": "no_content_extracted",
                "filename": original_name,
                "user_id": user_id,
                "num_chunks_added_to_qdrant": 0,
                "raw_text_for_analysis": ""
            }), 200

        response_payload = {
            "message": f"Successfully processed '{original_name}' for Qdrant. Status: {processing_status}.",
            "status": "added",
            "filename": original_name,
            "user_id": user_id,
            "num_chunks_added_to_qdrant": num_chunks_added_to_qdrant,
            "raw_text_for_analysis": raw_text_for_node_analysis if raw_text_for_node_analysis is not None else "",
            "chunks_with_metadata": chunks_with_metadata_for_kg # Pass this to Node.js for KG worker
        }
        current_app.logger.info(f"Successfully processed '{original_name}' for Qdrant. Returning raw text and Qdrant status.")
        return jsonify(response_payload), 201

    except FileNotFoundError as e:
        current_app.logger.error(f"Add Document (Qdrant) Error for '{original_name}' - FileNotFoundError: {e}", exc_info=True)
        return create_error_response(f"File not found during Qdrant processing: {str(e)}", 404)
    except config.TESSERACT_ERROR:
        current_app.logger.critical(f"Add Document (Qdrant) Error for '{original_name}' - Tesseract (OCR) not found.")
        return create_error_response("OCR engine (Tesseract) not found or not configured correctly on the server.", 500)
    except ValueError as e:
        current_app.logger.error(f"Add Document (Qdrant) Error for '{original_name}' - ValueError: {e}", exc_info=True)
        return create_error_response(f"Configuration or data error for Qdrant: {str(e)}", 400)
    except Exception as e:
        current_app.logger.error(f"Add Document (Qdrant) Error for '{original_name}' - Unexpected Exception: {e}\n{traceback.format_exc()}", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}' for Qdrant due to an internal error.", 500)

@app.route('/query', methods=['POST'])
def search_qdrant_documents_and_get_kg(): # Renamed for clarity
    current_app.logger.info("--- /query Request (Qdrant Search + KG Retrieval) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)
    
    # Also check Neo4j driver availability for KG retrieval
    try:
        neo4j_handler.get_driver_instance() # Will raise ConnectionError if not available
    except ConnectionError:
        return create_error_response("Knowledge Graph service (Neo4j) is not available.", 503)


    data = request.get_json()
    query_text = data.get('query')
    user_id_from_request = data.get('user_id') # <<< NEW: Expect user_id
    k = data.get('k', config.QDRANT_DEFAULT_SEARCH_K)
    filter_payload_from_request = data.get('filter') # This is the Qdrant filter
    
    qdrant_filters = None

    if not query_text:
        return create_error_response("Missing 'query' field in request body", 400)
    if not user_id_from_request: # <<< NEW: Validate user_id
        return create_error_response("Missing 'user_id' field in request body", 400)

    # --- Qdrant Filter Setup ---
    # The filter_payload_from_request is for Qdrant. It might contain user_id and original_name.
    # It's important that if a filter is used for Qdrant, it's consistent with the user_id
    # we'll use for KG retrieval. The client (Node.js) should ensure this consistency.
    if filter_payload_from_request and isinstance(filter_payload_from_request, dict):
        from qdrant_client import models as qdrant_models # Import here for safety
        conditions = []
        for key, value in filter_payload_from_request.items():
            conditions.append(qdrant_models.FieldCondition(key=key, match=qdrant_models.MatchValue(value=value)))
        if conditions:
            qdrant_filters = qdrant_models.Filter(must=conditions)
            current_app.logger.info(f"Applying Qdrant filter: {filter_payload_from_request}")
    else:
        current_app.logger.info("No Qdrant filter explicitly provided by client in this query.")


    try:
        k = int(k)
    except ValueError:
        return create_error_response("'k' must be an integer", 400)

    current_app.logger.info(f"Performing Qdrant search for user '{user_id_from_request}', query (first 50): '{query_text[:50]}...' with k={k}")

    try:
        # 1. Perform Qdrant Search
        # `docs` is List[Document], `formatted_context` is str, `docs_map` is Dict
        qdrant_retrieved_docs, formatted_context_snippet, qdrant_docs_map = vector_service.search_documents(
            query=query_text,
            k=k,
            filter_conditions=qdrant_filters # Use the filter passed from client
        )

        # 2. Prepare KG Retrieval based on Qdrant results
        knowledge_graphs_data = {} # To store KGs, mapping documentName to KG object
        
        if qdrant_retrieved_docs:
            unique_doc_names_for_kg = set()
            for doc_obj in qdrant_retrieved_docs:
                # We need 'documentName' or 'original_name' from the Qdrant doc metadata
                # to identify which KG to fetch.
                doc_meta = doc_obj.metadata
                doc_name_for_kg = doc_meta.get('documentName', doc_meta.get('original_name', doc_meta.get('file_name')))
                
                if doc_name_for_kg:
                    unique_doc_names_for_kg.add(doc_name_for_kg)
                else:
                    current_app.logger.warning(f"Qdrant doc metadata missing document identifier (documentName/original_name/file_name) for chunk ID {doc_meta.get('qdrant_id', 'N/A')}. Cannot fetch KG for this chunk's document.")
            
            current_app.logger.info(f"Found {len(unique_doc_names_for_kg)} unique document(s) in Qdrant results to fetch KGs for: {list(unique_doc_names_for_kg)}")

            for doc_name in unique_doc_names_for_kg:
                try:
                    current_app.logger.info(f"Fetching KG for document '{doc_name}' (User: {user_id_from_request})")
                    kg_content = neo4j_handler.get_knowledge_graph(user_id_from_request, doc_name)
                    if kg_content: # get_knowledge_graph returns None if not found
                        knowledge_graphs_data[doc_name] = kg_content
                        current_app.logger.info(f"Successfully retrieved KG for '{doc_name}'. Nodes: {len(kg_content.get('nodes',[]))}, Edges: {len(kg_content.get('edges',[]))}")
                    else:
                        current_app.logger.info(f"No KG found in Neo4j for document '{doc_name}' (User: {user_id_from_request}).")
                        knowledge_graphs_data[doc_name] = {"nodes": [], "edges": [], "message": "KG not found"} # Indicate not found
                except Exception as kg_err:
                    current_app.logger.error(f"Error retrieving KG for document '{doc_name}': {kg_err}", exc_info=True)
                    knowledge_graphs_data[doc_name] = {"nodes": [], "edges": [], "error": f"Failed to retrieve KG: {str(kg_err)}"}


        # 3. Construct Final Response Payload
        response_payload = {
            "query": query_text,
            "k_requested": k,
            "user_id_processed": user_id_from_request, # Echo back user_id
            "qdrant_filter_applied": filter_payload_from_request, # Show Qdrant filter used
            "qdrant_results_count": len(qdrant_retrieved_docs),
            "formatted_context_snippet": formatted_context_snippet,
            "retrieved_documents_map": qdrant_docs_map, # From Qdrant vector_service
            "retrieved_documents_list": [doc.to_dict() for doc in qdrant_retrieved_docs], # From Qdrant vector_service
            "knowledge_graphs": knowledge_graphs_data # <<< NEW: KG data
        }
        
        current_app.logger.info(f"Qdrant search and KG retrieval successful. Returning {len(qdrant_retrieved_docs)} Qdrant docs and KGs for {len(knowledge_graphs_data)} documents.")
        return jsonify(response_payload), 200

    except ConnectionError as ce: # Catch Neo4j or Qdrant connection errors
        current_app.logger.error(f"Service connection error during /query processing: {ce}", exc_info=True)
        return create_error_response(f"A dependent service is unavailable: {str(ce)}", 503)
    except neo4j_exceptions.Neo4jError as ne: # Catch specific Neo4j errors
        current_app.logger.error(f"Neo4j database error during /query processing: {ne}", exc_info=True)
        return create_error_response(f"Neo4j database operation failed: {ne.message}", 500)
    except Exception as e:
        current_app.logger.error(f"/query processing failed: {e}\n{traceback.format_exc()}", exc_info=True)
        return create_error_response(f"Error during query processing: {str(e)}", 500)

@app.route('/delete_qdrant_document_data', methods=['DELETE']) # Ensure this route is defined
def delete_qdrant_data_route():
    current_app.logger.info("--- DELETE /delete_qdrant_document_data Request ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    user_id = data.get('user_id')
    document_name = data.get('document_name') 

    if not user_id or not document_name:
        return create_error_response("Missing 'user_id' or 'document_name' in request body.", 400)

    try:
        # This assumes you have a method 'delete_document_vectors' in your vector_service
        result = vector_service.delete_document_vectors(user_id, document_name) 
        
        if result.get("success"):
            return jsonify({"message": result.get("message", "Qdrant vectors for document processed for deletion.")}), 200
        else:
            return create_error_response(result.get("message", "Failed to delete Qdrant vectors."), 500) # Or a more specific code
            
    except ConnectionError as ce:
        current_app.logger.error(f"Qdrant connection error during /delete_qdrant_document_data for user {user_id}, doc {document_name}: {ce}", exc_info=True)
        return create_error_response(f"Qdrant service connection error: {str(ce)}", 503)
    except Exception as e:
        current_app.logger.error(f"/delete_qdrant_document_data for user {user_id}, doc {document_name} failed: {e}", exc_info=True)
        return create_error_response(f"Error during Qdrant data deletion: {str(e)}", 500)

# === KG (Neo4j) Endpoints ===

@app.route('/kg', methods=['POST'])
def add_or_update_kg_route():
    current_app.logger.info("--- POST /kg Request (Neo4j Ingestion) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('userId') # Key from Node.js
    original_name = data.get('originalName') # Key from Node.js
    nodes = data.get('nodes')
    edges = data.get('edges')

    if not all([user_id, original_name, isinstance(nodes, list), isinstance(edges, list)]):
        missing_fields = []
        if not user_id: missing_fields.append("userId")
        if not original_name: missing_fields.append("originalName")
        if not isinstance(nodes, list): missing_fields.append("nodes (must be a list)")
        if not isinstance(edges, list): missing_fields.append("edges (must be a list)")
        return create_error_response(f"Missing or invalid fields: {', '.join(missing_fields)}", 400,
                                     details=f"Received: userId type {type(user_id)}, originalName type {type(original_name)}, nodes type {type(nodes)}, edges type {type(edges)}")

    logger.info(f"Attempting to ingest KG for user '{user_id}', document '{original_name}'. Nodes: {len(nodes)}, Edges: {len(edges)}")

    try:
        result = neo4j_handler.ingest_knowledge_graph(user_id, original_name, nodes, edges)
        if result["success"]:
            return jsonify({
                "message": result["message"],
                "userId": user_id,
                "documentName": original_name, # Consistent key name
                "nodes_affected": result["nodes_affected"],
                "edges_affected": result["edges_affected"],
                "status": "completed" # Status field as expected by Node.js
            }), 201
        else: # Should not happen if ingest_knowledge_graph raises on error
            return create_error_response(result.get("message", "KG ingestion failed."), 500)
            
    except ConnectionError as e:
        logger.error(f"Neo4j connection error during KG ingestion for '{original_name}': {e}", exc_info=True)
        return create_error_response(f"Neo4j connection error: {str(e)}. Please check service.", 503)
    except neo4j_exceptions.Neo4jError as e:
        logger.error(f"Neo4jError during KG ingestion for '{original_name}': {e}", exc_info=True)
        return create_error_response(f"Neo4j database error: {e.message}", 500)
    except Exception as e:
        logger.error(f"Unexpected error during KG ingestion for '{original_name}': {e}\n{traceback.format_exc()}", exc_info=True)
        return create_error_response(f"Failed to ingest Knowledge Graph: {str(e)}", 500)


@app.route('/kg/<user_id>/<path:document_name>', methods=['GET']) # Use <path:document_name> to allow slashes
def get_kg_route(user_id, document_name):
    current_app.logger.info(f"--- GET /kg/{user_id}/{document_name} Request (Neo4j Retrieval) ---")

    # Basic sanitization (you might want more robust URL segment sanitization if needed)
    sanitized_user_id = user_id.replace("..","").strip()
    sanitized_document_name = document_name.replace("..","").strip()

    if not sanitized_user_id or not sanitized_document_name:
        return create_error_response("User ID and Document Name URL parameters are required and cannot be empty.", 400)

    logger.info(f"Retrieving KG for user '{sanitized_user_id}', document '{sanitized_document_name}'.")

    try:
        kg_data = neo4j_handler.get_knowledge_graph(sanitized_user_id, sanitized_document_name)

        if kg_data is None: # Handler returns None if not found
            logger.info(f"No KG data found for user '{sanitized_user_id}', document '{sanitized_document_name}'.")
            return create_error_response("Knowledge Graph not found for the specified user and document.", 404)

        logger.info(f"Successfully retrieved KG for document '{sanitized_document_name}'. Nodes: {len(kg_data.get('nodes',[]))}, Edges: {len(kg_data.get('edges',[]))}")
        return jsonify(kg_data), 200

    except ConnectionError as e:
        logger.error(f"Neo4j connection error during KG retrieval: {e}", exc_info=True)
        return create_error_response(f"Neo4j connection error: {str(e)}. Please check service.", 503)
    except neo4j_exceptions.Neo4jError as e:
        logger.error(f"Neo4jError during KG retrieval: {e}", exc_info=True)
        return create_error_response(f"Neo4j database error: {e.message}", 500)
    except Exception as e:
        logger.error(f"Unexpected error during KG retrieval: {e}\n{traceback.format_exc()}", exc_info=True)
        return create_error_response(f"Failed to retrieve Knowledge Graph: {str(e)}", 500)


@app.route('/kg/<user_id>/<path:document_name>', methods=['DELETE']) # Use <path:document_name>
def delete_kg_route(user_id, document_name):
    current_app.logger.info(f"--- DELETE /kg/{user_id}/{document_name} Request (Neo4j Deletion) ---")

    sanitized_user_id = user_id.replace("..","").strip()
    sanitized_document_name = document_name.replace("..","").strip()

    if not sanitized_user_id or not sanitized_document_name:
        return create_error_response("User ID and Document Name URL parameters are required and cannot be empty.", 400)

    logger.info(f"Attempting to delete KG for user '{sanitized_user_id}', document '{sanitized_document_name}'.")

    try:
        deleted = neo4j_handler.delete_knowledge_graph(sanitized_user_id, sanitized_document_name)
        if deleted:
            logger.info(f"Knowledge Graph for document '{sanitized_document_name}' (User: {sanitized_user_id}) deleted successfully.")
            return jsonify({"message": "Knowledge Graph deleted successfully."}), 200
        else:
            logger.info(f"No Knowledge Graph found for document '{sanitized_document_name}' (User: {sanitized_user_id}) to delete.")
            return create_error_response("Knowledge Graph not found for deletion.", 404)

    except ConnectionError as e:
        logger.error(f"Neo4j connection error during KG deletion: {e}", exc_info=True)
        return create_error_response(f"Neo4j connection error: {str(e)}. Please check service.", 503)
    except neo4j_exceptions.Neo4jError as e:
        logger.error(f"Neo4jError during KG deletion: {e}", exc_info=True)
        return create_error_response(f"Neo4j database error: {e.message}", 500)
    except Exception as e:
        logger.error(f"Unexpected error during KG deletion: {e}\n{traceback.format_exc()}", exc_info=True)
        return create_error_response(f"Failed to delete Knowledge Graph: {str(e)}", 500)


if __name__ == '__main__':
    logger.info(f"--- Starting RAG API Service (with KG) on port {config.API_PORT} ---")
    logger.info(f"Qdrant Host: {config.QDRANT_HOST}, Port: {config.QDRANT_PORT}, Collection: {config.QDRANT_COLLECTION_NAME}")
    logger.info(f"Neo4j URI: {config.NEO4J_URI}, User: {config.NEO4J_USERNAME}, DB: {config.NEO4J_DATABASE}")
    logger.info(f"Document Embedding Model (ai_core): {config.DOCUMENT_EMBEDDING_MODEL_NAME} (Dim: {config.DOCUMENT_VECTOR_DIMENSION})")
    logger.info(f"Query Embedding Model (vector_db_service): {config.QUERY_EMBEDDING_MODEL_NAME} (Dim: {config.QUERY_VECTOR_DIMENSION})")
    
    app.run(host='0.0.0.0', port=config.API_PORT, debug=True) # debug=True for development
```

`server/rag_service/code.txt`

```
============ ./ai_core.py ============
import os
process_pdf_for_embeddings
from PIL import Image # For image processing
import pytesseract # OCR
import fitz # PyMuPDF for PDF parsing
import pdfplumber # For tables and layout
import PyPDF2 # For basic metadata
import re # Regular expressions for cleaning



============ ./app.py ============
# server/rag_service/app.py

import os
import sys
from flask import Flask, request, jsonify
from process_document import process_uploaded_document
import ai_core

# Add server directory to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
sys.path.insert(0, server_dir) # Ensure rag_service can be imported

# Now import local modules AFTER adjusting sys.path
from rag_service import config
import rag_service.file_parser as file_parser
import rag_service.faiss_handler as faiss_handler
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

def create_error_response(message, status_code=500):
    logger.error(f"API Error Response ({status_code}): {message}")
    return jsonify({"error": message}), status_code

@app.route('/health', methods=['GET'])
def health_check():
    logger.info("\n--- Received request at /health ---")
    status_details = {
        "status": "error",
        "embedding_model_type": config.EMBEDDING_TYPE,
        "embedding_model_name": config.EMBEDDING_MODEL_NAME,
        "embedding_dimension": None,
        "sentence_transformer_load": None,
        "default_index_loaded": False,
        "default_index_vectors": 0,
        "default_index_dim": None,
        "message": ""
    }
    http_status_code = 503

    try:
        # Check Embedding Model
        model = faiss_handler.embedding_model
        if model is None:
            status_details["message"] = "Embedding model could not be initialized during startup."
            status_details["sentence_transformer_load"] = "Failed"
            raise RuntimeError(status_details["message"])
        else:
            status_details["sentence_transformer_load"] = "OK"
            try:
                 status_details["embedding_dimension"] = faiss_handler.get_embedding_dimension(model)
            except Exception as dim_err:
                 status_details["embedding_dimension"] = f"Error: {dim_err}"


        # Check Default Index
        if config.DEFAULT_INDEX_USER_ID in faiss_handler.loaded_indices:
            status_details["default_index_loaded"] = True
            default_index = faiss_handler.loaded_indices[config.DEFAULT_INDEX_USER_ID]
            if hasattr(default_index, 'index') and default_index.index:
                status_details["default_index_vectors"] = default_index.index.ntotal
                status_details["default_index_dim"] = default_index.index.d
            logger.info("Default index found in cache.")
        else:
            logger.info("Attempting to load default index for health check...")
            try:
                default_index = faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID)
                status_details["default_index_loaded"] = True
                if hasattr(default_index, 'index') and default_index.index:
                    status_details["default_index_vectors"] = default_index.index.ntotal
                    status_details["default_index_dim"] = default_index.index.d
                logger.info("Default index loaded successfully during health check.")
            except Exception as index_load_err:
                logger.error(f"Health check failed to load default index: {index_load_err}", exc_info=True)
                status_details["message"] = f"Failed to load default index: {index_load_err}"
                status_details["default_index_loaded"] = False
                raise # Re-raise to indicate failure

        # Final Status
        status_details["status"] = "ok"
        status_details["message"] = "RAG service is running, embedding model accessible, default index loaded."
        http_status_code = 200
        logger.info("Health check successful.")

    except Exception as e:
        logger.error(f"--- Health Check Error ---", exc_info=True)
        if not status_details["message"]: # Avoid overwriting specific error messages
            status_details["message"] = f"Health check failed: {str(e)}"
        # Ensure status is error if exception occurred
        status_details["status"] = "error"
        http_status_code = 503 # Service unavailable if health check fails critically

    return jsonify(status_details), http_status_code


@app.route('/add_document', methods=['POST'])
def add_document():
    logger.info("\n--- Received request at /add_document ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    file_path = data.get('file_path')
    original_name = data.get('original_name')

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    logger.info(f"Processing file: {original_name} for user: {user_id}")
    logger.info(f"File path: {file_path}")

    if not os.path.exists(file_path):
        return create_error_response(f"File not found at path: {file_path}", 404)

    try:
        logger.info(f"Initialising [ process_file function ] for : {original_name} for user: {user_id}")

        final_chunks_with_embeddings = ai_core.process_pdf_for_embeddings(file_path, original_name, user_id)

        if not final_chunks_with_embeddings:
            logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
            return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200
        
        logger.info(f"Final Chunks with Embeddings: {final_chunks_with_embeddings}")

    except Exception as e:
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)

        
    # try:
    #     # 1. Parse File
    #     username = "abcd"
    #     text_content = process_uploaded_document(file_path, username=username)
    #     if text_content is None:
    #         logger.warning(f"Skipping embedding for {original_name}: File type not supported or parsing failed.")
    #         return jsonify({"message": f"File type of '{original_name}' not supported for RAG or parsing failed.", "filename": original_name, "status": "skipped"}), 200

    #     logger.info("Text_Content : {text_content}")
    #     # if not text_content.strip():
    #     #     logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
    #     #     return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 2. Chunk Text
    #     # documents = file_parser.chunk_text(text_content, original_name, user_id)
    #     # if not documents:
    #     #     logger.warning(f"No chunks created for {original_name}. Skipping add.")
    #     #     return jsonify({"message": f"No text chunks generated for '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 3. Add to Index (faiss_handler now handles dimension checks/recreation)
    #     # faiss_handler.add_documents_to_index(user_id, documents)

    #     # logger.info(f"Successfully processed and added document: {original_name} for user: {user_id}")
    #     # return jsonify({
    #     #     "message": f"Document '{original_name}' processed and added to index.",
    #     #     "filename": original_name,
    #     #     "chunks_added": len(documents),
    #     #     "status": "added"
    #     # }), 200
    # except Exception as e:
        # Log the specific error from faiss_handler if it raised one
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)


@app.route('/query', methods=['POST'])
def query_index_route():
    print("Called")
    logger.info("\n--- Received request at /query ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    query = data.get('query')
    k = data.get('k', 5) # Default to k=5 now

    if not user_id or not query:
        return create_error_response("Missing required fields: user_id, query", 400)

    logger.info(f"Querying for user: {user_id} with k={k}")
    # Avoid logging potentially sensitive query text in production
    logger.debug(f"Query text: '{query[:100]}...'")

    try:
        results = faiss_handler.query_index(user_id, query, k=k)

        formatted_results = []
        for doc, score in results:
            # --- SEND FULL CONTENT ---
            content = doc.page_content
            # --- (No snippet generation needed) ---

            formatted_results.append({
                "documentName": doc.metadata.get("documentName", "Unknown"),
                "score": float(score),
                "content": content, # Send the full content
                # Removed "content_snippet"
            })

        logger.info(f"Query successful for user {user_id}. Returning {len(formatted_results)} results.")
        return jsonify({"relevantDocs": formatted_results}), 200
    except Exception as e:
        logger.error(f"--- Query Error ---", exc_info=True)
        return create_error_response(f"Failed to query index: {str(e)}", 500)

if __name__ == '__main__':
    # Ensure base FAISS directory exists on startup
    try:
        faiss_handler.ensure_faiss_dir()
    except Exception as e:
        logger.critical(f"CRITICAL: Could not create FAISS base directory '{config.FAISS_INDEX_DIR}'. Exiting. Error: {e}", exc_info=True)
        sys.exit(1) # Exit if base dir cannot be created

    # Attempt to initialize embedding model on startup
    try:
        faiss_handler.get_embedding_model() # This also determines the dimension
        logger.info("Embedding model initialized successfully on startup.")
    except Exception as e:
        logger.error(f"CRITICAL: Embedding model failed to initialize on startup: {e}", exc_info=True)
        logger.error("Endpoints requiring embeddings (/add_document, /query) will fail.")
        # Decide if you want to exit or run in a degraded state
        sys.exit(1) # Exit if embedding model fails - essential service

    # Attempt to load/check the default index on startup
    try:
        faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID) # This checks/creates/validates dimension
        logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' loaded/checked/created on startup.")
    except Exception as e:
        logger.warning(f"Warning: Could not load/create default index '{config.DEFAULT_INDEX_USER_ID}' on startup: {e}", exc_info=True)
        # Don't necessarily exit, but log clearly. Queries might only use user indices.

    # Start Flask App
    port = config.RAG_SERVICE_PORT
    logger.info(f"--- Starting RAG service ---")
    logger.info(f"Listening on: http://0.0.0.0:{port}")
    logger.info(f"Using Embedding: {config.EMBEDDING_TYPE} ({config.EMBEDDING_MODEL_NAME})")
    try:
        logger.info(f"Embedding Dimension: {faiss_handler.get_embedding_dimension(faiss_handler.embedding_model)}")
    except: pass # Dimension already logged or failed earlier
    logger.info(f"FAISS Index Path: {config.FAISS_INDEX_DIR}")
    logger.info("-----------------------------")
    # Use waitress or gunicorn for production instead of Flask's development server
    app.run(host='0.0.0.0', port=port, debug=os.getenv('FLASK_DEBUG') == '1')



============ ./code.txt ============



============ ./config.py ============
# server/rag_service/config.py
import os
import sys

# --- Determine Server Directory ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SERVER_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '..')) # Path to the 'server' directory
print(f"[rag_service/config.py] Determined SERVER_DIR: {SERVER_DIR}")

# --- Embedding Model Configuration ---
# Switch to Sentence Transformer
EMBEDDING_TYPE = 'sentence-transformer'
# Specify the model name (make sure you have internet access for download on first run)
# Or choose another model compatible with sentence-transformers library
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'BAAI/bge-large-en-v1.5')
EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'mixedbread-ai/mxbai-embed-large-v1')
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'e5-large-v2')
EMBEDDING_MODEL_NAME = EMBEDDING_MODEL_NAME_ST
print(f"Using Sentence Transformer model: {EMBEDDING_MODEL_NAME}")

# --- FAISS Configuration ---
FAISS_INDEX_DIR = os.path.join(SERVER_DIR, 'faiss_indices')
DEFAULT_ASSETS_DIR = os.path.join(SERVER_DIR, 'default_assets', 'engineering')
DEFAULT_INDEX_USER_ID = '__DEFAULT__'

# --- Text Splitting Configuration ---
CHUNK_SIZE = 512#1000
CHUNK_OVERLAP = 100#150

# --- API Configuration ---
RAG_SERVICE_PORT = int(os.getenv('RAG_SERVICE_PORT', 5002))

# --- Print effective configuration ---
print(f"FAISS Index Directory: {FAISS_INDEX_DIR}")
print(f"Default Assets Directory (for default.py): {DEFAULT_ASSETS_DIR}")
print(f"RAG Service Port: {RAG_SERVICE_PORT}")
print(f"Default Index User ID: {DEFAULT_INDEX_USER_ID}")
print(f"Chunk Size: {CHUNK_SIZE}, Chunk Overlap: {CHUNK_OVERLAP}")





============ ./default.py ============
import os
import logging
import sys
import traceback

# --- Path Setup ---
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
project_root_dir = os.path.dirname(server_dir)
sys.path.insert(0, server_dir)
# --- End Path Setup ---

try:
    from rag_service import config
    from rag_service import faiss_handler
    from rag_service import file_parser
except ImportError as e:
     print("ImportError:", e)
     print("Failed to import modules. Ensure the script is run correctly relative to the project structure.")
     print("Current sys.path:", sys.path)
     exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


class DefaultVectorDBBuilder:
    def __init__(self):
        logger.info("Initializing embedding model...")
        try:
            # Use SentenceTransformer as configured in config.py
            self.embed_model = faiss_handler.get_embedding_model()
            if self.embed_model is None:
                 raise RuntimeError("Failed to initialize Sentence Transformer embedding model.")
        except Exception as e:
            logger.error(f"Fatal error initializing embedding model: {e}", exc_info=True)
            raise

        self.chunk_size = config.CHUNK_SIZE
        self.chunk_overlap = config.CHUNK_OVERLAP

        self.default_docs_dir = config.DEFAULT_ASSETS_DIR
        self.index_dir = config.FAISS_INDEX_DIR
        self.default_user_id = config.DEFAULT_INDEX_USER_ID

        self.default_index_user_path = faiss_handler.get_user_index_path(self.default_user_id)
        self.index_file_path = os.path.join(self.default_index_user_path, "index.faiss")
        self.pkl_file_path = os.path.join(self.default_index_user_path, "index.pkl")

        try:
            faiss_handler.ensure_faiss_dir()
            os.makedirs(self.default_index_user_path, exist_ok=True)
        except Exception as e:
             logger.error(f"Failed to create necessary directories: {e}")
             raise

        logger.info(f"Default assets directory: {self.default_docs_dir}")
        logger.info(f"Default index directory: {self.default_index_user_path}")


    def create_default_index(self, force_rebuild=True): # Keep force_rebuild flag
        """Scans default assets, parses files, creates embeddings, and saves the FAISS index."""
        logger.info("--- Starting Default Index Creation ---")

        # --- Force Rebuild Logic ---
        if force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
            logger.warning(f"force_rebuild=True. Deleting existing default index files in {self.default_index_user_path}.")
            try:
                if os.path.exists(self.index_file_path): os.remove(self.index_file_path)
                if os.path.exists(self.pkl_file_path): os.remove(self.pkl_file_path)
                # Clear from cache if loaded
                if self.default_user_id in faiss_handler.loaded_indices:
                    del faiss_handler.loaded_indices[self.default_user_id]
                logger.info("Removed existing default index files and cleared cache.")
            except OSError as e:
                logger.error(f"Error removing existing index files: {e}")
                return False # Stop if we can't remove old files
        elif not force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
             logger.info("Default index already exists and force_rebuild=False. Skipping creation.")
             # Try loading it to confirm validity
             try:
                 faiss_handler.load_or_create_index(self.default_user_id)
                 logger.info("Existing default index loaded successfully.")
                 return True
             except Exception as load_err:
                 logger.error(f"Failed to load existing default index: {load_err}. Consider running with force_rebuild=True.")
                 return False


        # --- Process Documents ---
        all_documents = []
        files_processed = 0
        files_skipped = 0

        logger.info(f"Scanning for processable files in: {self.default_docs_dir}")
        if not os.path.isdir(self.default_docs_dir):
            logger.error(f"Default assets directory not found: {self.default_docs_dir}")
            return False

        for root, _, files in os.walk(self.default_docs_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                # logger.debug(f"Found file: {filename}")
                try:
                    text_content = file_parser.parse_file(file_path)
                    if text_content and text_content.strip():
                        langchain_docs = file_parser.chunk_text(
                            text_content, filename, self.default_user_id
                        )
                        if langchain_docs:
                            all_documents.extend(langchain_docs)
                            files_processed += 1
                            logger.info(f"Parsed and chunked: {filename} ({len(langchain_docs)} chunks)")
                        else:
                            logger.warning(f"Skipped {filename}: No chunks generated.")
                            files_skipped += 1
                    else:
                        logger.warning(f"Skipped {filename}: No text content or unsupported type.")
                        files_skipped += 1
                except Exception as e:
                    logger.error(f"Error processing file {filename}: {e}")
                    traceback.print_exc()
                    files_skipped += 1

        if not all_documents:
            logger.error(f"No processable documents found or generated in {self.default_docs_dir}. Cannot create index.")
            # Still create an empty index structure if the directory was valid
            try:
                 logger.info("Creating an empty index structure as no documents were found.")
                 faiss_handler.load_or_create_index(self.default_user_id) # Creates empty index
                 logger.info("Empty default index created successfully.")
                 return True # Success, but empty
            except Exception as empty_create_err:
                 logger.error(f"Failed to create empty index structure: {empty_create_err}", exc_info=True)
                 return False


        logger.info(f"Total files processed: {files_processed}, skipped: {files_skipped}")
        logger.info(f"Creating embeddings and adding {len(all_documents)} total chunks to index...")

        try:
            # The load_or_create_index function will handle creating the empty structure
            # if it doesn't exist (or after deletion if force_rebuild=True)
            logger.info("Ensuring FAISS index structure exists...")
            index_instance = faiss_handler.load_or_create_index(self.default_user_id)

            logger.info(f"Adding {len(all_documents)} documents to the default index '{self.default_user_id}'...")
            # Use the updated handler function which now manages IDs correctly
            faiss_handler.add_documents_to_index(self.default_user_id, all_documents)

            # Verify save occurred
            if not os.path.exists(self.index_file_path) or not os.path.exists(self.pkl_file_path):
                 logger.error("Index files were not found after adding documents. Check permissions or disk space.")
                 return False

            logger.info(f"Successfully created/updated and saved default index ({self.default_user_id}) with {len(all_documents)} document chunks.")
            logger.info("--- Default Index Creation Finished ---")
            return True

        except Exception as e:
            logger.error(f"Failed during embedding or index creation: {e}", exc_info=True)
            logger.error("--- Default Index Creation Failed ---")
            return False

def main():
    print("--- Running Default Index Builder ---")
    try:
        builder = DefaultVectorDBBuilder()
    except Exception as init_err:
        print(f"FATAL: Failed to initialize builder: {init_err}")
        sys.exit(1)

    if not os.path.isdir(builder.default_docs_dir):
         logger.error(f"Default assets directory '{builder.default_docs_dir}' is missing.")
         sys.exit(1)

    # --- Always force rebuild as requested ---
    force = True
    logger.info(f"Starting index creation (force_rebuild={force})...")
    if not builder.create_default_index(force_rebuild=force):
        logger.error("Index creation process failed.")
        sys.exit(1)
    else:
        logger.info("Default index creation process completed successfully.")
        sys.exit(0)

if __name__ == "__main__":
    main()



============ ./faiss_handler.py ============
# server/rag_service/faiss_handler.py

import os
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.embeddings import Embeddings as LangchainEmbeddings
from langchain_core.documents import Document as LangchainDocument
from langchain_community.docstore import InMemoryDocstore
from rag_service import config
import numpy as np
import time
import logging
import pickle
import uuid
import shutil # Import shutil for removing directories

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)

embedding_model: LangchainEmbeddings | None = None
loaded_indices = {}
_embedding_dimension = None # Cache the dimension

def get_embedding_dimension(embedder: LangchainEmbeddings) -> int:
    """Gets and caches the embedding dimension."""
    global _embedding_dimension
    if _embedding_dimension is None:
        try:
            logger.info("Determining embedding dimension...")
            dummy_embedding = embedder.embed_query("dimension_check")
            dimension = len(dummy_embedding)
            if not isinstance(dimension, int) or dimension <= 0:
                raise ValueError(f"Invalid embedding dimension obtained: {dimension}")
            _embedding_dimension = dimension
            logger.info(f"Detected embedding dimension: {_embedding_dimension}")
        except Exception as e:
            logger.error(f"CRITICAL ERROR determining embedding dimension: {e}", exc_info=True)
            raise RuntimeError(f"Failed to determine embedding dimension: {e}")
    return _embedding_dimension

def get_embedding_model():
    global embedding_model
    if embedding_model is None:
        if config.EMBEDDING_TYPE == 'sentence-transformer':
            logger.info(f"Initializing HuggingFace Embeddings for Sentence Transformer (Model: {config.EMBEDDING_MODEL_NAME})")
            try:
                # Try CUDA first, fallback to CPU
                try:
                    if faiss.get_num_gpus() > 0:
                        device = 'cuda'
                        logger.info("CUDA detected. Using GPU for embeddings.")
                    else:
                        raise RuntimeError("No GPU found") # Force fallback
                except Exception:
                    device = 'cpu'
                    logger.warning("CUDA not available or GPU check failed. Using CPU for embeddings. This might be slow.")

                embedding_model = HuggingFaceEmbeddings(
                    model_name=config.EMBEDDING_MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True} # Often recommended for cosine similarity / MIPS with FAISS
                )
                # Determine and cache dimension on successful load
                get_embedding_dimension(embedding_model)

                logger.info("Testing embedding function...")
                test_embedding_doc = embedding_model.embed_documents(["test document"])
                test_embedding_query = embedding_model.embed_query("test query")
                if not test_embedding_doc or not test_embedding_query:
                    raise ValueError("Embedding test failed, returned empty results.")
                logger.info(f"Embedding test successful.")

            except Exception as e:
                logger.error(f"Error loading HuggingFace Embeddings for '{config.EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
                embedding_model = None
                raise RuntimeError(f"Failed to load embedding model: {e}")
        else:
            raise ValueError(f"Unsupported embedding type in config: {config.EMBEDDING_TYPE}. Expected 'sentence-transformer'.")
    return embedding_model

def get_user_index_path(user_id):
    safe_user_id = str(user_id).replace('.', '_').replace('/', '_').replace('\\', '_')
    user_dir = os.path.join(config.FAISS_INDEX_DIR, f"user_{safe_user_id}")
    return user_dir

def _delete_index_files(index_path, user_id):
    """Safely deletes index files for a user."""
    logger.warning(f"Deleting potentially incompatible index files for user '{user_id}' at {index_path}")
    try:
        if os.path.isdir(index_path):
            shutil.rmtree(index_path)
            logger.info(f"Successfully deleted directory: {index_path}")
        # If only loose files exist (less likely with save_local)
        index_file = os.path.join(index_path, "index.faiss")
        pkl_file = os.path.join(index_path, "index.pkl")
        if os.path.exists(index_file): os.remove(index_file)
        if os.path.exists(pkl_file): os.remove(pkl_file)
    except OSError as e:
        logger.error(f"Error deleting index files/directory for user '{user_id}' at {index_path}: {e}", exc_info=True)
        # Don't raise here, allow fallback to creating new index if possible

def load_or_create_index(user_id):
    global loaded_indices
    if user_id in loaded_indices:
        # **Even if cached, re-verify dimension on subsequent loads in case model changed**
        index = loaded_indices[user_id]
        embedder = get_embedding_model() # Ensure model is loaded
        current_dim = get_embedding_dimension(embedder)
        if hasattr(index, 'index') and index.index is not None and index.index.d != current_dim:
            logger.warning(f"Cached index for user '{user_id}' has dimension {index.index.d}, but current model has dimension {current_dim}. Discarding cache and forcing reload/recreate.")
            del loaded_indices[user_id] # Remove from cache
            # Fall through to load/create logic below
        else:
            logger.debug(f"Returning cached index for user '{user_id}'.")
            return index # Return cached and verified index

    index_path = get_user_index_path(user_id)
    index_file = os.path.join(index_path, "index.faiss")
    pkl_file = os.path.join(index_path, "index.pkl")

    embedder = get_embedding_model()
    if embedder is None:
        raise RuntimeError("Embedding model is not available.")
    current_embedding_dim = get_embedding_dimension(embedder)

    force_recreate = False
    if os.path.exists(index_file) and os.path.exists(pkl_file):
        logger.info(f"Attempting to load existing FAISS index for user '{user_id}' from {index_path}")
        try:
            start_time = time.time()
            # Temporarily load to check dimension
            index = FAISS.load_local(
                folder_path=index_path,
                embeddings=embedder,
                allow_dangerous_deserialization=True # Use with caution if index source isn't trusted
            )
            end_time = time.time()

            # --- CRITICAL DIMENSION CHECK ---
            if not hasattr(index, 'index') or index.index is None:
                 logger.warning(f"Loaded index for user '{user_id}' has no 'index' attribute or it's None. Forcing recreation.")
                 force_recreate = True
            elif index.index.d != current_embedding_dim:
                logger.warning(f"DIMENSION MISMATCH! Index for user '{user_id}' has dimension {index.index.d}, but current embedding model has dimension {current_embedding_dim}. Index is incompatible and will be recreated.")
                force_recreate = True
            elif index.index.ntotal == 0:
                logger.info(f"Loaded index for user '{user_id}' is empty (0 vectors). Will use it but note it's empty.")
                 # Not forcing recreate, just noting it's empty
            # --- END DIMENSION CHECK ---

            if force_recreate:
                _delete_index_files(index_path, user_id)
                # Don't return the incompatible index, fall through to create new one
            else:
                # If dimensions match and index is valid
                logger.info(f"Index for user '{user_id}' loaded successfully in {end_time - start_time:.2f} seconds. Dimension ({index.index.d}) matches. Contains {index.index.ntotal} vectors.")
                loaded_indices[user_id] = index
                return index

        except (pickle.UnpicklingError, EOFError, ModuleNotFoundError, AttributeError, ValueError) as load_err:
            logger.error(f"Error loading index for user '{user_id}' from {index_path}: {load_err}")
            logger.warning("Index files might be corrupted or incompatible. Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs
        except Exception as e:
            logger.error(f"Unexpected error loading index for user '{user_id}': {e}", exc_info=True)
            logger.warning("Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs

    # --- Create New Index Logic ---
    # This block runs if files didn't exist OR force_recreate is True
    logger.info(f"Creating new FAISS index structure for user '{user_id}' at {index_path} with dimension {current_embedding_dim}")
    try:
        # Ensure directory exists (it might have been deleted)
        os.makedirs(index_path, exist_ok=True)

        # Use the already determined dimension
        # Use IndexFlatIP if embeddings are normalized (recommended)
        faiss_index = faiss.IndexIDMap(faiss.IndexFlatIP(current_embedding_dim))
        # faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(current_embedding_dim)) # Use L2 if not normalized

        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}

        index = FAISS(
            embedding_function=embedder,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            normalize_L2=False # Set True if using IndexFlatIP and normalized embeddings (which we are with encode_kwargs)
        )

        logger.info(f"Initialized empty index structure for user '{user_id}'.")
        loaded_indices[user_id] = index # Add to cache immediately
        save_index(user_id) # Save the empty structure
        logger.info(f"New empty index for user '{user_id}' created and saved.")
        return index
    except Exception as e:
        logger.error(f"CRITICAL ERROR creating new index for user '{user_id}': {e}", exc_info=True)
        if user_id in loaded_indices:
            del loaded_indices[user_id] # Clean up cache on failure
        # Attempt to clean up directory if creation failed badly
        _delete_index_files(index_path, user_id)
        raise RuntimeError(f"Failed to initialize FAISS index for user '{user_id}'")


def add_documents_to_index(user_id, documents: list[LangchainDocument]):
    if not documents:
        logger.warning(f"No documents provided to add for user '{user_id}'.")
        return

    try:
        index = load_or_create_index(user_id) # This now handles dimension checks/recreation
        embedder = get_embedding_model() # Ensure model is loaded

        # --- VERIFY DIMENSIONS AGAIN before adding (paranoid check) ---
        current_dim = get_embedding_dimension(embedder)
        if not hasattr(index, 'index') or index.index is None:
             logger.error(f"Index object for user '{user_id}' is invalid after load/create. Cannot add documents.")
             raise RuntimeError("Failed to get valid index structure.")
        if index.index.d != current_dim:
             logger.error(f"FATAL: Dimension mismatch just before adding documents for user '{user_id}'. Index: {index.index.d}, Model: {current_dim}. This shouldn't happen if load_or_create_index worked.")
             # Attempt recovery by deleting and trying again? Risky loop potential.
             _delete_index_files(get_user_index_path(user_id), user_id)
             if user_id in loaded_indices: del loaded_indices[user_id]
             raise RuntimeError(f"Inconsistent index dimension detected for user '{user_id}'. Please retry.")
        # --- END VERIFY ---

        logger.info(f"Adding {len(documents)} documents to index for user '{user_id}' (Index dim: {index.index.d})...")
        start_time = time.time()

        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        # Generate embeddings using the current model
        embeddings = embedder.embed_documents(texts)
        if not embeddings or len(embeddings) != len(texts):
             logger.error(f"Embedding generation failed or returned unexpected number of vectors for user '{user_id}'.")
             raise ValueError("Embedding generation failed.")
        if len(embeddings[0]) != current_dim:
             logger.error(f"Generated embeddings have incorrect dimension ({len(embeddings[0])}) for user '{user_id}', expected {current_dim}.")
             raise ValueError("Generated embedding dimension mismatch.")

        embeddings_np = np.array(embeddings, dtype=np.float32)

        # Generate unique IDs for FAISS
        ids = [str(uuid.uuid4()) for _ in texts]
        ids_np = np.array([uuid.UUID(id_).int & (2**63 - 1) for id_ in ids], dtype=np.int64)


        # Add embeddings and their corresponding IDs to the FAISS index
        index.index.add_with_ids(embeddings_np, ids_np)

        # Add the original documents and their metadata to the Langchain Docstore,
        # using the generated string UUIDs as keys.
        # Map the FAISS integer ID back to the string UUID used in the docstore.
        docstore_additions = {doc_id: doc for doc_id, doc in zip(ids, documents)}
        index.docstore.add(docstore_additions)
        for i, faiss_id in enumerate(ids_np):
            index.index_to_docstore_id[int(faiss_id)] = ids[i] # Map FAISS int ID -> string UUID

        end_time = time.time()
        logger.info(f"Successfully added {len(documents)} vectors/documents for user '{user_id}' in {end_time - start_time:.2f} seconds. Total vectors: {index.index.ntotal}")
        save_index(user_id)
    except Exception as e:
        logger.error(f"Error adding documents for user '{user_id}': {e}", exc_info=True)
        # Don't re-raise here if app.py handles it, but ensure logging is clear
        raise # Re-raise the exception so app.py can catch it and return 500

def query_index(user_id, query_text, k=3):
    all_results_with_scores = []
    embedder = get_embedding_model()

    if embedder is None:
        logger.error("Embedding model is not available for query.")
        raise ConnectionError("Embedding model is not available for query.")

    try:
        start_time = time.time()
        user_index = None # Initialize to None
        default_index = None # Initialize to None

        # Query User Index
        try:
            user_index = load_or_create_index(user_id) # Assign to user_index
            if hasattr(user_index, 'index') and user_index.index is not None and user_index.index.ntotal > 0:
                logger.info(f"Querying index for user: '{user_id}' (Dim: {user_index.index.d}, Vectors: {user_index.index.ntotal}) with k={k}")
                user_results = user_index.similarity_search_with_score(query_text, k=k)
                logger.info(f"User index '{user_id}' query returned {len(user_results)} results.")
                all_results_with_scores.extend(user_results)
            else:
                logger.info(f"Skipping query for user '{user_id}': Index is empty or invalid.")
        except FileNotFoundError:
            logger.warning(f"User index files for '{user_id}' not found on disk (might be first time). Skipping query for this index.")
        except RuntimeError as e:
            logger.error(f"Could not load or create user index for '{user_id}': {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error querying user index for '{user_id}': {e}", exc_info=True)


        # Query Default Index (if different from user_id)
        if user_id != config.DEFAULT_INDEX_USER_ID:
            try:
                default_index = load_or_create_index(config.DEFAULT_INDEX_USER_ID) # Assign to default_index
                if hasattr(default_index, 'index') and default_index.index is not None and default_index.index.ntotal > 0:
                    logger.info(f"Querying default index '{config.DEFAULT_INDEX_USER_ID}' (Dim: {default_index.index.d}, Vectors: {default_index.index.ntotal}) with k={k}")
                    default_results = default_index.similarity_search_with_score(query_text, k=k)
                    logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' query returned {len(default_results)} results.")
                    all_results_with_scores.extend(default_results)
                else:
                    logger.info(f"Skipping query for default index '{config.DEFAULT_INDEX_USER_ID}': Index is empty or invalid.")
            except FileNotFoundError:
                 logger.warning(f"Default index '{config.DEFAULT_INDEX_USER_ID}' not found on disk (run default.py?). Skipping query.")
            except RuntimeError as e:
                logger.error(f"Could not load or create default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)
            except Exception as e:
                logger.error(f"Unexpected error querying default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)

        query_time = time.time()
        logger.info(f"Completed all index queries in {query_time - start_time:.2f} seconds. Found {len(all_results_with_scores)} raw results.")

        # --- Deduplication and Sorting ---
        unique_results = {}
        for doc, score in all_results_with_scores:
            if not doc or not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                logger.warning(f"Skipping invalid document object in results: {doc}")
                continue

            # Use the fallback content-based key
            content_key = f"{doc.metadata.get('documentName', 'Unknown')}_{doc.page_content[:200]}"
            unique_key = content_key # Use the content key directly

            # Add or update if the new score is better (lower for L2 distance / IP distance if normalized)
            if unique_key not in unique_results or score < unique_results[unique_key][1]:
                unique_results[unique_key] = (doc, score)

        # Sort by score (ascending for L2 distance / IP distance)
        sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
        final_results = sorted_results[:k] # Get top k unique results

        logger.info(f"Returning {len(final_results)} unique results after filtering and sorting.")
        return final_results
    except Exception as e:
        logger.error(f"Error during query processing for user '{user_id}': {e}", exc_info=True)
        return [] # Return empty list on error


def save_index(user_id):
    global loaded_indices
    if user_id not in loaded_indices:
        logger.warning(f"Index for user '{user_id}' not found in cache, cannot save.")
        return

    index = loaded_indices[user_id]
    index_path = get_user_index_path(user_id)

    if not isinstance(index, FAISS) or not hasattr(index, 'index') or not hasattr(index, 'docstore') or not hasattr(index, 'index_to_docstore_id'):
        logger.error(f"Cannot save index for user '{user_id}': Invalid index object in cache.")
        return

    # Ensure the target directory exists before saving
    try:
        os.makedirs(index_path, exist_ok=True)
        logger.info(f"Saving FAISS index for user '{user_id}' to {index_path} (Vectors: {index.index.ntotal if hasattr(index.index, 'ntotal') else 'N/A'})...")
        start_time = time.time()
        # This saves index.faiss and index.pkl
        index.save_local(folder_path=index_path)
        end_time = time.time()
        logger.info(f"Index for user '{user_id}' saved successfully in {end_time - start_time:.2f} seconds.")
    except Exception as e:
        logger.error(f"Error saving FAISS index for user '{user_id}' to {index_path}: {e}", exc_info=True)

# --- ADD THIS FUNCTION DEFINITION BACK ---
def ensure_faiss_dir():
    """Ensures the base FAISS index directory exists."""
    try:
        os.makedirs(config.FAISS_INDEX_DIR, exist_ok=True)
        logger.info(f"Ensured FAISS base directory exists: {config.FAISS_INDEX_DIR}")
    except OSError as e:
        logger.error(f"Could not create FAISS base directory {config.FAISS_INDEX_DIR}: {e}")
        raise # Raise the error to prevent startup if dir creation fails
# --- END OF ADDED FUNCTION ---



============ ./file_parser.py ============
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error



============ ./generate_code.sh ============
#!/bin/bash

# Output file
OUTPUT_FILE="code.txt"

# Clear previous output
> "$OUTPUT_FILE"

# Traverse files and directories, excluding __pycache__ and default.faiss
find . -type f \
    ! -path "*/__pycache__/*" \
    ! -name "*.pyc" \
    ! -name "default.faiss" \
    | while read file; do
        echo "============ $file ============" >> "$OUTPUT_FILE"
        cat "$file" >> "$OUTPUT_FILE"
        echo -e "\n\n" >> "$OUTPUT_FILE"
    done

echo " Code has been saved to $OUTPUT_FILE"



============ ./indexes/default_metadata.json ============
[
  {
    "content": "C h a p t e r 6\nD e e p F e e d f orw ard N e t w orks\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\nThegoalofafeedforwardnetworkistoapproximatesomefunction f.Forexample,\nforaclassier, y= f(x)mapsaninputxtoacategory y.Afeedforwardnetwork\ndenesamappingy= f(x;)andlearnsthevalueoftheparametersthatresult\ninthebestfunctionapproximation.\nThesemodelsarecalledfeedforwardbecauseinformationowsthroughthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "functionbeingevaluatedfromx,throughtheintermediate computations usedto\ndene f,andnallytotheoutputy.Therearenofeedbackconnectionsinwhich\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\nnetworks,presentedinchapter.10\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\nlanguageapplications.\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\nrepresentedbycomposingtogethermanydierentfunctions.Themodelisasso-\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "together.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtherst\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\n168",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\nthenamedeeplearningarises.Thenallayerofafeedforwardnetworkiscalled\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f(x).\nThetrainingdataprovidesuswithnoisy,approximateexamplesof f(x) evaluated\natdierenttrainingpoints.Eachexamplexisaccompanied byalabel y f(x).\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "x;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\nnotdirectlyspeciedbythetrainingdata.Thelearningalgorithmmustdecide\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\ndecidehowtousetheselayerstobestimplementanapproximation of f.Because\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\nlayersarecalledhiddenlayers.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "layersarecalledhiddenlayers.\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\nactivationvalue.Theideaofusingmanylayersofvector-valuedrepresentation\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\ntheserepresentationsisalsolooselyguidedbyneuroscienticobservationsabout\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations.Linearmodels,suchaslogistic\nregressionandlinearregression,areappealingbecausetheymaybeteciently",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "andreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\nthelinearmodelnottoxitselfbuttoatransformedinput (x),where isa\n1 6 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\nthe mapping.Wecanthinkof asprovidingasetoffeaturesdescribingx,or\nasprovidinganewrepresentationfor.x\nThequestionisthenhowtochoosethemapping. \n1.Oneoptionistouseaverygeneric ,suchastheinnite-dimens ional that\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.If (x)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "ofhighenoughdimension,wecanalwayshaveenoughcapacitytotthe\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\nproblems.\n2.Anotheroptionistomanuallyengineer .Untiltheadventofdeeplearning,\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\neortforeachseparatetask,withpractitionersspecializingin dierent",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "domainssuchasspeechrecognition orcomputer vision,andwith little\ntransferbetweendomains.\n3.Thestrategyofdeeplearningistolearn .Inthisapproach,wehaveamodel\ny= f(x;w ,) = (x;)w.Wenowhaveparametersthatweusetolearn\nfromabroadclassoffunctions,andparameterswthatmapfrom (x)to\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\ndeningahiddenlayer.Thisapproachistheonlyoneofthethreethat\ngivesupontheconvexityofthetrainingproblem,butthebenetsoutweigh",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "theharms.Inthisapproach,weparametrizetherepresentationas (x;)\nandusetheoptimization algorithmtondthethatcorrespondstoagood\nrepresentation.Ifwewish,thisapproachcancapturethebenetoftherst\napproachbybeinghighlygenericwedosobyusingaverybroadfamily\n(x;).Thisapproachcanalsocapturethebenetofthesecondapproach.\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\ndesigningfamilies (x;)thattheyexpectwillperformwell.Theadvantage",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "isthatthehumandesigneronlyneedstondtherightgeneralfunction\nfamilyratherthanndingpreciselytherightfunction.\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\n1 7 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmappingsfromxtoythatlackfeedbackconnections.Othermodelspresented\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "layersshouldbeconnectedtoeachother,and howmanyunitsshouldbein\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\nmoderngeneralizations ,whichcanbeusedtoecientlycomputethesegradients.\nFinally,weclosewithsomehistoricalperspective.\n6. 1 E x am p l e: L earni n g X O R\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "theXORfunction.\nTheXORfunction(exclusiveor)isanoperationontwobinaryvalues, x 1\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\ny= f(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;)and\nourlearningalgorithmwilladapttheparameterstomake fassimilaraspossible\nto f.\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0],[0 ,1],\n[1 ,0],and[1 ,1]}.Wewilltrainthenetworkonallfourofthesepoints.The\nonlychallengeistotthetrainingset.\nWecantreatthisproblemasaregressionproblemanduseameansquared\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n1 7 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\naredescribedinsection.6.2.2.2\nEvaluatedonourwholetrainingset,theMSElossfunctionis\nJ() =1\n4\nx X( f() (;))x fx2. (6.1)\nNowwemustchoosetheformofourmodel, f(x;).Supposethatwechoose\nalinearmodel,withconsistingofand.Ourmodelisdenedtobe w b\nf , b (;xw) = xw+ b . (6.2)\nWecanminimize J()inclosedformwithrespecttowand busingthenormal\nequations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "equations.\nAftersolvingthenormalequations,weobtainw= 0and b=1\n2.Thelinear\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\nthisproblemistouseamodelthatlearnsadierentfeaturespaceinwhicha\nlinearmodelisabletorepresentthesolution.\nSpecically,wewillintroduceaverysimplefeedforwardnetworkwithone\nhiddenlayercontainingtwohiddenunits.Seegureforanillustrationof 6.2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "f , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =Wx\nand f( 2 )(h) =hw.Then f(x) =wWx.Wecouldrepresentthisfunctionas\nf() = xxwwherew= Ww.\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networksdosousingananetransformationcontrolledbylearnedparameters,\nfollowedbyaxed,nonlinearfunctioncalledanactivationfunction.Weusethat\nstrategyhere,bydeningh= g(Wx+c) ,whereWprovidestheweightsofa\nlineartransformationandcthebiases.Previously,todescribealinearregression\n1 7 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 1\nx 101x 2O r i g i n a l s p a c e x\n0 1 2\nh 101h 2L e a r n e d s p a c e h\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\nfunction.When x1= 0,themodelsoutputmustincreaseas x2increases.When x1= 1,\nthemodelsoutputmustdecreaseas x 2increases.Alinearmodelmustapplyaxed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "coecient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\nthecoecienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\nmappedbothx= [1 ,0]andx= [0 ,1]toasinglepointinfeaturespace,h= [1 ,0].",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcantthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize.\n1 7 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nyy\nhh\nx xWwyy\nh 1 h 1\nx 1 x 1h 2 h 2\nx 2 x 2\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodierentstyles.Specically,\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "eachentirevectorrepresentingalayersactivations.Thisstyleismuchmorecompact.\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\nofdrawing.\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "anetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\nananetransformationfromavectorxtoavectorh,soanentirevectorofbias\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\nthatisappliedelement-wise,with h i= g(xW : , i+ c i).Inmodernneuralnetworks,\nthedefaultrecommendation istousetherectiedlinearunitorReLU(Jarrett\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)denedbytheactivation 2011a\nfunction depictedingure. g z , z () = max0{} 6.3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "function depictedingure. g z , z () = max0{} 6.3\nWecannowspecifyourcompletenetworkas\nf , , , b (;xWcw) = wmax0{ ,Wxc+}+ b . (6.3)\nWecannowspecifyasolutiontotheXORproblem.Let\nW=11\n11\n, (6.4)\nc=\n0\n1\n, (6.5)\n1 7 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0\nz0g z ( ) = m a x 0{ , z}\nFigure6.3:Therectiedlinearactivationfunction.Thisactivationfunctionisthedefault\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\nfunctionwithtwolinearpieces.Becauserectiedlinearunitsarenearlylinear,they",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "preservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\ncomplicatedsystemsfromminimalcomponents.MuchasaTuringmachinesmemory\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\nfromrectiedlinearfunctions.\n1 7 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nw=1\n2\n, (6.6)\nand. b= 0\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\nwithoneexampleperrow:\nX=\n00\n01\n10\n11\n. (6.7)\nTherststepintheneuralnetworkistomultiplytheinputmatrixbytherst\nlayersweightmatrix:\nXW=\n00\n11\n11\n22\n. (6.8)\nNext,weaddthebiasvector,toobtainc\n\n0 1\n10\n10\n21\n. (6.9)\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\nAlinearmodelcannotimplementsuchafunction.Tonishcomputingthevalue\nofforeachexample,weapplytherectiedlineartransformation: h\n\n00\n10\n10\n21\n. (6.10)\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\nlongerlieonasingleline.Asshowningure,theynowlieinaspacewherea 6.1\nlinearmodelcansolvetheproblem.\nWenishbymultiplyingbytheweightvector:w\n\n0\n1\n1\n0\n. (6.11)\n1 7 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\nInthisexample,wesimplyspeciedthesolution,thenshowedthatitobtained\nzeroerror.Inarealsituation,theremightbebillionsofmodelparametersand\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\nhere.Instead,agradient-basedoptimization algorithmcanndparametersthat\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "globalminimumofthelossfunction,sogradientdescentcouldconvergetothis\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\ndescentcouldalsond.Theconvergencepointofgradientdescentdependsonthe\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\nndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\nhere.\n6. 2 Gradi en t - Bas e d L earni n g\nDesigningandtraininganeuralnetworkisnotmuchdierentfromtrainingany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\nacostfunction,andamodelfamily.\nThelargestdierencebetweenthelinearmodelswehaveseensofarandneural\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\nconvergesstartingfromanyinitialparameters(intheoryinpracticeitisvery\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "importanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\nparticulardiscussedinsection.Forthemoment,itsucestounderstandthat 8.4\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\ncostfunctioninonewayoranother.The specicalgorithmsareimprovements",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "andrenementsontheideasofgradientdescent,introducedinsection,and,4.3\n1 7 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmorespecically,aremostoftenimprovementsofthestochasticgradientdescent\nalgorithm,introducedinsection.5.9\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\nmuchdierentfromtraininganyothermodel.Computingthegradientisslightly\nmorecomplicatedforaneuralnetwork,butcanstillbedoneecientlyandexactly.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\ntheneuralnetworksscenario.\n6.2.1CostFunctions\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\nInmostcases,ourparametricmodeldenesadistribution p(yx|;)and\nwesimplyusetheprincipleofmaximumlikelihood.Thismeansweusethe\ncross-entropybetweenthetrainingdataandthemodelspredictionsasthecost\nfunction.\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "on.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "describedinchapter.7\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n1 7 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\ncostfunctionisgivenby\nJ() =   E x y ,  pdatalog p m o de l( )yx| . (6.12)\nThespecicformofthecostfunctionchangesfrommodeltomodel,depending\nonthespecicformoflog p m o de l.Theexpansionoftheaboveequationtypically\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;) ,I),\nthenwerecoverthemeansquarederrorcost,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "thenwerecoverthemeansquarederrorcost,\nJ () =1\n2E x y ,  pdata|| ||y f(;)x2+const , (6.13)\nuptoascalingfactorof1\n2andatermthatdoesnotdependon.Thediscarded\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Anadvantageofthisapproachofderivingthecostfunctionfrommaximum\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\nforthelearningalgorithm.Functionsthatsaturate(becomeveryat)undermine\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\nhiddenunitsortheoutputunitssaturate.Thenegativelog-likelihoodhelpsto\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\nsection.6.2.2\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n1 7 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\ncross-entropyapproachingnegativeinnity.Regularizationtechniquesdescribed\ninchapterprovideseveraldierentwaysofmodifyingthelearningproblemso 7\nthatthemodelcannotreapunlimitedrewardinthisway.\n6.2.1.2LearningConditionalStatistics",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "6.2.1.2LearningConditionalStatistics\nInsteadoflearningafullprobabilitydistribution p(yx|;)weoftenwanttolearn\njustoneconditionalstatisticofgiven.yx\nForexample,wemayhaveapredictor f(x;) thatwewishtopredictthemean\nof.y\nIfweuseasucientlypowerfulneuralnetwork,wecanthinkoftheneural\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\nratherthanbyhavingaspecicparametricform.Fromthispointofview,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "canviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespecic\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "toolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\nusedtoderivethefollowingtworesults.\nOurrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\ntionproblem\nf= argmin\nfE x y , pdata|| ||y f()x2(6.14)\nyields\nf() = x E y pdata ( ) y x|[]y , (6.15)\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "couldtrainoninnitelymanysamplesfromthetruedatageneratingdistribution,\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\nmeanofforeachvalueof. y x\n1 8 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nDierentcostfunctionsgivedierentstatistics.Asecondresultderivedusing\ncalculusofvariationsisthat\nf= argmin\nfE x y , pdata|| ||y f()x 1 (6.16)\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\nfunctioniscommonlycalled . meanabsoluteerror\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "saturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\nentiredistribution. p( )yx|\n6.2.2OutputUnits\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\nmodeldistribution.Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "theformofthecross-entropyfunction.\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\nsetofhiddenfeaturesdenedbyh= f(x;).Theroleoftheoutputlayeristhen\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thatthenetworkmustperform.\n6.2.2.1LinearUnitsforGaussianOutputDistributions\nOnesimplekindofoutputunitisanoutputunitbasedonananetransformation\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\nGivenfeaturesh,alayeroflinearoutputunitsproducesavectory=Wh+b.\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| NyyI ,) . (6.17)\n1 8 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\nerror.\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\ndenitematrixforallinputs.Itisdiculttosatisfysuchconstraintswithalinear\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\nBecauselinearunitsdonotsaturate,theyposelittledicultyforgradient-\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\nalgorithms.\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\nManytasksrequirepredictingthevalueofabinaryvariable y.Classication\nproblemswithtwoclassescanbecastinthisform.\nThemaximum-likelihoodapproachistodeneaBernoullidistributionover y\nconditionedon.x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "conditionedon.x\nABernoullidistributionisdenedbyjustasinglenumber.Theneuralnet\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\nmustlieintheinterval[0,1].\nSatisfyingthisconstraintrequiressomecarefuldesigneort.Supposewewere\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\nP y(= 1 ) = max |x\n0min ,\n1 ,wh+ b\n.(6.18)\nThiswouldindeeddeneavalidconditionaldistribution,butwewouldnotbeable\ntotrainitveryeectivelywithgradientdescent.Anytimethatwh+ bstrayed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\nparameters.\nInstead,itisbettertouseadierentapproachthatensuresthereisalwaysa\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\nAsigmoidoutputunitisdenedby\n y = \nwh+ b\n(6.19)\n1 8 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwhereisthelogisticsigmoidfunctiondescribedinsection.  3.10\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\nusesalinearlayertocompute z=wh+ b.Next,itusesthesigmoidactivation\nfunctiontoconvertintoaprobability. z\nWeomitthedependenceonxforthemomenttodiscusshowtodenea\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\nbyconstructinganunnormalized probabilitydistribution P( y),whichdoesnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\ncontrolledbyasigmoidaltransformationof: z\nlog P y y z () = (6.20)\n P y y z () = exp() (6.21)\nP y() =exp() y z1\ny= 0exp( yz)(6.22)\nP y  y z . () = ((21)) (6.23)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "P y  y z . () = ((21)) (6.23)\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\nthroughoutthestatisticalmodelingliterature.The zvariabledeningsucha\ndistributionoverbinaryvariablesiscalleda.logit\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\nlikelihoodislog P( y|x),theloginthecostfunctionundoestheexpofthe\nsigmoid.Withoutthiseect,thesaturationofthesigmoidcouldpreventgradient-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "basedlearningfrommakinggoodprogress.Thelossfunctionformaximum\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\nJ P y () = log  (|x) (6.24)\n= log((2 1))   y z (6.25)\n= ((12))   y z . (6.26)\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n(12 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\nhastherightanswerwhen y= 1and zisverypositive,or y= 0and zisvery",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "negative.When zhasthewrongsign,theargumenttothesoftplusfunction,\n1 8 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n(12 y) z,maybesimpliedto|| z.As|| zbecomeslargewhile zhasthewrongsign,\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\ncorrectamistaken. z\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "saturateanytime ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\noutputunits.\nAnalytically,thelogarithmofthesigmoidisalwaysdenedandnite,because",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "thesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\nfunctionof z,ratherthanasafunctionof y= ( z).Ifthesigmoidfunction\nunderowstozero,thentakingthelogarithmof yyieldsnegativeinnity.\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "with npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\ndistributionoverabinaryvariable.\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassier,torepresent\ntheprobabilitydistributionover ndierentclasses.Morerarely,softmaxfunctions\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\nndierentoptionsforsomeinternalvariable.\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": " y P y . = (= 1 )|x (6.27)\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\nthelog-likelihood,wechosetoinsteadpredictanumber z=log P( y=1|x).\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction.\n1 8 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\ntoproduceavectory,with  y i= P( y= i|x).Werequirenotonlythateach\nelementof y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\nlayerpredictsunnormalized logprobabilities:\nzW= hb+ , (6.28)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "zW= hb+ , (6.28)\nwhere z i=log P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\nnormalizetoobtainthedesired z y.Formally,thesoftmaxfunctionisgivenby\nsoftmax()z i=exp( z i)\njexp( z j). (6.29)\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deningthe\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\ntheofthesoftmax: exp",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theofthesoftmax: exp\nlogsoftmax()z i= z ilog\njexp( z j) . (6.30)\nThersttermofequationshowsthattheinput 6.30 z ialwayshasadirect\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,therst 6.30\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\npusheddown.Togainsomeintuitionforthesecondterm,log\njexp( z j),observe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j),observe\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\nbasedontheideathatexp( z k) isinsignicantforany z kthatisnoticeablylessthan\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\nthe z itermandthelog\njexp( z j)max j z j= z itermswillroughlycancel.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j)max j z j= z itermswillroughlycancel.\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassied.\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n1 8 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\nsoftmax((;))zx im\nj = 1 1y() j= i , x() j= xm\nj = 1 1x() j = x. (6.31)\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\nmodelisonlyabletoapproximatethesefractions.\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "withthesoftmaxfunction.Specically,objectivefunctionsthatdonotusealogto\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\noutput,evenwhenthemodelmakeshighlycondentincorrectpredictions(,Bridle\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\nthesoftmaxfunctionitself.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesoftmaxfunctionitself.\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\noutputvaluescansaturatewhenthedierencesbetweeninputvaluesbecome\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\nToseethatthesoftmaxfunctionrespondstothedierencebetweenitsinputs,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\ninputs:\nsoftmax() = softmax(+) zz c . (6.32)\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\nsoftmax() = softmax( max zz\niz i) . (6.33)\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\nbytheamountthatitsargumentsdeviatefrommax i z i.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "bytheamountthatitsargumentsdeviatefrommax i z i.\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\n1 8 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancausesimilardicultiesforlearningifthelossfunctionisnotdesignedto\ncompensateforit.\nTheargumentztothesoftmaxfunctioncanbeproducedintwodierentways.\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\neveryelementofz,asdescribedaboveusingthelinearlayerz=Wh+b.While\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n1parametersare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "necessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\nrst n1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\nofzbexed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\nwhatthesigmoidunitdoes.Dening P( y= 1|x) = ( z)isequivalenttodening\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n1\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\nsetofprobabilitydistributions,buthavedierentlearningdynamics.Inpractice,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "thereisrarelymuchdierencebetweenusingtheoverparametrized versionorthe\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\nFromaneuroscienticpointofview,itisinterestingtothinkofthesoftmaxas\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "extreme(whenthedierencebetweenthemaximal a iandtheothersislargein\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\nandtheothersarenearly0).\nThenamesoftmaxcanbesomewhatconfusing.Thefunctionismoreclosely\nrelatedtotheargmaxfunctionthanthemaxfunction.Thetermsoftderives\nfromthefactthatthesoftmaxfunctioniscontinuousanddierentiable. The\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\nordierentiable. Thesoftmaxfunctionthusprovidesasoftenedversionofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "argmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)z.\nItwouldperhapsbebettertocallthesoftmaxfunctionsoftargmax,butthe\ncurrentnameisanentrenchedconvention.\n6.2.2.4OtherOutputTypes\nThelinear,sigmoid,andsoftmaxoutputunitsdescribedabovearethemost\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n1 8 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nagoodcostfunctionfornearlyanykindofoutputlayer.\nIngeneral,ifwedeneaconditionaldistribution p(yx|;),theprincipleof\nmaximumlikelihoodsuggestsweuse asourcostfunction.  | log( pyx;)\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;).\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;) =providestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas . log(;()) p yx",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "canthenbeinterpretedas . log(;()) p yx\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\ngiven x.Inthesimplecase,wherethevariance 2isaconstant,thereisaclosed\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\nempiricalmeanofthesquareddierencebetweenobservations yandtheirexpected\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "distribution p( y|x)thatiscontrolledby= f(x;).Thenegativelog-likelihood\nlog p(y;(x))willthenprovideacostfunctionwiththeappropriateterms\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\ncanmakeanewparameterinthenetworkthatiscopieddirectlyinto.Thisnew\nparametermightbe itselforcouldbeaparameter vrepresenting 2oritcould\nbeaparameter representing1\n2,dependingonhowwechoosetoparametrize",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "2,dependingonhowwechoosetoparametrize\nthedistribution.Wemaywishourmodeltopredictadierentamountofvariance\nin yfordierentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\nheteroscedasticcase,wesimplymakethespecicationofthevariancebeoneof\nthevaluesoutputby f( x;).AtypicalwaytodothisistoformulatetheGaussian\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\ndiag (6.34) () .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "diag (6.34) () .\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\nlog-likelihoodoftheGaussiandistributionparametrized byinvolvesonlymul-\ntiplicationby  iandadditionoflog i.Thegradientofmultiplication, addition,\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "arbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\ncanvanishnearzero,makingitdiculttolearnparametersthataresquared.\n1 8 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivedenite.Because\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\npositivedenite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Ifwesupposethataistherawactivationofthemodelusedtodeterminethe\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\nvector:= (a) .Thissamestrategyappliesequallyifusingvarianceorstandard\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\ndiagonalmatrix.\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\ndiagonal.Ifthecovarianceisfullandconditional,thenaparametrization must",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "bechosenthatguaranteespositive-denitenessofthepredictedcovariancematrix.\nThiscanbeachievedbywriting () = ()xBxB()x,whereBisanunconstrained\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\nlikelihoodisexpensive,witha d dmatrixrequiring O( d3)computationforthe\ndeterminantandinverseof (x)(orequivalently,andmorecommonlydone,its\neigendecompositionorthatof).Bx()\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldierent\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdenedby\ntheconditionalprobabilitydistribution\np( ) =yx|n\ni = 1p i (= c |Nx)(;y( ) i()x , ( ) i())x .(6.35)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Theneuralnetworkmusthavethreeoutputs:avectordening p(c= i|x),a\nmatrixproviding( ) i(x)forall i,andatensorproviding ( ) i(x)forall i.These\noutputsmustsatisfydierentconstraints:\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\noverthe ndierentcomponentsassociatedwithlatentvariable1c,andcan\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "y , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\nra n d o m v a ria b l e .\n1 8 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\nthattheseoutputsarepositiveandsumto1.\n2.Means( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\nan n dmatrixcontainingall nofthese d-dimensionalvectors.Learning\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "learningthemeansofadistributionwithonlyoneoutputmode.Weonly\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\neachexamplescontributiontothelossforeachcomponentbytheprobability\nthatthecomponentproducedtheexample.\n3.Covariances ( ) i(x):thesespecifythecovariancematrixforeachcomponent\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\nspecicationofthenegativelog-likelihoodunderthemixturemodel.\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,). MurrayandLarochelle2014\nGaussianmixtureoutputsareparticularlyeectiveingenerativemodelsof\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\ndensitynetworkisshowningure.6.4\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\nthatformsasentence.Inthesecases,wemaycontinuetousetheprinciple\nofmaximumlikelihoodappliedtoourmodel p(y;(x)),butthemodelweuse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "1 9 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nxy\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "predictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\nChapterdescribeshowtouserecurrentneuralnetworkstodenesuchmodels 10\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\nprobabilitydistributions.\n6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "arecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\nmodel.\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\nyethavemanydenitiveguidingtheoreticalprinciples.\nRectiedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\ntypesofhiddenunitsareavailable.Itcanbediculttodeterminewhentouse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "whichkind(thoughrectiedlinearunitsareusuallyanacceptablechoice).We\n1 9 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\nonavalidationset.\nSomeofthehiddenunitsincludedinthislistarenotactuallydierentiableat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "allinputpoints.Forexample,therectiedlinearfunction g( z) =max{0 , z}isnot\ndierentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\nforthesemodelstobeusedformachinelearningtasks.Thisisinpartbecause\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\nthecostfunction,butinsteadmerelyreduceitsvaluesignicantly,asshownin\ngure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "expecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\nfortheminimaofthecostfunctiontocorrespondtopointswithundenedgradient.\nHiddenunitsthatarenotdierentiableareusuallynon-dierentiable atonlya\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedened\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\ndenedbytheslopeofthefunctionimmediately totherightof z.Afunction\nisdierentiableat zonlyifboththeleftderivativeandtherightderivativeare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "denedandequaltoeachother.Thefunctionsusedinthecontextofneural\nnetworksusuallyhavedenedleftderivativesanddenedrightderivatives.Inthe\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\ntheone-sidedderivativesratherthanreportingthatthederivativeisundenedor\nraisinganerror.Thismaybeheuristicallyjustiedbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 thatwasrounded\nto.Insomecontexts,moretheoreticallypleasingjusticationsareavailable,but 0\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\ninpracticeonecansafelydisregardthenon-dierentiabilityofthehiddenunit\nactivationfunctionsdescribedbelow.\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "avectorofinputsx,computingananetransformationz=Wx+b,and\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\nfunction. g()z\n1 9 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1RectiedLinearUnitsandTheirGeneralizations\nRectiedlinearunitsusetheactivationfunction . g z , z () = max0{}\nRectiedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\nunits.Theonlydierencebetweenalinearunitandarectiedlinearunitis\nthatarectiedlinearunitoutputszeroacrosshalfitsdomain.This makesthe\nderivativesthrougharectiedlinearunitremainlargewhenevertheunitisactive.\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "rectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\nthatintroducesecond-ordereects.\nRectiedlinearunitsaretypicallyusedontopofananetransformation:\nhW= ( gxb+) . (6.36)\nWheninitializingtheparametersoftheanetransformation,itcanbeagood\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "itverylikelythattherectiedlinearunitswillbeinitiallyactiveformostinputs\ninthetrainingsetandallowthederivativestopassthrough.\nSeveralgeneralizations ofrectiedlinearunitsexist.Mostofthesegeneral-\nizationsperformcomparablytorectiedlinearunitsandoccasionallyperform\nbetter.\nOnedrawbacktorectiedlinearunitsisthattheycannotlearnviagradient-\nbasedmethodsonexamplesforwhichtheiractiv ationiszero.Avarietyof\ngeneralizations ofrectiedlinearunitsguaranteethattheyreceivegradientevery-\nwhere.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "where.\nThreegeneralizations ofrectiedlinearunitsarebasedonusinganon-zero\nslope  iwhen z i <0: h i= g(z ,) i=max(0 , z i)+  imin(0 , z i).Absolutevalue\nrecticationxes  i=1toobtain g( z) =|| z.Itisusedforobjectrecognition\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\nofrectiedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "2013)xes  itoasmallvaluelike0.01whileaparametricReLUorPReLU\ntreats  iasalearnableparameter(,). He e t a l .2015\nMaxoutunits( ,)generalizerectiedlinearunits Goodfellow e t a l .2013a\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\n1 9 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\noneofthesegroups:\ng()z i=max\nj G() iz j (6.37)\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i1) k+1 , . . . , i k}.\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\ndirectionsintheinputspace.x\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "learntoapproximateanyconvexfunctionwitharbitrarydelity.Inparticular,\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\ninputxasatraditionallayerusingtherectiedlinearactivationfunction,absolute\nvaluerecticationfunction,ortheleakyorparametricReLU,orcanlearnto\nimplementatotallydierentfunctionaltogether.Themaxoutlayerwillofcourse\nbeparametrized dierentlyfromanyoftheseotherlayertypes,sothelearning\ndynamicswillbedierenteveninthecaseswheremaxoutlearnstoimplementthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "samefunctionofasoneoftheotherlayertypes. x\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\nsomaxoutunitstypicallyneedmoreregularizationthanrectiedlinearunits.They\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\npiecesperunitiskeptlow(,). Cai e t a l .2013\nMaxoutunitshaveafewotherbenets.Insomecases,onecangainsomesta-\ntisticalandcomputational advantagesbyrequiringfewerparameters.Specically,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "ifthefeaturescapturedby ndierentlinearlterscanbesummarizedwithout\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\nlayercangetbywithtimesfewerweights. k\nBecauseeachunitisdrivenbymultiplelters,maxoutunitshavesomeredun-\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\nthepast( ,). Goodfellow e t a l .2014a\nRectiedlinearunitsandallofthesegeneralizations ofthemarebasedonthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "1 9 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\narchitectures,theLSTM,propagatesinformationthroughtimeviasummationa\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\ninsection.10.10\n6.3.2LogisticSigmoidandHyperbolicTangent\nPriortotheintroduction ofrectiedlinearunits,mostneuralnetworksusedthe\nlogisticsigmoidactivationfunction\ng z  z () = () (6.38)\northehyperbolictangentactivationfunction\ng z z . () = tanh( ) (6.39)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "g z z . () = tanh( ) (6.39)\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z  z\nWehavealreadyseen sigmoidunitsasoutputunits,usedtopredictthe\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\nunitssaturateacrossmostoftheirdomainthey saturatetoahighvaluewhen\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "sigmoidalunitscanmakegradient-basedlearningverydicult.Forthisreason,\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\nlayer.\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while (0) =1\n2.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "2.\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\nnetwork y=wtanh(Utanh(Vx))resemblestrainingalinearmodel y=\nwUVxsolongastheactivationsofthenetworkcanbekeptsmall.This\nmakestrainingthenetworkeasier. tanh\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\ndrawbacksofsaturation.\n1 9 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.3OtherHiddenUnits\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\nIngeneral,awidevarietyofdierentiable functionsperformperfectlywell.\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "functions.Duringresearchanddevelopmentofnewtechniques,itiscommon\ntotestmanydierentactivationfunctionsandndthatseveralvariationson\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasignicant\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\ntypesaresocommonastobeuninteresting.\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "intheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "aneuralnetworklayerwith ninputsand poutputs,h= g(Wx+b).Wemay\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\nusingweightmatrixV.Iftherstlayerhasnoactivationfunction,thenwehave\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\nfactoredapproachistocomputeh= g(VUx+b).IfUproduces qoutputs,\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "comesatthecostofconstrainingthelineartransformationtobelow-rank,but\ntheselow-rankrelationshipsareoftensucient.Linearhiddenunitsthusoeran\neectivewayofreducingthenumberofparametersinanetwork.\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "unitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\nmanipulatememory,describedinsection.10.12\n1 9 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAfewotherreasonablycommonhiddenunittypesinclude:\nRadialbasisfunctionorRBFunit: h i=exp\n1\n2\ni||W : , i||x2\n.This\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\nsaturatestoformost,itcanbediculttooptimize. 0x\nSoftplus: g( a) = ( a) =log(1+ ea).Thisisasmoothversionoftherectier,\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "models. ()comparedthesoftplusandrectierandfound Glorot e t a l .2011a\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\nbeverycounterintuitiveonemightexpectittohaveanadvantageover\ntherectierduetobeingdierentiableeverywhereorduetosaturatingless\ncompletely,butempiricallyitdoesnot.\nHardtanh:thisisshapedsimilarlytothetanhandtherectierbutunlike\nthelatter,itisbounded, g( a)=max(1 ,min(1 , a)).Itwasintroduced",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "by(). Collobert2004\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\nunittypesremaintobediscovered.\n6. 4 A rc h i t ec t u re D es i gn\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers.Most\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "layerbeingafunctionofthelayerthatprecededit.Inthisstructure,therstlayer\nisgivenby\nh( 1 )= g( 1 )\nW( 1 )xb+( 1 )\n, (6.40)\nthesecondlayerisgivenby\nh( 2 )= g( 2 )\nW( 2 )h( 1 )+b( 2 )\n, (6.41)\nandsoon.\n1 9 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\nanetworkwithevenonehiddenlayerissucienttotthetrainingset.Deeper\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize.The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "monitoringthevalidationseterror.\n6.4.1UniversalApproximationPropertiesandDepth\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\nbydenitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\nAtrstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\nmationframework.Specically,theuniversalapproximationtheorem(Hornik\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\nlayerandatleastonehiddenlayerwithanysquashingactivationfunction(such\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\nfunctionfromonenite-dimensional spacetoanotherwithanydesirednon-zero\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "derivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\nisbeyondthescopeofthisbook;forourpurposesitsucestosaythatany\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\nalsoapproximateanyfunctionmappingfromanynitedimensionaldiscretespace\ntoanother.Whiletheoriginaltheoremswererststatedintermsofunitswith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "activationfunctionsthatsaturatebothforverynegativeandforverypositive\narguments,universalapproximation theoremshavealsobeenprovedforawider\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiedlinear\nunit( ,). Leshno e t a l .1993\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "to l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\ncanfailfortwodierentreasons.First,theoptimizationalgorithmusedfortraining\n1 9 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmaynotbeabletondthevalueoftheparametersthatcorrespondstothedesired\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\novertting.Recallfromsectionthatthenofreelunchtheoremshowsthat 5.2.1\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "isnouniversalprocedureforexaminingatrainingsetofspecicexamplesand\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "withonehiddenunitcorrespondingtoeachinputcongurationthatneedstobe\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\nnumberofpossiblebinaryfunctionsonvectorsv{0 ,1}nis22nandselecting\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\nfreedom.\nInsummary,afeedforwardnetworkwithasinglelayerissucienttorepresent\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\namountofgeneralization error.\nThereexistfamiliesoffunctionswhichcanbeapproximated ecientlybyan\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n.Suchresults\nwererstprovedformodelsthatdonotresemblethecontinuous,dierentiable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\nmodels.Therstresultswereforcircuitsoflogicgates(,).Later Hstad1986\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\n( ,; ,),andthentonetworkswith HstadandGoldmann1991Hajnal e t a l .1993\ncontinuous-valuedactivations(,; ,).Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserectiedlinearunits. ()demonstrated Leshno e t a l .1993",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\nincludingrectiedlinearunits,haveuniversalapproximation properties,butthese\nresultsdonotaddressthequestionsofdepthoreciencytheyspecifyonlythat\nasucientlywiderectiernetworkcouldrepresentanyfunction.Montufar e t a l .\n1 9 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprectiernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\nfromrectiernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\nanetworkwithabsolutevaluerecticationcreatesmirrorimagesofthefunction",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "computedontopofsomehiddenunit,withrespecttotheinputofthathidden\nunit.Eachhiddenunitspecieswheretofoldtheinputspaceinordertocreate\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "rectiernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectication\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\nofsymmetryisgivenbythehyperplanedenedbytheweightsandbiasoftheunit.A\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "befoldedontopoftherst(byanotherdownstreamunit)toobtainanothersymmetry\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\npermissionfrom (). Montufar e t a l .2014\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\nnumberoflinearregionscarvedoutbyadeeprectiernetworkwith dinputs,\ndepth,andunitsperhiddenlayer,is l n\nOn\ndd l ( 1 )\nnd\n, (6.42)\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithltersper l k\nunit,thenumberoflinearregionsis\nO",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "unit,thenumberoflinearregionsis\nO\nk( 1 ) + l d\n. (6.43)\n2 0 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\nWemayalsowanttochooseadeepmodelforstatisticalreasons.Anytime\nwechooseaspecicmachinelearningalgorithm,weareimplicitlystatingsome\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\nmultiplesteps,whereeachstepmakesuseofthepreviousstepsoutput.These",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "intermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,;2014dSzegedy ,).Seegureandgureforexamplesof 2014a 6.6 6.7\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\n6.4.2OtherArchitecturalConsiderations\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\nInpractice,neuralnetworksshowconsiderablymorediversity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "Manyneuralnetworkarchitectures havebeendevelopedforspecictasks.\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\nhavetheirownarchitecturalconsiderations.\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoowfrom\noutputlayerstolayersnearertheinput.\n2 0 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n3 4 5 6 7 8 9 1 0 1 1\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\ne t a l .().Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth.See 2014d",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "gureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesameeect.\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "theoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\nthatareveryeectiveforcomputervisionproblems.Inthischapter,itisdicult\ntogivemuchmorespecicadviceconcerningthearchitectureofagenericneural\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "havebeenfoundtoworkwellfordierentapplicationdomains.\n2 0 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\nN u m b e r o f p a r a m e t e r s  1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\n3,fullyconnected\n11,convolutional\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "nearlyaseectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\ncontextovertataround20millionparameterswhiledeeponescanbenetfromhaving\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\nthespaceoffunctionsthemodelcanlearn.Specically,itexpressesabeliefthatthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "functionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\ncornersdenedintermsofedges)orinlearningaprogramwithsequentiallydependent\nsteps(e.g.,rstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\nthem).\n2 0 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i  eren t i at i on A l go-\nri t h m s\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\noutput y,informationowsforwardthroughthenetwork.Theinputsxprovide\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\nandnallyproduces y.Thisiscalledforwardpropagation.Duringtraining,\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J().",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\nbackprop,allowstheinformationfromthecosttothenowbackwardsthrough\nthenetwork,inordertocomputethegradient.\nComputingananalyticalexpressionforthegradientisstraightforward,but\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\nThetermback-propagation isoftenmisunders toodasmeaningthewhole",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\nFurthermore,back-propagation isoftenmisunderstoodasbeingspecictomulti-\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\nfunctionisundened).Specically,wewilldescribehowtocomputethegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": " x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\n  J().Manymachinelearningtasksinvolvecomputingotherderivatives,either\naspartofthelearningprocess,orto analyzethelearnedmodel. Theback-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\nusedcasewherehasasingleoutput. f\n2 0 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.1ComputationalGraphs\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\nmoreprecise language. computationalgraph\nManywaysofformalizingcomputationasgraphsarepossible.\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\ntogether.\nWithoutlossofgenerality,wedeneanoperationtoreturnonlyasingle\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\nconceptualunderstanding.\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\nwedrawadirectededgefrom xto y.Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext.\nExamplesofcomputational graphsareshowningure.6.8\n6.5.2ChainRuleofCalculus\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\nchainrule,withaspecicorderofoperationsthatishighlyecient.\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x. (6.44)\nWecangeneralizethisbeyondthescalarcase.Supposethatx Rm,y Rn,\n2 0 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxx yy\n( a)\nx x ww\n( b)u( 1 )u( 1 )\nd o t\nbbu( 2 )u( 2 )\n+ y  y\n\n( c )XX WWU( 1 )U( 1 )\nm a t m u l\nb bU( 2 )U( 2 )\n+HH\nr e l u\nx x ww\n( d) y y\nd o t\n u( 1 )u( 1 )\ns q ru( 2 )u( 2 )\ns u mu( 3 )u( 3 )\n\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) operationto\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b )  y= \nxw+ b\n.\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "butneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\nmatrixofrectiedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\nofinputsX.Examplesacappliedatmostoneoperationtoeachvariable,butit ( d )\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\nweightsareusedtomakeboththeprediction yandtheweightdecaypenalty ",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "iw2\ni.\n2 0 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\n z\n x i=\nj z\n y j y j\n x i. (6.45)\nInvectornotation,thismaybeequivalentlywrittenas\n x z=y\nx\n y z , (6.46)\nwhere y\n xistheJacobianmatrixof. n m g\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\naJacobianmatrix y\n xbyagradient y z.Theback-propagation algorithmconsists\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\nsameasback-propagation withvectors.Theonlydierenceishowthenumbers\narearrangedinagridtoformatensor.Wecouldimagineatteningeachtensor\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\nback-propagationisstilljustmultiplyingJacobiansbygradients.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "Todenotethegradientofavalue zwithrespecttoatensor X,wewrite  X z,\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinatesfor\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\npossibleindextuples i,( X z) igives z\n X i.Thisisexactlythesameashowforall\npossibleintegerindices iintoavector,( x z) igives z\n x i.Usingthisnotation,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": " x i.Usingthisnotation,we\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\n X z=\nj( X Y j) z\n Y j. (6.47)\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\nintroducessomeextraconsiderations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "introducessomeextraconsiderations.\nSpecically,manysubexpressionsmayberepeatedseveraltimeswithinthe\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n2 0 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\ngure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\nbewasteful.Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "reducememoryconsumptionatthecostofhigherruntime.\nWerstbeginbyaversionoftheback-propagationalgorithmthatspeciesthe\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\ncomputations orviewthedescriptionofthealgorithmasasymbolicspecication\nofthecomputational graphforcomputingtheback-propagation. However,this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "formulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\nsymbolicgraphthatperformsthegradientcomputation.Such aformulationis\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\nnodesthatcontainarbitrarytensors.\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).In\notherwordswewishtocompute u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "otherwordswewishtocompute u() n\n u() iforall i{1 ,2 , . . . , n i}.Intheapplication\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\ncorrespondtotheparametersofthemodel.\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\ngoingupto u( ) n.Asdenedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "operation f( ) iandiscomputedbyevaluatingthefunction\nu( ) i= ( f A( ) i) (6.48)\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\nThatalgorithmspeciestheforwardpropagationcomputation,whichwecould\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\nthederivative u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "thederivative u() n\n u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\n2 0 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\nu( 1 )to u( n i )toanoutput u( ) n.Thisdenesacomputational graphwhereeachnode\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a ( u( ) i).The\ninputtothecomputational graphisthevectorx,andissetintotherst n inodes\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadothelast(output)\nnode u( ) n.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "node u( ) n.\nfor i , . . . , n = 1 ido\nu( ) i x i\nendfor\nfor i n= i+1 , . . . , ndo\nA( ) i{ u( ) j| j P a u(( ) i)}\nu( ) i f( ) i( A( ) i)\nendfor\nreturn u( ) n\nusingthechainrulewithrespecttoscalaroutput u( ) n:\n u( ) n\n u( ) j=\ni j P a u : (() i ) u( ) n\n u( ) i u( ) i\n u( ) j(6.49)\nasspeciedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\nthecomputationof u() i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "thecomputationof u() i\n u() j.Inaddition,adotproductisperformedforeachnode,\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\nof u( ) jandthevectorcontainingthepartialderivatives u() i\n u() jforthesamechildren\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nodewithrespecttooneofitsparents)aswellasperformingonemultiplication\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\necientimplementations.\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\nsubexpressionswithoutregardtomemory.Specically,itperformsontheorder\nofoneJacobianproductpernodeinthegraph.Thiscanbeseenfromthefact\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "thegraphexactlyonceinordertoobtaintheassociatedpartialderivative u() i\n u() j.\n2 0 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.2Simpliedversionoftheback-propagation algorithmforcomputing\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\nintendedtofurtherunderstandingbyshowingasimpliedcasewhereallvariables\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\nThissimpliedversioncomputesthederivativesofallnodesinthegraph.The\ncomputational costofthisalgorithmisproportional tothenumberofedgesin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\ntheforwardpropagation. Each u() i\n u() jisafunctionoftheparents u( ) jof u( ) i,thus\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\ngraph.\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\ntionsofthenetwork\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "beencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\n u() n\n u() i.\ng r a d t a b l e_ [ u( ) n] 1\nfor do j n= 1downto1\nThenextlinecomputes u() n\n u() j=\ni j P a u : (() i ) u() n\n u() i u() i\n u() jusingstoredvalues:\ng r a d t a b l e_ [ u( ) j] \ni j P a u : (() i ) g r a d t a b l e_ [ u( ) i] u() i\n u() j\nendfor\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\nsimplicationsonthecomputational graph,ormaybeabletoconservememoryby\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\nafterdescribingtheback-propagation algorithmitself.\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\nToclarifytheabovedenitionoftheback-propagation computation,letusconsider\nthespecicgraphassociatedwithafully-connected multi-layerMLP.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "Algorithmrstshowstheforwardpropagation, whichmapsparametersto 6.3\nthesupervisedloss L(yy ,)associatedwithasingle(input,target) trainingexample\n( )xy ,,with ytheoutputoftheneuralnetworkwhenisprovidedininput. x\nAlgorithm thenshowsthecorrespondingcomputationto bedonefor 6.4\n2 1 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfff\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\nthegradient.Let w Rbetheinputtothegraph.Weusethesamefunction f: R R\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\nTocompute z\n w,weapplyequationandobtain: 6.44\n z\n w(6.50)\n= z\n y y\n x x\n w(6.51)\n= f() y f() x f() w (6.52)\n= f((())) f f w f(()) f w f() w (6.53)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "= f((())) f f w f(()) f w f() w (6.53)\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "runtime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\nusefulwhenmemoryislimited.\n2 1 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napplyingtheback-propagation algorithmtothisgraph.\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However,theyarespecializedtoonespecic\nproblem.\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\ncomputation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "computation.\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\nthecomputationofthecostfunction.Theloss L(yy ,)dependsontheoutput\nyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer ( ),where \ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "useaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . . , l , {1 }theweightmatricesofthemodel\nRequire:b( ) i, i , . . . , l , {1 }thebiasparametersofthemodel\nRequire:x,theinputtoprocess\nRequire:y,thetargetoutput\nh( 0 )= x\nfordo k , . . . , l = 1\na( ) k= b( ) k+W( ) kh( 1 ) k\nh( ) k= ( fa( ) k)\nendfor\nyh= ( ) l\nJ L= (yy ,)+()  \n6.5.5Symbol-to-SymbolDerivatives\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "variablesthatdonothavespecicvalues.Thesealgebraicand graph-based\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\ntrainaneuralnetwork,wemustassignspecicvaluestothesesymbols.We\nreplaceasymbolicinputtothenetworkxwithaspecicnumericvalue,suchas\n[123765 18] . , . , ..\n2 1 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\noutputlayerandgoingbackwardstothersthiddenlayer.Fromthesegradients,\nwhichcanbeinterpretedasanindicationofhoweachlayersoutputshouldchange\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "gradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\ncomputed)orusedwithothergradient-basedoptimization methods.\nAftertheforwardcomputation,computethegradientontheoutputlayer:\ng  y J=   y L(yy ,)\nfor do k l , l , . . . , = 1 1\nConvertthegradientonthelayersoutputintoagradientintothepre-\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\nga() k J f = g(a( ) k)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "ga() k J f = g(a( ) k)\nComputegradientsonweightsandbiases(includingtheregularizationterm,\nwhereneeded):\nb() k J  = +g b() k() \nW() k J= gh( 1 ) k+ W() k() \nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayersactivations:\ngh(1) k  J= W( ) kg\nendfor\n2 1 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfffz z\nxxyy\nw wfff\nd z\nd yd z\nd yf\nd y\nd xd y\nd xf\nd z\nd xd z\nd x\nd x\nd wd x\nd wf\nd z\nd wd z\nd w\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\nspecicnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "derivativesforanyspecicnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\nittoconstructthegraphfortheexpressioncorrespondingtod z\nd w.Inthisexample,wedo\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\nderivative.\nSomeapproachestoback-propagationtakeacomputational graphandaset",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "ofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproachsymbol-\nto-numberdierentiation. ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCae(,). Collobert e t a l .2011b Jia2013\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "andTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\nisillustratedingure.Theprimaryadvantageofthisapproachisthat 6.10\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\nback-propagationagain,dierentiating thederivativesinordertoobtainhigher\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "2 1 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\ngraphmaythenbeevaluatedusingspecicnumericalvaluesatalatertime.This\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\nparentsvaluesareavailable.\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "performingexactlythesamecomputations asaredoneinthegraphbuiltbythe\nsymbol-to-symbolapproach.Thekeydierenceisthatthesymbol-to-number\napproachdoesnotexposethegraph.\n6.5.6GeneralBack-Propagation\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\nthatthegradientwithrespectto zisgivenbyd z\nd z=1.Wecanthencompute\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "currentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\ntwoormorepaths,wesimplysumthegradientsarrivingfromdierentpathsat\nthatnode.\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\nmaximumgenerality,wedescribethisvariableasbeingatensor V.Tensorcan\ningeneralhaveanynumberofdimensions.Theysubsumescalars,vectors,and\nmatrices.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "matrices.\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\n g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\noperation,andtheget_operationfunction.Supposewehaveavariablethat\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\nreturnsapointertoaninstanceofthecorrespondingC++class.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": " g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\nVinthecomputational graph.G\n G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\ninthecomputational graph.G\n2 1 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "ofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\nisresponsiblefordeningtwoback-propagation rules,oneforeachofitsinput\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\nisgivenbyGB.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "b p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAG.The\nback-propagationalgorithmitselfdoesnotneedtoknowanydierentiation rules.It\nonlyneedstocalleachoperationsbpropruleswiththerightarguments.Formally,\no p b p r o p i n p u t s . ( , , X G)mustreturn\n\ni( X o p f i n p u t s .( ) i) G i , (6.54)\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "mathematical functionthattheoperationimplements, Xistheinputwhosegradient\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\nderivativeon. x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "derivativeon. x\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\nanynewoperationsmanually.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "anynewoperationsmanually.\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\n2 1 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\ninthe subroutineofalgorithm build_grad 6.6.\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\nRequire:G,thecomputational graph\nRequire: z,thevariabletobedierentiated\nLetGbeGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin. T",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "ofnodesin. T\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\ng r a d t a b l e_ [] 1 z\nfordo Vin T\nb u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )\nendfor\nReturn restrictedto grad_table T\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\nNowthatwehavespeciedtheback-propagationalgorithm,wecanunderstandits",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "computational cost.Ifweassumethateachoperationevaluationhasroughlythe\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "O( n2) operations.Herewearecountingoperationsinthecomputational graph,not\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n2 1 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG, g r a d t a b l e_ )of\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdened\ninalgorithm .6.5\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\nRequire:G,thegraphtomodify.\nRequire:G,therestrictionoftonodesthatparticipateinthegradient. G\nRequire:grad_table,adatastructuremappingnodestotheirgradients\nif then Visingrad_table\nReturn_ g r a d t a b l e[] V\nendif\ni1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "Return_ g r a d t a b l e[] V\nendif\ni1\nfor C V in_ g e t c o n s u m e r s( ,G)do\no p g e t o p e r a t i o n _ () C\nD C  b u i l d g r a d_ ( , ,GG, g r a d t a b l e_ )\nG( ) i G o p b p r o p g e t i n p u t s . (_ ( C ,) ) , , V D\ni i+1\nendfor\nG\ni G( ) i\ng r a d t a b l e_ [] = V G\nInsertandtheoperationscreatingitinto G G\nReturn G\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\ntherecursivechainrule(equation)non-recursively: 6.49\n u( ) n\n u( ) j=\npa t h ( u( 1), u( 2), . . . , u(  t)) ,\nf r o m 1 = t o j  t = nt\nk = 2 u(  k )\n u(  k 1 ). (6.55)\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "graph.Thislargecostwouldbeincurredbecausethesamecomputationfor\n u() i\n u() jwouldberedonemanytimes.Toavoidsuchrecomputation, wecanthink\nofback-propagation asatable-llingalgorithmthattakesadvantageofstoring\nintermediateresults u() n\n u() i.Eachnodeinthegraphhasacorrespondingslotina\ntabletostorethegradientforthatnode.Byllinginthesetableentriesinorder,\n2 1 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-lling\nstrategyissometimescalled . dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron.\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "singleminibatch.Specically,weuseaminibatchofexamplesfromthetraining\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "operationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\ndistributiondenedbytheseunnormalized logprobabilities. Theresultingcross-\nentropydenesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\nlikelihoodestimationoftheclassier.However,tomakethisexamplemorerealistic,\nwealsoincludearegularizationterm.Thetotalcost\nJ J= M LE+ \n\ni , j\nW( 1 )\ni , j2\n+\ni , j\nW( 2 )\ni , j2\n (6.56)\nconsistsofthecross-entropyandaweightdecaytermwithcoecient .The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "computational graphisillustratedingure.6.11\nThecomputational graphforthegradientofthisexampleislargeenoughthat\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebenets\noftheback-propagation algorithm,whichisthatitcanautomatically generate\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\nderivemanually.\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\nbylookingattheforwardpropagationgraphingure.Totrain,wewish 6.11",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "tocomputebothW(1) Jand W(2) J.Therearetwodierentpathsleading\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\nalwayscontribute 2 W( ) itothegradientonW( ) i.\n2 1 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nXXW( 1 )W( 1 )U( 1 )U( 1 )\nm a t m u lHH\nr e l u\nU( 3 )U( 3 )\ns q ru( 4 )u( 4 )\ns u m  u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\nm a t m u ly yJ M L E J M L E\nc r o s s _ e n t r o p y\nU( 5 )U( 5 )\ns q ru( 6 )u( 6 )\ns u mu( 8 )u( 8 )J J\n+\n\n+\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\nexploretwodierentbranches.Ontheshorterbranch,itaddsHGtothe\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\ncomputes  H J=GW( 2 )usingtheback-propagationrulefortherstargument",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "tothematrixmultiplication operation.Next,thereluoperationusesitsback-\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G.Thelaststepofthe\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\nargumentoftheoperationtoadd matmul XGtothegradientonW( 1 ).\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "updatetheparameters.\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\n2 2 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "numberofexamplesintheminibatchand n histhenumberofhiddenunits.\n6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice.\nAsnotedabove,wehaverestrictedthedenitionofanoperationtobea\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "besttocomputebothinasinglepassthroughmemory,soitismostecientto\nimplementthisprocedureasasingleoperationwithtwooutputs.\nWehavenotdescribedhowtocontrolthememoryconsumptionofback-\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuerand\naddingeachvaluetothatbuerasitiscomputed.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "addingeachvaluetothatbuerasitiscomputed.\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\ndatatypes,suchas32-bitoatingpoint,64-bitoatingpoint,andintegervalues.\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\nSomeoperationshaveundenedgradients,anditisimportanttotrackthese\ncasesanddeterminewhetherthegradientrequestedbytheuserisundened.\nVariousothertechnicalitiesmakereal-worlddierentiation morecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\nthatmanymoresubtletiesexist.\n6.5.9DierentiationoutsidetheDeepLearningCommunity\nThedeeplearningcomm unityhasbeensomewhatisolatedfromthebroader\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n2 2 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nconcerninghowtoperformdierentiation. Moregenerally,theeldofautomatic\ndierentiationisconcernedwithhowtocomputederivativesalgorithmically .\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\ndierentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\nruleindierentorders.Ingeneral,determining theorderofevaluationthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "resultsinthelowestcomputational costisadicultproblem.Findingtheoptimal\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\nexpensiveform.\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\nwedene\nq i=exp( z i)\niexp( z i), (6.57)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "wedene\nq i=exp( z i)\niexp( z i), (6.57)\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\noperations,andconstructa cross-entropyloss J=\ni p ilog q i.Ahuman\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\nsimpleform: q i p i.Theback-propagation algorithmisnotcapableofsimplifying\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "librariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\nbythepureback-propagation algorithm.\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\n u() i\n u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\nthesameorderasthenumberofcomputations fortheforwardcomputation: this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "canbeseeninalgorithm becauseeachlocalpartialderivative 6.2 u() i\n u() jneedsto\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\ntask.ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "matchingknownsimplicationpatternsinordertoiterativelyattempttosimplify\nthegraph.Wedenedback-propagation onlyforthecomputationofagradientofa\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\nof kdierentscalarnodesinthegraph,orofatensor-valuednodecontaining k\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\n2 2 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\nanotherformofautomaticdierentiationcalledforwardmodeaccumulation.\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\nocomputational eciencyformemory.Therelationshipbetweenforwardmode\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\nright-multiplyingasequenceofmatrices,suchas\nABCD , (6.58)\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "andgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\nthemultiplications left-to-right,correspondingtotheforwardmode.\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\nplementdierentiationsoftwarethatactsdirectlyontraditionalprogramming",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "languagecode,suchasPythonorCcode,andautomatically generatesprograms\nthatdierentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\nrequiringthelibrarydevelopertodenethebpropmethodsforeveryoperation\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendened.\nHowever,thespecializedapproachalsohasthebenetofallowingcustomized",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "back-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\nwouldpresumablybeunabletoreplicate.\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\nlearningcommunityverywell.Inthefuture,dierentiation technologyfordeep\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\ninthebroadereldofautomaticdierentiation.\n2 2 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.10Higher-OrderDerivatives\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\nderivativesastheyusetodescribetheoriginalfunctionbeingdierentiated.This\nmeansthatthesymbolicdierentiation machinerycanbeappliedtoderivatives.\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\nmatrix.Ifwehaveafunction f: Rn R,thentheHessianmatrixisofsize n n.\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\nthusinfeasibletoevenrepresent.\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "performingvariousoperationslikeapproximately invertingamatrixornding\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\notherthanmatrix-vector products.\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\nHv=  x\n( x f x())v\n. (6.59)\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\ntakesthegradientofafunctionoftheinnergradientexpression.\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\nspecifythattheautomaticdierentiationsoftwareshouldnotdierentiatethrough\nthegraphthatproduced.v\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\ne( ) iistheone-hotvectorwith e( ) i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "e( ) iistheone-hotvectorwith e( ) i\ni= 1andallotherentriesequalto0.\n6. 6 Hi s t or i c a l Not es\nFeedforwardnetworkscanbeseenasecientnonlinearfunctionapproximators\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\n2 2 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask.\nThechainrulethatunderliestheback-propagation algorithmwasinvented\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676LHpital1696\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\noptimization problemsuntilthe19thcentury(Cauchy1847,).",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\nseveraloftheawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\nceptronandameansofcomputingthegradientthroughsuchamodel.Ecient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "applicationsofthechainrulebasedondynamicprogramming begantoappear\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\ntechniquestotrainingarticialneuralnetworks.Theideawasnallydeveloped\ninpracticeafterbeingindependentlyrediscoveredindierentways(,;LeCun1985\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "cessingpresentedtheresultsofsomeoftherstsuccessfulexperimentswith\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\nresearchinmulti-layerneuralnetworks.However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation.Theyincludecrucialideasaboutthepossiblecomputational\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "underthenameofconnectionism becauseoftheimportancethisschoolofthought\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\ne t a l .,).1986\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\nbeganin2006.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "beganin2006.\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s.Thesameback-propagationalgorithmandthesame\n2 2 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\nnetworksnoticeably.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "networksnoticeably.\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\nhadpreviouslysueredfromsaturationandslowlearningwhenusingthemean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "squarederrorloss.\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\nlinearhiddenunits,suchasrectiedlinearunits.Recticationusingthemax{0 , z}\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\nmodelsdidnotuserectiedlinearunits,butinsteadappliedrecticationto",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "nonlinearfunctions.Despitetheearlypopularityofrectication,recticationwas\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiedlinearunits\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\nnon-dierentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\nJarrett2009 e t a l .()observedthatusingarectifyingnonlinearityisthesinglemost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "importantfactorinimprovingtheperformanceofarecognitionsystemamong\nseveraldierentfactorsofneuralnetworkarchitecturedesign.\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\nRandomweightsaresucienttopropagateusefulinformationthrougharectied\nlinearnetwork,allowingtheclassierlayeratthetoptolearnhowtomapdierent\nfeaturevectorstoclassidentities.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "featurevectorstoclassidentities.\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\nshowedthatlearningisfareasierindeeprectiedlinearnetworksthanindeep\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\n2 2 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRectiedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohaveaninuenceonthedevelopmentofdeep\nlearningalgorithms. ()motivaterectiedlinearunitsfrom Glorot e t a l .2011a\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\ncompletelyinactive.2)Forsomeinputs,abiologicalneuronsoutputisproportional",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "toitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "ismorecommontousesupervisedlearningtosupportunsupervisedlearning.\nFeedforwardnetworkscontinuetohaveunfullledpotential.Inthefuture,we\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\nsubsequentchapters,weturntohowtousethesemodelshowtoregularizeand\ntrainthem.\n2 2 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 0\nS e qu e n ce Mo d e l i n g: Recurren t\nan d Recursiv e N e t s\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) .Justasconvolutionalnetworks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\nlength.\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\nthe1980s:sharingparametersacrossdierentpartsofamodel.Parametersharing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "makesitpossibletoextendandapplythemodeltoexamplesofdierentforms\n(dierentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\nseenduringtraining,norsharestatisticalstrengthacrossdierentsequencelengths\nandacrossdierentpositionsintime.Suchsharingisparticularlyimportantwhen\naspecicpieceofinformationcanoccuratmultiplepositionswithinthesequence.\nForexample,considerthetwosentencesIwenttoNepalin2009andIn2009,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "IwenttoNepal.Ifweaskamachinelearningmodeltoreadeachsentenceand\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n373",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\nnetworkthatprocessessentencesofxedlength.Atraditionalfullyconnected\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\nacrossseveraltimesteps.\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\nstep.Recurrentnetworksshareparametersinadierentway.Eachmemberofthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph.\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 .In\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\nwithadierentsequencelength foreachmemberoftheminibatch.Wehave",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\nThischapterextendstheideaofacomputational graphtoincludecycles.These",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "cyclesrepresenttheinuenceofthepresentvalueofavariableonitsownvalue\natafuturetimestep.Suchcomputational graphsallowustodenerecurrent\nneuralnetworks.Wethendescribemanydierentwaystoconstruct,train,and\nuserecurrentneuralnetworks.\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\nchapter,wereferthereadertothetextbookofGraves2012().\n3 7 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.1UnfoldingComputationalGraphs\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure.\nForexample,considertheclassicalformofadynamicalsystem:\ns( ) t= ( fs( 1 ) t ;) , (10.1)\nwheres( ) tiscalledthestateofthesystem.\nEquationisrecurrentbecausethedenitionof 10.1 sattime trefersbackto\nthesamedenitionattime. t1\nForanitenumberoftimesteps ,thegraphcanbeunfoldedbyapplying\nthedenition 1times.Forexample,ifweunfoldequationfor10.1 = 3time\nsteps,weobtain\ns( 3 )=( fs( 2 );) (10.2)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "steps,weobtain\ns( 3 )=( fs( 2 );) (10.2)\n=(( f fs( 1 ););) (10.3)\nUnfoldingtheequationbyrepeatedlyapplyingthedenitioninthiswayhas\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\nnowberepresentedbyatraditionaldirectedacycliccomputational graph.The\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\ngure.10.1\ns( t  1 )s( t  1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\nf f f f f f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "f f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph.Eachnoderepresentsthestateatsometime tandthe\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\nofusedtoparametrize)areusedforalltimesteps.  f\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\nsignalx( ) t,\ns( ) t= ( fs( 1 ) t ,x( ) t;) , (10.4)\n3 7 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\nRecurrentneuralnetworkscanbebuiltinmanydierentways.Muchas\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\ndenethevaluesoftheirhiddenunits.Toindicatethatthestateisthehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\nthestate:\nh( ) t= ( fh( 1 ) t ,x( ) t;) , (10.5)\nillustratedingure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n(x( ) t,x( 1 ) t ,x( 2 ) t , . . . ,x( 2 ),x( 1 ))toaxedlengthvectorh( ) t.Dependingonthe\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "uptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\n(chapter).14\nf fhh\nx xh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t  1 )x( t  1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\nf f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "informationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance.\nEquationcanbedrawnintwodierentways.OnewaytodrawtheRNN 10.5\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n3 7 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\nview,thenetworkdenesacircuitthatoperatesinrealtime,withphysicalparts\nwhosecurrentstatecaninuencetheirfuturestate,asintheleftofgure.10.2\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "unfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\ndierentvariables,withonevariablepertimestep,representingthestateofthe\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\nseparatenodeofthecomputational graph,asintherightofgure.Whatwe10.2\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideofthegure\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\ngraphnowhasasizethatdependsonthesequencelength.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "graphnowhasasizethatdependsonthesequencelength.\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\nh( ) t= g( ) t(x( ) t,x( 1 ) t ,x( 2 ) t , . . . ,x( 2 ),x( 1 )) (10.6)\n=( fh( 1 ) t ,x( ) t;) (10.7)\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t ,x( 2 ) t , . . . ,x( 2 ),x( 1 ))\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "processthusintroducestwomajoradvantages:\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\ninputsize,becauseitisspeciedintermsoftransitionfromonestateto\nanotherstate,ratherthanspeciedintermsofavariable-length historyof\nstates.\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep.\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\nrequiredwithoutparametersharing.\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\n3 7 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninformationowforwardintime(computingoutputsandlosses)andbackward\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\ninformationows.\n10.2RecurrentNeuralNetworks\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\ncandesignawidevarietyofrecurrentneuralnetworks.\nUUV V\nWWo( t  1 )o( t  1 )\nhhooy y\nLL",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "UUV V\nWWo( t  1 )o( t  1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t  1 )L( t  1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t  1 )y( t  1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t  1 )x( t  1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "thatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\ncomputesy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aweightmatrixV.Equationdenesforwardpropagationinthismodel. 10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance.\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\nincludethefollowing:\n3 7 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nRecurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsbetweenhiddenunits,illustratedingure.10.3\nRecurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\nunitsatthenexttimestep,illustratedingure10.4\nRecurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\ngure.10.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "gure.10.5\ngureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\nmostofthechapter.\nTherecurrentneuralnetworkofgureandequationisuniversalinthe 10.3 10.8\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\narecurrentnetworkofanitesize.TheoutputcanbereadfromtheRNNafter\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\nfunctionsinthissettingusingasinglespecicRNNofnitesize(Siegelmannand\nSontag1995()use886units).TheinputoftheTuringmachineisaspecication",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "ofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\nmachineissucientforallproblems.ThetheoreticalRNNusedfortheproof\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\nrationalnumbersofunboundedprecision.\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\ngure.Theguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "theguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\nobtainavectoryofnormalizedprobabilitiesovertheoutput.Forwardpropagation\nbeginswithaspecicationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\n3 7 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nUV\nWo( t  1 )o( t  1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t  1 )L( t  1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t  1 )y( t  1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t  1 )x( t  1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\nh( ) . . .h( ) . . .V V V\nU U UU nf ol d\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "tothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbygure.TheRNN 10.3\ningurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\nrepresentationhandtransmithtothefuture.TheRNNinthisgureistrainedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "putaspecicoutputvalueintoo,andoistheonlyinformationitisallowedtosend\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\nfromthepast.ThismakestheRNNinthisgurelesspowerful,butitmaybeeasierto\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\nparallelizationduringtraining,asdescribedinsection.10.2.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "3 8 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nt t  = 1to= ,weapplythefollowingupdateequations:\na( ) t= +bWh( 1 ) t +Ux( ) t(10.8)\nh( ) t=tanh(a( ) t) (10.9)\no( ) t= +cVh( ) t(10.10)\ny( ) t=softmax(o( ) t) (10.11)\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\nL\n{x( 1 ), . . . ,x( ) }{ ,y( 1 ), . . . ,y( ) }\n(10.12)\n=\ntL( ) t(10.13)\n=\ntlog p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\n, (10.14)\nwhere p m o de l\ny( ) t|{x( 1 ), . . . ,x( ) t}\nisgivenbyreadingtheentryfor y( ) tfromthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "isgivenbyreadingtheentryfor y( ) tfromthe\nmodelsoutputvectory( ) t.Computingthegradientofthislossfunctionwithrespect\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\noftheunrolledgraphingure,followedbyabackwardpropagationpass 10.3\nmovingrighttoleftthroughthegraph.Theruntimeis O( ) andcannotbereduced\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "eachtimestepmayonlybecomputedafterthepreviousone.Statescomputed\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\npass,sothememorycostisalso O( ).Theback-propagation algorithmapplied\ntotheunrolledgraphwith O( )costiscalledback-propagationthroughtime\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\nalternative?\n10.2.1TeacherForcingandNetworkswithOutputRecurrence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\nthehiddenunitsatthenexttimestep(showningure)isstrictlylesspowerful 10.4\n3 8 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\nneedtocomputetheoutputfortheprevioustimesteprst,becausethetraining\nsetprovidestheidealvalueofthatoutput.\nh( t  1 )h( t  1 )\nWh( ) th( ) t . . . . . .",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "h( t  1 )h( t  1 )\nWh( ) th( ) t . . . . . .\nx( t  1 )x( t  1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\nU U Uh( ) h( ) \nx( ) x( ) W\nUo( ) o( ) y( ) y( ) L( ) L( ) \nV\n. . . . . .\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\nxed-sizerepresentationusedasinputforfurtherprocessing.Theremightbeatarget\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "back-propagatingfromfurtherdownstreammodules.\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.Wecansee\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n3 8 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t  1 )o( t  1 )o( ) to( ) t\nh( t  1 )h( t  1 )h( ) th( ) t\nx( t  1 )x( t  1 )x( ) tx( ) tW\nV V\nU Uo( t  1 )o( t  1 )o( ) to( ) tL( t  1 )L( t  1 )L( ) tL( ) ty( t  1 )y( t  1 )y( ) ty( ) t\nh( t  1 )h( t  1 )h( ) th( ) t\nx( t  1 )x( t  1 )x( ) tx( ) tW\nV V\nU U\nT r ai n t i m e T e s t  t i m e\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodelsoutput\no( ) t,andfeedtheoutputbackintothemodel.\n3 8 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nlikelihoodcriterionis\nlog p\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\n(10.15)\n=log p\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\n+log p\ny( 1 )|x( 1 ),x( 2 )\n(10.16)\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\nvaluefromthetrainingset.Maximumlikelihoodthusspeciesthatduringtraining,\nratherthanfeedingthemodelsownoutputbackintoitself,theseconnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\nThisisillustratedingure.10.6\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "timesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\nwithbothteacherforcingandBPTT.\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\ntheoutputdistribution)fedbackasinput.Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitedierentfromthekindofinputs\nthatitwillseeattesttime.Onewaytomitigatethisproblemistotrainwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "bothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "inputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\nusemoreofthegeneratedvaluesasinput.\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n3 8 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\ngradient-basedtechniquestotrainanRNN.\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "tforx( ) t,h( ) t,o( ) tand L( ) t.Foreachnode Nweneedtocomputethegradient\n N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingthenalloss\n L\n L( ) t= 1 . (10.17)\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\nsoftmaxfunctiontoobtainthevectoryofprobabilitiesovertheoutput.Wealso\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "inputsofar.Thegradiento( ) t Lontheoutputsattimestep t,forall i , t,isas\nfollows:\n(o( ) t L)i= L\n o( ) t\ni= L\n L( ) t L( ) t\n o( ) t\ni= y( ) t\ni 1i , y( ) t .(10.18)\nWeworkourwaybackwards,startingfromtheendofthesequence.Atthenal\ntimestep, h( ) onlyhaso( ) asadescendent,soitsgradientissimple:\nh( )  L= Vo( )  L. (10.19)\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\nfrom t= 1downto t= 1,notingthath( ) t(for t < )hasasdescendentsboth",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "o( ) tandh( + 1 ) t.Itsgradientisthusgivenby\nh( ) t L=\nh( + 1 ) t\nh( ) t\n(h( +1) t L)+\no( ) t\nh( ) t\n(o( ) t L) (10.20)\n= W(h( +1) t L)diag\n1\nh( + 1 ) t2\n+V(o( ) t L)(10.21)\nwhere diag\n1\nh( + 1 ) t2\nindicatesthediagonalmatrixcontainingtheelements\n1( h( + 1 ) t\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\nhiddenunitattime. i t+1\n3 8 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nOncethegradientsontheinternalnodesofthecomputational graphare\nobtained,wecanobtainthegradientsontheparameternodes.Becausethe\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\nofasingleedgeinthecomputational graphtothegradient.However,the W f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "operatorusedincalculustakesintoaccountthecontributionofWtothevalue\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\nintroducedummyvariablesW( ) tthataredenedtobecopiesofWbutwitheach\nW( ) tusedonlyattimestep t.WemaythenuseW( ) ttodenotethecontribution\noftheweightsattimesteptothegradient. t\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\n c L=\nt\no( ) t\nc\no( ) t L=\nto( ) t L (10.22)\n b L=\nt\nh( ) t\nb( ) t\nh( ) t L=\ntdiag\n1\nh( ) t2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "b( ) t\nh( ) t L=\ntdiag\n1\nh( ) t2\nh( ) t L(10.23)\n V L=\nt\ni\n L\n o( ) t\ni\n V o( ) t\ni=\nt(o( ) t L)h( ) t(10.24)\n W L=\nt\ni\n L\n h( ) t\ni\nW( ) t h( ) t\ni (10.25)\n=\ntdiag\n1\nh( ) t2\n(h( ) t L)h( 1 ) t (10.26)\n U L=\nt\ni\n L\n h( ) t\ni\nU( ) t h( ) t\ni (10.27)\n=\ntdiag\n1\nh( ) t2\n(h( ) t L)x( ) t(10.28)\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\nitdoesnothaveanyparametersasancestorsinthecomputational graphdening\ntheloss.\n3 8 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.2.3RecurrentNetworksasDirectedGraphicalModels\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "weusuallyusethecross-entropyassociatedwiththatdistributiontodenetheloss.\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\nWhenweuseapredictivelog-likelihoodtrainingobjective,suchasequa-\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\nthelog-likelihood\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "log( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t ) . (10.30)\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "fromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\nvaluesinthepasttothecurrent y( ) tvalue.\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) },withnoadditionalinputs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "x.Theinputattimestep tissimplytheoutputattimestep t1.TheRNNthen\ndenesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\nprobabilities:\nP P () = Y ( y( 1 ), . . . , y( ) ) =\nt = 1P( y( ) t| y( 1 ) t , y( 2 ) t , . . . , y( 1 ))(10.31)\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) }accordingtosuchamodel\n3 8 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\npastobservation y( ) imayinuencetheconditionaldistributionofsome y( ) t(for t > i),\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\ngraph(asinequation)mightbeveryinecient,withanevergrowingnumberof 10.6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\nconnectivitybutecientparametrization,asillustratedingure.10.8\nis\nL=\ntL( ) t(10.32)\nwhere\nL( ) t= log(  Py( ) t= y( ) t| y( 1 ) t , y( 2 ) t , . . . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\necientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\nsharethesameparameterswiththeotherstages.\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\neciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\n3 8 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\nshouldonlycontainedgesfrom{y( ) t k , . . . ,y( 1 ) t }toy( ) t,ratherthancontaining\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\ninputsshouldhaveaninuenceonthenextelementofthesequence.RNNsare\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\nfromthedistantpastinawaythatisnotcapturedbytheeectofy( ) iony( 1 ) t .",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\ndeningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\nvalueswiththecompletegraphstructureisshowningure.Thecomplete10.7\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\nmarginalizing themoutofthemodel.\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryecient\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\nrepresentationanarraycontainingaseparateentryforeachpossibleassignment\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\noccurring.If ycantakeon kdierentvalues,thetabularrepresentationwould\nhave O( k)parameters.Bycomparison,duetoparametersharing,thenumberof",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "parametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\nlong-termrelationshipsbetweenvariableseciently,usingrecurrentapplications\nofthesamefunction fandsameparametersateachtimestep.Figure10.8\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "quantitybetweenthem.Avariable y( ) iinthedistantpastmayinuenceavariable\ny( ) tviaitseectonh.Thestructureofthisgraphshowsthatthemodelcanbe\necientlyparametrized byusingthesameconditionalprobabilitydistributionsat\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\njointassignmentofallvariablescanbeevaluatedeciently.\nEvenwiththeecientparametrization ofthegraphicalmodel,someoperations\nremaincomputationally challenging.Forexample,itisdiculttopredictmissing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\nh i d d e n u n i t s .\n3 8 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence.\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybedicult. o p t i m i z i ng\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\nthatthesameparameterscanbeusedfordierenttimesteps.Equivalently,the\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "betweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\ndierenttimesteps.Thiswouldalreadybemuchbetterthanusingadierent\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\nextrapolatewhenfacedwithnewvaluesof. t\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\nsimplytosamplefromtheconditionaldistributionateachtimestep.However,\nthereisoneadditionalcomplication.The RNNmusthavesomemechanismfor\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "weinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) \nineachtrainingexample.\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\nsequenceendsorcontinuesateachtimestep.\nAnotherwaytodeterminethesequencelength istoaddanextraoutputto\nthemodelthatpredictstheinteger itself.Themodelcansampleavalueof \nandthensample stepsworthofdata.Thisapproachrequiresaddinganextra\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\ncaneitherconsistofthevalueof orcanconsistof  t,thenumberofremaining\n3 9 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\nendabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\nbasedonthedecomposition\nP(x( 1 ), . . . ,x( ) ) = ()( P  Px( 1 ), . . . ,x( ) |  .)(10.34)\nThestrategyofpredicting directlyisusedforexamplebyGoodfellow e t a l .\n().2014d\n10.2.4ModelingSequencesConditionedonContextwithRNNs\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of\ncourse,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8\ninputsx( 1 ),x( 2 ), . . . ,x( ) .Ingeneral,RNNsallowtheextensionofthegraphical\nmodelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut\nalsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof\nfeedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;)\ncanbereinterpretedasamodelrepresentingaconditionaldistribution P(y|)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "with=.Wecanextendsuchamodeltorepresentadistribution P(yx|)by\nusingthesame P(y|)asbefore,butmakingafunctionofx.Inthecaseof\nanRNN,thiscanbeachievedindierentways.Wereviewherethemostcommon\nandobviouschoices.\nPreviously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\nt=1 , . . . , asinput.Anotheroptionistotakeonlyasinglevectorxasinput.\nWhenxisaxed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto\nanRNNare:",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "anRNNare:\n1.asanextrainputateachtimestep,or\n2.astheinitialstateh( 0 ),or\n3.both.\nTherstandmostcommonapproachisillustratedingure.Theinteraction10.9\nbetweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly\nintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequence\nof yvalues.ThesameproductxRisaddedasadditionalinputtothehidden\nunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue\n3 9 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nofxRthatiseectivelyanewbiasparameterusedforeachofthehiddenunits.\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparametersofthenon-conditional modelandturningtheminto,where\nthebiasparameterswithinarenowafunctionoftheinput. \no( t  1 )o( t  1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t  1 )L( t  1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t  1 )y( t  1 )y( ) ty( ) ty( +1 ) ty( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\nx xy( ) . . .y( ) . . .\nR R R R R\nFigure10.9:AnRNNthatmapsaxed-lengthvectorxintoadistributionoversequences\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) |x( 1 ), . . . ,x( ) )thatmakesa\nconditionalindependence assumptionthatthisdistributionfactorizesas\n\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\ntheoutputattime ttothehiddenunitattime t+1,asshowningure.The10.10",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\n3 9 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t  1 )o( t  1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t  1 )L( t  1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t  1 )y( t  1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\nU U U\nx( t  1 )x( t  1 )R\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\nFigure10.10:Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "ofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\ngure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\ngivensequencesofxofthesamelength.TheRNNofgureisonlyabletorepresent 10.3\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\nthevalues.x\n3 9 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\no( t  1 )o( t  1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t  1 )L( t  1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t  1 )y( t  1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t  1 )x( t  1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t  1 )g( t  1 )g( ) tg( ) tg( +1 ) tg( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Figure10.11:Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\npoint t,theoutputunitso( ) tcanbenetfromarelevantsummaryofthepastinitsh( ) t\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\n10.3BidirectionalRNNs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "10.3BidirectionalRNNs\nAlloftherecurrentnetworkswehaveconsidereduptonowhaveacausalstruc-\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\nx( 1 ), . . . ,x( 1 ) t ,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoaectthecurrentstatewhenthey\nvaluesareavailable.\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\n3 9 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\ntasks,describedinthenextsection.\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\ne t a l .,).1999",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).1999\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\nsub-RNNthatmovesbackwardthroughtime.Thisallowstheoutputunitso( ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "tocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\nxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\naconvolutionalnetwork,oraregularRNNwithaxed-sizelook-aheadbuer).\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\nhavingRNNs,eachonegoinginoneofthefourdirections:up, down,left, f o u r\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "representationthatwouldcapturemostlylocalinformationbutcouldalsodepend\nonlong-range inputs,iftheRNNisabletolearntocarrythatinformation.\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\ninteractions.\n3 9 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\ntures\nWehaveseeningurehowanRNNcanmapaninputsequencetoaxed-size 10.5\nvector.WehaveseeningurehowanRNNcanmapaxed-sizevectortoa 10.9\nsequence.Wehaveseeningures,,andhowanRNNcan 10.310.410.1010.11\nmapaninputsequencetoanoutputsequenceofthesamelength.\nE nc ode r\n\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\n",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "D e c ode r\n\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\nFigure10.12:Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\ngivenoutputsequence).ThenalhiddenstateoftheencoderRNNisusedtocomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "generallyxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\nsequenceandisgivenasinputtothedecoderRNN.\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength.This comesupin\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\n3 9 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\nnotofthesamelength(althoughtheirlengthsmightberelated).\nWeoftencalltheinputtotheRNNthecontext.Wewanttoproducea\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\nThesimplestRNNarchitectureformappingavariable-length sequenceto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "anothervariable-length sequencewasrstproposedby ()and Cho e t a l .2014a\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\ntectureandwerethersttoobtainstate-of-the-art translationusingthisapproach.\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\nthetranslations.Theseauthorsrespectivelycalledthisarchitecture, illustrated",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "ingure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\nnalhiddenstate.(2)adecoderorwriteroroutputRNNisconditionedon\nthatxed-lengthvector(justlikeingure)togeneratetheoutputsequence 10.9\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "varyfromeachother,whilepreviousarchitectures constrained n x= n y= .Ina\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\ndecoderRNN.\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\nateachtimestep.Thesetwowayscanalsobecombined.\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder.\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "sequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\nthanaxed-sizevector.Additionally,theyintroducedanattentionmechanism\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\n3 9 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequence.Seesectionformoredetails. 12.4.5.1\n10.5DeepRecurrentNetworks\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\nandassociatedtransformations:\n1.fromtheinputtothehiddenstate,\n2.fromtheprevioushiddenstatetothenexthiddenstate,and\n3.fromthehiddenstatetotheoutput.\nWiththeRNNarchitectureofgure,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "ofthesecorrespondstoashallowtransformation.Byashallowtransformation,\nwemeanatransformationthatwouldberepresentedbyasinglelayerwithin\nadeepMLP.Typicallythisisatransformationrepresentedbyalearnedane\ntransformationfollowedbyaxednonlinearity.\nWoulditbeadvantageoustointroducedepthineachoftheseoperations?\nExperimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests\nso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "depthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),\nElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs.\nGraves2013 e t a l .()werethersttoshowasignicantbenetofdecomposing\nthestateofanRNNintomultiplelayersasingure(left).Wecanthink 10.13\nofthelowerlayersinthehierarchydepictedingureaasplayingarole 10.13\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\nthehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\nenumeratedabove,asillustratedingureb.Considerationsofrepresentational 10.13\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\nsobyaddingdepthmayhurtlearningbymakingoptimization dicult.Ingeneral,\nitiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\ngurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable\nintimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\nshortestpathbetweenvariablesinanytwodierenttimesteps,comparedwiththe\nordinaryRNNofgure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this\n3 9 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhy\nxz\n( a) ( b) ( c )xhy\nxhy\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\nhidden,hidden-to-hiddenandhidden-to-outputparts.Thismaylengthentheshortest\npathlinkingdierenttimesteps.Thepath-lengtheningeectcanbemitigatedby ( c )\nintroducingskipconnections.\n3 9 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\nillustratedingurec.10.13\n10.6RecursiveNeuralNetworks\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\nx( 4 )x( 4 )Voo\nU W U WUW\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\nbemappedtoaxed-sizerepresentation(theoutputo),withaxedsetofparameters",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "(theweightmatricesU,V,W).Thegureillustratesasupervisedlearningcaseinwhich\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\nnetworks,withadierentkindofcomputational graph,whichisstructuredasa\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\ngraphforarecursivenetworkisillustratedingure.Recursiveneural 10.14",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "2W e s u g g e s t t o n o t a b b re v i a t e  re c u rs i v e n e u ra l n e t w o rk  a s  R NN t o a v o i d c o n f u s i o n with\n re c u rre n t n e u ra l n e t w o rk . \n4 0 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\ncomputervision( ,). Socher e t a l .2011b\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "ofthesamelength ,thedepth(measuredasthenumberofcompositionsof\nnonlinearoperations)canbedrasticallyreducedfrom to O(log ),whichmight\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\nlanguagesentences,thetreestructurefortherecursivenetworkcanbexedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\nparser( ,,).Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby(). Bottou2011\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\nandassociatetheinputsandtargetswithindividualnodesofthetree.The",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "computationperformedbyeachnodedoesnothavetobethetraditionalarticial\nneuroncomputation(anetransformationofallinputsfollowedbyamonotone\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\nrepresentedbycontinuousvectors(embeddings).\n10.7TheChallengeofLong-TermDependencies",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "10.7TheChallengeofLong-TermDependencies\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\nwithgradientsnotexploding),thedicultywithlong-termdependenciesarises",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "fromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\n4 0 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n   6 0 4 0 2 0 0 2 0 4 0 6 0\nI nput c o o r di na t e 4 3 2 101234P r o j e c t i o n o f o utput0\n1\n2\n3\n4\n5\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "downtoasingledimension,plottedonthe y-axis.The x-axisisthecoordinateofthe\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\nhasbeencomposed.\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\nbehavior,asillustratedingure.10.15\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\nh( ) t= Wh( 1 ) t (10.36)\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\nandlackinginputsx.Asdescribedinsection,thisrecurrencerelation 8.2.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "essentiallydescribesthepowermethod.Itmaybesimpliedto\nh( ) t=\nWth( 0 ), (10.37)\nandifadmitsaneigendecompositionoftheform W\nWQQ = , (10.38)\n4 0 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwithorthogonal ,therecurrencemaybesimpliedfurtherto Q\nh( ) t= QtQh( 0 ). (10.39)\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\nwilleventuallybediscarded.\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "multiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\nnetworkthathasadierentweight w( ) tateachtimestep,thesituationisdierent.\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\nt w( ) t.Suppose\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\ndesiredvariance vwemaychoosetheindividualweightswithvariance v=n",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "v.\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "mustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\n1994).Specically,wheneverthemodelisabletorepresentlongtermdependencies,\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\nthegradientofashortterminteraction.It doesnotmeanthatitisimpossible\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\nuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "in ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydicult,\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\nreaching0forsequencesofonlylength10or20.\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "approachesthathavebeenproposedtoreducethedicultyoflearninglong-\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\n4 0 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\noneofthemainchallengesindeeplearning.\n10.8EchoStateNetworks\nTherecurrentweightsmappingfromh( 1 ) t toh( ) tandtheinputweightsmapping\nfromx( ) ttoh( ) taresomeofthemostdicultparameterstolearninarecurrent\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\nJaeger2007b,)approachtoavoidingthisdicultyistosettherecurrentweights",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "reservoircomputing(LukoeviiusandJaeger2009,)todenotethefactthat\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturedierent\naspectsofthehistoryofinputs.\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\nhistoryofinputsuptotime t)intoaxed-lengthvector(therecurrentstateh( ) t),\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\nsimplelearningalgorithms(,). Jaeger2003\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "networkstate?Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\nJ( ) t= s( ) t\n s( 1 ) t .OfparticularimportanceisthespectralradiusofJ( ) t,denedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "bethemaximumoftheabsolutevaluesofitseigenvalues.\n4 0 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTounderstandtheeectofthespectralradius,considerthesimplecaseof\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\naneigenvectorvwithcorrespondingeigenvalue .Considerwhathappensaswe\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "stepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\naperturbedversionofg.Ifwebeginwithg+ v,thenafteronestep,wewill\nhaveJ(g+ v).After nsteps,wewillhaveJn(g+ v).Fromthiswecansee\nthatback-propagationstartingfromgandback-propagationstartingfromg+ v\ndivergeby Jnvafter nstepsofback-propagation.Ifvischosentobeaunit\neigenvectorofJwitheigenvalue ,thenmultiplicationbytheJacobiansimply\nscalesthedierenceateachstep.Thetwoexecutionsofback-propagationare",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "separatedbyadistanceof  ||n.Whenvcorrespondstothelargestvalueof|| ,\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\nofsize. \nWhen ||  >1,thedeviationsize  ||ngrowsexponentiallylarge.When ||  <1,\nthedeviationsizebecomesexponentiallysmall.\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "manytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\nradius.Indeed,themostrecentworkonechostatenetworksadvocatesusinga\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\nwherethestateh( + 1 ) t= h( ) t W.\nWhenalinearmapWalwaysshrinkshasmeasuredbythe L2norm,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "wesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\nthepastwhenweuseanitelevelofprecision(suchas32bitintegers)tostore\nthestatevector.\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "duringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n4 0 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\ncoecients,whenwemultiplythematrixbythevector.Aneigenvaluewith\nmagnitudegreaterthanonecorrespondstomagnication (exponentialgrowth,if",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "appliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\nWithanonlinearmap,theJacobianisfreetochangeateachstep.The\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\ndierencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\nbounded.Notethatitispossibleforback-propagation toretainunbounded",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\nThestrategyofechostatenetworksissimplytoxtheweightstohavesome\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\ndoesnotexplodeduetothestabilizingeectofsaturatingnonlinearities liketanh.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\nwiththesparseinitialization schemedescribedinsection.8.4\n10.9LeakyUnitsandOtherStrategiesforMultiple\nTimeScales",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "TimeScales\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\natmultipletimescales,sothatsomepartsofthemodeloperateatne-grained\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\nscalesandtransferinformationfromthedistantpasttothepresentmoreeciently.\nVariousstrategiesforbuildingbothneandcoarsetimescalesarepossible.These\nincludetheadditionofskipconnectionsacrosstime,leakyunitsthatintegrate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "signalswithdierenttimeconstants,andtheremovalofsomeoftheconnections\n4 0 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nusedtomodelne-grainedtimescales.\n10.9.1AddingSkipConnectionsthroughTime\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\ndiminishexponentiallyasafunctionof\ndratherthan .Sincethereareboth\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin .",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\nlong-termdependencies mayberepresentedwellinthisway.\n10.9.2LeakyUnitsandaSpectrumofDierentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\nWhenweaccumulatearunningaverage ( ) tofsomevalue v( ) tbyapplyingthe\nupdate ( ) t  ( 1 ) t +(1 ) v( ) tthe parameterisanexampleofalinearself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "connectionfrom ( 1 ) t to ( ) t.When isnearone,therunningaverageremembers\ninformationaboutthepastforalongtime,andwhen isnearzero,information\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\nunits.\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\nalwayslearntobeinuencedbyavaluefrom dtimestepsearlier.Theuseofa\nlinearself-connectionwithaweightnearoneisadierentwayofensuringthatthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\nthiseecttobeadaptedmoresmoothlyandexiblybyadjustingthereal-valued\nratherthanbyadjustingtheinteger-valuedskiplength.\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n(,). Jaeger e t a l .2007\n4 0 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\nunits.Onestrategyistomanuallyxthemtovaluesthatremainconstant,for\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\nHavingsuchleakyunitsatdierenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\n10.9.3RemovingConnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "10.9.3RemovingConnections\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\ninformationowingmoreeasilythroughlongdistancesattheslowertimescales.\nThisideadiersfromtheskipconnectionsthroughtimediscussedearlier\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\nwithlongerconnections.Unitsmodiedinsuchawayareforcedtooperateona",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "longtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\nfocusontheirothershort-termconnections.\nTherearedierentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdierenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavedierentgroupsofunitsassociatedwithdierentxedtimescales.\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "e t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\natdierenttimes,withadierentfrequencyfordierentgroupsofunits.Thisis\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\nwellonanumberofbenchmarkdatasets.\n10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themosteectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe . gatedrecurrentunit",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "networksbasedonthe . gatedrecurrentunit\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\ntimethathavederivativesthatneithervanishnorexplode.Leakyunitsdid\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n4 0 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nateachtimestep.\n\ni nput i nputgate f or ge t  gate outputgateoutput\ns t at es e l f - l oop\n+ \nFigure10.16:BlockdiagramoftheLSTMrecurrentnetworkcell.Cellsareconnected\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\nAninputfeatureiscomputedwitharegulararticialneuronunit.Itsvaluecanbe\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\nbeshutobytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\ninputunitcanhaveanysquashingnonlinearity.Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\nforaparticularfeatureorcategory)overalongduration.However,oncethat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "informationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n4 0 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\niswhatgatedRNNsdo.\n10.10.1LSTM\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\ncanowforlongdurationsisacorecontributionoftheinitiallongshort-term\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\nxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\nInthiscase,wemeanthatevenforanLSTMwithxedparameters,thetimescale\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\ninmanyapplications,suchasunconstrainedhandwriting recognition(Graves\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\nparsing(Vinyals2014a e t a l .,).\nTheLSTMblockdiagramisillustratedingure.Thecorresponding 10.16\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wisenonlinearitytotheanetransformationofinputsandrecurrentunits,LSTM\nrecurrentnetworkshaveLSTMcellsthathaveaninternalrecurrence(aself-loop),\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\nsystemofgatingunitsthatcontrolstheowofinformation. Themostimportant\ncomponentisthestateunit s( ) t\nithathasalinearself-loopsimilartotheleaky\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "associatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\ni(fortimestep t\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\nf( ) t\ni= \n bf\ni+\njUf\ni , j x( ) t\nj+\njWf\ni , j h( 1 ) t \nj\n ,(10.40)\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\n4 1 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\nf( ) t\ni:\ns( ) t\ni= f( ) t\ni s( 1 ) t \ni + g( ) t\ni \n b i+\njU i , j x( ) t\nj+\njW i , j h( 1 ) t \nj\n ,(10.41)\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\niiscomputed\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n0and1),butwithitsownparameters:\ng( ) t\ni= \n bg\ni+",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "g( ) t\ni= \n bg\ni+\njUg\ni , j x( ) t\nj+\njWg\ni , j h( 1 ) t \nj\n .(10.42)\nTheoutput h( ) t\nioftheLSTMcellcanalsobeshuto,viatheoutputgate q( ) t\ni,\nwhichalsousesasigmoidunitforgating:\nh( ) t\ni= tanh\ns( ) t\ni\nq( ) t\ni (10.43)\nq( ) t\ni= \n bo\ni+\njUo\ni , j x( ) t\nj+\njWo\ni , j h( 1 ) t \nj\n (10.44)\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\ni",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "i\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\ningure.Thiswouldrequirethreeadditionalparameters. 10.16\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\nthanthesimplerecurrentarchitectures,rstonarticialdatasetsdesignedfor\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "Graves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\nhavebeenstudiedandusedandarediscussednext.\n10.10.2OtherGatedRNNs\nWhichpiecesoftheLSTMarchitecture areactually necessary?Whatother\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\ncontrolthetimescaleandforgettingbehaviorofdierentunits?\n4 1 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\ndierencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\narethefollowing:\nh( ) t\ni= u( 1 ) t \ni h( 1 ) t \ni+(1 u( 1 ) t \ni) \n b i+",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "i h( 1 ) t \ni+(1 u( 1 ) t \ni) \n b i+\njU i , j x( 1 ) t \nj +\njW i , j r( 1 ) t \nj h( 1 ) t \nj\n ,\n(10.45)\nwhereustandsforupdategateandrforresetgate.Theirvalueisdenedas\nusual:\nu( ) t\ni= \n bu\ni+\njUu\ni , j x( ) t\nj+\njWu\ni , j h( ) t\nj\n (10.46)\nand\nr( ) t\ni= \n br\ni+\njUr\ni , j x( ) t\nj+\njWr\ni , j h( ) t\nj\n .(10.47)\nTheresetandupdatesgatescanindividuallyignorepartsofthestatevector.\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\nignoreit(attheotherextreme)byreplacingitbythenewtargetstatevalue\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\nadditionalnonlineareectintherelationshipbetweenpaststateandfuturestate.\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\nacrossawiderangeoftasks(,; Gre e t a l .2015Jozefowicz2015Gre e t a l .,).\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "advocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\nexploredarchitecturalvariants.\n4 1 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.11OptimizationforLong-TermDependencies\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\nderivativesmayvanishatthesametimethatrstderivativesvanish.Second-order\noptimization algorithmsmayroughlybeunderstoodasdividingtherstderivative",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "bythesecondderivative(inhigherdimension,multiplyingthegradientbythe\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotherst\nderivative,thentheratioofrstandsecondderivativesmayremainrelatively\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "methodssuchasNesterovmomentumwithcarefulinitialization couldachieve\nsimilarresults.SeeSutskever2012()formoredetail.Bothoftheseapproaches\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\npowerfuloptimization algorithm.\n10.11.1ClippingGradients\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "byarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\neitherverylargeorverysmallinmagnitude.Thisisillustratedingureand8.3\ngure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\nparameters)hasalandscapeinwhichonendsclis:wideandratherat\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\nformingakindofcli.\nThedicultythatarisesisthatwhentheparametergradientisverylarge,a\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "regionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\ncorrespondstothesteepestdescentwithinaninnitesimalregionsurroundingthe\ncurrentparameters.Outsideofthisinnitesimalregion,thecostfunctionmay\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n4 1 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\nlandscapeonthenextstep.\n\n \n               \n\n \n            \nFigure10.17:Exampleoftheeectofgradientclippinginarecurrentnetworkwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "twoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\nreasonablyinthevicinityofextremelysteepclis.Thesesteepcliscommonlyoccur\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\nThecliisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "fromthecliface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\nreactiontothecli.Whileitdoesascendthecliface,thestepsizeisrestrictedsothat\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\npermissionfromPascanu2013 e t a l .().\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\nclippingthegradient.Therearedierentinstancesofthisidea(Mikolov2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "Pascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\nupdate:\nif||||g > v (10.48)\ngg v\n||||g(10.49)\n4 1 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\ngradientofalltheparameters(includingdierentgroupsofparameters,suchas\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "normclipping,theparameterupdatevectornormisnowbounded.Thisbounded\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\ngradientisnumerically InforNan(consideredinniteornot-a-number),then\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\nnumericallyunstableconguration. Clippingthegradientnormper-minibatchwill",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "notchangethedirectionofthegradientforanindividualminibatch.However,\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\ntothenaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "conjecturethatallthesemethodsbehavesimilarly.\n10.11.2RegularizingtoEncourageInformationFlow\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "loopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\ntoregularizeorconstraintheparameterssoastoencourageinformationow.\nInparticular,wewouldlikethegradientvectorh( ) t Lbeingback-propagatedto\n4 1 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\nendofthesequence.Formally,wewant\n(h( ) t L)h( ) t\nh( 1 ) t (10.50)\ntobeaslargeas\nh( ) t L. (10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\n =\nt\n|(h( ) t L) h( ) t\n h( 1 ) t |\n||h( ) t L||1\n2\n. (10.52)\nComputingthegradientofthisregularizermayappeardicult,butPascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\nvectorsh( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\nthedependenciesthatanRNNcanlearn.BecauseitkeepstheRNNdynamics\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\nAkeyweaknessofthisapproachisthatitisnotaseectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling.\n10.12ExplicitMemory\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\ntherearedierentkindsofknowledge.Someknowledgecanbeimplicit,sub-\nconscious,anddiculttoverbalizesuchashowtowalk,orhowadoglooks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "dierentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\nstraightforwardtoputintowordseverydaycommonsense knowledge,likeacat\nisakindofanimal,orveryspecicfactsthatyouneedtoknowtoaccomplish\nyourcurrentgoals,likethemeetingwiththesalesteamisat3:00PMinroom\n141.\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\nmemorizefacts.Stochasticgradientdescentrequiresmanypresentationsofthe\n4 1 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nT ask  ne t w or k ,\nc ontrol l i ngth e  m e m o r yMe m or y  c e l l s\nW r i t i ng\nm e c hani s mR e adi ng\nm e c hani s m\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\ndistinguishtherepresentationpartofthemodel(thetasknetwork,herearecurrent\nnetinthebottom)fromthememorypartofthemodel(thesetofcells),whichcan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "storefacts.Thetasknetworklearnstocontrolthememory,decidingwheretoreadfrom\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\n4 1 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\ninformationthatarerelevanttoachievingsome goal.Suchexplicitmemory\ncomponentswouldallowoursystemsnotonlytorapidlyandintentionallystore",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "andretrievespecicfactsbutalsotosequentiallyreasonwiththem.Theneed\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\nresponsestotheinput(,). Hinton1990\nToresolvethisdiculty,Weston2014 e t a l .()introducedmemorynetworks\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\nLSTMsandGRUs.Thedierenceisthatthenetworkoutputsaninternalstate\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\ndigitalcomputerreadfromorwritetoaspecicaddress.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "digitalcomputerreadfromorwritetoaspecicaddress.\nItisdiculttooptimizefunctionsthatproduceexact,integeraddresses.To\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\nmodifymultiplecellsbydierentamounts.Thecoecientsfortheseoperations\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\ndescent.Thegradientonthesecoecientsindicateswhethereachofthemshould\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\nmemoryaddressesreceivingalargecoecient.\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\n4 1 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\ncostofaccessingamemorycell.Wepaythecomputational costofproducinga\ncoecientformanycells,butweexpectthesecoecientstoclusteraroundasmall\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\nosetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "theyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\nreadinstructionassaying,RetrievethelyricsofthesongthathasthechorusWe\nallliveinayellowsubmarine.Content-basedaddressingismoreusefulwhenwe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "maketheobjectstoberetrievedlargeifeveryletterofthesongwasstoredina\nseparatememorycell,wewouldnotbeabletondthemthisway.Bycomparison,\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\nWecanthinkofalocation-basedreadinstructionassayingRetrievethelyricsof\nthesonginslot347.Location-basedaddressingcanoftenbeaperfectlysensible\nmechanismevenwhenthememorycellsaresmall.\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "theinformationitcontainscanbepropagatedforwardintimeandthegradients\npropagatedbackwardintimewithouteithervanishingorexploding.\nTheexplicitmemoryapproachisillustratedingure,whereweseethat 10.18\nataskneuralnetworkiscoupledwithamemory.Althoughthattaskneural\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\nThetasknetworkcanchoosetoreadfromorwritetospecicmemoryaddresses.\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "RNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\nforverylongdurations.\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\ncells,wecaninterpretthememoryaddressingcoecientsasprobabilities and\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "insection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\ndecisions.\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\n4 1 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmechanismforchoosinganaddressisinitsformidenticaltotheattention\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\ntranslation( ,)anddiscussedinsection.Theidea Bahdanau e t a l .2015 12.4.5.1\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "thecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\nattentioncanmovetoacompletelydierentplace,comparedtothepreviousstep.\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\ntasks.\n4 2 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 5\nMac h i n e L e ar n i n g B asics\nDeeplearningisaspecickindofmachinelearning.Inordertounderstand\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\nontraditional machinelearningtechniquesthathavestronglyinuenced the\ndevelopmentofdeeplearningalgorithms.\nWebeginwithadenitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm.W ethenproceedtodescribehowthe\nchallengeofttingthetrainingdatadiersfromthechallengeofndingpatterns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\nonprovingcondenceintervalsaroundthesefunctions;wethereforepresentthe\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory.Mostdeeplearning\nalgorithmsarebasedonanoptimization algorithmcalledstochasticgradient\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\n98",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles.\n5.1LearningAlgorithms\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\nwhatdowemeanbylearning?Mitchell1997()providesthedenitionAcomputer",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\nimproveswithexperienceE.Onecanimagineaverywidevarietyofexperiences\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\nbooktoprovideaformaldenitionofwhatmaybeusedforeachoftheseentities.\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndierentkindsoftasks,performance measuresandexperiencesthatcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "toconstructmachinelearningalgorithms.\n5.1.1TheTask, T\nMachinelearningallowsustotackletasksthataretoodiculttosolvewith\nxedprogramswrittenanddesignedbyhumanbeings.Fromascienticand\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\nprinciplesthatunderlieintelligence.\nInthisrelativelyformaldenitionofthewordtask,theprocessoflearning\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspecieshowtowalkmanually.\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vectorx Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\n9 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\ncommonmachinelearningtasksincludethefollowing:\nClassication:Inthistypeoftask,thecomputerprogramisaskedtospecify\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\nalgorithmisusuallyaskedtoproduceafunctionf: Rn{1,...,k}.When\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\nidentiedbynumericcodey.Thereareothervariantsoftheclassication",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "task,forexample,wherefoutputsaprobabilitydistributionoverclasses.\nAnexampleofaclassicationtaskisobjectrecognition,wheretheinput\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\ndierentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "deeplearning( ,; ,).Object Krizhevskyetal.2012IoeandSzegedy2015\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\ntheirusers.\nClassicationwithmissinginputs:Classicationbecomesmorechal-\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassication",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "task,thelearningalgorithmonlyhastodeneafunctionmapping single\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\nbemissing,ratherthanprovidingasingleclassicationfunction,thelearning\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\nfyingxwithadierentsubsetofitsinputsmissing.Thiskindofsituation\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\nareexpensiveorinvasive.Onewaytoecientlydenesuchalargeset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "offunctionsistolearnaprobabilitydistributionoveralloftherelevant\nvariables,thensolvetheclassicationtaskbymarginalizing outthemissing\nvariables.Withninputvariables,wecannowobtainall2ndierentclassi-\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "sectioncanalsobegeneralizedtoworkwithmissinginputs;classication\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\n1 0 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nRegression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\nisaskedtooutputafunctionf: Rn R.Thistypeoftaskissimilarto\nclassication,exceptthattheformatofoutputisdierent.Anexampleof\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\nalgorithmictrading.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "algorithmictrading.\nTranscription:Inthistypeoftask,themachinelearningsystemisasked\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\nrecognition,thecomputerprogramisshownaphotographcontainingan\nimageoftextandisaskedtoreturnthistextintheformofasequence\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\n2012b).\nMachinetranslation:Inamachinetranslationtask,theinputalready\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "mustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\nStructuredoutput:Structuredoutputtasksinvolveanytaskwherethe\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\nimportantrelationshipsbetweenthedierentelements.Thisisabroad",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "category,andsubsumesthetranscriptionandtranslationtasksdescribed\nabove,butalsomanyothertasks.Oneexampleisparsingmappinga\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\ntask.Anotherexampleispixel-wisesegmentationofimages,wherethe\ncomputerprogramassignseverypixelinanimagetoaspeciccategory.For\n1 0 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "calledstructuredoutputtasksbecausetheprogrammustoutputseveral\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\nanimagecaptioningprogrammustformavalidsentence.\nAnomalydetection:Inthistypeoftask,thecomputerprogramsifts\nthroughasetofeventsorobjects,andagssomeofthemasbeingunusual\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "information,thethiefspurchaseswilloftencomefromadierentprobability\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\nsurveyofanomalydetectionmethods.\nSynthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata.Synthesisandsamplingviamachinelearningcanbeuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "formediaapplicationswhereitcanbeexpensiveorboringforanartistto\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespecic\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "containingaspokenversionofthatsentence.Thisisakindofstructured\noutputtask,butwiththeaddedqualicationthatthereisnosinglecorrect\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\nImputationofmissingvalues:Inthistypeoftask,themachinelearning\nalgorithmisgivenanewexamplex Rn,butwithsomeentriesx iofx\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\nentries.\n1 0 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nDenoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\ninputacorruptedexamplex Rnobtainedbyanunknowncorruptionprocess\nfromacleanexamplex Rn.Thelearnermustpredictthecleanexample\nxfromitscorruptedversionx,ormoregenerallypredicttheconditional\nprobabilitydistributionp(x|x).\nDensityestimationorprobabilitymassfunctionestimation:In\nthedensityestimationproblem,themachinelearningalgorithmisasked\ntolearnafunctionpmodel: Rn R,wherepmodel(x)canbeinterpreted",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "asaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistribution.Densityestimationallowsustoexplicitlycapture\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\navaluex iismissingandalloftheothervalues,denotedx  i,aregiven,\nthenweknowthedistributionoveritisgivenbyp(x i|x  i).Inpractice,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\nintractable.\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\ndo,nottodenearigidtaxonomyoftasks.\n5.1.2ThePerformanceMeasure, P\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "specictothetaskbeingcarriedoutbythesystem. T\nFortaskssuchasclassication,classicationwithmissinginputs,andtran-\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n1 0 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\nifitiscorrectlyclassiedand1ifitisnot.Fortaskssuchasdensityestimation,\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\nloss.Instead,wemustuseadierentperformancemetricthatgivesthemodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "acontinuous-valuedscoreforeachexample.Themostcommonapproachisto\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\nlearningsystem.\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "butitisoftendiculttochooseaperformancemeasurethatcorrespondswellto\nthedesiredbehaviorofthesystem.\nInsomecases,thisisbecauseitisdiculttodecidewhatshouldbemeasured.\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\nofthesystemattranscribingentiresequences,orshouldweuseamorene-grained\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\naspecicpointinspaceinmanysuchmodelsisintractable.Inthesecases,one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\nordesignagoodapproximationtothedesiredcriterion.\n5.1.3TheExperience, E\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\nlearningprocess.\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\n1 0 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndenedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\ndierentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "alsorecordswhichspecieseachplantbelongedto.Threedierentspeciesare\nrepresentedinthedataset.\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "performotherroles,likeclustering,whichconsistsofdividingthedatasetinto\nclustersofsimilarexamples.\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\ndierentspeciesbasedontheirmeasurements.\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\nUnsupervisedlearningandsupervisedlearningarenotformallydenedterms.\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\nthatforavector x Rn,thejointdistributioncanbedecomposedas\np() = xn\ni=1p(x i|x1,...,x i 1). (5.1)\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "modelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\n1 0 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\nunsupervisedlearningtechnologiestolearnthejointdistributionp( x,y)and\ninferring\npy(| x) =p,y( x)\nyp,y( x). (5.2)\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classication",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "andstructuredoutputproblemsassupervisedlearning.Densityestimationin\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "withdeepmodels,seeKotzias 2015etal.().\nSomemachinelearningalgorithmsdonotjustexperienceaxeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.Such\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures.\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\nmatrixisamatrixcontainingadierentexampleineachrow.Eachcolumnofthe\nmatrixcorrespondstoadierentfeature.Forinstance,theIrisdatasetcontains\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thedatasetwithadesignmatrixX R1504 ,whereX i ,1isthesepallengthof\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\nwithdierentwidthsandheights,thendierentphotographswillcontaindierent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "numbersofpixels,sonotallofthephotographs maybedescribedwiththesame\nlengthofvector.Sectionandchapterdescribehowtohandledierent 9.7 10\n1 0 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\nx() iandx() jhavethesamesize.\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "appearsineachofthephotos.Wemightdothiswithanumericcode,with0\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Justasthereisnoformaldenitionofsupervisedandunsupervisedlearning,\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\n5.1.4Example:LinearRegression\nOurdenitionofamachinelearningalgorithmasanalgorithmthatiscapable\nofimprovingacomputerprogramsperformanceatsometaskviaexperienceis\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "simplemachinelearningalgorithm:linearregression.Wewillreturntothis\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\nunderstanditsbehavior.\nAsthenameimplies,linearregressionsolvesaregressionproblem.Inother\nwords,thegoalistobuildasystemthatcantakeavectorx Rnasinputand\npredictthevalueofascalary Rasitsoutput.Inthecaseoflinearregression,\ntheoutputisalinearfunctionoftheinput.Letybethevaluethatourmodel\npredictsshouldtakeon.Wedenetheoutputtobe y\ny= wx (5.3)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "y= wx (5.3)\nwherew Rnisavectorof .parameters\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\nthecoecientthatwemultiplybyfeaturex ibeforesummingupthecontributions\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\neachfeatureaectstheprediction.If afeaturex ireceivesapositiveweightw i,\n1 0 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction y.\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\ndecreasesthevalueofourprediction.Ifafeaturesweightislargeinmagnitude,\nthenithasalargeeectontheprediction.Ifafeaturesweightiszero,ithasno\neectontheprediction.\nWethushaveadenitionofourtaskT:topredictyfromxbyoutputting\ny= wx.Nextweneedadenitionofourperformancemeasure,.P",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "Supposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\ntargetsasy()test.\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\nsquarederrorofthemodelonthetestset.Ify()testgivesthepredictionsofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "modelonthetestset,thenthemeansquarederrorisgivenby\nMSEtest=1\nm\ni(y()testy()test)2\ni. (5.4)\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when y()test=y()test.\nWecanalsoseethat\nMSEtest=1\nm||y()testy()test||2\n2, (5.5)\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\nandthetargetsincreases.\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "isallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\nminimizethemeansquarederroronthetrainingset,MSEtrain.\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\n wMSEtrain= 0 (5.6)\n w1\nm||y()trainy()train||2\n2= 0 (5.7)\n1\nm w||X()trainwy()train||2\n2= 0 (5.8)\n1 0 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n  1 0 . 0 5 0 0 0 5 1 0 . . . .\nx1 3 2 10123yL i n ea r r eg r es s i o n ex a m p l e\n0 5 1 0 1 5 . . .\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "tosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\n w\nX()trainwy()train\nX()trainwy()train\n= 0(5.9)\n w\nwX()train X()trainww2X()train y()train+y()train y()train\n= 0\n(5.10)\n2X()train X()trainwX2()train y()train= 0(5.11)\nw=\nX()train X()train1\nX()train y()train(5.12)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "X()train X()train1\nX()train y()train(5.12)\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\nseegure.5.1\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\naslightlymoresophisticatedmodelwithoneadditionalparameteran intercept\nterm.Inthismodelb\ny= wx+b (5.13)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "term.Inthismodelb\ny= wx+b (5.13)\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\nmappingfromfeaturestopredictionsisnowananefunction.Thisextensionto\nanefunctionsmeansthattheplotofthemodelspredictionsstilllookslikea\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n1 0 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\nplaystheroleofthebiasparameter.Wewillfrequentlyusethetermlinearwhen\nreferringtoanefunctionsthroughoutthisbook.\nTheintercepttermbisoftencalledthebiasparameteroftheanetransfor-\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "isdierentfromtheideaofastatisticalbias,inwhichastatisticalestimation\nalgorithmsexpectedestimateofaquantityisnotequaltothetruequantity.\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\nlearningalgorithms.\n5.2Capacity,OverttingandUndertting",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,OverttingandUndertting\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\npreviouslyunseeninputsnotjustthoseonwhichourmodelwastrained.The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "anoptimization problem.Whatseparatesmachinelearningfromoptimization is\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\nwell.Thegeneralization errorisdenedastheexpectedvalueoftheerrorona\nnewinput.Heretheexpectationistakenacrossdierentpossibleinputs,drawn\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "fromthetrainingset.\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwy()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwy()test||2\n2.\nHowcanweaectperformanceonthetestsetwhenwegettoobserveonlythe\n1 1 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntrainingset?Theeldofstatisticallearningtheoryprovidessomeanswers.If\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\ndo.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\nsetarecollected,thenwecanmakesomeprogress.\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\ncalledthedatageneratingprocess.Wetypicallymakeasetofassumptions\nknowncollectivelyasthei.i.d.assumptions.Theseassumptionsarethatthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "examplesineachdatasetareindependentfromeachother,andthatthetrain\nsetandtestsetareidenticallydistributed,drawnfromthesameprobability\ndistributionaseachother.Thisassumptionallowsustodescribethedatagen-\neratingprocesswithaprobabilitydistributionoverasingleexample.Thesame\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\nWecallthatsharedunderlyingdistributionthedatageneratingdistribution,\ndenotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "mathematically studytherelationshipbetweentrainingerrorandtesterror.\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\nexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\np(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset.Forsomexedvaluew,theexpectedtrainingseterrorisexactlythesameas\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "datasetsamplingprocess.Theonlydierencebetweenthetwoconditionsisthe\nnameweassigntothedatasetwesample.\nOfcourse,whenweuseamachinelearningalgorithm,w edonotxthe\nparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\ntestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\ntheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\nlearningalgorithmwillperformareitsabilityto:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmwillperformareitsabilityto:\n1.Makethetrainingerrorsmall.\n2.Makethegapbetweentrainingandtesterrorsmall.\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunderttingandovertting.Underttingoccurswhenthemodelisnotableto\nobtainasucientlylowerrorvalueonthetrainingset.Overttingoccurswhen\nthegapbetweenthetrainingerrorandtesterroristoolarge.\nWecancontrolwhetheramodelismorelikelytoovertorundertbyaltering",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "itscapacity.Informally,amodelscapacityisitsabilitytotawidevarietyof\n1 1 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nfunctions.Modelswithlowcapacitymaystruggletotthetrainingset.Models\nwithhighcapacitycanovertbymemorizingpropertiesofthetrainingsetthatdo\nnotservethemwellonthetestset.\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\nhypothesisspace.Doingsoincreasesthemodelscapacity.\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\narealreadyfamiliar,withprediction\nybwx. = + (5.15)\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\ncanlearnamodelthatisquadraticasafunctionof:x\nybw = +1xw+2x2. (5.16)\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "stillalinearfunctionoftheparameters,sowecanstillusethenormalequations\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\nyb= +9\ni=1w ixi. (5.17)\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\namountoftrainingdatatheyareprovidedwith.Modelswithinsucientcapacity\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "tasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\nmayovert.\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\nanddegree-9predictorattemptingtotaproblemwherethetrueunderlying\nfunctionisquadratic.Thelinearfunctionisunabletocapturethecurvaturein\nthetrueunderlyingproblem,soitunderts.Thedegree-9predictoriscapableof\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginnitely\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n1 1 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\nasolutionthatgeneralizeswellwhensomanywildlydierentsolutionsexist.In\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\nthetasksoitgeneralizeswelltonewdata.\n          \n                  \n          \nFigure5.2:Wetthreemodelstothisexampletrainingset.Thetrainingdatawas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "generatedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\nbyevaluatingaquadraticfunction. ( L e f t )Alinearfunctionttothedatasuersfrom\nunderttingitcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\nquadraticfunctionttothedatageneralizeswelltounseenpoints.Itdoesnotsuerfrom\nasignicantamountofoverttingorundertting.Apolynomialofdegree9tto ( R i g h t )\nthedatasuersfromovertting.HereweusedtheMoore-Penrosepseudoinversetosolve",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea.\nSofarwehavedescribedonlyonewayofchangingamodelscapacity:by\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "parametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\namodelscapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\nmodelspecieswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\ntherepresentationalcapacityofthemodel.Inmanycases,ndingthebest\nfunctionwithinthisfamilyisaverydicultoptimization problem.Inpractice,\nthelearningalgorithmdoesnotactuallyndthebestfunction,butmerelyone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "thatsignicantlyreducesthetrainingerror.Theseadditionallimitations,suchas\n1 1 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithms\neectivecapacitymaybelessthantherepresentationalcapacityofthemodel\nfamily.\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\nmodelsarerenementsofthoughtdatingbacktophilosophersatleastasearly\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\nmostwidelyknownasOccamsrazor(c.1287-1347).Thisprinciplestatesthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "amongcompetinghypothesesthatexplainknownobservationsequallywell,one\nshouldchoosethesimplestone.Thisideawasformalizedandmademoreprecise\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassier.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "VCdimensionisdenedasbeingthelargestpossiblevalueofmforwhichthere\nexistsatrainingsetofmdierentxpointsthattheclassiercanlabelarbitrarily.\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\nintellectualjusticationthatmachinelearningalgorithmscanwork,buttheyare\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\ndiculttodeterminethecapacityofdeeplearningalgorithms.Theproblemof\ndeterminingthecapacityofadeeplearningmodelisespeciallydicultbecausethe\neectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "wehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\nproblemsinvolvedindeeplearning.\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\nsucientlycomplexhypothesistoachievelowtrainingerror.Typically,training\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "generalization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\nillustratedingure.5.3\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\n1 1 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0 O pti m a l C a pa c i t y\nC a pa c i t yE r r o rU nde r tti ng z o ne O v e r tti ng z o ne\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\nG e n e r a l i z a t i o n e r r o r\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\nbehavedierently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\narebothhigh.Thisistheunderttingregime.Asweincreasecapacity,trainingerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheovertting\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\nbyaparametervectorwhosesizeisniteandxedbeforeanydataisobserved.\nNon-parametric modelshavenosuchlimitation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "Non-parametric modelshavenosuchlimitation.\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\nwhichhasaxed-lengthvectorofweights,thenearestneighborregressionmodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "simplystorestheXandyfromthetrainingset.Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,y=y iwherei=argmin||X i ,:||x2\n2.\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdierent\noutputs)onanyregressiondataset.\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\n1 1 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\npolynomialexpansionoftheinput.\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata.Evensuchamodelwillstillincursomeerroronmany\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "ofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\ndistributioniscalledthe p,y(x)Bayeserror.\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thebestpossibleerrorisachieved.Anyxedparametricmodelwithlessthan\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\ngureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\nexamples.\n5.2.1TheNoFreeLunchTheorem\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "anitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\nisnotlogicallyvalid.Tologicallyinferaruledescribingeverymemberofaset,\nonemusthaveinformationabouteverymemberofthatset.\nInpart,machinelearningavoidsthisproblembyoeringonlyprobabilisticrules,\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning.Machine\nlearningpromisestondrulesthatareprobably most correctaboutmembersof\nthesettheyconcern.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "thesettheyconcern.\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\nallpossibledatageneratingdistributions,everyclassicationalgorithmhasthe\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n1 1 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n      \n                                                    \n                \n              \n                     \n                       \n      \n                                                     ",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Figure5.4:Theeectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\nandthengeneratedseveraldierentsizesoftrainingset.Foreachsize,wegenerated40\ndierenttrainingsetsinordertoploterrorbarsshowing95percentcondenceintervals.\n( T o p )TheMSEonthetrainingandtestsetfortwodierentmodels:aquadraticmodel,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andamodelwithdegreechosentominimizethetesterror.Botharetinclosedform.For\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\nThisisbecauselargerdatasetsarehardertot.Simultaneously,thetesterrordecreases,\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\ntomemorizespecicinstancesofthetrainingset.Asthetrainingsizeincreasestoinnity,\nthetrainingerrorofanyxed-capacitymodel(here,thequadraticmodel)mustrisetoat\nleasttheBayeserror.Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases.Theoptimal\ncapacityplateausafterreachingsucientcomplexitytosolvethetask.\n1 1 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\ntothesameclass.\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\nalgorithmsthatperformwellonthesedistributions.\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\nunderstandwhatkindsofdistributionsarerelevanttotherealworldthatanAI\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\n5.2.2Regularization\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\nalgorithmstoperformwellonaspecictask.Wedosobybuildingasetof\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "thelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\nconcretelyistoincreaseordecreasethemodelsrepresentationalcapacitybyadding\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\nisabletochoose.Wegavethespecicexampleofincreasingordecreasingthe\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\nfarisoversimplied.\nThebehaviorofouralgorithmisstronglyaectednotjustbyhowlargewe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "makethesetoffunctionsallowedinitshypothesisspace,butbythespecicidentity\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "controltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\nfunctions.\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\nispreferred.Theunpreferredsolutionwillbechosenonlyifittsthetraining\n1 1 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndatasignicantlybetterthanthepreferredsolution.\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Specically,\nJ() = wMSEtrain+ww, (5.18)\nwhereisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "forsmallerweights.When= 0,weimposenopreference,andlargerforcesthe\nweightstobecomesmaller.MinimizingJ(w)resultsinachoiceofweightsthat\nmakeatradeobetweenttingthetrainingdataandbeingsmall.Thisgivesus\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\nexampleofhowwecancontrolamodelstendencytoovertorundertviaweight\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdierentvalues\nof.Seegurefortheresults.  5.5\n           \n         ",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "           \n         \n                       \n         \n          \n    \nFigure5.5:Wetahigh-degreepolynomialregressionmodeltoourexampletrainingset\nfromgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromovertting.\n( L e f t )Withverylarge,wecanforcethemodeltolearnafunctionwithnoslopeat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "all.Thisundertsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. \nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\ncoecients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "degree-9polynomialovertssignicantly,aswesawingure.5.2\n1 1 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;)by\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\ndecay,theregularizeris(w) =ww.Inchapter,wewillseethatmanyother 7\nregularizersarepossible.\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\nofcontrollingamodelscapacitythanincludingorexcludingmembersfromthe\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "expressinganinnitelystrongpreferenceagainstthatfunction.\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\ndenedwithsmallerweightsexplicitly,viaanextraterminthecriterionwe\nminimize.Therearemanyotherwaysofexpressing preferencesfordierent\nsolutions,bothimplicitlyandexplicitly.Together,thesedierentapproaches\nareknownasregularization.Regularizationisanymodicationwemaketoa\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "trainingerror.Regularizationisoneofthecentralconcernsoftheeldofmachine\nlearning,rivaledinitsimportanceonlybyoptimization.\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "thatpeoplecando)mayallbesolvedeectivelyusingverygeneral-purposeforms\nofregularization.\n5.3HyperparametersandValidationSets\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\nitself(thoughwecandesignanestedlearningprocedurewhere onelearning\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Inthepolynomialregressionexamplewesawingure,thereisasingle 5.2\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\nparameter.Thevalueusedtocontrolthestrengthofweightdecayisanother\nexampleofahyperparameter.\nSometimesasettingischosentobeahyperparameter thatthelearningal-\ngorithmdoesnotlearnbecauseitisdiculttooptimize.Morefrequently,the\n1 2 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\nalwayschoosethemaximumpossiblemodelcapacity,resultinginovertting(refer\ntogure).Forexample,wecanalwaystthetrainingsetbetterwithahigher 5.3\ndegreepolynomialandaweightdecaysettingof= 0thanwecouldwithalower\ndegreepolynomialandapositiveweightdecaysetting.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "degreepolynomialandapositiveweightdecaysetting.\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve.\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\nitshyperparameters .Forthisreason,noexamplefromthetestsetcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\ntrainingdata.Specically,wesplitthetrainingdataintotwodisjointsubsets.One\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\n20%forvalidation.Sincethevalidationsetisusedtotrainthehyperparameters ,\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\nInpractice,when thesametestsethasbeenusedrepeatedlytoevaluate",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "performanceofdierentalgorithmsovermanyyears,andespeciallyifweconsider\nalltheattemptsfromthescienticcommunityatbeatingthereportedstate-of-\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreectthe\ntrueeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\n1 2 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.3.1Cross-Validation\nDividingthedatasetintoaxedtrainingsetandaxedtestsetcanbeproblematic\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\naroundtheestimatedaveragetesterror,makingitdiculttoclaimthatalgorithm\nAworksbetterthanalgorithmonthegiventask. B\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "tousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\nthetrainingandtestingcomputationondierentrandomlychosensubsetsorsplits\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "dataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\nused.\n5.4Estimators,BiasandVariance\nTheeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "toformallycharacterizenotionsofgeneralization, underttingandovertting.\n5.4.1PointEstimation\nPointestimationistheattempttoprovidethesinglebestpredictionofsome\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterby .",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed\n1 2 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "inthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\nofunsupervisedlearning.The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error.Theerrorson\nindividualexamplescanbeusedtocomputeacondenceintervalaroundthemean\n(equation).Whilethesecondenceintervalsarenotwell-justiedafterthe 5.47\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\nalgorithmAisbetterthanalgorithmBonlyifthecondenceintervaloftheerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "ofalgorithmAliesbelowanddoesnotintersectthecondenceintervalofalgorithm\nB.\nDeneKFoldXV(): D,A,L,k\nRequire: D,thegivendataset,withelementsz() i\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\ninputandoutputsalearnedfunction\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\nanexamplez() i  Dtoascalar R\nRequire:k,thenumberoffolds\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\nfordoikfromto1\nf i= (A D D\\ i)\nforz() jin D ido\ne j= (Lf i,z() j)\nendfor",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "forz() jin D ido\ne j= (Lf i,z() j)\nendfor\nendfor\nReturne\n1 2 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\n m= (gx(1),...,x() m). (5.19)\nThedenitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\noreventhattherangeofgisthesameasthesetofallowablevaluesof.\nThisdenitionofapointestimatorisverygeneralandallowsthedesignerofan\nestimatorgreatexibility.Whilealmostanyfunctionthusqualiesasanestimator,\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingthat\ngeneratedthetrainingdata.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "generatedthetrainingdata.\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\nthatthetrueparametervalueisxedbutunknown,whilethepointestimate\nisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\nfunctionofthedataisrandom.Therefore isarandomvariable.\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators.\nFunctionEstimationAswementionedabove,sometimesweareinterestedin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "performingfunctionestimation(orfunctionapproximation).Herewearetryingto\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\nwemayassumethaty=f(x)+,wherestandsforthepartofythatisnot\npredictablefromx.Infunctionestimation,weareinterestedinapproximating\nfwithamodelorestimate f.Functionestimationisreallyjustthesameas\nestimatingaparameter;thefunctionestimator fissimplyapointestimatorin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "functionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\nafunction f y mappingfromtox.\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\ndiscusswhattheytellusabouttheseestimators.\n5.4.2Bias\nThebiasofanestimatorisdenedas:\nbias( m) = ( E m) (5.20)\n1 2 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\nandisthetrueunderlyingvalueofusedtodenethedatageneratingdistri-\nbution.Anestimator  missaidtobeunbiasedifbias( m) = 0,whichimplies\nthat E( m)=.Anestimator  missaidtobeasymptoticallyunbiasedif\nlim m  bias( m) = 0,whichimpliesthatlim m   E( m) = .\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\nbutionwithmean:\nPx(() i;) = x() i(1 )(1  x() i). (5.21)\nAcommonestimatorfortheparameterofthisdistributionisthemeanofthe\ntrainingsamples:\n m=1\nmm\ni=1x() i. (5.22)\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\nintoequation:5.20\nbias( m) = [ E m] (5.23)\n= E\n1\nmm\ni=1x() i\n (5.24)\n=1\nmm\ni=1E\nx() i\n (5.25)\n=1\nmm\ni=11\nx() i=0\nx() ix() i(1 )(1  x() i)\n(5.26)\n=1\nmm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "x() ix() i(1 )(1  x() i)\n(5.26)\n=1\nmm\ni=1() (5.27)\n= = 0 (5.28)\nSince bias() = 0,wesaythatourestimator isunbiased.\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\naccordingtoaGaussiandistributionp(x() i) =N(x() i;,2),wherei{1,...,m}.\n1 2 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\npx(() i;,2) =1\n22exp\n1\n2(x() i)2\n2\n.(5.29)\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\nmean:\n m=1\nmm\ni=1x() i(5.30)\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\nitsexpectation:\nbias( m) = [ E m] (5.31)\n= E\n1\nmm\ni=1x() i\n (5.32)\n=\n1\nmm\ni=1E\nx() i\n (5.33)\n=\n1\nmm\ni=1\n (5.34)\n= = 0 (5.35)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "=\n1\nmm\ni=1\n (5.34)\n= = 0 (5.35)\nThuswendthatthesamplemeanisanunbiasedestimatorofGaussianmean\nparameter.\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodierentestimatorsofthevarianceparameter2ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\nTherstestimatorof2weconsiderisknownasthesamplevariance:\n2\nm=1\nmm\ni=1\nx() i m2\n, (5.36)\nwhere  misthesamplemean,denedabove.Moreformally,weareinterestedin\ncomputing\nbias(2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "computing\nbias(2\nm) = [ E2\nm]2(5.37)\n1 2 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nWebeginbyevaluatingtheterm E[2\nm]:\nE[2\nm] = E\n1\nmm\ni=1\nx() i m2\n(5.38)\n=m1\nm2(5.39)\nReturningtoequation,weconcludethatthebiasof 5.37 2\nmis2/m.Therefore,\nthesamplevarianceisabiasedestimator.\nTheunbiasedsamplevarianceestimator\n2\nm=1\nm1m\ni=1\nx() i m2\n(5.40)\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\nThatis,wendthat E[2\nm] = 2:\nE[2\nm] = E\n1\nm1m\ni=1\nx() i m2\n(5.41)\n=m\nm1E[2\nm] (5.42)\n=m\nm1m1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "(5.41)\n=m\nm1E[2\nm] (5.42)\n=m\nm1m1\nm2\n(5.43)\n= 2. (5.44)\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\nestimatorsareclearlydesirable,theyarenotalwaysthebestestimators.Aswe\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\n5.4.3VarianceandStandardError\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "Thevarianceofanestimatorissimplythevariance\nVar() (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE().\n1 2 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\nlowvariance.\nWhenwecomputeanystatisticusinganitenumberofsamples,ourestimate\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\nbeendierent.Theexpecteddegreeofvariationinanyestimatorisasourceof\nerrorthatwewanttoquantify.\nThestandarderrorofthemeanisgivenby\nSE( m) =Var\n1\nmm\ni=1x() i\n=m, (5.46)\nwhere2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\nestimatedbyusinganestimateof.Unfortunately,neitherthesquarerootof\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "provideanunbiasedestimateofthestandarddeviation.Bothapproachestend\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\nForlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\nfallsinanychoseninterval.Forexample,the95%condenceintervalcenteredon\nthemean  mis\n( m196SE( . m), m+196SE( . m)), (5.47)\nunderthenormaldistributionwithmean  mandvariance SE( m)2.Inmachine\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Biftheupperboundofthe95%condenceintervalfortheerrorofalgorithmAis\nlessthanthelowerboundofthe95%condenceintervalfortheerrorofalgorithm\nB.\n1 2 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nExample:BernoulliDistributionWeonceagainconsiderasetofsamples\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\n(recallP(x() i;) =x() i(1)(1  x() i)).Thistimeweareinterestedincomputing\nthevarianceoftheestimator  m=1\nmm\ni=1x() i.\nVar\n m\n= Var\n1\nmm\ni=1x() i\n(5.48)\n=1\nm2m\ni=1Var\nx() i\n(5.49)\n=1\nm2m\ni=1 (1) (5.50)\n=1\nm2m (1) (5.51)\n=1\nm (1) (5.52)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "=1\nm2m (1) (5.51)\n=1\nm (1) (5.52)\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\nreturntowhenwediscussconsistency(seesection).5.4.5\n5.4.4TradingoBiasandVariancetoMinimizeMeanSquared\nError\nBiasandvariancemeasuretwodierentsourcesoferrorinanestimator.Bias\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "estimatorvaluethatanyparticularsamplingofthedataislikelytocause.\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\ngureandweareonlyoeredthechoicebetweenamodelwithlargebiasand 5.2\nonethatsuersfromlargevariance.Howdowechoosebetweenthem?\nThemostcommonwaytonegotiatethistrade-oistousecross-validation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\nMSE = [( E m)2] (5.53)\n= Bias( m)2+Var( m) (5.54)\n1 2 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheMSEmeasurestheoverallexpecteddeviationin asquarederrorsense\nbetweentheestimatorandthetruevalueoftheparameter.Asisclearfrom\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\nC apac i t yB i as Ge ne r al i z at i on\ne r r orV ar i anc e\nO pt i m al\nc apac i t yO v e r t t i ngz o n e U nde r t t i ngz o n e",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withundertting\nwhenthecapacityisbelowthisoptimumandoverttingwhenitisabove.Thisrelationship\nissimilartotherelationshipbetweencapacity,undertting,andovertting,discussedin\nsectionandgure. 5.2 5.3\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "learningconceptsofcapacity,underttingandovertting.Inthecasewheregen-\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\nanddecreasebias.Thisisillustratedingure,whereweseeagaintheU-shaped 5.6\ncurveofgeneralization errorasafunctionofcapacity.\n5.4.5Consistency\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\nxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\n1 3 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\nplimm   m= . (5.55)\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany>0,\nP(| m|>)0asm.Theconditiondescribedbyequationis5.55\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\nstrongconsistencyreferringtothealmostsureconvergenceofto.Almost\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\noccurswhenp(lim m   x() m= ) = 1x.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "occurswhenp(lim m   x() m= ) = 1x.\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\nnumberofdataexamplesgrows.However,thereverseisnottrueasymptotic\nunbiasednessdoesnotimplyconsistency.Forexample,considerestimatingthe\nmeanparameterofanormaldistributionN(x;,2),withadatasetconsisting\nofmsamples:{x(1),...,x() m}.Wecouldusetherstsamplex(1)ofthedataset\nasanunbiasedestimator:=x(1).Inthatcase, E( m)=sotheestimator",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\nestimatorasitisthecasethat not  m mas.\n5.5MaximumLikelihoodEstimation\nPreviously,wehaveseensomedenitionsofcommonestimatorsandanalyzed\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespecic",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "functionsthataregoodestimatorsfordierentmodels.\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\nthetruebutunknowndatageneratingdistributionpdata() x.\nLetpmodel( x;)beaparametricfamilyofprobabilitydistributionsoverthe\nsamespaceindexedby.Inotherwords,pmodel(x;)mapsanycongurationx\ntoarealnumberestimatingthetrueprobabilitypdata()x.\nThemaximumlikelihoodestimatorforisthendenedas \nML= argmax\npmodel(;) X (5.56)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "ML= argmax\npmodel(;) X (5.56)\n= argmax\nm\ni=1pmodel(x() i;) (5.57)\n1 3 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\nForexample,itispronetonumericalunderow.Toobtainamoreconvenient\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\nintoasum:\nML= argmax\nm\ni=1logpmodel(x() i;). (5.58)\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "withrespecttotheempiricaldistributionpdatadenedbythetrainingdata:\nML= argmax\nE x  pdatalogpmodel(;)x. (5.59)\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\nthedissimilaritybetweentheempiricaldistributionpdatadenedbythetraining\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\nDKL(pdatapmodel) = E x  pdata[log pdata()logxpmodel()]x.(5.60)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\nneedonlyminimize\n E x  pdata[logpmodel()]x (5.61)\nwhichisofcoursethesameasthemaximization inequation.5.59\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\nentropybetweenthedistributions.Manyauthorsusethetermcross-entropyto\nidentifyspecicallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\nentropybetweentheempiricaldistributiondenedbythetrainingsetandthe\nprobabilitydistributiondenedbymodel.Forexample,meansquarederroristhe\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\ntributionmatchtheempiricaldistributionpdata.Ideally,wewouldliketomatch\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\ndistribution.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "distribution.\nWhiletheoptimalisthesameregardlessofwhetherwearemaximizingthe\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n1 3 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\naredierent.Insoftware,weoftenphrasebothasminimizingacostfunction.\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "5.5.1ConditionalLog-LikelihoodandMeanSquaredError\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\nourgoalistoestimateaconditionalprobabilityP( y x|;)inordertopredict y\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\ntargets,thentheconditionalmaximumlikelihoodestimatoris\nML= argmax\nP. ( ;)YX| (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\nML= argmax\nm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "ML= argmax\nm\ni=1log(Py() i|x() i;). (5.63)\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\nintroducedearlierinsection,maybejustiedasamaximumlikelihood 5.1.4\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\ntotakeaninputxandproduceanoutputvalue y.Themappingfromxtoyis\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimation.Insteadofproducingasingleprediction y,wenowthink\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\nthatwithaninnitelylargetrainingset,wemightseeseveraltrainingexamples\nwiththesameinputvaluexbutdierentvaluesofy.Thegoalofthelearning\nalgorithmisnowtotthedistributionp(y|x)toallofthosedierentyvalues\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\nweobtainedbefore,wedenep(y|x) =N(y;y(x;w),2).Thefunction y(x;w)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\nthevarianceisxedtosomeconstant2chosenbytheuser.Wewillseethatthis\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\n1 3 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\ngivenby\nm\ni=1log(py() i|x() i;) (5.64)\n= log mm\n2log(2)m\ni=1y() iy() i2\n22,(5.65)\nwhere y() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\nsquarederror,\nMSEtrain=1\nmm\ni=1||y() iy() i||2, (5.66)\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "thesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\nThetwocriteriahavedierentvaluesbutthesamelocationoftheoptimum.This\njustiestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\n5.5.2PropertiesofMaximumLikelihood\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\nbethebestestimatorasymptotically,asthenumberofexamplesm,interms\nofitsrateofconvergenceasincreases.m",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "ofitsrateofconvergenceasincreases.m\nUnderappropriateconditions,the maximumlikelihoodestimatorhasthe\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\noftrainingexamplesapproachesinnity,themaximumlikelihoodestimateofa\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\nThetruedistributionpdatamustliewithinthemodelfamilypmodel(;).\nOtherwise,noestimatorcanrecoverpdata.\nThetruedistributionpdatamustcorrespondtoexactlyonevalueof.Other-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. \nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\ntor,manyofwhichsharethepropertyofbeingconsistentestimators.However,\n1 3 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nconsistentestimatorscandierintheirstatisticeciency,meaningthatone\nconsistentestimatormayobtainlowergeneralization errorforaxednumberof\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaxedlevelof\ngeneralization error.\nStatisticaleciencyistypicallystudiedintheparametriccase(likeinlinear\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "measurehowclosewearetothetrueparameterisbytheexpectedmeansquared\nerror,computingthesquareddierencebetweentheestimatedandtrueparameter\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\nformlarge,theCramr-Raolowerbound(,;,)showsthatno Rao1945Cramr1946\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\nestimator.\nForthesereasons(consistencyandeciency),maximumlikelihoodisoften",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "consideredthepreferredestimatortouseformachinelearning.Whenthenumber\nofexamplesissmallenoughtoyieldoverttingbehavior,regularizationstrategies\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\nthathaslessvariancewhentrainingdataislimited.\n5.6BayesianStatistics\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\ningasinglevalueof,thenmakingallpredictionsthereafterbasedonthatone\nestimate.Anotherapproachistoconsiderallpossiblevaluesofwhenmakinga",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "prediction.ThelatteristhedomainofBayesianstatistics.\nAsdiscussedinsection,thefrequen tistperspectiveisthatthetrue 5.4.1\nparametervalueisxedbutunknown,whilethepointestimate isarandom\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\nTheBayesianperspectiveonstatisticsisquitedierent.The Bayesianuses\nprobabilitytoreectdegreesofcertaintyofstatesofknowledge.Thedatasetis\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameter",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "isunknownoruncertainandthusisrepresentedasarandomvariable.\nBeforeobservingthedata,werepresentourknowledgeofusingtheprior\nprobabilitydistribution,p()(sometimesreferredtoassimplytheprior).\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\nquitebroad(i.e.withhighentropy)toreectahighdegreeofuncertaintyinthe\n1 3 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofbeforeobservinganydata.Forexample,onemightassume that apriori\nliesinsomeniterangeorvolume,withauniformdistribution.Manypriors\ninsteadreectapreferenceforsimplersolutions(suchassmallermagnitude\ncoecients,orafunctionthatisclosertobeingconstant).\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\nrecovertheeectofdataonourbeliefaboutbycombiningthedatalikelihood\npx((1),...,x() m|)withthepriorviaBayesrule:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "px((1),...,x() m|)withthepriorviaBayesrule:\npx(|(1),...,x() m) =px((1),...,x() m|)(p)\npx((1),...,x() m)(5.67)\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\nfewhighlylikelyvaluesoftheparameters.\nRelativetomaximumlikelihoodestimation,Bayesianestimationoerstwo",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "importantdierences.First,unlikethemaximumlikelihoodapproachthatmakes\npredictionsusingapointestimateof,theBayesianapproachistomakepredictions\nusingafulldistributionover.Forexample,afterobservingmexamples,the\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\npx((+1) m|x(1),...,x() m) =\npx((+1) m| |)(px(1),...,x() m)d.(5.68)\nHereeachvalueofwithpositiveprobabilitydensitycontributestotheprediction\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueof,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake.\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\ntaintyinagivenpointestimateofbyevaluatingitsvariance.Thevarianceof\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "protectwellagainstovertting.Thisintegralisofcoursejustanapplicationof\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\nestimate.\nThesecondimportantdierencebetweentheBayesianapproachtoestimation\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n1 3 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\npriordistribution.Thepriorhasaninuencebyshiftingprobabilitymassdensity\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\njudgmentimpactingthepredictions.\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\nisavailable,buttypicallysuerfromhighcomputational costwhenthenumberof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "trainingexamplesislarge.\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\nwelearnalinearmappingfromaninputvectorx Rntopredictthevalueofa\nscalar.Thepredictionisparametrized bythevector y R w Rn:\ny= wx. (5.69)\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\nofovertheentiretrainingsetas: y\ny()train= X()trainw. (5.70)\nExpressedasaGaussianconditionaldistributionony()train,wehave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "p(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\nexp\n1\n2(y()trainX()trainw)(y()trainX()trainw)\n,\n(5.72)\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\n(X()train,y()train) ( ) assimplyXy,.\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\nrstneedtospecifyapriordistribution.Thepriorshouldreectournaivebelief\naboutthevalueoftheseparameters.Whileitissometimesdicultorunnatural",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\nabout.Forreal-valuedparametersitiscommontouseaGaussianasaprior\ndistribution:\np() = (;w Nw0, 0) exp\n1\n2(w0)1\n0(w0)\n,(5.73)\n1 3 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwhere0and 0arethepriordistributionmeanvectorandcovariancematrix\nrespectively.1\nWiththepriorthusspecied,wecannowproceedindeterminingtheposterior\ndistributionoverthemodelparameters.\np,p,p (wX|y) (yX|w)()w (5.74)\nexp\n1\n2( )yXw( )yXw\nexp\n1\n2(w0)1\n0(w0)\n(5.75)\nexp\n1\n2\n2yXww+XXww+1\n0w2\n0 1\n0w\n.\n(5.76)\nWenowdene  m=\nXX+ 1\n0 1and m=  m\nXy+ 1\n00\n.Using\nthesenewvariables,wendthattheposteriormayberewrittenasaGaussian",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "distribution:\np, (wX|y) exp\n1\n2(w m)1\nm(w m)+1\n2\nm 1\nm m\n(5.77)\nexp\n1\n2(w m)1\nm(w m)\n. (5.78)\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\neectofBayesianinference.Inmostsituations,weset0to 0.Ifweset 0=1\nI,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "I,\nthen mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\nweightdecaypenaltyofww.OnedierenceisthattheBayesianestimateis\nundenedifissettozero-wearenotallowedtobegintheBayesianlearning\nprocesswithaninnitelywideprioronw.Themoreimportantdierenceisthat\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\ndierentvaluesofare,ratherthanprovidingonlytheestimate w  m.\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\nposteriordistributionovertheparameter,itisstilloftendesirabletohavea\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\nd i a g o n a l c o v a ria n c e m a t rix 0= diag( 0) .\n1 3 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate.Onecommonreasonfordesiringapointestimateisthat\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\nintractable,andapointestimateoersatractableapproximation.Ratherthan\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\nthebenetoftheBayesianapproachbyallowingthepriortoinuencethechoice\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\ncaseofcontinuous):\nMAP= argmax\np( ) = argmaxx|\nlog( )+log() px|p.(5.79)\nWerecognize,aboveontherighthandside,logp(x|),i.e.thestandardlog-\nlikelihoodterm,and,correspondingtothepriordistribution. log()p\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\nI2),thenthelog-priortermin\nequationisproportional tothefamiliar 5.79 wwweightdecaypenalty,plusa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "termthatdoesnotdependonwanddoesnotaectthelearningprocess.MAP\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\ndecay.\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\nthepriceofincreasedbias.\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\naddinganextratermtotheobjectivefunctionthatcorrespondstologp().Not\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\ndistributionisnotallowedtodo.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "distributionisnotallowedtodo.\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\ndistribution,astheprior(NowlanandHinton1992,).\n1 3 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.7SupervisedLearningAlgorithms\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\ntrainingsetofexamplesofinputsxandoutputsy.Inmanycasestheoutputs\nymaybediculttocollectautomatically andmustbeprovidedbyahuman\nsupervisor,butthetermstillappliesevenwhenthetrainingsettargetswere\ncollectedautomatically .\n5.7.1ProbabilisticSupervisedLearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "5.7.1ProbabilisticSupervisedLearning\nMostsupervisedlearningalgorithmsinthisbookarebasedon estimatinga\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\nlikelihoodestimationtondthebestparametervectorforaparametricfamily\nofdistributions .py(|x;)\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\npyy (| Nx;) = (;xI,). (5.80)\nWecangeneralizelinearregressiontotheclassicationscenariobydeninga\ndierentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\nmustaddupto1.\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "interval(0,1)andinterpretthatvalueasaprobability:\npy  (= 1 ;) = |x (x). (5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassicationratherthanregression).\nInthecaseoflinearregression,wewereabletondtheoptimalweightsby\nsolvingthenormalequations.Logisticregressionissomewhatmoredicult.There\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "log-likelihood(NLL)usinggradientdescent.\n1 4 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\ntherightkindofinputandoutputvariables.\n5.7.2SupportVectorMachines\nOneofthemostinuentialapproachestosupervisedlearningisthesupportvector\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\nlogisticregressioninthatitisdrivenbyalinearfunctionwx+b.Unlikelogistic",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "regression,thesupportvectormachinedoesnotprovideprobabilities, butonly\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\nwx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\nwx+bisnegative.\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "bere-writtenas\nwx+= +bbm\ni=1 ixx() i(5.82)\nwherex() iisatrainingexampleandisavectorofcoecients.Rewritingthe\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\nfunction(x) andthedotproductwithafunctionk(xx,() i) =(x)(x() i) called\nakernel.The operatorrepresentsaninnerproductanalogousto(x)(x() i).\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\nsomeinnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "example,innerproductsbasedonintegrationratherthansummation.Acomplete\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +\ni ik,(xx() i). (5.83)\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetween(x)\nandf(x)islinear.Also,therelationshipbetweenandf(x)islinear.The\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\n1 4 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nguaranteedtoconvergeeciently.Thisispossiblebecauseweconsiderxedand\noptimizeonly,i.e.,theoptimization algorithmcanviewthedecisionfunction\nasbeinglinearinadierentspace.Second,thekernelfunctionkoftenadmits\nanimplementationthatissignicantlymorecomputational ecientthannaively\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. ()x\nInsomecases,(x)canevenbeinnitedimensional,whichwouldresultin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "aninnitecomputational costforthenaive,explicitapproach.Inmanycases,\nk(xx,)isanonlinear,tractablefunctionofxevenwhen(x)isintractable.As\nanexampleofaninnite-dimens ionalfeaturespacewithatractablekernel,we\nconstructafeaturemapping(x)overthenon-negativeintegersx.Supposethat\nthismappingreturnsavectorcontainingxonesfollowedbyinnitelymanyzeros.\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\ntothecorrespondinginnite-dimens ionaldotproduct.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "tothecorrespondinginnite-dimens ionaldotproduct.\nThemostcommonlyusedkernelistheGaussiankernel\nk, , (uvuv ) = (N ;02I) (5.84)\nwhere N(x;, )isthestandardnormaldensity.Thiskernelisalsoknownas\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\nproductinaninnite-dimens ionalspace,butthederivationofthisspaceisless\nstraightforwardthaninourexampleofthekernelovertheintegers. min",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "WecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\nforclassy.WhenatestpointxisnearxaccordingtoEuclideandistance,the\nGaussiankernelhasalargeresponse,indicatingthatxisverysimilartothex\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\nsimilarityofthecorrespondingtrainingexamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "similarityofthecorrespondingtrainingexamples.\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\norkernelmethods( ,; WilliamsandRasmussen1996Schlkopf1999etal.,).\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "contributesaterm ik(xx,() i)tothedecisionfunction.Supportvectormachines\nareabletomitigatethisbylearninganvectorthatcontainsmostlyzeros.\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\nthetrainingexamplesthathavenon-zero i.Thesetrainingexamplesareknown\n1 4 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nassupportvectors.\nKernelmachinesalsosuerfromahighcomputational costoftrainingwhen\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "ontheMNISTbenchmark.\n5.7.3OtherSimpleSupervisedLearningAlgorithms\nWehavealreadybrieyencounteredanothernon-probabilis ticsupervisedlearning\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\nafamilyoftechniquesthatcanbeusedforclassicationorregression.Asa\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaxed\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\nwendthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\nanykindofsupervisedlearningwherewecandeneanaverageoveryvalues.In\nthecaseofclassication,wecanaverageoverone-hotcodevectorscwithc y= 1\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\nsupposewehaveamulticlassclassicationtaskandmeasureperformancewith0-1\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\nnumberoftrainingexamplesapproachesinnity.TheerrorinexcessoftheBayes\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\ndistantneighborsrandomly.Whenthereisinnitetrainingdata,alltestpointsx",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "willhaveinnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\nofthem,theprocedureconvergestotheBayeserrorrate.Thehighcapacityof\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\ngivenasmall,nitetrainingset.Oneweaknessofk-nearestneighborsisthatit\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "imaginewehavearegressiontaskwithx R100drawnfromanisotropicGaussian\n1 4 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\nfeaturesx2throughx100,notbythelonefeaturex1.Thustheoutputonsmall\ntrainingsetswillessentiallyberandom.\n1 4 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\ndisplayedwithabinarystringidentiercorrespondingtoitspositioninthetree,obtained\nbyappendingabittoitsparentidentier(0=chooseleftortop,1=chooserightorbottom).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodene,soitis\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\nnumberoftrainingexamples.\n1 4 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\n1984)anditsmanyvariants.Asshowningure,eachnodeofthedecision 5.7\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\ncut).Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\nwithaxis-alignedsteps.\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\nlearningalgorithmsbythinkingaboutthesimilaritiesanddierencesbetween\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\nSee (),(), ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\n5.8UnsupervisedLearningAlgorithms\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "onlyfeaturesbutnotasupervisionsignal.Thedistinctionbetweensupervised\nandunsupervisedalgorithmsisnotformallyandrigidlydenedbecausethereisno\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "ndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n1 4 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nrelatedexamples.\nAclassicunsupervisedlearningtaskistondthebestrepresentationofthe\ndata.Bybestwecanmeandierentthings,butgenerallyspeakingwearelooking\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\nmoreaccessiblethanitself.x\nTherearemultiplewaysofdeningarepresentation.Threeofthe simpler\nmostcommonincludelowerdimensionalrepresentations,sparserepresentations",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "andindependentrepresentations.Low-dimensionalrepresentationsattemptto\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "overallstructureoftherepresentationthattendstodistributedataalongtheaxes\noftherepresentationspace.Independentrepresentationsattempttodisentangle\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\noftherepresentationarestatisticallyindependent.\nOfcoursethesethreecriteriaarecertainlynotmutuallyexclusive.Low-\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "reducethesizeofarepresentationistondandremoveredundancies.Identifying\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\nachievemorecompressionwhilediscardinglessinformation.\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\ndevelopthesecriteriaindierentwaysorintroduceothercriteria.\n5.8.1PrincipalComponentsAnalysis\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n1 4 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n  2 0 1 0 0 1 0 2 0\nx 1 2 0 1 001 02 0x 2\n  2 0 1 0 0 1 0 2 0\nz 1 2 0 1 001 02 0z 2\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned. ( R i g h t )The\ntransformeddataz=xWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\nmostvarianceisnowalongz 2.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "mostvarianceisnowalongz 2.\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\nisarststeptowardthecriterionoflearningrepresentationswhoseelementsare\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "inputxtoarepresentationzasshowningure.Insection,wesawthat 5.8 2.12\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\ncorrespondstotherstprincipalcomponentofthedata.ThuswecanusePCA\nasasimpleandeectivedimensionalityreductionmethodthatpreservesasmuch\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "decorrelatestheoriginaldatarepresentation.X\nLetusconsiderthemn-dimensionaldesignmatrixX.Wewillassumethat\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nm1XX. (5.85)\n1 4 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nPCAndsarepresentation(throughlineartransformation)z=xWwhere\nVar[]zisdiagonal.\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\naregivenbytheeigenvectorsofXX.Fromthisview,\nXXWW = . (5.86)\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\nSpecically,theyaretherightsingularvectorsofX.Toseethis,letWbethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "rightsingularvectorsinthedecompositionX=UW .Wethenrecoverthe\noriginaleigenvectorequationwithastheeigenvectorbasis: W\nXX=\nUW \nUW = W 2W.(5.87)\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\nSVDof,wecanexpressthevarianceofas: X X\nVar[] =x1\nm1XX (5.88)\n=1\nm1(UW )UW (5.89)\n=1\nm1W UUW (5.90)\n=1\nm1W 2W, (5.91)\nwhereweusethefactthatUU=IbecausetheUmatrixofthesingularvalue\ndecompositionisdenedtobeorthogonal.Thisshowsthatifwetakez=xW,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "wecanensurethatthecovarianceofisdiagonalasrequired: z\nVar[] =z1\nm1ZZ (5.92)\n=1\nm1WXXW (5.93)\n=1\nm1WW 2WW (5.94)\n=1\nm12, (5.95)\nwherethistimeweusethefactthatWW=I,againfromthedenitionofthe\nSVD.\n1 4 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\n(asgivenby 2)whichimmediatelyimpliesthattheindividualelementsofzare\nmutuallyuncorrelated.\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "variationunderlyingthedata.InthecaseofPCA,thisdisentanglingtakesthe\nformofndingarotationoftheinputspace(describedbyW)thatalignsthe\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\nwith.z\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\ncanbedonewithasimplelineartransformation.\n5.8.2-meansClustering k",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "5.8.2-meansClustering k\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\nThek-meansclusteringalgorithmdividesthetrainingsetintokdierentclusters\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\nzero.\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "representation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\nwewilldevelopotheralgorithmsthatlearnmoreexiblesparserepresentations,\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\nareanextremeexampleofsparserepresentationsthatlosemanyofthebenets\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\nsimilartoeachother)anditconfersthecomputational advantagethattheentire",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "representationmaybecapturedbyasingleinteger.\nThek-meansalgorithmworksbyinitializingkdierentcentroids{(1),...,() k}\ntodierentvalues,thenalternatingbetweentwodierentstepsuntilconvergence.\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\nthenearestcentroid() i.Intheotherstep,eachcentroid() iisupdatedtothe\nmeanofalltrainingexamplesx() jassignedtocluster.i\n1 5 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nOnedicultypertainingtoclusteringisthattheclusteringproblemisinherently\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "clusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\nmaybemanydierentclusteringsthatallcorrespondwelltosomepropertyof\ntherealworld.Wemayhopetondaclusteringthatrelatestoonefeaturebut\nobtainadierent,equallyvalidclusteringthatisnotrelevanttoourtask.For\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\ncars.Ifweaskeachclusteringalgorithmtondtwoclusters,onealgorithmmay",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "ndaclusterofcarsandaclusteroftrucks,whileanothermayndaclusterof\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\nlostinformationaboutsimilarity.Redcarsareinadierentclusterfromgray\ncars,justastheyareinadierentclusterfromgraytrucks.Theoutputofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\nthantheyaretograytrucks.Theyaredierentfromboththings,andthatisall\nweknow.\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\ntwoattributesforeachvehicleonerepresentingitscolorandonerepresenting\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "twoattributesweareinterestedinarecolorandcar-versus-truckratherthan\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\nsimilaritybetweenobjectsinane-grainedwaybycomparingmanyattributes\ninsteadofjusttestingwhetheroneattributematches.\n5.9StochasticGradientDescent\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\n1 5 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ngradientdescentalgorithmintroducedinsection.4.3\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\nexpensive.\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\nJ() =  E x ,y  pdataL,y,(x) =1\nmm\ni=1L(x() i,y() i,)(5.96)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "mm\ni=1L(x() i,y() i,)(5.96)\nwhereistheper-exampleloss L L,y,py. (x) = log (|x;)\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\n J() =1\nmm\ni=1 L(x() i,y() i,.) (5.97)\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\nlong.\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "Specically,oneachstepofthealgorithm,wecansampleaminibatchofexamples\nB={x(1),...,x( m)}drawnuniformlyfromthetrainingset.Theminibatchsize\nmistypicallychosentobearelativelysmallnumberofexamples,rangingfrom\n1toafewhundred.Crucially,misusuallyheldxedasthetrainingsetsizem\ngrows.Wemaytatrainingsetwithbillionsofexamplesusingupdatescomputed\nononlyahundredexamples.\nTheestimateofthegradientisformedas\ng=1\nm m\ni=1L(x() i,y() i,.) (5.98)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "g=1\nm m\ni=1L(x() i,y() i,.) (5.98)\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\nthenfollowstheestimatedgradientdownhill:\ng  , (5.99)\nwhereisthelearningrate. \n1 5 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\nlocalminimuminareasonableamountoftime,butitoftenndsaverylowvalue\nofthecostfunctionquicklyenoughtobeuseful.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "ofthecostfunctionquicklyenoughtobeuseful.\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\ndatasets.Foraxedmodelsize,thecostperSGDupdatedoesnotdependonthe\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\nconvergenceusuallyincreaseswithtrainingsetsize.However,asmapproaches",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "innity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\nextendtheamountoftrainingtimeneededtoreachthemodelsbestpossibletest\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\namodelwithSGDisasafunctionof. O(1) m\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "algorithmsrequireconstructinganmmmatrixG i , j=k(x() i,x() j).Constructing\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\nwithbillions ofexamples. Inacademia, startingin2006,deeplearning was\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\ndatasets.\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\ninchapter.8\n5.10BuildingaMachineLearningAlgorithm\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\nafairlysimplerecipe:combineaspecicationofadataset,acostfunction,an\noptimization procedureandamodel.\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n1 5 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nXyand,thecostfunction\nJ,b(w) =  E x ,y  pdatalogpmodel( )y|x, (5.100)\nthemodelspecicationpmodel(y|x) =N(y;xw+b,1),and,inmostcases,the\noptimization algorithmdenedbysolvingforwherethegradientofthecostiszero\nusingthenormalequations.\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "processtoperformstatisticalestimation.Themostcommoncostfunctionisthe\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\nlikelihoodestimation.\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\ntoobtain\nJ,b (w) = ||||w2\n2 E x ,y  pdatalogpmodel( )y|x.(5.101)\nThisstillallowsclosed-formoptimization.\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\noptimization procedure,suchasgradientdescent.\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\nlearningcanbesupportedbydeningadatasetthatcontainsonlyXandproviding\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintherst",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "PCAvectorbyspecifyingthatourlossfunctionis\nJ() = w E x  pdata|| ||xr(;)xw2\n2 (5.102)\nwhileourmodelisdenedtohavewwithnormoneandreconstructionfunction\nr() = xwxw.\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\napproximatingitsgradients.\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "immediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n1 5 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\ntheircostfunctionshaveatregionsthatmaketheminappropriate forminimization\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\ncanbedescribedusingthisrecipehelpstoseethedierentalgorithmsaspartofa\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "thanasalonglistofalgorithmsthateachhaveseparatejustications.\n5.11ChallengesMotivatingDeepLearning\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymoredicultwhenworkingwithhigh-dimensionaldata,andhow\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\nareinsucienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\novercometheseandotherobstacles.\n5.11.1TheCurseofDimensionality\nManymachinelearningproblemsbecomeexceedinglydicultwhenthenumber\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\ncongurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\nincreases.\n1 5 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofcongurationsofinterestmaygrowexponentially. ( L e f t )Inthis\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "eachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\ndimensionsitismorediculttodistinguish10dierentvaluesofeachvariable.Weneed\ntokeeptrackofupto1010=100regions,andweneedatleastthatmanyexamplesto\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\naxis,weseemtoneedO(vd)regionsandexamples.Thisisaninstanceofthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.FiguregraciouslyprovidedbyNicolasChapados.\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\nespeciallysoinmachinelearning.\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\nAsillustratedingure,astatisticalchallengearisesbecausethenumberof 5.9\npossiblecongurations ofxismuchlargerthanthenumberoftrainingexamples.\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\ngrid,likeinthegure.Wecandescribelow-dimensional spacewithalownumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell.Ifwearedoingregressionwecanaveragethetarget",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\ncongurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n1 5 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nmeaningfulaboutthesenewcongurations? Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint.\n5.11.2LocalConstancyandSmoothnessRegularization\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "overparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\ndirectlyinuencingtheitselfandonlyindirectlyactingontheparameters function\nviatheireectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\ndegreeofbeliefinvariousfunctions.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "degreeofbeliefinvariousfunctions.\nAmongthemostwidelyusedoftheseimplicitpriorsisthesmoothness\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\nshouldnotchangeverymuchwithinasmallregion.\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\nadditional(explicitandimplicit)priorsinordertoreducethegeneralization",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\ninsucientforthesetasks.\nTherearemanydierentwaystoimplicitlyorexplicitlyexpressapriorbelief\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesedierent\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionfthat\nsatisesthecondition\nf() xf(+)x (5.103)\nformostcongurationsxandsmallchange.Inotherwords,ifweknowagood\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "answerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\npossible.\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n1 5 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\nthanthenumberoftrainingexamples.\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\nresembleseachtrainingexamplex() i.Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,). Bengioetal.2006b\nDecisiontreesalsosuerfromthelimitationsofexclusivelysmoothness-based\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "leavesanduseaseparateparameter(orsometimesmanyparametersforextensions\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\nrequiredtotthetree.Amultipleofnisneededtoachievesomelevelofstatistical\ncondenceinthepredictedoutput.\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "associatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\nwhereeachtrainingexamplecanbeusedtodeneatmostoneregion,isillustrated\ningure.5.10\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\nForexample,imagine thatthetargetfunctionisakindofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "exampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\n1 5 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.10:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintoregions.Anexample(representedherebyacircle)withineachregiondenesthe\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\ndeneswhattheoutputshouldbeforallpointswithinthecorrespondingregion.The\nregionsdenedbynearestneighbormatchingformageometricpatterncalledaVoronoi\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "oftrainingexamples.Whilethisgureillustratesthebehaviorofthenearestneighbor\nalgorithmspecically,othermachinelearningalgorithmsthatrelyexclusivelyonthe\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\nsurroundingthatexample.\n1 5 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "dierentwayalongeachdimension.Ifthefunctionadditionallybehavesdierently\nindierentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\ngeneralizewell?\nTheanswertobothofthesequestionswhetheritispossibletorepresent\nacomplicatedfunctioneciently,andwhetheritispossiblefortheestimated\nfunctiontogeneralizewelltonewinputsisyes.Thekeyinsightisthatavery",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "largenumberofregions,e.g.,O(2k),canbedenedwithO(k)examples,solong\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\ndierentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\nOtherapproachestomachinelearningoftenmakestronger,task-specicas-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 288,
      "type": "default"
    }
  },
  {
    "content": "sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\nstrong,task-specicassumptionsintoneuralnetworkssothattheycangeneralize\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\ncomplextobelimitedtosimple,manuallyspeciedpropertiessuchasperiodicity,\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 289,
      "type": "default"
    }
  },
  {
    "content": "thecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms.Theseapparentlymildassumptionsallowanexponentialgaininthe\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 290,
      "type": "default"
    }
  },
  {
    "content": "ofdimensionality.\n1 6 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 291,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.11.3ManifoldLearning\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\nmanifold.\nAmanifoldisaconnectedregion. Mathematically ,itisasetofpoints,\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n3-Dspace.\nThedenitionofaneighborhoodsurroundingeachpointimpliestheexistence",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 292,
      "type": "default"
    }
  },
  {
    "content": "oftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\ntoaneighboringone.Intheexampleoftheworldssurfaceasamanifold,onecan\nwalknorth,south,east,orwest.\nAlthoughthereisaformalmathematical meaningtothetermmanifold,in\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\ndimensioncorrespondstoalocaldirectionofvariation.Seegureforan5.11",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 293,
      "type": "default"
    }
  },
  {
    "content": "exampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\nofthemanifoldtovaryfromonepointtoanother.This oftenhappenswhena\nmanifoldintersectsitself.Forexample,agureeightisamanifoldthathasasingle\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . . 1 0 . 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 294,
      "type": "default"
    }
  },
  {
    "content": "Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\ntheunderlyingmanifoldthatthelearnershouldinfer.\n1 6 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 295,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\nof Rnconsistsofinvalidinputs,andthatinterestinginputsoccuronlyalong\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 296,
      "type": "default"
    }
  },
  {
    "content": "thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\nhighlyconcentrated.\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 297,
      "type": "default"
    }
  },
  {
    "content": "thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\noftwocategoriesofobservations.\nTherstobservationinfavorofthemanifoldhypothesisisthattheproba-\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\nfromthesedomains.Figureshowshow,instead,uniformlysampledpoints 5.12",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 298,
      "type": "default"
    }
  },
  {
    "content": "looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\n1 6 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 299,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\nvolumeofimagespace.\nOfcourse,concentratedprobabilitydistributionsarenotsucienttoshow",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 300,
      "type": "default"
    }
  },
  {
    "content": "thatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\n1 6 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 301,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 302,
      "type": "default"
    }
  },
  {
    "content": "moveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\nconnectedtothemanifoldofimagesofcatfaces.\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\ntuitivereasonssupportingit.Morerigorousexperiments(Cayton2005Narayanan,;\nandMitter2010Schlkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 303,
      "type": "default"
    }
  },
  {
    "content": "2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\ninterestinAI.\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\nspecicaddressesintermsofaddressnumbersalongthese1-Droads,notinterms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 304,
      "type": "default"
    }
  },
  {
    "content": "ofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\nmethodsnecessarytolearnsuchamanifoldstructure.Ingure,wewillsee 20.6\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 305,
      "type": "default"
    }
  },
  {
    "content": "andmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\n1 6 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 306,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\nfeat.\n1 6 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 307,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 3\nL i n e ar F act or Mo d e l s\nManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel\noftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto\npredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many\nofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|.\nTheselatentvariablesprovideanothermeansofrepresentingthedata.Distributed",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationsbasedonlatentvariablescanobtainalloftheadvantagesof\nrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrent\nnetworks.\nInthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\nlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\nblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;\nRoweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "alsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat\nthemoreadvanceddeepmodelswillextendfurther.\nAlinearfactormodelisdenedbytheuseofastochastic,lineardecoder\nfunctionthatgeneratesbyaddingnoisetoalineartransformationof. x h\nThesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\nfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\nmadethesemodelssomeoftherstlatentvariablemodelstobeextensivelystudied.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,\nwesampletheexplanatoryfactorsfromadistribution h\nh p ,() h (13.1)\nwhere p( h)isafactorialdistribution,with p( h) =\ni p( h i),sothatitiseasyto\n489",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:\nx W h b = ++noise (13.2)\nwherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\nThisisillustratedingure.13.1\nh 1 h 1 h 2 h 2 h 3 h 3\nx 1 x 1 x 2 x 2 x 3 x 3\nx h n  o i s  e x h n  o i s  e = W + + b = W + + b\nFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\nwhichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "independentlatentfactors h,plussomenoise.Dierentmodels,suchasprobabilistic\nPCA,factoranalysisorICA,makedierentchoicesabouttheformofthenoiseandof\ntheprior. p() h\n13.1ProbabilisticPCAandFactorAnalysis\nProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\nfactormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\ndierinthechoicesmadeforthenoisedistributionandthemodelspriorover\nlatentvariablesbeforeobserving. h x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "latentvariablesbeforeobserving. h x\nIn f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994\npriorisjusttheunitvarianceGaussian\nh 0 N(; h , I) (13.3)\nwhiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,\ngiven h.Specically,thenoiseisassumedtobedrawnfromadiagonalco-\nvarianceGaussian distribution,withcovariancematrix =diag( 2),with\n2= [ 2\n1 , 2\n2 , . . . , 2\nn]avectorofper-variablevariances.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "2 , . . . , 2\nn]avectorofper-variablevariances.\nTheroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween\nthedierentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta\nmultivariatenormalrandomvariable,with\nxN(; x b W W ,+)  . (13.4)\n490",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nInordertocastPCAinaprobabilisticframework,wecanmakeaslight\nmodicationtothefactoranalysismodel,makingtheconditionalvariances 2\ni\nequaltoeachother.Inthatcasethecovarianceof xisjust W W+ 2I,where\n2isnowascalar.Thisyieldstheconditionaldistribution\nxN(; x b W W ,+ 2I) (13.5)\norequivalently\nx h z = W ++ b  (13.6)\nwhere zN( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999\niterativeEMalgorithmforestimatingtheparameters and W 2.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "This pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost\nvariationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall\nresidual r e c o nst r u c t i o n e r r o r 2.Asshownby (), TippingandBishop1999\nprobabilisticPCAbecomesPCAas 0.Inthatcase,theconditionalexpected\nvalueof hgiven xbecomesanorthogonalprojectionof x bontothespace\nspannedbythecolumnsof,likeinPCA. d W\nAs 0,thedensitymodeldenedbyprobabilisticPCAbecomesverysharp",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "aroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe\nmodelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster\nnearahyperplane.\n13.2IndependentComponentAnalysis(ICA)\nIndependentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\nalgorithms( ,; ,;,; HeraultandAns1984JuttenandHerault1991Comon1994\nHyvrinen1999Hyvrinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,).\nItisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\ntheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\nmerelydecorrelatedfromeachother.1\nManydierentspecicmethodologiesarereferredtoasICA.Thevariant\nthatismostsimilartotheothergenerativemodelswehavedescribedhereisa\nvariant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992\npriordistributionovertheunderlyingfactors, p( h),mustbexedaheadoftimeby",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma\n1Seesectionforadiscussionofthedierencebetweenuncorrelatedvariablesandindepen- 3.8\ndentvariables.\n491",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nnonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning\nthemodelthenproceedsasusual,usingmaximumlikelihood.\nThemotivationforthisapproachisthatbychoosing p( h)tobeindependent,\nwecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.\nThisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto\nrecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each\ntrainingexampleisonemomentintime,each x iisonesensorsobservationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "themixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For\nexample,wemighthave npeoplespeakingsimultaneously.Ifwehave ndierent\nmicrophonesplacedindierentlocations,ICAcandetectthechangesinthevolume\nbetweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso\nthateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused\ninneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical\nsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubjects",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "headareusedtomeasuremanyelectricalsignalscomingfromthebody.The\nexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom\nthesubjectsheartandeyesarestrongenoughtoconfoundmeasurementstaken\natthesubjectsscalp.Thesignalsarriveattheelectrodesmixedtogether,so\nICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals\noriginatinginthebrain,andtoseparatesignalsindierentbrainregionsfrom\neachother.\nAsmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "inthegenerationof xratherthanusingadeterministicdecoder.Mostdonot\nusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\nh= W 1xindependentfromeachother.Manycriteriathataccomplishthisgoal\narepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe\nanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\nproblematicoperationbyconstrainingtobeorthogonal. W\nAllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "isanindependentpriorwithGaussiancomponents,then Wisnotidentiable.\nWecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery\ndierentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\nthatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe\nmodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe\nuserexplicitlyspeciesthedistribution,atypicalchoiceistouse p( h i) =d\nd h i( h i).",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "d h i( h i).\nTypicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\ndoestheGaussiandistribution,sowecanalsoseemostimplementations ofICA\naslearningsparsefeatures.\n492",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\nphrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples\nfromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but\ndonothaveanywayofrepresenting p( h),andthusdonotimposeadistribution\nover p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\nh= W 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "accomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore\noftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\ndataorestimatingitsdensity.\nJustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\nchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\nweuseanonlinearfunction ftogeneratetheobserveddata.SeeHyvrinenand\nPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "ensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000\nAnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent\nc o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014\nofinvertibletransformations(encoderstages)thathavethepropertythatthe\ndeterminantoftheJacobianofeachtransformationcanbecomputedeciently.\nThismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "totransformthedataintoaspacewhereithasafactorizedmarginaldistribution,\nbutismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\nisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto\ngeneratesamplesfromthemodel(byrstsamplingfrom p( h)andthenapplying\nthedecoder).\nAnothergeneralization ofICAistolearngroupsoffeatures,withstatistical\ndependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyvrinenand",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Hoyer1999Hyvrinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen\ntobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso\npossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\ngroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\nfeatures.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns\nGaborlters,suchthatneighboringfeatureshavesimilarorientation,locationor",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "frequency.ManydierentphaseosetsofsimilarGaborfunctionsoccurwithin\neachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\n13.3SlowFeatureAnalysis\nSl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom\n493",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ntimesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\nSlowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\nprinciple.Theideaisthattheimportantcharacteristicsofsceneschangevery\nslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\nscene.Forexample,incomputervision,individualpixelvaluescanchangevery\nrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "willrapidlychangefromblacktowhiteandbackagainasthezebrasstripespass\noverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe\nimagewillnotchangeatall,andthefeaturedescribingthezebraspositionwill\nchangeslowly.Wethereforemaywishtoregularizeourmodeltolearnfeatures\nthatchangeslowlyovertime.\nTheslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\ntoawidevarietyofmodels(,;,; ,; Hinton1989Fldik1989Mobahi e t a l .2009",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\ndierentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\nintroducedbyaddingatermtothecostfunctionoftheform\n\ntL f(( x( + 1 ) t)( , f x( ) t)) (13.7)\nwhere isahyperparameter determiningthestrengthoftheslownessregularization\nterm, tistheindexintoatimesequenceofexamples, fisthefeatureextractor\ntoberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "and f( x( + 1 ) t).Acommonchoiceforisthemeansquareddierence. L\nSlowfeatureanalysisisaparticularlyecientapplicationoftheslowness\nprinciple.Itisecientbecauseitisappliedtoalinearfeatureextractor,andcan\nthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\ngenerativemodelperse,inthesensethatitdenesalinearmapbetweeninput\nspaceandfeaturespacebutdoesnotdeneaprioroverfeaturespaceandthus\ndoesnotimposeadistributiononinputspace. p() x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "doesnotimposeadistributiononinputspace. p() x\nTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdening f( x; )\ntobealineartransformation,andsolvingtheoptimization problem\nmin\nE t(( f x( + 1 ) t) i f( x( ) t) i)2(13.8)\nsubjecttotheconstraints\nE t f( x( ) t) i= 0 (13.9)\nand\nE t[( f x( ) t)2\ni] = 1 . (13.10)\n494",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\nproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\nvaluesandobtainadierentsolutionwithequalvalueoftheslownessobjective.\nTheconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\npathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\nareordered,withtherstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\nmustalsoaddtheconstraint",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "mustalsoaddtheconstraint\n i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11)\nThisspeciesthatthelearnedfeaturesmustbelinearlydecorrelated fromeach\nother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe\noneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\nreconstructionerror,toforcethefeaturestodiversify,butthisdecorrelation\nmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\nproblemmaybesolvedinclosedformbyalinearalgebrapackage.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\nexpansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe\nquadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear\nSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors\nbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis\nexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron\ntopofthatexpansion.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "topofthatexpansion.\nWhentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\nquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\nthoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\nonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep\nSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\nbyneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA\nthusseemstobeareasonablybiologicallyplausiblemodel.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich\nfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical\npredictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\nconguration space(e.g.,inthecaseofrandommotion inthe3-Drendered\nenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\ndistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\ntheunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\nappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\n495",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nThisisincomparisontootherlearningalgorithmswherethecostfunctiondepends\nhighlyonspecicpixelvalues,makingitmuchmorediculttodeterminewhat\nfeaturesthemodelwilllearn.\nDeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\nestimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome\nthebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited\nitsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\nitwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\nonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\nwhethertheobjectsvelocityishighorlow,buttheslownessprincipleencourages\nthemodeltoignorethepositionofobjectsthathavehighvelocity.\n13.4SparseCoding\nSpar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996\nbeenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "mechanism.Strictlyspeaking,thetermsparsecodingreferstotheprocessof\ninferringthevalueof hinthismodel,whilesparsemodelingreferstotheprocess\nofdesigningandlearningthemodel,butthetermsparsecodingisoftenusedto\nrefertoboth.\nLikemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\nobtainreconstructionsof x,asspeciedinequation.Morespecically,sparse 13.2\ncodingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\nisotropicprecision: \np , ( ) = (; + x h| N x W h b1\nI) . (13.12)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "p , ( ) = (; + x h| N x W h b1\nI) . (13.12)\nThedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen\nandField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\nStudent- tdistributions.Forexample,theLaplacepriorparametrized intermsof\nthesparsitypenaltycoecientisgivenby \np h( i) = Laplace( h i;0 ,2\n) =\n4e1\n2 h| i|(13.13)\nandtheStudent-priorby t\np h( i) 1\n(1+h2\ni\n) +1\n2. (13.14)\n496",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\ntrainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\nreconstructthedatagiventheencoding.Thisapproachwillbejustiedfurtheras\naprincipledapproximation tomaximumlikelihoodlater,insection.19.3\nFormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\nthatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\nisanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek\nthesinglemostlikelycodevalue:\nh= () = argmax f x\nhp . ( ) h x| (13.15)\nWhencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12\noptimization problem:\nargmax\nhp( ) h x| (13.16)\n=argmax\nhlog( ) p h x| (13.17)\n=argmin\nh|||| h 1+ || || x W h2\n2 , (13.18)\nwherewehavedroppedtermsnotdependingon handdividedbypositivescaling",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "factorstosimplifytheequation.\nDuetotheimpositionofan L1normon h,thisprocedurewillyieldasparse\nh(Seesection).7.1.2\nTotrainthemodelratherthanjustperforminference,wealternatebetween\nminimization withrespectto handminimization withrespectto W.Inthis\npresentation,wetreat asahyperparameter.Typicallyitissetto1becauseits\nroleinthisoptimization problemissharedwith andthereisnoneedforboth\nhyperparameters.Inprinciple,wecouldalsotreat asaparameterofthemodel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\non hbutdodependon .Tolearn ,thesetermsmustbeincluded,or will\ncollapseto.0\nNotallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|).\nOftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\nvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\nIfwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor\nanelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "sparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d\n497",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ninferenceinadierentmodelfamily,thespikeandslabsparsecodingmodel,for\nwhichsamplesfromthepriorusuallycontaintruezeros.\nThesparsecodingapproachcombinedwiththeuseofthenon-parametric\nencodercaninprincipleminimizethecombinationofreconstructionerrorand\nlog-priorbetterthananyspecicparametricencoder.Anotheradvantageisthat\nthereisnogeneralization errortotheencoder.Aparametricencodermustlearn\nhowtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "thetrainingdata,alearned,parametricencodermayfailtondan hthatresults\ninaccuratereconstructionorasparsecode.Forthevastmajorityofformulations\nofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\nprocedurewillalwaysndtheoptimalcode(unlessdegeneratecasessuchas\nreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\ncanstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe\ndecoderweights,ratherthangeneralization errorintheencoder.Thelackof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "generalization errorinsparsecodingsoptimization-based encodingprocessmay\nresultinbettergeneralization whensparsecodingisusedasafeatureextractorfor\naclassierthanwhenaparametricfunctionisusedtopredictthecode.Coates\nandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\nobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\nencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l .",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\nextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\nlabelsperclass).\nTheprimarydisadvantageofthenon-parametric encoderisthatitrequires\ngreatertimetocompute hgiven xbecausethenon-parametric approachrequires\nrunninganiterativealgorithm.Theparametricautoencoderapproach,developed\ninchapter,usesonlyaxedn umberoflayers,oftenonlyone.Another 14",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe\nnon-parametric encoder,whichmakesitdiculttopretrainasparsecodingmodel\nwithanunsupervisedcriterionandthenne-tuneitusingasupervisedcriterion.\nModiedversionsofsparsecodingthatpermitapproximate derivativesdoexist\nbutarenotwidelyused( ,). BagnellandBradley2009\nSparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\nshowningure.Thishappensevenwhenthemodelisabletoreconstruct 13.2",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thedatawellandprovideusefulfeaturesforaclassier.Thereasonisthateach\nindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\nresultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated\nsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-\n498",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nFigure13.2:Example samplesandweightsfromaspikeandslabsparsecodingmodel\ntrainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe\ntrainingexamples.Atrstglance,onemightassumethemodelispoorlyt.The ( R i g h t )\nweightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\ndigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\noverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\ngenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\nreproducedwithpermissionfromGoodfellow2013d e t a l .().\nfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore\nsophisticatedshallowmodels.\n13.5ManifoldInterpretationofPCA\nLinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\nlearningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "deningathinpancake-shapedregionofhighprobabilityaGaussiandistribution\nthatisverynarrowalongsomeaxes,justasapancakeisveryatalongitsvertical\naxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\naxes.Thisisillustratedingure.PCAcanbeinterpretedasaligningthis 13.3\npancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation\nappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\nmatrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "xaspossible,\nLettheencoderbe\nh x W = ( f) = ( ) x  . (13.19)\n499",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheencodercomputesalow-dimensional representationof h.Withtheautoencoder\nview,wehaveadecodercomputingthereconstruction\n x h b V h = ( g) = + . (13.20)\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\nmanifold.Thegureshowstheupperhalfofthepancakeabovethemanifoldplane\nwhichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis\nverysmall(arrowpointingoutofplane)andcanbeconsideredlikenoise,whiletheother",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "variancesarelarge(arrowsintheplane)andcorrespondtosignal,andacoordinate\nsystemforthereduced-dimensiondata.\nThechoicesoflinearencoderanddecoderthatminimizereconstructionerror\nE[|| x x||2] (13.21)\ncorrespondto V= W, = b= E[ x]andthecolumnsof Wformanorthonormal\nbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\nmatrix\nC x  x  = [( E )()] . (13.22)\nInthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\nOnecanalsoshowthateigenvalue  iof Ccorrespondstothevarianceof x\ninthedirectionofeigenvector v( ) i.If x RDand h Rdwith d < D,thenthe\n500",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\noptimalreconstructionerror(choosing,,andasabove)is  b V W\nmin[ E|| x x||2] =D \ni d = + 1 i . (13.23)\nHence,ifthecovariancehasrank d,theeigenvalues  d + 1to  Dare0andrecon-\nstructionerroris0.\nFurthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\nmaximizingthevariancesoftheelementsof h,underorthogonal W,insteadof\nminimizingreconstructionerror.\nLinearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "simplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiersand\nlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\nfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\nmodelsthatperformthesametasksbutwithamuchmorepowerfulandexible\nmodelfamily.\n501",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "Deep L ea r ni n g\nI a n G o o d f e l l o w\nY o s h u a B e n g i o\nA a r o n C o u r v i l l e",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "C on t e n t s\nWebsite vii\nAcknowledgments viii\nNotation xi\n1Introduction 1\n1.1WhoShouldReadThisBook?............... .....8\n1.2HistoricalTrendsinDeepLearning......... ........ 11\nIAppliedMathandMachineLearningBasics 29\n2LinearAlgebra 31\n2.1Scalars,Vectors,MatricesandTensors......... ......31\n2.2MultiplyingMatricesandVectors......... ........ .34\n2.3IdentityandInverseMatrices......... ........ ...36",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "2.4LinearDependenceandSpan......... ........ ...37\n2.5Norms......... ........ ........ ........ 39\n2.6SpecialKindsofMatricesandVectors............... 40\n2.7Eigendecomposition.......... ........ ........ 42\n2.8SingularValueDecomposition........ ........ ....44\n2.9TheMoore-PenrosePseudoinverse......... ........ .45",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "2.10TheTraceOperator......... ........ ........ 46\n2.11TheDeterminant.......... ........ ......... 47\n2.12Example:PrincipalComponentsAnalysis......... ....48\n3ProbabilityandInformationTheory 53\n3.1WhyProbability?.............. ........ .....54\ni",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n3.2RandomVariables............. ......... ....56\n3.3ProbabilityDistributions......... ........ ......56\n3.4MarginalProbability......... ......... ....... 58\n3.5ConditionalProbability.......... ........ .....59\n3.6TheChainRuleofConditionalProbabilities......... ...59\n3.7IndependenceandConditionalIndependence......... ...60",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "3.8Expectation,VarianceandCovariance.......... .....60\n3.9CommonProbabilityDistributions............... ..62\n3.10UsefulPropertiesofCommonFunctions............ ..67\n3.11BayesRule.......... ........ ........ ....70\n3.12TechnicalDetailsofContinuousVariables............. 71\n3.13InformationTheory.......... ........ ........ 73\n3.14StructuredProbabilisticModels............ ....... 75",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "4NumericalComputation 80\n4.1OverowandUnderow......... ........ ......80\n4.2PoorConditioning ......... ........ ......... 82\n4.3Gradient-BasedOptimization ............... .....82\n4.4ConstrainedOptimization ............. ........ .93\n4.5Example:LinearLeastSquares................ ...96\n5MachineLearningBasics 98\n5.1LearningAlgorithms........... ........ ......99",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,OverttingandUndertting.......... .....110\n5.3HyperparametersandValidationSets......... .......120\n5.4Estimators,BiasandVariance.............. ......122\n5.5MaximumLikelihoodEstimation............... ...131\n5.6BayesianStatistics........... ........ .......135\n5.7SupervisedLearningAlgorithms........... ........ 140\n5.8UnsupervisedLearningAlgorithms............... ..146",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "5.9StochasticGradientDescent............. ........ 151\n5.10BuildingaMachineLearningAlgorithm............. .153\n5.11ChallengesMotivatingDeepLearning.............. ..155\nIIDeepNetworks:ModernPractices 166\n6DeepFeedforwardNetworks 168\n6.1Example:LearningXOR.......... ........ .....171\n6.2Gradient-BasedLearning......... ........ ......177\ni i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n6.3HiddenUnits.............. ......... ......191\n6.4ArchitectureDesign......... ........ ........ .197\n6.5Back-PropagationandOtherDierentiationAlgorithms.....204\n6.6HistoricalNotes............... ......... ....224\n7RegularizationforDeepLearning 228\n7.1ParameterNormPenalties.............. ........ 230\n7.2NormPenaltiesasConstrainedOptimization ........ ....237",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "7.3RegularizationandUnder-ConstrainedProblems.........239\n7.4DatasetAugmentation.......... ......... .....240\n7.5NoiseRobustness......... ........ ........ ..242\n7.6Semi-SupervisedLearning................ ......243\n7.7Multi-TaskLearning.............. ......... ..244\n7.8EarlyStopping......... ........ ........ ...246",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "7.9ParameterTyingandParameterSharing.............. 253\n7.10SparseRepresentations......... ........ .......254\n7.11BaggingandOtherEnsembleMethods.......... .....256\n7.12Dropout........ ......... ........ .......258\n7.13AdversarialTraining........ ......... ........ 268\n7.14TangentDistance,TangentProp,andManifoldTangentClassier270\n8OptimizationforTrainingDeepModels 274",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "8OptimizationforTrainingDeepModels 274\n8.1HowLearningDiersfromPureOptimization ........... 275\n8.2ChallengesinNeuralNetworkOptimization ............282\n8.3BasicAlgorithms............. ........ ......294\n8.4ParameterInitialization Strategies.......... .......301\n8.5AlgorithmswithAdaptiveLearningRates.............306\n8.6ApproximateSecond-Order Methods............. ...310",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "8.7Optimization StrategiesandMeta-Algorithms...........317\n9ConvolutionalNetworks 330\n9.1TheConvolutionOperation................ .....331\n9.2Motivation.......... ......... ........ ....335\n9.3Pooling............. ........ ......... ...339\n9.4ConvolutionandPoolingasanInnitelyStrongPrior.......345\n9.5VariantsoftheBasicConvolutionFunction............ 347",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "9.6StructuredOutputs......... ......... ........ 358\n9.7DataTypes.............. ........ ........ 360\n9.8EcientConvolutionAlgorithms........ ........ ..362\n9.9RandomorUnsupervisedFeatures........ ........ .363\ni i i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n9.10TheNeuroscienticBasisforConvolutionalNetworks.......364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning....371\n10SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs............. .....375\n10.2RecurrentNeuralNetworks............ ........ .378\n10.3BidirectionalRNNs.............. ......... ...394\n10.4Encoder-DecoderSequence-to-SequenceArchitectures.......396",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "10.5DeepRecurrentNetworks........ ......... .....398\n10.6RecursiveNeuralNetworks.............. ........ 400\n10.7TheChallengeofLong-TermDependencies.......... ...401\n10.8EchoStateNetworks.......... ......... ......404\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales....406\n10.10TheLongShort-TermMemoryandOtherGatedRNNs......408\n10.11Optimization forLong-TermDependencies........ .....413",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "10.12Explicit Memory.......... ......... ........ 416\n11PracticalMethodology 421\n11.1PerformanceMetrics.......... ........ .......422\n11.2DefaultBaselineModels........ ........ .......425\n11.3DeterminingWhethertoGatherMoreData............ 426\n11.4SelectingHyperparameters......... ........ .....427\n11.5DebuggingStrategies............. ......... ...436",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "11.6Example:Multi-DigitNumberRecognition............. 440\n12Applications 443\n12.1Large-ScaleDeepLearning......... ........ .....443\n12.2ComputerVision......... ........ ........ ..452\n12.3SpeechRecognition.............. ......... ...458\n12.4NaturalLanguageProcessing........... ........ .461\n12.5OtherApplications......... ........ ........ .478\nIIIDeepLearningResearch 486",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "IIIDeepLearningResearch 486\n13LinearFactorModels 489\n13.1ProbabilisticPCAandFactorAnalysis............... 490\n13.2IndependentComponentAnalysis(ICA)............ ..491\n13.3SlowFeatureAnalysis............... ........ .493\n13.4SparseCoding.............. ......... ......496\ni v",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n13.5ManifoldInterpretation ofPCA................. ..499\n14Autoencoders 502\n14.1Undercomplete Autoencoders.......... ........ ..503\n14.2RegularizedAutoencoders......... ........ .....504\n14.3RepresentationalPower,LayerSizeandDepth...........508\n14.4StochasticEncodersandDecoders........... .......509\n14.5DenoisingAutoencoders.......... ......... ....510",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "14.6LearningManifoldswithAutoencoders............... 515\n14.7ContractiveAutoencoders......... ........ .....521\n14.8PredictiveSparseDecomposition........ ......... .523\n14.9ApplicationsofAutoencoders.............. ......524\n15RepresentationLearning 526\n15.1GreedyLayer-WiseUnsupervisedPretraining...........528\n15.2TransferLearningandDomainAdaptation............ .536",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "15.3Semi-SupervisedDisentanglingofCausalFactors.........541\n15.4DistributedRepresentation............ ......... .546\n15.5ExponentialGainsfromDepth.......... ........ .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses..........554\n16StructuredProbabilisticModelsforDeepLearning558\n16.1TheChallengeofUnstructuredModeling......... .....559\n16.2UsingGraphstoDescribeModelStructure.......... ...563",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "16.3SamplingfromGraphicalModels........... .......580\n16.4AdvantagesofStructuredModeling.......... .......582\n16.5LearningaboutDependencies............ ........ 582\n16.6InferenceandApproximateInference......... .......584\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\n17MonteCarloMethods 590\n17.1SamplingandMonteCarloMethods........ ........ 590",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "17.2ImportanceSampling............ ........ .....592\n17.3MarkovChainMonteCarloMethods............. ...595\n17.4GibbsSampling................ ........ ....599\n17.5TheChallengeofMixingbetweenSeparatedModes........599\n18ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient......... ......... ..606\n18.2StochasticMaximumLikelihoodandContrastiveDivergence...607\nv",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n18.3Pseudolikelihood........... ......... .......615\n18.4ScoreMatchingandRatioMatching........ ........ 617\n18.5DenoisingScoreMatching......... ........ .....619\n18.6Noise-ContrastiveEstimation............. .......620\n18.7EstimatingthePartitionFunction........... .......623\n19ApproximateInference 631\n19.1InferenceasOptimization .......... ........ ....633",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "19.2ExpectationMaximization .......... ......... ...634\n19.3MAPInferenceandSparseCoding.......... .......635\n19.4VariationalInferenceandLearning............... ..638\n19.5LearnedApproximateInference............ .......651\n20DeepGenerativeModels 654\n20.1BoltzmannMachines........... ......... .....654\n20.2RestrictedBoltzmannMachines................ ...656",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "20.3DeepBeliefNetworks......... ........ ........ 660\n20.4DeepBoltzmannMachines.......... ........ ....663\n20.5BoltzmannMachinesforReal-ValuedData......... ....676\n20.6ConvolutionalBoltzmannMachines................ .683\n20.7BoltzmannMachinesforStructuredorSequentialOutputs....685\n20.8OtherBoltzmannMachines............. ........ 686\n20.9Back-PropagationthroughRandomOperations..........687",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "20.10DirectedGenerativeNets............ ......... ..692\n20.11DrawingSamplesfromAutoencoders.............. ..711\n20.12Generativ eStochasticNetworks........... ........ 714\n20.13OtherGenerationSchemes............. ......... 716\n20.14EvaluatingGenerativeModels............. .......717\n20.15Conclus ion........ ......... ........ ......720\nBibliography 721\nIndex 777\nv i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "W e b s i t e\nwww.deeplearningb ook.org\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\nvii",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 2\nL i n e ar A l ge b ra\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\nthekeylinearalgebraprerequisites.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "thekeylinearalgebraprerequisites.\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Shilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\ntopicsthatarenotessentialforunderstandingdeeplearning.\n2.1Scalars,Vectors,MatricesandTensors\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\nScalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\n31",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nexample,wemightsayLet s Rbetheslopeoftheline,whiledeninga\nreal-valuedscalar,orLet n Nbethenumberofunits,whiledeninga\nnaturalnumberscalar.\nVectors:Avectorisanarrayofnumbers.Thenumbersarearrangedin\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\nasx.Theelementsofthevectorareidentiedbywritingitsnameinitalic\ntypeface,withasubscript.Therstelementofxis x 1,thesecondelement",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "is x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\nwewritethemasacolumnenclosedinsquarebrackets:\nx=\nx 1\nx 2\n...\nx n\n. (2.1)\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\ngivingthecoordinatealongadierentaxis.\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "deneasetcontainingtheindicesandwritethesetasasubscript.For\nexample,toaccess x 1, x 3and x 6,wedenetheset S={1 ,3 ,6}andwrite\nx S.Weusethesigntoindexthecomplementofaset.Forexamplex  1is\nthevectorcontainingallelementsofxexceptfor x 1,andx  Sisthevector\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\nMatrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentied\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "aheightof mandawidthof n,thenwesaythatA Rm n .Weusually\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\nofthenumberswithverticalcoordinate ibywritingaforthehorizontal :\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\n3 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nA =\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\nA 3 1 , A 3 2 ,\n  A=A 1 1 , A 2 1 , A 3 1 ,\nA 1 2 , A 2 2 , A 3 2 ,\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\nmaindiagonal.\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\nA 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\n. (2.2)\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\nofthematrixcomputedbyapplyingthefunctionto. fA\nTensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\nvariablenumberofaxesisknownasatensor.WedenoteatensornamedA\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\nbywriting A i , j , k.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "bywriting A i , j , k.\nOneimportantoperationonmatricesisthetranspose.Thetransposeofa\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\ngureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\nmatrixasAA,anditisdenedsuchthat\n(A) i , j= A j , i . (2.3)\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n3 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ndeneavectorbywritingoutitselementsinthetextinlineasarowmatrix,\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\nx= [ x 1 , x 2 , x 3].\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\ncanseethatascalarisitsowntranspose: a a= .\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "byperformingthatoperationoneachelementofamatrix:D= aB+ cwhere\nD i , j= a B i , j+ c.\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\nmatrix.Thisshorthandeliminatestheneedtodeneamatrixwithbcopiedinto\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\niscalled .broadcasting\n2.2MultiplyingMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "2.2MultiplyingMatricesandVectors\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\norderforthisproducttobedened,Amusthavethesamenumberofcolumnsas\nBhasrows.IfAisofshape m nandBisofshape n p,thenCisofshape\nm p.Wecanwritethematrixproductjustbyplacingtwoormorematrices\ntogether,e.g.\nCAB= . (2.4)\nTheproductoperationisdenedby\nC i , j=\nkA i , k B k, j . (2.5)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "C i , j=\nkA i , k B k, j . (2.5)\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\nThedotproductbetweentwovectorsxandyofthesamedimensionality\nisthematrixproductxy.WecanthinkofthematrixproductC=ABas\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\n3 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\nanalysisofmatricesmoreconvenient.Forexample,matrixm ultiplicationis\ndistributive:\nABCABAC (+) = + . (2.6)\nItisalsoassociative:\nABCABC ( ) = ( ) . (2.7)\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\nvectorsiscommutative:\nxyy= x . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )AB= BA. (2.9)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "( )AB= BA. (2.9)\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\nxy=\nxy\n= yx . (2.10)\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\nthereadershouldbeawarethatmanymoreexist.\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\nequations:\nAxb= (2.11)\nwhereA Rm n isaknownmatrix,b Rmisaknownvector,andx Rnisa",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "vectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\nconstraint.Wecanrewriteequationas:2.11\nA 1 : ,x= b 1 (2.12)\nA 2 : ,x= b 2 (2.13)\n. . . (2.14)\nA m , :x= b m (2.15)\nor,evenmoreexplicitly,as:\nA 1 1 , x 1+A 1 2 , x 2+ +A 1 , n x n= b 1 (2.16)\n3 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n\n100\n010\n001\n\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\nA 2 1 , x 1+A 2 2 , x 2+ +A 2 , n x n= b 2 (2.17)\n. . . (2.18)\nA m , 1 x 1+A m , 2 x 2+ +A m , n x n= b m . (2.19)\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\nequationsofthisform.\n2.3IdentityandInverseMatrices\nLinearalgebraoersapowerfultoolcalledmatrixinversionthatallowsusto\nanalyticallysolveequationformanyvaluesof. 2.11 A",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "analyticallysolveequationformanyvaluesof. 2.11 A\nTodescribematrixinversion,werstneedtodenetheconceptofanidentity\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\nn-dimensionalvectorsasI n.Formally,I n Rn n ,and\nx Rn,I nxx= . (2.20)\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\ndiagonalare1,whilealloftheotherentriesarezero.Seegureforanexample.2.2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "ThematrixinverseofAisdenotedasA 1,anditisdenedasthematrix\nsuchthat\nA 1AI= n . (2.21)\nWecannowsolveequationbythefollowingsteps: 2.11\nAxb= (2.22)\nA 1AxA=  1b (2.23)\nI nxA=  1b (2.24)\n3 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nxA=  1b . (2.25)\nOfcourse,thisprocessdependsonitbeingpossibletondA 1.Wediscuss\ntheconditionsfortheexistenceofA 1inthefollowingsection.\nWhenA 1exists,severaldierentalgorithmsexistforndingitinclosedform.\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\ntimesfordierentvaluesofb.However,A 1isprimarilyusefulasatheoretical\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "BecauseA 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\nestimatesof.x\n2.4LinearDependenceandSpan\nInorderforA 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\nsolutionsorinnitelymanysolutionsforsomevaluesofb.Itisnotpossibleto\nhavemorethanonebutlessthaninnitelymanysolutionsforaparticularb;if\nbothandaresolutionsthen xy\nzxy = +(1 )  (2.26)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "bothandaresolutionsthen xy\nzxy = +(1 )  (2.26)\nisalsoasolutionforanyreal. \nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\nofAasspecifyingdierentdirectionswecantravelfromtheorigin(thepoint\nspeciedbythevectorofallzeros),anddeterminehowmanywaysthereareof\nreachingb.Inthisview,eachelementofxspecieshowfarweshouldtravelin\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\ncolumn: i\nAx=\nix iA : , i . (2.27)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "column: i\nAx=\nix iA : , i . (2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\neachvectorv( ) ibyacorrespondingscalarcoecientandaddingtheresults:\n\nic iv( ) i. (2.28)\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\noftheoriginalvectors.\n3 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\nspacerangeortheof.A\nInorderforthesystemAx=btohaveasolutionforallvaluesofb Rm,\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\nimmediately thatAmusthaveatleast mcolumns,i.e., n m.Otherwise, the",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "dimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\n32matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\nifandonlyifliesonthatplane.b\nHaving n misonlyanecessaryconditionforeverypointtohaveasolution.\nItisnotasucientcondition,becauseitispossibleforsomeofthecolumnsto\nberedundant.Considera22matrixwherebothofthecolumnsareidentical.\nThishasthesamecolumnspaceasa21matrixcontainingonlyonecopyofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "replicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\nencompassallof R2,eventhoughtherearetwocolumns.\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors.Ifweaddavectortoasetthatisalinearcombinationof\ntheothervectorsintheset,thenewvectordoesnotaddanypointstothesets\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\nconditionisbothnecessaryandsucientforequationtohaveasolutionfor 2.11\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\nmcolumnsmayhavemorethanonesuchset.\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "equationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\nwayofparametrizing eachsolution.\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\nwithlinearlydependentcolumnsisknownas.singular\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\nequation.However,wecannotusethemethodofmatrixinversiontondthe\n3 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nsolution.\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\nalsopossibletodeneaninversethatismultipliedontheright:\nAA 1= I . (2.29)\nForsquarematrices,theleftinverseandrightinverseareequal.\n2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p .  R1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "||||x p=\ni| x i|p 1\np\n(2.30)\nfor p , p .  R1\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatises\nthefollowingproperties:\n  f() = 0 xx= 0\n  f(+) xy f f ()+x ()y(thetriangleinequality)\n ||  R , f (x) =  f()x\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\nEuclideandistancefromtheorigintothepointidentiedbyx.The L2normis",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "usedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\nthesquared L2norm,whichcanbecalculatedsimplyasxx.\nThesquared L2normismoreconvenienttoworkwithmathematically and\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\n3 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\nthe L1norm.The L1normmaybesimpliedto\n||||x 1=\ni| x i| . (2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthedierencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the  L1normincreasesby. ",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "awayfrom0by,the  L1normincreasesby. \nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\nelements.Someauthorsrefertothisfunctionasthe L0norm,butthisisincorrect\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\nscalingthevectorby doesnotchangethenumberofnonzeroentries.The L1\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\nOneothernormthatcommonlyarisesinmachinelearningisthe Lnorm,\nalsoknownasthemaxnorm.Thisnormsimpliestotheabsolutevalueofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "elementwiththelargestmagnitudeinthevector,\n||||x = max\ni| x i| . (2.32)\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\nFrobeniusnorm:\n|||| A F=\ni , jA2\ni , j , (2.33)\nwhichisanalogoustothe L2normofavector.\nThedotproductoftwovectorscanberewrittenintermsofnorms.Specically,\nxyx= |||| 2||||y 2cos  (2.34)\nwhereistheanglebetweenand.  xy\n2.6SpecialKindsofMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2.6SpecialKindsofMatricesandVectors\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\nthemaindiagonal.Formally,amatrixDisdiagonalifandonlyif D i , j=0for\n4 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nall i= j.Wehavealreadyseenoneexampleofadiagonalmatrix:theidentity\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\nisverycomputationally ecient.Tocomputediag(v)x,weonlyneedtoscaleeach\nelement x iby v i.Inotherwords,diag(v)x=vx.Invertingasquarediagonal",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "matrixisalsoecient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\nandinthatcase,diag(v) 1=diag([1 /v 1 , . . . ,1 /v n]).Inmanycases,wemay\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\nmatricestobediagonal.\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "possibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\nelementsofthevectorifiswiderthanitistall. D\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\nAA= . (2.35)\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\nA isavectorwith : unitvectorunitnorm\n||||x 2= 1 . (2.36)\nAvectorxandavectoryareorthogonaltoeachotherifxy= 0.Ifboth\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\northonormal.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "orthonormal.\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\nmalandwhosecolumnsaremutuallyorthonormal:\nAAAA= = I . (2.37)\n4 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThisimpliesthat\nA 1= A, (2.38)\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\nPaycarefulattentiontothedenitionoforthogonalmatrices.Counterintuitively,\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\n2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,orndingsomepropertiesofthemthatareuniversal,notcaused",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "bythewaywechoosetorepresentthem.\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\norinbinary,butitwillalwaysbetruethat12 = 223.Fromthisrepresentation\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\nintegermultipleofwillbedivisibleby. 12 3\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\nrepresentationofthematrixasanarrayofelements.\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\neigenvalues.\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv=  . (2.39)\nThescalar isknownastheeigenvaluecorrespondingtothiseigenvector.(One",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "canalsondalefteigenvectorsuchthatvA= v,butweareusually\nconcernedwithrighteigenvectors).\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s  R= 0.\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\nforuniteigenvectors.\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues {  1 , . . . ,  n}.Wemayconcatenateallofthe\n4 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n\u0000 \u0000 \u0000    \n\u0000\u0000\u0000 \n                     \n\u0000 \u0000 \u0000    \n\n\u0000\u0000\u0000\n   \n                                                     \nFigure2.3:Anexampleoftheeectofeigenvectorsandeigenvalues.Here,wehave\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue  1andv( 2 )with",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "eigenvalue  2. ( L e f t )Weplotthesetofallunitvectorsu R2asaunitcircle. ( R i g h t )We\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\ncanseethatitscalesspaceindirectionv( ) iby  i.\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavector= [  1 , . . . ,\n n].The ofisthengivenby eigendecompositionA\nAVV = diag() 1. (2.40)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "AVV = diag() 1. (2.40)\nWehaveseenthatconstructingmatriceswithspeciceigenvaluesandeigenvec-\ntorsallowsustostretchspaceindesireddirections.Ho wever,weoftenwantto\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspecicclassof\nmatricesthathaveasimpledecomposition.Specically,everyrealsymmetric\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\nandeigenvalues:\nAQQ = , (2.41)\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and isa\ndiagonalmatrix.Theeigenvalue i , iisassociatedwiththeeigenvectorincolumn i",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "ofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\nscalingspaceby  iindirectionv( ) i.Seegureforanexample.2.3\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof ",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\nifalloftheeigenvaluesareunique.\nTheeigendecompositionofamatrixtellsusmanyusefulfactsaboutthe\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\nquadraticexpressionsoftheform f(x) =xAxsubjectto||||x 2= 1.Wheneverx\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivedenite.A\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemide-\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedenite,and\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemidenite.Positive\nsemidenitematricesareinterestingbecausetheyguaranteethatxx ,Ax0.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "PositivedenitematricesadditionallyguaranteethatxAxx = 0  = 0.\n2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\n4 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdened,andwe\nmustuseasingularvaluedecompositioninstead.\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\namatrixVofeigenvectorsandavectorofeigenvaluessuchthatwecanrewrite\nAas\nAVV = diag() 1. (2.42)\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "asaproductofthreematrices:\nAUDV = . (2.43)\nSupposethatAisan m nmatrix.ThenUisdenedtobean m mmatrix,\nD V tobeanmatrix,and m n tobeanmatrix. n n\nEachofthesematricesisdenedtohaveaspecialstructure.ThematricesU\nandVarebothdenedtobeorthogonalmatrices.ThematrixDisdenedtobe\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\ncolumnsofareknownasasthe V right-singularvectors.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "columnsofareknownasasthe V right-singularvectors.\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\neigenvectorsofAA.Theright-singularvectorsofAaretheeigenvectorsofAA.\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAA.\nThesameistrueforAA.\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "section.\n2.9TheMoore-PenrosePseudoinverse\nMatrixinversionisnotdenedformatricesthatarenotsquare.Supposewewant\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\nAxy= (2.44)\n4 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nbyleft-multiplyingeachsidetoobtain\nxBy= . (2.45)\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\nuniquemappingfromto.AB\nIfAistallerthanitiswide,thenitispossibleforthisequationtohave\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\nsolutions.\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisdenedasamatrix A\nA+=lim\n  0(AAI+ ) 1A. (2.46)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "A+=lim\n  0(AAI+ ) 1A. (2.46)\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeni-\ntion,butrathertheformula\nA+= VD+U, (2.47)\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\nelementsthentakingthetransposeoftheresultingmatrix.\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\npseudoinverseprovidesoneofthemanypossiblesolutions.Specically,itprovides",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\nsolutions.\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\npossibletointermsofEuclideannorm y ||||Axy 2.\n2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =A\niA i , i . (2.48)\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\ndiculttospecifywithoutresortingtosummationnotationcanbespeciedusing",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "4 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\nanalternativewayofwritingtheFrobeniusnormofamatrix:\n|||| A F=\nTr(AA) . (2.49)\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\nmanipulatetheexpressionusingmanyusefulidentities.Forexample,thetrace\noperatorisinvarianttothetransposeoperator:\nTr() = Tr(AA) . (2.50)\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "movingthelastfactorintotherstposition,iftheshapesofthecorresponding\nmatricesallowtheresultingproducttobedened:\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\normoregenerally,\nTr(n\ni = 1F( ) i) = Tr(F( ) nn  1\ni = 1F( ) i) . (2.52)\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\ndierentshape.Forexample,forA Rm n andB Rn m ,wehave\nTr( ) = Tr( )ABBA (2.53)\neventhoughAB Rm m andBA Rn n .\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\n2.11TheDeterminant",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "2.11TheDeterminant\nThedeterminant ofasquarematrix,denoted det(A),isafunctionmapping\nmatricestorealscalars.Thedeterminant isequaltotheproductofallthe\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\nthetransformationpreservesvolume.\n4 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n2.12Example:PrincipalComponentsAnalysis\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\nWewouldliketoloseaslittleprecisionaspossible.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Wewouldliketoloseaslittleprecisionaspossible.\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\nofthem.Foreachpointx( ) i Rnwewillndacorrespondingcodevectorc( ) i Rl.\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\noriginaldata.Wewillwanttondsomeencodingfunctionthatproducesthecode\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\ninputgivenitscode, .xx  g f(())\nPCAisdenedbyourchoiceofthedecodingfunction.Specically,tomakethe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\ninto Rn.Let,where g() = cDcD Rn l isthematrixdeningthedecoding.\nComputingtheoptimalcodeforthisdecodercouldbeadicultproblem.To\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\ntoeachother.(NotethatDisstillnottechnicallyanorthogonalmatrixunless\nl n= )\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "theproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\nnorm.\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,therst\nthingweneedtodoisgureouthowtogeneratetheoptimalcodepointcfor\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\ninputpointxanditsreconstruction, g(c).Wecanmeasurethisdistanceusinga\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\nc= argmin\nc|| ||x g()c 2 . (2.54)\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "bothareminimizedbythesamevalueofc.Bothareminimizedbythesame\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\n4 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmonotonically increasingfornon-negative arguments.\nc= argmin\nc|| ||x g()c2\n2 . (2.55)\nThefunctionbeingminimizedsimpliesto\n( ())x gc( ())x gc (2.56)\n(bythedenitionofthe L2norm,equation)2.30\n= xxxg g ()c()cxc+( g)g()c (2.57)\n(bythedistributiveproperty)\n= xxx2g g ()+c ()cg()c (2.58)\n(becausethescalar g()cxisequaltothetransposeofitself).\nWecannowchangethefunctionbeingminimizedagain,toomittherstterm,\nsincethistermdoesnotdependon:c\nc= argmin",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "sincethistermdoesnotdependon:c\nc= argmin\nc2xg g ()+c ()cg .()c (2.59)\nTomakefurtherprogress,wemustsubstituteinthedenitionof: g()c\nc= argmin\nc2xDcc+DDc (2.60)\n= argmin\nc2xDcc+I lc (2.61)\n(bytheorthogonalityandunitnormconstraintson)D\n= argmin\nc2xDcc+c (2.62)\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\nyoudonotknowhowtodothis):\n c(2xDcc+c) = 0 (2.63)\n2Dxc+2= 0 (2.64)\ncD= x . (2.65)\n4 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThismakesthealgorithmecient:wecanoptimallyencodexjustusinga\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\nf() = xDx . (2.66)\nUsingafurthermatrixmultiplication, wecanalsodenethePCAreconstruction\noperation:\nr g f () = x (()) = xDDx . (2.67)\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\noferrorscomputedoveralldimensionsandallpoints:\nD= argmin\nD\ni , j\nx( ) i\nj r(x( ) i) j2\nsubjecttoDDI= l(2.68)\nToderivethealgorithmforndingD,wewillstartbyconsideringthecase\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\nd= argmin\nd\ni||x( ) iddx( ) i||2\n2subjectto||||d 2= 1 .(2.69)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "2subjectto||||d 2= 1 .(2.69)\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\nscalarvaluedx( ) iontherightofthevectord.Itismoreconventionaltowrite\nscalarcoecientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\nsuchaformulaas\nd= argmin\nd\ni||x( ) idx( ) id||2\n2subjectto||||d 2= 1 ,(2.70)\nor,exploitingthefactthatascalarisitsowntranspose,as\nd= argmin\nd\ni||x( ) ix( ) i dd||2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "d= argmin\nd\ni||x( ) ix( ) i dd||2\n2subjectto||||d 2= 1 .(2.71)\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\n5 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\nThiswillallowustousemorecompactnotation.LetX Rm n bethematrix\ndenedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i.\nWecannowrewritetheproblemas\nd= argmin\nd||XXdd||2\nFsubjecttodd= 1 .(2.72)\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\nportionasfollows:\nargmin\nd||XXdd||2\nF (2.73)\n= argmin\ndTr",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "argmin\nd||XXdd||2\nF (2.73)\n= argmin\ndTr\nXXdd \nXXdd \n(2.74)\n(byequation)2.49\n= argmin\ndTr(XXXXddddXXdd+XXdd)(2.75)\n= argmin\ndTr(XX)Tr(XXdd)Tr(ddXX)+Tr(ddXXdd)\n(2.76)\n= argmin\ndTr(XXdd)Tr(ddXX)+Tr(ddXXdd)(2.77)\n(becausetermsnotinvolvingdonotaectthe) d argmin\n= argmin\nd2Tr(XXdd)+Tr(ddXXdd)(2.78)\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n= argmin\nd2Tr(XXdd)+Tr(XXdddd)(2.79)\n(usingthesamepropertyagain)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "(usingthesamepropertyagain)\nAtthispoint,were-introducetheconstraint:\nargmin\nd2Tr(XXdd)+Tr(XXdddd)subjecttodd= 1(2.80)\n= argmin\nd2Tr(XXdd)+Tr(XXdd)subjecttodd= 1(2.81)\n(duetotheconstraint)\n= argmin\ndTr(XXdd)subjecttodd= 1(2.82)\n5 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n= argmax\ndTr(XXdd)subjecttodd= 1(2.83)\n= argmax\ndTr(dXXdd )subjecttod= 1(2.84)\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Specically,\ntheoptimaldisgivenbytheeigenvectorofXXcorrespondingtothelargest\neigenvalue.\nThisderivationisspecictothecaseof l=1andrecoversonlytherst\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\nwritingthisproofasanexercise.\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\n5 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 2\nA p p l i cat i on s\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\nspecicapplicationareasthatdeeplearninghasbeenusedtosolve.Whileone\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\nLanguagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature.\n12. 1 L arge- S c a l e D eep L earni n g\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\nimprovementinneuralnetworksaccuracyandtheimprovementofthecomplexity\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\ngrownexponentiallyforthepastthreedecades,yetarticialneuralnetworksare\nonlyaslargeasthenervoussystemsofinsects.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "onlyaslargeasthenervoussystemsofinsects.\nBecausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure.\n12.1.1FastCPUImplementations\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\nToday,thisapproachisgenerallyconsideredinsucient.WenowmostlyuseGPU\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "AdescriptionofhowtoimplementecientnumericalCPUcodeisbeyond\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\nspecicCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingxed-point\narithmeticratherthanoating-pointarithmetic.Bycreatingacarefullytunedxed-\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\nastrongoating-pointsystem.EachnewmodelofCPUhasdierentperformance",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "characteristics,sosometimesoating-pointimplementations canbefastertoo.\nTheimportantprincipleisthatcarefulspecializationofnumericalcomputation\nroutinescanyieldalargepayo.Otherstrategies,besideschoosingwhethertouse\nxedoroatingpoint,includeoptimizingdatastructurestoavoidcachemisses\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\nimplementationdetails,butwhentheperformanceofanimplementation restricts\nthesizeofthemodel,theaccuracyofthemodelsuers.\n12.1.2GPUImplementations",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "12.1.2GPUImplementations\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\nbenecialforneuralnetworksaswell.\nVideogamerenderingrequiresperformingmanyoperationsinparallelquickly.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "Modelsofcharactersand environmentsarespeciedintermsoflistsof3-D\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\nateachpixelinparalleltodeterminethecolorofeachpixel.Inbothcases,the\n4 4 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuersof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "memory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\nCPUs.\nNeuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\nlargeandnumerousbuersofparameters,activationvalues,andgradientvalues,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\nbuersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.\nGPUsoeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.\nNeuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\nnetworkscanbedividedintomultipleindividualneuronsthatcanbeprocessed",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "independentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\nbenetfromtheparallelismofGPUcomputing.\nGPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\ngraphicstasks.Overtime,GPUhardwarebecamemoreexible,allowingcustom\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\nbasedonarenderingtask.TheseGPUscouldbeusedforscienticcomputingby",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "writingtheoutputofacomputationtoabuerofpixelvalues.Steinkrau e t a l .\n()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\nacceleratesupervisedconvolutionalnetworks.\nThepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "code,notjustrenderingsubroutines.NVIDIAsCUDAprogramming language\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\n4 4 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nbandwidth,GP-GPUsnowoeranidealplatformforneuralnetworkprogramming.\nThisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\nWritingecientcodeforGP-GPUsremainsadiculttaskbestlefttospe-\ncialists.ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\ndierentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "writablememorylocationsarenotcached,soitcanactuallybefastertocompute\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.\nGPUcodeisalsoinherentlymulti-threaded andthedierentthreadsmustbe\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\ntransaction.DierentmodelsofGPUsareabletocoalescedierentkindsofread",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "orwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\nof2.TheexactspecicationsdierbetweenmodelsofGPU.Anothercommon\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\nsameinstructionsimultaneously.Thismeansthatbranchingcanbediculton\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\nexecutesthesameinstructionduringeachcycle,soifdierentthreadswithinthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "samewarpneedtoexecutedierentcodepaths,thesedierentcodepathsmust\nbetraversedsequentiallyratherthaninparallel.\nDuetothedicultyofwritinghighperformanceGPUcode,researchersshould\nstructuretheirworkowtoavoidneedingtowritenewGPUcodeinordertotest\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "machinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciesallofits\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l .\n2011b)providesimilarfeatures.\n4 4 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.1.3Large-ScaleDistributedImplementations\nInmanycases,thecomputational resourcesavailableonasinglemachineare\ninsucient.Wethereforewanttodistributetheworkloadoftrainingandinference\nacrossmanymachines.\nDistributinginferenceissimple,becauseeachinputexamplewewanttoprocess\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\ntogetheronasingledatapoint,witheachmachinerunningadierentpartofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model.Thisisfeasibleforbothinferenceandtraining.\nDataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\nthestandarddenitionofgradientdescentisasacompletelysequentialalgorithm:\nthegradientatstepisafunctionoftheparametersproducedbystep. t t1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.\nThisreducestheaverageamountofimprovementthateachgradientdescentstep\nyields,becausesomeofthecoresoverwriteeachothersprogress,buttheincreased\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\n2015).Academicdeeplearningresearcherstypicallycannotaordthesamescale\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "distributednetworkswithrelativelylow-costhardwareavailableintheuniversity\nsetting( ,). Coates e t a l .2013\n12.1.4ModelCompression\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\n4 4 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\npowerfulcomputercluster,thendeployitonmobilephones.\nAkeystrategyforreducingthecostofinferenceismodelcompression(Bu-\ncilua2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "storeandevaluate.\nModelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\nprimarilybyaneedtopreventovertting.Inmostcases,themodelwiththe\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.\nEvaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\nTheselargemodelslearnsomefunction f(x),butdosousingmanymore\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thelimitednumberoftrainingexamples.Assoonaswehavetthisfunction\nf(x),wecangenerateatrainingsetcontaininginnitelymanyexamples,simply\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\nmodeltomatch f(x)onthesepoints.Inordertomostecientlyusethecapacity\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "modeltrainedontheoriginaltrainingset.\nAlternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).\n12.1.5DynamicStructure\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\ncomputation(,; ,).Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n4 4 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\nDynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\ngenerallythroughoutthesoftwareengineeringdiscipline.Thesimplestversions\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\nbeappliedtoaparticularinput.\nAvenerablestrategyforacceleratinginferenceinaclassieristouseacascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ofclassiers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\nwemustuseasophisticatedclassierwithhighcapacity,thatisexpensivetorun.\nHowever,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\nasequenceofclassiers.Therstclassiersinthesequencehavelowcapacity,\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "wedonotwronglyrejectaninputwhentheobjectispresent.Thenalclassier\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\nclassiersinasequence,abandoninganyexampleassoonasanyoneelementin\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\nhighcondence,usingahighcapacitymodel,butdoesnotforceustopaythecost\noffullinferenceforeveryexample.Therearetwodierentwaysthatthecascade\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\nhighcapacity,becausesomeofitsindividualmembersdo.Itisalsopossibleto\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassier\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\nthatrstlocatestheaddressnumberwithonemachinelearningmodelandthen\ntranscribesitwithanother(Goodfellow2014d e t a l .,).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\nAsimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n4 4 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\ndonewiththeprimarygoalofacceleratinginferencecomputations.\nInthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\ngiventhecurrentinput.Therstversionofthisideaiscalledthemixtureof\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\nandthenaloutputisobtainedbytheweightedcombinationoftheoutputof\ntheexperts.Inthatcase,theuseofthegaterdoesnotoerareductionin\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "whenwewanttoselectdierentsubsetsofunitsorparameters,itisnotpossible\ntouseasoftswitchbecauseitrequiresenumerating(andcomputingoutputsfor)\nallthegatercongurations. Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l .\n()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\nqualityoftheapproximation.\nAnotherkindofdynamicstructureisaswitch,whereahiddenunit can\nreceiveinputfromdierentunitsdependingonthecontext.Thisdynamicrouting\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotproveneectiveonlarge-scaleapplications.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\nandthusdonotachieveallofthepossiblecomputational benetsofdynamic\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\ndegreeofparallelismthatresultsfromthesystemfollowingdierentcodebranches\nfordierentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "canwritemorespecializedsub-routinesthatconvolveeachexamplewithdierent\nkernelsormultiplyeachrowofadesignmatrixbyadierentsetofcolumns\nofweights.Unfortunately,thesemorespecializedsubroutinesaredicultto\nimplementeciently.CPUimplementations willbeslowduetothelackofcache\n4 5 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\ndierentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\nofexamplessimultaneously.Thiscanbeanacceptablestrategyforminimizing\nthetimerequiredtoprocessaxedamountofexamplesinanoinesetting.In",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "areal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\nmachinetoprocesstherststepinacascadeandanothermachinetoprocess\nthelaststepinacascade,thentherstwilltendtobeoverloadedandthelast\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\nimplementdierentnodesofaneuraldecisiontree.\n12.1.6SpecializedHardwareImplementationsofDeepNetworks",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l .\n2003MisraandSaha2010 ; ,).\nDierentformsofspecializedhardware(GrafandJackel1989Meadand ,;\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "beendevelopedoverthelastdecades,eitherwithASICs(application-specicinte-\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\n(combiningdigitalandanalogcomponents).InrecentyearsmoreexibleFPGA\n(eldprogrammable gatedarray)implementations(wheretheparticularsofthe\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\nandGPUs)typicallyuse32or64bitsofprecisiontorepresentoatingpoint\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n4 5 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\nGPUs).Thisisverydierentfromthesituationofthe1990s(thepreviousneural\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\nnaturallanguage).\nRecentworkonlow-precisionimplementationsofbackprop-based neuralnets\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\nthatbetween8and16bitsofprecisioncansuceforusingortrainingdeep\nneuralnetworkswithback-propagation.Whatisclearisthatmoreprecisionis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\nxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\nrequiredpernumber.Traditionalxedpointnumbersarerestrictedtoaxed\nrange(whichcorrespondstoagivenexponentinaoatingpointrepresentation).\nDynamicxedpointrepresentationssharethatrangeamongasetofnumbers\n(suchasalltheweightsinonelayer).Usingxedpointratherthanoatingpoint\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "powerrequirementsandcomputingtimeneededforperformingmultiplications,\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\ntrainamoderndeepnetworkwithbackprop.\n12. 2 C om p u t er V i s i on\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\ndeeplearningapplications,becausevisionisataskthatiseortlessforhumans\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "ofobjectrecognitionoropticalcharacterrecognition.\nComputervisionisaverybroadeldencompassingawidevarietyofways\nofprocessingimages,andanamazingdiversityofapplications.Applicationsof\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\n4 5 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\nremovingobjectsfromimages.\n12.2.1Preprocessing\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\ninputcomesinaformthatisdicultformanydeeplearningarchitecturesto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\nsame,reasonablerange,like[0,1]or[-1,1].Mixingimagesthatliein[0,1]with\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\ncroppedorscaledtotthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\nimage( ,). Hadsell e t a l .2007\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\nthemodelmanydierentversionsofthesameinput(forexample,thesameimage\ncroppedatslightlydierentlocations)andhavethedierentinstantiationsofthe\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\nensembleapproach,andhelpstoreducegeneralization error.\nOtherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n4 5 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\nthemodelneededtotthetrainingset.Simplertasksmaybesolvedbysmaller\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\niscondenthasnorelevancetothetask.Whentrainingwithlargedatasetsand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "largemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\ne t a l .,).2012\n12.2.1.1ContrastNormalization\nOneofthemostobvioussourcesofvariationthatcanbesafelyremovedfor\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthedierencebetweenthebrightandthedarkpixelsinanimage.\nTherearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\nX Rr c3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\nentireimageisgivenby\n1\n3 r cr \ni=1c \nj=13 \nk=1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "1\n3 r cr \ni=1c \nj=13 \nk=1\nX i , j , k X2(12.1)\nwhere  Xisthemeanintensityoftheentireimage:\n X=1\n3 r cr \ni=1c \nj=13 \nk=1X i , j , k . (12.2)\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage,then\nrescalingitsothatthestandarddeviationacross itspixelsisequaltosome\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "intensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n4 5 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\nmotivatesintroducingasmall,positiveregularizationparameter tobiasthe\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\ntobeatleast .Givenaninputimage X,GCNproducesanoutputimage X,\ndenedsuchthat\nX\ni , j , k= sX i , j , k X\nmax\n ,\n+1\n3 r cr\ni=1c\nj=13\nk=1\nX i , j , k X2 .(12.3)\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\ntopracticallyignorethesmalldenominator problembysetting = 0andavoid\ndivisionby0inextremelyrarecasesbysetting toanextremelylowvaluelike\n108.Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\n  = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\npreferabletodeneGCNintermsofstandarddeviationratherthan L2norm\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "basedonstandarddeviationallowsthesame stobeusedregardlessofimage\nsize.However,theobservationthatthe L2normisproportionaltothestandard\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\nexamplestoasphericalshell.Seegureforanillustration.Thiscanbea 12.1\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdierent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "biases.Suchcoordinationcanbedicultforthelearningalgorithmtodiscover.\nAdditionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\nreducingeachexampletoadirectionratherthanadirectionandadistance.\nCounterintuitively,thereisapreprocessingoperationknownasspheringand\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\n4 5 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n 1 5 0 0 1 5 . . .\nx 0 1 5 .0 0 .1 5 .x 1Rawinput\n 1 5 0 0 1 5 . . .\nx 0GCN, = 10  2\n 1 5 0 0 1 5 . . .\nx 0GCN, = 0 \nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\n( C e n t e r )GCNwith = 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\ns= 1and = 10 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "GCN,with  >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore. s \nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\nsphericalcontours.Spheringismorecommonlyknownas .whitening\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\nlargedierencebetweenthebrightnessofthedarkareaandthebrightnessofthe\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.\nThismotivateslocalcontrastnormalization.Localcontrastnormalization\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\ntheimageasawhole.Seegureforacomparisonofglobalandlocalcontrast 12.2\nnormalization.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "normalization.\nVariousdenitionsoflocalcontrastnormalization arepossible.Inallcases,\nonemodieseachpixelbysubtractingameanofnearbypixelsanddividingby\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\npixeltobemodied(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\nbemodied.Inthecaseofcolorimages,somestrategiesprocessdierentcolor\n4 5 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInputimage GCN LCN\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeects\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\ncontrastnormalizationmodiestheimagemuchmore,discardingallregionsofconstant\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofnetexture,\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "normalizationkernelbeingtoohigh.\nchannelsseparatelywhileotherscombineinformationfromdierentchannelsto\nnormalizeeachpixel( ,). Sermanet e t a l .2012\nLocalcontrastnormalization canusuallybeimplemented ecientlybyusing\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\ndivisionondierentfeaturemaps.\nLocalcontrastnormalization isadierentiable operationandcanalsobeusedas",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\noperationappliedtotheinput.\nAswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.\n4 5 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.2.1.2DatasetAugmentation\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassier 7.4\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\nexamplesthathavebeenmodiedwithtransformationsthatdonotchangethe\nclass.Objectrecognitionisaclassicationtaskthatisespeciallyamenableto\nthisformofdatasetaugmentationbecausetheclassisinvarianttosomany\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "operations.Asdescribedbefore,classierscanbenetfromrandomtranslations,\nrotations,andinsomecases,ipsoftheinputtoaugmentthedataset.Inspecialized\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12. 3 S p eec h R ec ogn i t i o n\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "asequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\ntaskconsistsofcreatingafunction f\nASRthatcomputesthemostprobablelinguistic\nsequencegiventheacousticsequence: y X\nf\nASR() = argmaxX\nyP( = ) y X|X (12.4)\nwhere PisthetrueconditionaldistributionrelatingtheinputsXtothetargets\ny.\nSincethe1980sanduntilabout20092012,state-of-theartspeechrecognition\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "phonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987\nTheGMM-HMMmodelfamilytreatsacousticwaveformsasbeinggenerated\nbythefollowingprocess:rstanHMMgeneratesasequenceofphonemesand\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\n4 5 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\nspeechrecognitionwasactuallyoneoftherstareaswhereneuralnetworkswere\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performanceofASRbasedonneuralnetsapproximately matchedtheperformance\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\nphonemestodiscriminatebetween),whichwasbetterthanorcomparableto\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\nHowever,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "speechrecognitionandtheeortthathadbeeninvestedinbuildingthesesystems\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\nLater,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "forthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).\nStartingin2009,speechresearchersappliedaformofdeeplearningbasedon\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "Thesenetworkstakespectralacousticrepresentationsinaxed-sizeinputwindow\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosignicantlyimprove\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "recognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n4 5 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\nontechniquessuchasrectiedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\ne t a l .,).Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\narenowdeployedinproductssuchasmobilephones.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "arenowdeployedinproductssuchasmobilephones.\nLater,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\nunnecessaryordidnotbringanysignicantimprovement.\nThesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "longperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\ntrainingsets(seegure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\nalgorithmsandarchitectures forASR,whichisstillongoingtoday.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l .\n2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\nfrequencyofspectralcomponents.\nAnotherimportantpush,stillongoing,hasbeentowardsend-to-enddeep",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "learningspeechrecognitionsystemsthatcompletelyremovetheHMM.Therst\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding.This",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\nappliedinothersettings.\nAnothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\nsystemlearnhowtoaligntheacoustic-levelinformationwiththephonetic-level\n4 6 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\nspecializedlanguagesdesignedtoallowecientandunambiguousparsingbysimple\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\ndescription.Naturallanguageprocessingincludesapplicationssuchasmachine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "translation,inwhichthelearnermustreadasentenceinonehumanlanguageand\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\narebasedonlanguagemodelsthatdeneaprobabilitydistributionoversequences\nofwords,charactersorbytesinanaturallanguage.\nAswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\nHowever,toachieveexcellentperformanceandtoscalewelltolargeapplications,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "somedomain-specicstrategiesbecomeimportant.Tobuildanecientmodelof\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\nbeendevelopedtomakemodelsofsuchaspaceecient,bothinacomputational",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andinastatisticalsense.\n12.4.1-grams n\nAlanguagemodeldenesaprobabilitydistributionoversequencesoftokens\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\nearliestsuccessfullanguagemodelswerebasedonmodelsofxed-lengthsequences\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\nModelsbasedon n-gramsdenetheconditionalprobabilityofthe n-thtoken\ngiventhepreceding n1tokens.Themodelusesproductsoftheseconditional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "distributionstodenetheprobabilitydistributionoverlongersequences:\nP x(1 , . . . , x ) = ( P x1 , . . . , x n1)\nt n=P x( t| x t n+1 , . . . , x t1) .(12.5)\n4 6 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nThisdecompositionisjustiedbythechainruleofprobability.Theprobability\ndistributionovertheinitialsequence P( x1 , . . . , x n1)maybemodeledbyadierent\nmodelwithasmallervalueof. n\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "Mercer1980Katz1987ChenandGoodman1999 ,;,; ,).\nForsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\nfor n=2,andtrigramfor n=3.ThesenamesderivefromtheLatinprexesfor\nthecorrespondingnumbersandtheGreeksux-gramdenotingsomethingthat\niswritten.\nUsuallywetrainbothan n-grammodelandan n1 grammodelsimultaneously.\nThismakesiteasytocompute\nP x( t| x t n+1 , . . . , x t1) =P n( x t n+1 , . . . , x t)\nP n1( x t n+1 , . . . , x t1)(12.6)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "P n1( x t n+1 , . . . , x t1)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomitthenalcharacterfromeachsequencewhenwe\ntrain P n1.\nAsanexample,wedemonstratehowatrigrammodelcomputestheprobability\nofthesentenceTHEDOGRANAWAY.Therstwordsofthesentencecannotbe\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "abilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N).\nFinally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\nweobtain:\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N .\n(12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thoughthetuple ( x t n+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\ndierentkindsofcatastrophicoutcomes.When P n1iszero,theratioisundened,\nsothemodeldoesnotevenproduceasensibleoutput.When P n1isnon-zerobut\nP niszero,thetestlog-likelihoodis.Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\nSee ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\nsymbolvalues.ThismethodcanbejustiedasBayesianinferencewithauniform\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "higher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\nmorelikelytoavoidcountsofzero.Back-omethodslook-upthelower-order\nn-gramsifthefrequencyofthecontext x t1 , . . . , x t n+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k + , . . . , x t1,forincreasing k,untilasucientlyreliableestimateis\nfound.\nClassical n-grammodelsareparticularlyvulnerabletothecurseofdimension-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "ality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset.\nOnewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\nisevenmoreseverethanusual,becauseanytwodierentwordshavethesamedis-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "tancefromeachotherinone-hotvectorspace.Itisthusdiculttoleveragemuch\ninformationfromanyneighborsonlytrainingexamplesthatrepeatliterallythe\nsamecontextareusefulforlocalgeneralization.T oovercometheseproblems,a\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\nsemanticallysimilarwords.\nToimprovethestatisticaleciencyof n-grammodels,class-basedlanguage\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-oare\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\nlostinthisrepresentation.\n4 6 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.4.2NeuralLanguageModels\nNeurallanguagemodelsorNLMsareaclassoflanguagemodeldesigned\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\nsequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "word(anditscontext)andothersimilarwordsandcontexts.Thedistributed\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "transferringinformationfromeachtrainingsentencetoanexponentiallylarge\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\nexponentialnumberofsimilarsentences.\nWesometimescallthesewordrepresentationswordembeddings.Inthis\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\naone-hotvector,soeverypairofwordsisatEuclideandistance\n2fromeach\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n(oranypairofwordssharingsomefeatureslearnedbythemodel)arecloseto\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\nFigurezoomsinonspecicareasofalearnedwordembeddingspacetoshow 12.3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\nNeuralnetworksinotherdomainsalsodeneembeddings.Forexample,a\nhiddenlayerofaconvolutionalnetworkprovidesanimageembedding.Usually\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\nrepresented.\nThebasicideaofusingdistributedrepresentationstoimprovemodelsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n4 6 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmultiplelatentvariables(MnihandHinton2007,).\n     343230282614131211109876\nCanadaEuropeOntario\nNorthEnglish\nCanadianUnionAfricanAfrica\nBritishFrance\nRussianChina\nGermanyFrench\nAssemblyEU JapanIraq\nSouthEuropean\n350355360365370375380 . . . . . . .171819202122\n1995199619971998199920002001\n200220032004\n20052006200720082009\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "machinetranslationmodel( ,),zoominginonspecicareaswhere Bahdanau e t a l .2015\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\n12.4.3High-DimensionalOutputs\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\nrepresentingsuchadistributionistoapplyananetransformationfromahidden\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "componentofthisanetransformationisverylarge,becauseitsoutputdimension\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\ntimeaswellastesttimewecannotcalculateonlythedotproductwiththeweight\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "attesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\n4 6 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlossfunctions,thegradientcanbecomputedeciently( ,),but Vincent e t a l .2015\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\nmanydiculties.\nSupposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\ny.IfweparametrizethetransformationfromhtoywithlearnedweightsW\nandlearnedbiasesb,thentheane-softmaxoutputlayerperformsthefollowing\ncomputations:\na i= b i+\njW i j h j{ ||} i1 , . . . , V , (12.8)\n y i=ea i\n|| V\ni=1 eai . (12.9)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": " y i=ea i\n|| V\ni=1 eai . (12.9)\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\ncomputationofmostneurallanguagemodels.\n12.4.3.1UseofaShortList\nTherstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "builtuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\nfrequentwords(handledbytheneuralnet)andatail T= V L\\ofmorerarewords\n(handledbyan n-grammodel).Tobeabletocombinethetwopredictions,the\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\nunittoprovideanestimateof P( i C | T ).Theextraoutputcanthenbeusedto\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "P y i C (= |) =1 i L P y i C, i P i C (= |   L)(1 (| T ))\n+1 i T P y i C, i P i C (= |  T)(| T )(12.10)\nwhere P( y= i C, i|  L)isprovidedbytheneurallanguagemodeland P( y= i|\nC, i T) isprovidedbythe n-grammodel.Withslightmodication,thisapproach\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodelssoftmax\nlayer,ratherthanaseparatesigmoidunit.\nAnobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "4 6 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nwords,where,arguably,itistheleastuseful.Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow.\n12.4.3.2HierarchicalSoftmax\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "the|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\nmodels.\nOnecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.\nThesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\nthetreehasdepth O(log|| V).Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "everynodeonapathfromtherootofthetreetotheleafcontainingtheword.\nFigureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\nsummationoverallofthepathsthatleadtothatword.\nTopredicttheconditionalprobabilities requiredateachnodeofthetree,we\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "samecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\nBecausetheoutputlog-likelihoodcanbecomputedeciently(aslowaslog|| V\nratherthan|| V),itsgradientsmayalsobecomputedeciently.Thisincludesnot\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "withrespecttothehiddenlayeractivations.\nItispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\n4 6 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n( 1) ( 0)\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspecicwords.\nInternalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "ofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissucientlybalanced,\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\nthenumberofwords|| V:thechoiceofoneoutof|| Vwordscanbeobtainedbydoing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "O(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "nodeindexedbytheprexofthesebits.Forexample,node(1 ,0)correspondstothe\nprex( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\n4 6 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npractice,thecomputational savingsaretypicallynotworththeeortbecausethe\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\nactivationsgrowsasas O( l n2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "activationsgrowsasas O( l n2\nh)whiletheoutputcomputations growas O( n h n b).\nAslongas n b l n h,wecanreducecomputationmorebyshrinking n hthanby\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\nexceedsamillionwordsandlog2(106)20,itispossibletoreduce n btoabout,20\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\natreewithabranchingfactorof,onecaninsteaddeneatreewithdepthtwo 2\nandabranchingfactorof\n|| V.Suchatreecorrespondstosimplydeningaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "|| V.Suchatreecorrespondstosimplydeningaset\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\ntwocapturesmostofthecomputational benetofthehierarchicalstrategy.\nOnequestionthatremainssomewhatopenishowtobestdenetheseword\nclasses,orhowtodenethewordhierarchyingeneral.Earlyworkusedexisting\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\njointlywiththeneurallanguagemodel.Learningthehierarchyisdicult.Anexact",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "optimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization. However,\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\nwordsintowordclasses.\nAnimportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\ntionalbenetsbothattrainingtimeandattesttime,ifattesttimewewantto\ncomputetheprobabilityofspecicwords.\nOfcourse,computingtheprobabilityofall|| Vwordswillremainexpensive",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "evenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\nprovideanecientandexactsolutiontothisproblem.\nAdisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses.\n12.4.3.3ImportanceSampling\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "computingthecontributionofthegradientfromallofthewordsthatdonotappear\n4 6 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\ninequation,thegradientcanbewrittenasfollows: 12.8\n P y C log(|)\n =logsoftmax y()a\n (12.13)\n=\n logea y\n\ni ea i(12.14)\n=\n ( a ylog\niea i) (12.15)\n= a y\n \niP y i C (= |) a i\n (12.16)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "= a y\n \niP y i C (= |) a i\n (12.16)\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\nperword.Thersttermisthepositivephaseterm(pushing a yup)whilethe\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\nSamplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\nwhichispreciselywhatwearetryingtoavoid.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "whichispreciselywhatwearetryingtoavoid.\nInsteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\nSncal2003BengioandSncal2008 ,; ,).Thisisanapplicationofamoregeneral\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\ninsection.Unfortunately,evenexactimportancesamplingisnotecient 17.2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "becauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\nassociatedgradientisweightedby\nw i=p n i /q n iN\nj=1 p n j /q n j. (12.17)\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\n4 7 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ngradient:\n|| V\ni=1P i C(|) a i\n 1\nmm \ni=1w i a n i\n . (12.18)\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\ntheparameters,itisalsopossibletosamplefromsuchadistributionveryeciently.\nImportancesamplingisnotonlyusefulforspeedingupmodelswithlarge\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.\nMachinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\ncomputational benettousingsparseoutputs,becausethemodelmaychooseto\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\nDauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\nimportancesampling.Theecientalgorithmminimizesthelossreconstructionfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "thepositivewords(thosethatarenon-zerointhetarget)andanequalnumber\nofnegativewords.Thenegativewordsarechosenrandomly,usingaheuristicto\nsamplewordsthataremorelikelytobemistaken.Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.\nInallofthesecases,thecomputational complexityofgradientestimationfor\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\nratherthanproportionaltothesizeoftheoutputvector.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "ratherthanproportionaltothesizeoftheoutputvector.\n12.4.3.4Noise-ContrastiveEstimationandRankingLoss\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\n4 7 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nscores a i.Therankinglossproposedthenis\nL=\nimax(01 , a y+ a i) . (12.19)\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n(includingconditionaltextgenerationtaskssuchastranslation).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\nandKavukcuoglu2013,).\n12.4.4CombiningNeuralLanguageModelswith-grams n\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\nofcapacity.Incomparison,doublinganeuralnetworksnumberofparameters\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "networks,canaddparameterswhilereducingthedegreeofparametersharing\ninordertomaintainthesameamountofcomputation. However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters.\nOneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "theensemblemembersmakeindependentmistakes.Theeldofensemblelearning\nprovidesmanywaysofcombiningtheensemblememberspredictions,including\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\nItisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\n4 7 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\nishugethenewportionofthearchitecturecontainsupto|| s Vnparametersbut\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\ntheextrainputsareverysparse.\n12.4.5NeuralMachineTranslation",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage.Mac hine\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\ntranslationswillnotbegrammaticalduetodierencesbetweenthelanguages.For\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "directlytheyyieldphrasessuchasapplered.Theproposalmechanismsuggests\nmanyvariantsofthesuggestedtranslation,ideallyincludingredapple.Asecond\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\ntranslations,andcanscoreredappleasbetterthanapplered.\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "usedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\nmachinetranslationincludenotjusttraditionalback-o n-grammodels(Jelinek\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\nentropylanguagemodels(,),inwhichanane-softmaxlayer Berger e t a l .1996\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\nthatdenesamarginaldistributionoversomevariabletodeneaconditional\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "inthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels. n\n4 7 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nD e c ode rO ut putob j e c t  ( E ngl i s h\ns e nt e nc e )\nI nt e r m e di at e ,  s e m a n t i c  r e pr e s e nt a t i o n\nSourc e  ob j e c t  ( F r e nc h s e n t e nc e  or  i m a g e )E nc ode r\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\nByusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\ntomachinetranslationbutalsotocaptiongenerationfromimages.\nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\npreprocessedtobeofxedlength.Tomakethetranslationmoreexible,wewould",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "liketouseamodelthatcanaccommodatevariablelengthinputsandvariable\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\nwhentheinputisasequence.Inallcases,onemodelrstreadstheinputsequence\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\nsummarythecontext C.Thecontext Cmaybealistofvectors,oritmaybea",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "vectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\nnetwork(KalchbrennerandBlunsom2013,).Asecondmodel,usuallyanRNN,\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\ningure.12.5\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\nmodelmusthaveawaytorepresenttheentiresourcesentence.Earliermodels",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wereonlyabletorepresentindividualwordsorphrases.Fromarepresentation\n4 7 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\nexploredrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\n( t  1 )( t  1 )( ) t( ) t( + 1 ) t( + 1 ) t\nh( t  1 )h( t  1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\n     +\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\noffeaturevectorsh( ) twithweights ( ) t.Insomeapplications,thefeaturevectorshare",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\nweights ( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\napproximatesreadingthatonespecictimestepprecisely.Theweights ( ) tareusually\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "indexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\nattentionmechanismbasedonweightedaveragesisasmooth,dierentiableapproximation\nthatcanbetrainedwithexistingoptimizationalgorithms.\nUsingaxed-sizerepresentationtocaptureallthesemanticdetailsofavery\nlongsentenceofsay60wordsisverydicult.Itcanbeachievedbytraininga\nsucientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\ne t a l .()and2014aSutskever2014 e t a l .().However,amoreecientapproachis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n4 7 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\nfocusingonadierentpartoftheinputsentenceinordertogatherthesemantic\ndetailsthatarerequiredtoproducethenextoutputword.Thatisexactlythe\nideathat ()rstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\ntofocusonspecicpartsoftheinputsequenceateachtimestepisillustratedin\ngure.12.6\nWecanthinkofanattention-basedsystemashavingthreecomponents:",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "1.Aprocessthat r e a d srawdata(suchassourcewordsinasourcesentence),\nandconvertsthemintodistributedrepresentations,withonefeaturevector\nassociatedwitheachwordposition.\n2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\nunderstoodasacontainingasequenceoffacts,whichcanbe m e m o r y\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\nofthem.\n3.Aprocessthatthecontentofthememorytosequentiallyperform e x p l o i t s",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "atask,ateachtimestephavingtheabilityputattentiononthecontentof\nonememoryelement(orafew,withadierentweight).\nThethirdcomponentgeneratesthetranslatedsentence.\nWhenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "wordembeddingsinanother(Koisk2014 e t a l .,),yieldingloweralignmenterror\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\nThereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\n2012).Manyextensionstothisapproacharepossible.Forexample,moreecient\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\n12.4.6HistoricalPerspective\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()inoneoftherstexplorationsofback-propagation, withsymbols 1986a\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\nsuchas(Colin,Mother,Victoria).The rstlayeroftheneuralnetworklearned\narepresentationofeachfamilymember.Forexample,thefeaturesforColin\n4 7 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\nthemotherofColin.\nTheideaofforminganembeddingforasymbolwasextendedtotheideaofan\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\nThehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\npopularityofdierentwaysofrepresentingtheinputtothemodel.Following\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\ntheinputasasequenceofcharacters.\nBengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "neuralmodelshavescaledupfromdeningrepresentationsofasmallsetofsymbols\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\nmodernapplications.Thiscomputational scalingeortledtotheinventionofthe\ntechniquesdescribedaboveinsection.12.4.3\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\nimprovedlanguagemodeling performance( ,).Tothisday, Bengio e t a l .2001\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\nmodelingindividualbytesofUnicodecharacters.\nTheideasbehindneurallanguagemodelshavebeenextendedintoseveral\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\ntasks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "tasks.\nTwo-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proleappli-\ncationtovisualizationwordembeddingsbyJosephTurianin2009.\n4 7 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12. 5 O t h er A p p l i c a t i o n s\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\naredierentfromthestandardobjectrecognition,speechrecognitionandnatural\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\nscopeevenfurthertotasksthatremainprimarilyresearchareas.\n12.5.1RecommenderSystems\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\nmaderegardingthatproducttothatuser.Theinternetiscurrentlynancedin",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "greatpartbyvariousformsofonlineadvertising.Therearemajorpartsofthe\neconomythatrelyononlineshopping.CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .\nSometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\nforvideogames,ormatchingpeopleindatingservices.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "forvideogames,ormatchingpeopleindatingservices.\nOften,thisassociationproblemishandledlikeasupervisedlearningproblem:\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\ninterest(userclicksonad,userentersarating,userclicksonalikebutton,user\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "classicationproblem(predictingtheconditionalprobabilityofsomediscrete\nevent).\nTheearlyworkonrecommendersystemsreliedonminimalinformationas\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\nthetargetvariablefordierentusersorfordierentitems.Supposethatuser1\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n4 7 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\nthenameofcollaborativeltering.Bothnon-parametric approaches(suchas\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\ndependonlyoneithertheuserIDortheitemID).LetRbethematrixcontaining\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\npredictionisthusobtainedasfollows:\n R u , i= b u+ c i+\njA u , j B j , i . (12.20)\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\n R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\nconvenientlyvisualizedwhentheyarerstreducedtoalowdimension(twoor\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "likewordembeddings.One waytoobtaintheseembeddingsisbyperforminga\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings).\nThiscorrespondstofactorizingR=UDV(oranormalizedvariant)intothe\nproductoftwofactors,thelowerrankmatricesA=UDandB=V.One\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\nbasedoptimization. TheSVDandthebilinearpredictionofequation both12.20\nperformedverywellinthecompetitionfortheNetixprize( , BennettandLanning\n2007),aimingatpredictingratingsforlms,basedonlyonpreviousratingsby\nalargesetofanonymoususers.Manymachinelearningexpertsparticipatedin\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "improvementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\n4 7 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npresentedbymostofthecompetitors,includingthewinners( ,; Tscher e t a l .2009\nKoren2009,).\nBeyondthesebilinearmodelswithdistributedrepresentations,oneoftherst\nusesofneuralnetworksforcollaborativelteringisbasedontheRBMundirected\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\noftheensembleofmethodsthatwontheNetixcompetition(Tscher2009 e t a l .,;\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\nMnih2008,).\nHowever,thereisabasiclimitationofcollaborativelteringsystems:whena\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\nthecold-startrecommendation problemistointroduceextrainformationabout",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "theindividualusersanditems.Forexample,thisextrainformationcouldbeuser\nproleinformationorfeaturesofeachitem.Systems thatusesuchinformation\narecalledcontent-basedrecommendersystems.Themappingfromarich\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,).\nSpecializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "audiotracks,formusicrecommendation (vandenOrd2013 e t a l .,).Inthatwork,\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.\n12.5.1.1ExplorationVersusExploitation\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\ntheywererecommendedandnottotheotheritems.Inaddition,insomecases\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\n4 8 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\nwouldbeliketrainingaclassierbypickingoneclass yforeachtrainingexample\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "label yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "someinputvariablethatcaninformthedecision.Forexample,weatleastknow\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\nissueinthereinforcementlearningandbanditsliterature.\nReinforcementlearningrequireschoosingatradeobetweenexplorationand\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "bestversionofthelearnedpolicyactionsthatweknowwillachieveahighreward.\nExplorationreferstotakingactionsspecicallyinordertoobtainmoretraining\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\nrewardof1.However,wemayalsowanttoexplorebytryingaction a.Wedonot\nknowwhatwillhappenifwetryaction a.Wehopetogetarewardof,butwe 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\nrewardandthemodelsamountofuncertaintyaboutthatreward.\n4 8 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\nOneofthemostprominentfactorsisthetimescaleweareinterestedin.Ifthe\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\nmoreexplorationsothatfutureactionscanbeplannedmoreeectivelywithmore\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\nmoreexploitation.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "moreexploitation.\nSupervisedlearninghasnotradeobetweenexplorationandexploitation\nbecausethesupervisionsignalalwaysspecieswhichoutputiscorrectforeach\ninput.Thereisnoneedtotryoutdierentoutputstodetermineifoneisbetter\nthanthemodelscurrentoutputwealwaysknowthatthelabelisthebestoutput.\nAnotherdicultyarisinginthecontextofreinforcementlearning,besidesthe\nexploration-exploitationtrade-o,isthedicultyofevaluatingandcomparing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "dierentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\nevaluatethelearnersperformanceusingaxedsetoftestsetinputvalues.The\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits.\n12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\nswering\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "translationandnaturallanguageprocessingduetotheuseofembeddingsfor\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\nrepresentations.\n12.5.2.1Knowledge,RelationsandQuestionAnswering",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "12.5.2.1Knowledge,RelationsandQuestionAnswering\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\nInmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\n4 8 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ndonot.Forexample,wecandenetherelationislessthanonthesetofentities\n{1 ,2 ,3}bydeningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\nrelationisdened,wecanuseitlikeaverb.Because(1 ,2) S,wesaythat1is\nlessthan2.Because(2 ,1) S,wecannotsaythat2islessthan1.Ofcourse,the\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddenea\nrelation containingtupleslike(,). is_a_type_of dogmammal\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\nsentencestaketheformofatripletoftokens\n(subjectverbobject) , , (12.21)\nwithvalues\n(entityi ,relation j ,entityk) . (12.22)\nWecanalsodeneanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) . (12.23)\nForexample,wecoulddenethehas_furattribute,andapplyittoentitieslike\ndog.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "dog.\nManyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\nHowshouldwebestdothiswithinthecontextofneuralnetworks?\nMachinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\nTherearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\nkindofinformation,alb eitnotformattedasthreetokensentences.Whena",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "databaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\nexpertknowledgeaboutanapplicationareatoanarticialintelligencesystem,\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).2013a\n1R e s p e c t i v e l y a v a i l a b l e  f ro m t h e s e  w e b  s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t .\np r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtodeneamodelfamilytotrain.\nAcommonapproachistoextendneurallanguagemodelstomodelentitiesand\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.\nEarlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "2000)positedhighlyconstrainedparametricforms(linearrelationalembeddings),\noftenusingadierentformofrepresentationfortherelationthanfortheentities.\nForexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\ne t a l .,),allowingustomakestatementsaboutrelations,butmoreexibilityis 2012",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\nApracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .\n(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "application.\nEvaluatingtheperformanceofamodelonalinkpredictiontaskisdicult\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\nbetrue).Ifthemodelproposesafactthatisnotinthedataset,weareunsure\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\nintherelationwithadierententityselectedatrandom.Thepopularprecisionat\n10%metriccountshowmanytimesthemodelranksacorrectfactamongthe\ntop10%ofallcorruptedversionsofthatfact.\n4 8 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\none,insomecontext.\nEventually,knowledgeofrelationscombinedwithareasoningprocessand\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "inputinformationandrememberimportantfacts,organizedinawaythatenables\nittoretrieveandreasonaboutthemlater.Thisremainsadicultopenproblem\nwhichcanonlybesolvedinrestrictedtoyenvironments.Currently,thebest\napproachtorememberingandretrievingspecicdeclarativefactsistousean\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\nrstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).\ne t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "theinputintothememoryandtoproducetheanswergiventhecontentsofthe\nmemory.\nDeeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\nasofthiswriting.\nThisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "networks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\nmethodsinvolveusingthegradientofacostfunctiontondtheparametersofa\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\nterritoryofresearchmethodsthataredesignedtoworkwithlesstrainingdata\nortoperformagreatervarietyoftasks,wherethechallengesaremoredicult\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\n4 8 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "P a rt I\nAppliedMathandMachine\nLearningBasics\n29",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\nallo w us t o dene f unctions of many v ariables ,  nd t he highes t and low e s t p oints\non t hes e f unctions and q uantify degrees of b e lief.\nN e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "t o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction.\nThis e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep.In t he",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "s ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\nf r amew ork.\n3 0",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 4\nA u t o e n co d e rs\nAn aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput\ntoitsoutput.Internally ,ithasahiddenlayer hthatdescribesa c o deusedto\nrepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\nencoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h).\nThisarchitectureispresentedingure.Ifanautoencodersucceedsinsimply 14.1\nlearningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\nrestrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly\ninputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\nwhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\ndata.\nModernautoencodershavegeneralizedtheidea ofanencoderandade-\ncoderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and\npdecoder( ) x h|.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "pdecoder( ) x h|.\nTheideaofautoencodershasbeenpartofthehistoricallandscapeofneural\nnetworksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\n1994).Traditionally,autoencoderswereusedfordimensionalityreductionor\nfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersand\nlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\nmodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\naspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "techniques,typicallyminibatchgradientdescentfollowinggradientscomputed\nbyback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\nalsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning\nalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\n502",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\ntotheactivationsonthereconstructedinput.Recirculationisregardedasmore\nbiologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning\napplications.\nxx rrh h\nf g\nFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\n(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder\nhastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto\nr).\n14.1UndercompleteAutoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "r).\n14.1UndercompleteAutoencoders\nCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\ninterestedintheoutputofthedecoder. Instead,wehopethattrainingthe\nautoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful\nproperties.\nOnewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto\nhavesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless\nthantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "representationforcestheautoencodertocapturethemostsalientfeaturesofthe\ntrainingdata.\nThelearningprocessisdescribedsimplyasminimizingalossfunction\nL , g f ( x(())) x (14.1)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthemeansquarederror.\nWhenthedecoderislinearand Listhemeansquarederror,anundercomplete\nautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\ntrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\ntrainingdataasaside-eect.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "trainingdataasaside-eect.\nAutoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-\ntions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-\n5 0 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\ncanlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\nthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\nwithaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto\nrepresenteachtrainingexample x() iwiththecode i.Thedecodercouldlearnto\nmaptheseintegerindicesbacktothevaluesofspecictrainingexamples.This",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "specicscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\ncodertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\nthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\n14.2RegularizedAutoencoders\nUndercomplete autoencoders,withcodedimensionlessthantheinputdimension,\ncanlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\ntheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\ngiventoomuchcapacity.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "giventoomuchcapacity.\nAsimilarproblemoccursifthehiddencodeisallowedtohavedimension\nequaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas\ndimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear\ndecodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\naboutthedatadistribution.\nIdeally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\nthecodedimensionandthecapacityoftheencoderanddecoderbasedonthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\nabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\nanddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\nfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheability\ntocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\nrepresentation,smallnessofthederivativeoftherepresentation,androbustness\ntonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "overcompletebutstilllearnsomethingusefulaboutthedatadistributionevenif\nthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.\nInadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted\nasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\nandequippedwithaninferenceprocedure(forcomputinglatentrepresentations\ngiveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\nmodelingapproachesthatemphasizethisconnectionwithautoencodersarethe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "descendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b\n5 0 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12\nThesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\nanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\narenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\ntheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\n1 4 . 2 . 1 S p a rse A u t o en co d ers\nAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "sparsitypenalty( h)onthecodelayer h,inadditiontothereconstructionerror:\nL , g f ( x(()))+() x h (14.2)\nwhere g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder\noutput.\nSparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch\nasclassication.Anautoencoderthathasbeenregularizedtobesparsemust\nrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\nthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\nfeaturesasabyproduct.\nWecanthinkofthepenalty ( h)simplyasaregularizertermaddedto\nafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\n(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\n(withasupervisedlearningob jective)thatdependsonthesesparsefeatures.\nUnlikeotherregularizerssuchasweightdecay,thereisnotastraightforward",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Bayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1\nwithweightdecayandotherregularizationpenaltiescanbeinterpretedasa\nMAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\ncorrespondingtoapriorprobabilitydistributionoverthemodelparameters.In\nthisview,regularizedmaximumlikelihoodcorrespondstomaximizing p(  x|),\nwhichisequivalenttomaximizing log p( x |)+log p( ).The log p( x |)term\nistheusualdatalog-likelihoodtermandthelog p( )term,thelog-priorover",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "parameters,incorporatesthepreferenceoverparticularvaluesof .Thisviewwas\ndescribedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\nbecausetheregularizerdependsonthedataandisthereforebydenitionnota\npriorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\ntermsasimplicitlyexpressingapreferenceoverfunctions.\nRatherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\ntask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\n5 0 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nmaximumlikelihoodtrainingofagenerativemodelthathaslatentvariables.\nSupposewehaveamodelwithvisiblevariables xandlatentvariables h,with\nanexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto\npmodel( h)asthemodelspriordistributionoverthelatentvariables,representing\nthemodelsbeliefspriortoseeing x.Thisisdierentfromthewaywehave\npreviouslyusedthewordprior,torefertothedistribution p( )encodingour",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "beliefsaboutthemodelsparametersbeforewehaveseenthetrainingdata.The\nlog-likelihoodcanbedecomposedas\nlog pmodel() = log x\nhpmodel( ) h x , . (14.3)\nWecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\nforjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative\nmodel(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather\nthantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof\nview,withthischosen,wearemaximizing h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "view,withthischosen,wearemaximizing h\nlog pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)\nThelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,\npmodel( h i) =\n2e|  h i|, (14.5)\ncorrespondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\nabsolutevaluepenalty,weobtain\n() = h \ni| h i| (14.6)\nlog pmodel() = h\ni\n h| i|log\n2\n= ()+const h (14.7)\nwheretheconstanttermdependsonlyon andnot h.Wetypicallytreat asa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "hyperparameteranddiscardtheconstanttermsinceitdoesnotaecttheparameter\nlearning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From\nthispointofviewofsparsityasresultingfromtheeectof pmodel( h)onapproximate\nmaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\nall.Itisjustaconsequenceofthemodelsdistributionoveritslatentvariables.\nThisviewprovidesadierentmotivationfortraininganautoencoder:itisaway",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ofapproximately trainingagenerativemodel.Italsoprovidesadierentreasonfor\n5 0 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\nvariablesthatexplaintheinput.\nEarlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008\nvariousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\nandthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected\nprobabilisticmodel p( x) =1\nZ p( x).Theideaisthatminimizing log Zpreventsa\nprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "onanautoencoderpreventstheautoencoderfromhavinglowreconstruction\nerroreverywhere.Inthiscase,theconnectionisonthelevelofanintuitive\nunderstandingofageneralmechanismratherthanamathematical correspondence.\nTheinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina\ndirectedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward.\nOnewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "wasintroducedin ().Theideaistouserectiedlinearunitsto Glorot e t a l .2011b\nproducethecodelayer.Withapriorthatactuallypushestherepresentationsto\nzero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\nnumberofzerosintherepresentation.\n1 4 . 2 . 2 D en o i s i n g A u t o en co d ers\nRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder  \nthatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\nfunction.\nTraditionally,autoencodersminimizesomefunction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Traditionally,autoencodersminimizesomefunction\nL , g f ( x(())) x (14.8)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthe L2normoftheirdierence.This encourages g ftolearntobemerelyan\nidentityfunctioniftheyhavethecapacitytodoso.\nA orDAEinsteadminimizes denoising aut o e nc o der\nL , g f ( x(( x))) , (14.9)\nwhere  xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising\nautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\ninput.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "input.\nDenoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),\nasshownby ()and ().Denoising AlainandBengio2013Bengio e t a l .2013c\n5 0 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\nasabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\nhowovercomplete,high-capacity modelsmaybeusedasautoencoderssolong\nascareistakentopreventthemfromlearningtheidentityfunction.Denoising\nautoencodersarepresentedinmoredetailinsection.14.5\n1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es\nAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse ",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "autoencoders,\nL , g f , , ( x(()))+( x h x) (14.10)\nbutwithadierentformof:\n( ) = h x , \ni|| x h i||2. (14.11)\nThisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\nchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\ntheautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\ndistribution.\nAnautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der\norCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail\ninsection.14.7\n14.3RepresentationalPower,LayerSizeandDepth\nAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\ndecoder.However,thisisnotarequirement.Infact,usingdeepencodersand\ndecodersoersmanyadvantages.\nRecallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\nwardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork\nasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\nbenetfromdepth.\nOnemajoradvantageofnon-trivialdepthisthattheuniversalapproximator\ntheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\nlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan\n5 0 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\narbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\nthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\nfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrom\ninputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\nconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\nleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any\nmappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Depthcanexponentiallyreducethecomputational costofrepresentingsome\nfunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\nneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\ndepthinfeedforwardnetworks.\nExperimentally,deepautoencodersyieldmuchbettercompressionthancorre-\nspondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\nAcommonstrategyfortrainingadeepautoencoderistogreedilypretrain\nthedeeparchitecturebytrainingastackofshallowautoencoders,soweoften",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "encountershallowautoencoders,evenwhentheultimategoalistotrainadeep\nautoencoder.\n14.4StochasticEncodersandDecoders\nAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\nunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\nautoencoders.\nAsdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\nandthelossfunctionofafeedforwardnetworkistodeneanoutputdistribution\np( y x|)andminimizethenegativelog-likelihoodlog p( y x|).Inthatsetting, y",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "wasavectoroftargets,suchasclasslabels.\nInthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,\nwecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay\nthinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|).We\nmaythentraintheautoencoderbyminimizing log pdecoder( ) x h|.Theexact\nformofthislossfunctionwillchangedependingontheformof pdecoder.Aswith\ntraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "themeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative\nlog-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues\ncorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid\noutputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon.\n5 0 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nTypically,theoutputvariablesaretreatedasbeingconditionallyindependent\ngiven hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome\ntechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs\nwithcorrelations.\nxx rrh h\np e n c o d e r ( ) h x| p d e c o d e r ( ) x h|\nFigure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\ndecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "theiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder\nand pdecoder( ) x h|forthedecoder.\nTomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\npreviously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to\nan e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedingure.14.2\nAnylatentvariablemodel pmodel( ) h x ,denesastochasticencoder\npencoder( ) = h x| pmodel( ) h x| (14.12)\nandastochasticdecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andastochasticdecoder\npdecoder( ) = x h| pmodel( ) x h| . (14.13)\nIngeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\ndistributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l .\n()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\nwilltendtomakethemcompatibleasymptotically(withenoughcapacityand\nexamples).\n14.5DenoisingAutoencoders\nThe denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\nasitsoutput.\nTheDAEtrainingprocedureisillustratedingure.Weintroducea 14.3\ncorruptionprocess C(x x|)whichrepresentsaconditionaldistrib utionover\n5 1 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n x  x L Lh h\nfg\nxxC (  x x| )\nFigure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,\nwhichistrainedtoreconstructthecleandatapoint xfromitscorruptedversion x.\nThisisaccomplishedbyminimizingtheloss L=log pdecoder( x h|= f( x)),where\n xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption\nprocess C( x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean\nparametersareemittedbyafeedforwardnetwork. g",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "parametersareemittedbyafeedforwardnetwork. g\ncorruptedsamples  x,givenadatasample x.Theautoencoderthenlearnsa\nr e c o nst r u c t i o n di st r i but i o n preconstruct( x| x)estimatedfromtrainingpairs\n( x , x),asfollows:\n1.Sampleatrainingexamplefromthetrainingdata. x\n2.Sampleacorruptedversion xfrom C( x x|= ) x.\n3.Use( x , x)asatrainingexampleforestimatingtheautoencoderreconstruction\ndistribution preconstruct( x|x) = pdecoder( x h|)with htheoutputofencoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "f( x)and pdecodertypicallydenedbyadecoder. g() h\nTypicallywecansimplyperformgradient-basedapproximate minimization (such\nasminibatchgradientdescent)onthenegativelog-likelihoodlog pdecoder( x h|).\nSolongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\nnetworkandmaybetrainedwithexactlythesametechniquesasanyother\nfeedforwardnetwork.\nWecanthereforeviewtheDAEasperformingstochasticgradientdescenton\nthefollowingexpectation:",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thefollowingexpectation:\n E x p d a t a() x E x C(x| x)log pdecoder( = ( x h| f x))(14.14)\nwhere  pdata() xisthetrainingdistribution.\n5 1 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nx x\ng f\n x\nC (  x x| )\nx\nFigure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapointxbackto\ntheoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara\nlow-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption\nprocess C(x x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\nhowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\n|| g( f( x))|| x2,thereconstruction g( f( x)) estimates E x , x p dat a()( x Cx x|)[ x| x].Thevector\ng( f(x)) xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(x))\nestimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenriseto x.The\nautoencoderthuslearnsavectoreld g( f( x)) xindicatedbythegreenarrows.This\nvectoreldestimatesthescore xlog pdata( x)uptoamultiplicativefactorthatisthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "averagerootmeansquarereconstructionerror.\n5 1 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n1 4 . 5 . 1 E s t i m a t i n g t h e S co re\nScorematching(,)isanalternativetomaximumlikelihood.It Hyvrinen2005\nprovidesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\nthemodeltohavethesame sc o r easthedatadistributionateverytrainingpoint\nx.Inthiscontext,thescoreisaparticulargradienteld:\n xlog() p x . (14.15)\nScorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4\nregardingautoencoders,itissucienttounderstandthatlearningthegradient",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "eldoflog pdataisonewaytolearnthestructureof pdataitself.\nAveryimportantpropertyofDAEsisthattheirtrainingcriterion(with\nconditionallyGaussian p( x h|))makestheautoencoderlearnavectoreld\n( g( f( x)) x)thatestimatesthescoreofthedatadistribution.Thisisillustrated\ningure.14.4\nDenoisingtrainingofaspecickindofautoencoder(sigmoidalhiddenunits,\nlinearreconstr uctionunits) usingGaussiannoiseandmeansquarederroras\nthereconstructioncostisequivalent(,)totrainingaspecickind Vincent2011",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.\nThiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1\ndiscussionitsucestoknowthatitisamodelthatprovidesanexplicit pmodel( x; ).\nWhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun\n2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\nautoencoder.Withaxednoiselevel,regularizedscorematchingisnotaconsistent",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "estimator;itinsteadrecoversablurredversionofthedistribution.However,if\nthenoiselevelischosentoapproach0whenthenumberofexamplesapproaches\ninnity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\nmoredetailinsection.18.5\nOtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\nappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\ncombinedwitharegularizationtermsimilartothecontractivepenaltyofthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen-\ncodergradientprovidesanapproximationtocontrastivedivergencetrainingof\nRBMs.\nForcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand\nreconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\ngeneralencoderanddecoderparametrizations ( ,).This AlainandBengio2013\nmeansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\n5 1 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nbytrainingwiththesquarederrorcriterion\n|| g f(( x x))||2(14.16)\nandcorruption\nC( x=x x|) = (N x x ;=  ,  = 2I) (14.17)\nwithnoisevariance 2.Seegureforanillustrationofhowthisworks. 14.5\nFigure14.5:Vectoreldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold\nnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe\nreconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "accordingtotheimplicitlyestimatedprobabilitydistribution.Thevectoreldhaszeros\natbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima\nofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof\nlocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\nthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength\nofthearrows)islarge,itmeansthatprobabilitycanbesignicantlyincreasedbymoving",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "inthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.\nTheautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.\nWhereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore\naccurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013\nIngeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe\ninput xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis\n5 1 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011\nwhere g( f( x)) xmaybeobtainedbytakingthederivativeofanotherfunction.\nKamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by\nidentifyingafamilyofshallowautoencoderssuchthat g( f( x)) xcorrespondsto\nascoreforallmembersofthefamily.\nSofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\naprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "agenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed\nlater,insection.20.11\n1 4 . 5 . 1 . 1 Hi st o r i c a l P e r spec t i v e\nTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987\nand ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001\nimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\nHowever,thenamedenoisingautoencoderreferstoamodelthatisintendednot\nmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "asasideeectoflearningtodenoise.Thisideacamemuchlater(Vincent\ne t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\ndeeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\nsparsecoding,contractiveautoencodersandotherregularizedautoencoders,the\nmotivationforDAEswastoallowthelearningofaveryhigh-capacity encoder\nwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\nPriortotheintroduction ofthemodernDAE,InayoshiandKurita2005()",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\nminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\nnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\ngeneralization byintroducingthe reconstructionerrorandtheinjectednoise.\nHowever,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction\nfamiliesaspowerfulascanthemodernDAE.\n14.6LearningManifoldswithAutoencoders\nLikemanyothermachinelearningalgorithms,auto encodersexploittheidea",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\nmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3\nthisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe\nmanifoldbutmayhaveunusualbehaviorifgivenaninputthatisothemanifold.\n5 1 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\nTounderstandhowautoencodersdothis,wemustpresentsomeimportant\ncharacteristicsofmanifolds.\nAnimportantcharacterization ofamanifoldisthesetofits t angen t pl anes.\nAtapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis\nvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\nillustratedingure,theselocaldirectionsspecifyhowonecanchange 14.6 x",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "innitesimallywhilestayingonthemanifold.\nAllautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\n1.Learningarepresentation hofatrainingexample xsuchthat xcanbe\napproximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn\nfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed\nnotsuccessfullyreconstructinputsthatarenotprobableunderthedata\ngeneratingdistribution.\n2.Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\naregularizationtermaddedtothereconstructioncost.Thesetechniques\ngenerallyprefersolutionsthatarelesssensitivetotheinput.\nClearly,neitherforcealonewouldbeusefulcopyingtheinputtotheoutput\nisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\nareusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\naboutthestructureofthedatageneratingdistribution.Theimportantprinciple",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "isthattheautoencodercanaordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d\nt o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates\nnearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture\nalocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\nmanifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder\nlearnsamappingfromtheinputspace xtoarepresentationspace,amappingthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\nchangesorthogonaltothemanifold.\nAone-dimensional exampleisillustratedingure,showingthat,bymaking 14.7\nthereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\ndatapoints,wecausetheautoencodertorecoverthemanifoldstructure.\nTounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\nstructivetocomparethemtootherapproaches.Whatismostcommonlylearned",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)\n5 1 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.6:Anillustrationoftheconceptofatangenthyperplane.Herewecreatea\none-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784\npixelsandtransformitbytranslatingitvertically.Theamountofverticaltranslation\ndenesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath\nthroughimagespace.Thisplotshowsafewpointsalongthismanifold.Forvisualization,\nwehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "manifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches\nthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.\nItdenesthespaceofdirectionsinwhichitispossibletomovewhileremainingon\nthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean\nexampletangentlineatonepoint,withanimageshowinghowthistangentdirection\nappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "thetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels\nthatdarken.\n5 1 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nx 0 x 1 x 2\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y\nO p t i m a l r e c o n s t r u c t i o n\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\nperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\nthemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\nlineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\narrowsatthebottomoftheplotindicatethe r( x) xreconstructiondirectionvector\natthebaseofthearrow,ininputspace,alwayspointingtowardsthenearestmanifold\n(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake\nthederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The\ncontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\nspacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\nthereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints\nbackontothemanifold.\nthemanifold.Sucharepresentationforaparticularexampleisalsocalledits\nembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions\nthantheambientspaceofwhichthemanifoldisalow-dimensionalsubset.Some",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "algorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly\nlearnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\nmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\npointintheambientspace(theinputspace)toitsembedding.\nManifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\nattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\nonlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "onthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample\nandedgesconnectingnearneighborstoeachother.Thesemethods(Schlkopf\ne t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;\n5 1 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph\ninwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor\nrelationships.Variousprocedurescanthusobtainthetangentplaneassociatedwitha\nneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\nexamplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize\nsucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\napproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l .\n2000).\nandNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\nandRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha\ntangentplanethatspansthedirectionsofvariationsassociatedwiththedierence\nvectorsbetweentheexampleanditsneighbors,asillustratedingure.14.8\nAglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "solvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9\nlargenumberoflocallylinearGaussian-likepatches(orpancakes,becausethe\nGaussiansareatinthetangentdirections).\nHowever,thereisafundamentaldicultywithsuchlocalnon-parametric\napproachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005\nmanifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),\nonemayneedaverylargenumberoftrainingexamplestocovereachoneof\n5 1 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.9:Ifthetangentplanes(seegure)ateachlocationareknown,thenthey 14.6\ncanbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\ncanbethoughtofasalocalEuclideancoordinatesystemorasalocallyatGaussian,or\npancake,withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\nverylargevarianceinthedirectionsdeningthecoordinatesystemonthepancake.A\nmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Parzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003\nvariant( ,). Bengio e t a l .2006c\nthesevariations,withnochancetogeneralizetounseenvariations.Indeed,these\nmethodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between\nneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan\nhaveverycomplicatedstructurethatcanbediculttocapturefromonlylocal\ninterpolation.Considerforexamplethemanifoldresultingfromtranslationshown",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "ingure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe\nimageistranslated,wewillobservethatonecoordinateencountersapeakora\ntroughinitsvalueonceforeverypeakortroughinbrightnessintheimage.In\notherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage\ntemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming\nsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentations\nanddeeplearningforcapturingmanifoldstructure.\n5 2 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n14.7ContractiveAutoencoders\nThecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab\nonthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:\n() = h  f() x\n x2\nF. (14.18)\nThepenalty( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\nJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\nThereisaconnectionbetweenthedenoisingautoencoderandthecontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\ninputnoise,thedenoisingreconstruction errorisequivalenttoacontractive\npenaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother\nwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\nnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\nfeatureextractionfunctionresistinnitesimalperturbationsoftheinput.When\nusingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "withaclassier,thebestclassicationaccuracyusuallyresultsfromapplyingthe\ncontractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)\nalsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1\nThename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speci-\ncally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\ntomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\nWecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "neighborhood.\nToclarify,theCAEiscontractiveonlylocallyallperturbationsofatraining\npoint xaremappednearto f( x).Globally,twodierentpoints xand xmaybe\nmappedto f( x)and f( x)pointsthatarefartherapartthantheoriginalpoints.\nItisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see\nforexamplewhathappensinthe1-Dtoyexampleofgure).Whenthe 14.7 ( h)\npenaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto\nmakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa\nbinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\nmostofthehypercubethatitssigmoidalhiddenunitscanspan.\nWecanthinkoftheJacobianmatrix Jatapoint xasapproximating the\nnonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword\ncontractivemoreformally.Inthetheoryoflinearoperators,alinearoperator\n5 2 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nissaidtobecontractiveifthenormof J xremainslessthanorequaltofor1\nallunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere.\nWecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear\napproximationof f( x)ateverytrainingpoint xinordertoencourageeachof\ntheselocallinearoperatortobecomeacontraction.\nAsdescribedinsection,regularized autoencoderslearnmanifoldsby 14.6\nbalancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "reconstructionerrorandthecontractivepenalty( h).Reconstructionerroralone\nwouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\nalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\nThecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\n f() x\n xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\nsmallnumberofdirectionsintheinput,mayhavesignicantderivatives.\nThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "xwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich\napproximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a\nand ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b\nof Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1\nsomesingularvaluesremainabove,becausethereconstructionerrorpenalty 1\nencouragestheCAEtoencodethedirectionswiththemostlocalvariance.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "directionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent\ndirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent\ndirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE\nappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas\nobjectsintheimagegraduallychangepose,asshowningure.Visualizations 14.6\noftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful\ntransformationsoftheinputimage,asshowningure.14.10",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\nischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\nmuchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\nRifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each\ntrainedtoreconstructthepreviousautoencodershiddenlayer.Thecomposition\noftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\nseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\ntheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\ncapturesmanyofthedesirablequalitativecharacteristics.\nAnotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\n5 2 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nInput\npointTangentvectors\nLocalPCA(nosharingacrossregions)\nContractiveautoencoder\nFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\nandbyacontractiveautoencoder.Thelocationonthemanifoldisdenedbytheinput\nimageofadogdrawnfromtheCIFAR-10dataset.Thetangentvectorsareestimated\nbytheleadingsingularvectorsoftheJacobianmatrix h\n xoftheinput-to-codemapping.\nAlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\nsharingacrossdierentlocationsthatshareasubsetofactivehiddenunits.TheCAE\ntangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\ntheheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c\nifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\ncouldconsistofmultiplyingtheinputbyasmallconstant andthedecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "couldconsistofdividingthecodeby .As approaches,theencoderdrivesthe 0\ncontractivepenalty( h)toapproachwithouthavinglearnedanythingaboutthe 0\ndistribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\ne t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare\nstandardneuralnetworklayersconsistingofananetransformationfollowedby\nanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g\ntobethetransposeoftheweightmatrixof. f",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "tobethetransposeoftheweightmatrixof. f\n14.8PredictiveSparseDecomposition\nP r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse\ncodingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric\nencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\nappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\n(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "asforaudio( ,).Themodelconsistsofanencoder Hena e t a l .2011 f( x)anda\ndecoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe\n5 2 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\noptimization algorithm.Trainingproceedsbyminimizing\n|| || x g() h2+ || h1+ ()  f || h x||2. (14.19)\nLikeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with\nrespectto handminimization withrespecttothemodelparameters.Minimization\nwithrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe\ncostfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent\ncanobtainreasonablevaluesofinasfewastensteps. h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "canobtainreasonablevaluesofinasfewastensteps. h\nThetrainingprocedureusedbyPSDisdierentfromrsttrainingasparse\ncodingmodelandthentraining f( x)topredictthevaluesofthesparsecoding\nfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparameters\nforwhichcaninfergoodcodevalues. f() x\nPredictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e.\nInsection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\nprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\nmodel.\nInpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring\ntraining.Theparametricencoder fisusedtocomputethelearnedfeatureswhen\nthemodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto\ninferring hviagradientdescent.Because fisadierentiableparametricfunction,\nPSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "withanothercriterion.\n14.9ApplicationsofAutoencoders\nAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\nmationretrievaltasks.Dimensionalityreductionwasoneoftherstapplications\nofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\nforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\nastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\nwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand\nthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\nunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\nLower-dimensionalrepresentationscanimproveperformanceonmanytasks,\nsuchasclassication.Modelsofsmallerspacesconsumelessmemoryandruntime.\n5 2 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\neachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l .\n().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\ngeneralization.\nOnetaskthatbenetsevenmorethanusualfromdimensionalityreductionis\ni nf o r m at i o n r e t r i e v al,thetaskofndingentriesinadatabasethatresemblea\nqueryentry.Thistaskderivestheusualbenetsfromdimensionalityreduction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thatothertasksdo,butalsoderivestheadditionalbenetthatsearchcanbecome\nextremelyecientincertainkindsoflowdimensionalspaces.Specically,if\nwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\ndimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary\nmappingbinarycodevectorstoentries.Thishashtableallowsustoperform\ninformationretrievalbyreturningalldatabaseentriesthathavethesamebinary\ncodeasthequery.Wecanalsosearchoverslightlylesssimilarentriesvery",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "eciently,justbyippingindividualbitsfromtheencodingofthequery.This\napproachtoinformationretrievalviadimensionalityreductionandbinarization\niscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas\nbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\nimages(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,).\nToproducebinarycodesforsemantichashing,onetypicallyusesanencoding\nfunctionwithsigmoidsonthenallayer.Thesigmoidunitsmustbetrainedtobe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\nthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\ntraining.Themagnitudeofthenoiseshouldincreaseovertime.Toghtthat\nnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\nmagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\nTheideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\ndirections,includingtheideaoftrainingtherepresentationssoastooptimize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "alossmoredirectlylinkedtothetaskofndingnearbyexamplesinthehash\ntable( ,). NorouziandFleet2011\n5 2 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "N ot at i o n\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\nconcepts,wedescribemostoftheseideasinchapters24.\nNum b e r s and Ar r a y s\naAscalar(integerorreal)\naAvector\nAAmatrix\nAAtensor\nI nIdentitymatrixwithrowsandcolumns n n\nIIdentitymatrixwithdimensionalityimpliedby\ncontext\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\n1atposition i\ndiag()aAsquare,diagonalmatrixwithdiagonalentries",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "diag()aAsquare,diagonalmatrixwithdiagonalentries\ngivenbya\naAscalarrandomvariable\naAvector-valuedrandomvariable\nAAmatrix-valuedrandomvariable\nxi",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nSet s and G r aphs\nAAset\nRThesetofrealnumbers\n{}01 ,Thesetcontaining0and1\n{ } 01 , , . . . , nThesetofallintegersbetweenand0 n\n[] a , bTherealintervalincludingand a b\n(] a , bTherealintervalexcludingbutincluding a b\nA B\\Setsubtraction,i.e.,thesetcontainingtheele-\nmentsofthatarenotin A B\nGAgraph\nP a G(x i)Theparentsofx iinG\nI ndexing\na iElement iofvectora,withindexingstartingat1\na  iAllelementsofvectorexceptforelementa i\nA i , jElementofmatrix i , jA\nA i , :Rowofmatrix iA",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "A i , :Rowofmatrix iA\nA : , iColumnofmatrix iA\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\nA : : , , i2-Dsliceofa3-Dtensor\na iElementoftherandomvector i a\nL i near Al g e br a O p e r at i o ns\nATransposeofmatrixA\nA+Moore-PenrosepseudoinverseofA\nABElement-wise(Hadamard)productofandAB\ndet()ADeterminantofA\nx i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCal c ul usd y\nd xDerivativeofwithrespectto y x\n y\n xPartialderivativeofwithrespectto y x\n x yGradientofwithrespectto y x\n X yMatrixderivativesofwithrespectto y X\n X yTensorcontainingderivativesof ywithrespectto\nX\n f\nxJacobianmatrixJ Rm n of f: Rn Rm\n2\nx f f f () (xorH)()xTheHessianmatrixofatinputpointx\nf d()xxDeniteintegralovertheentiredomainofx\n\nSf d()xx x Deniteintegralwithrespecttoovertheset S\nP r o babil i t y and I nf o r m at i o n T heor y",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "abTherandomvariablesaandbareindependent \nabcTheyareconditionallyindependentgivenc |\nP()aAprobabilitydistributionoveradiscretevariable\np()aAprobabilitydistributionoveracontinuousvari-\nable,oroveravariablewhosetypehasnotbeen\nspecied\na Randomvariableahasdistribution  P P\nE x  P[()] () () () f xor E f xExpectationof f xwithrespectto Px\nVar(()) f xVarianceofunderx f x() P()\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\nH()xShannonentropyoftherandomvariablex",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "H()xShannonentropyoftherandomvariablex\nD K L( ) P QKullback-LeiblerdivergenceofPandQ\nN(; )x ,Gaussiandistributionoverxwithmeanand\ncovariance\nx i i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nF unc t i o ns\nf f : A BThefunctionwithdomainandrange A B\nf g f g Compositionofthefunctionsand\nf(;)xAfunctionofxparametrized by.(Sometimes\nwewrite f(x)andomittheargumenttolighten\nnotation)\nlog x x Naturallogarithmof\n x()Logisticsigmoid,1\n1+exp() x\n x x () log(1+exp( Softplus, ))\n||||x p Lpnormofx\n||||x L2normofx\nx+Positivepartof,i.e., x max(0) , x\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\ntothearrayelement-wise. Forexample,if C= ( X),then C i , j , k= ( X i , j , k)forall\nvalidvaluesof,and. i j k\nD at aset s and D i st r i but i o n s\np da t aThedatageneratingdistribution\n p da t aTheempiricaldistributiondenedbythetraining\nset\nXAsetoftrainingexamples\nx( ) iThe-thexample(input)fromadataset i\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\ning\nXThe m nmatrixwithinputexamplex( ) iinrow",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "ing\nXThe m nmatrixwithinputexamplex( ) iinrow\nX i , :\nx i v",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 5\nRepresen t at i on L e ar n i n g\nInthischapter,werstdiscusswhatitmeanstolearnrepresentationsandhow\nthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss\nhowlearningalgorithmssharestatisticalstrengthacrossdierenttasks,including\nusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\nrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\nlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationexists.Finally,westepbackandargueaboutthereasonsforthe\nsuccessofrepresentationlearning,startingwiththetheoreticaladvantagesof\ndistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand\nendingwiththemoregeneralideaofunderlyingassumptionsaboutthedata\ngeneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.\nManyinformationprocessingtaskscanbeveryeasyorverydicultdepending\nonhowtheinformationisrepresented.Thisisageneralprincipleapplicableto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "dailylife,computerscienceingeneral,andtomachinelearning.Forexample,it\nisstraightforwardforapersontodivide210by6usinglongdivision.Thetask\nbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\nnumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\nbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\npermittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\nconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "appropriateorinappropriate representations.Forexample,insertinganumber\nintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe\nlistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa\nred-blacktree.\nInthecontextofmachinelearning,whatmakesonerepresentationbetterthan\n526",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\nlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\nofthesubsequentlearningtask.\nWecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\nformingakindofrepresentationlearning.Specically,thelastlayerofthenetwork\nistypicallyalinearclassier,suchasasoftmaxregressionclassier.Therestof\nthenetworklearnstoprovidearepresentationtothisclassier.Trainingwitha",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\nmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassication\ntaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\nfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\nlastlayercouldbeanotherkindofmodel,suchasanearestneighborclassier\n(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\nlearndierentpropertiesdependingonthetypeofthelastlayer.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\nanyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\nlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\nsomeparticularway.Forexample,supposewewanttolearnarepresentationthat\nmakesdensityestimationeasier.Distributionswithmoreindependencesareeasier\ntomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\noftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\nlearnarepresentationasasideeect.Regardlessofhowarepresentationwas\nobtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\nsupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\nrepresentation.\nMostrepresentationlearningproblemsfaceatradeobetweenpreservingas\nmuchinformationabouttheinputaspossibleandattainingniceproperties(such\nasindependence).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asindependence).\nRepresentationlearningisparticularlyinterestingbecauseitprovidesone\nwaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\nlargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\ndata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\nresultsinsevereovertting.Semi-supervisedlearningoersthechancetoresolve\nthisoverttingproblembyalsolearningfromtheunlabeleddata.Specically,\nwecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "representationstosolvethesupervisedlearningtask.\nHumansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\n5 2 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\nperformanceforexample,thebrainmayuseverylargeensemblesofclassiers\norBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\nabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\ntoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\nunlabeleddatacanbeusedtolearnagoodrepresentation.\n15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\nnetworks,enablingresearchersforthersttimetotrainadeepsupervisednetwork\nwithoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We\ncallthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -\nwi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow\narepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theshapeoftheinputdistribution)cansometimesbeusefulforanothertask\n(supervisedlearningwiththesameinputdomain).\nGreedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\ntationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\ncodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\npretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\nandproducingasoutputanewrepresentationofthedata,whosedistribution(or",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "itsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.\nSeealgorithm foraformaldescription. 15.1\nGreedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\nbeenusedtosidestepthedicultyofjointlytrainingthelayersofadeepneuralnet\nforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron\n(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\nthatthisgreedylearningprocedurecouldbeusedtondagoodinitialization for",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\ntosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;\nandSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,).\nPriortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\nresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\nthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "architectures,buttheunsupervisedpretrainingapproachwastherstmethodto\nsucceed.\nGreedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -\n5 2 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nr i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one\npieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se\nbecausetheseindependentpiecesarethelayersofthenetwork.Specically,greedy\nlayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile\nkeepingthepreviousonesxed.Inparticular,thelowerlayers(whicharetrained\nrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "v i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning\nalgorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe\nonlyarststepbeforeajointtrainingalgorithmisappliedto ne-t uneallthe\nlayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\nasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\ndecreasingtrainingerror)andaformofparameterinitialization.\nItiscommontousethewordpretrainingtorefernotonlytothepretraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "stageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining\nphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\ntrainingasimpleclassierontopofthefeatureslearnedinthepretrainingphase,\noritmayinvolvesupervisedne-tuningoftheentirenetworklearnedinthe\npretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\nwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining\nschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "willobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining\nfollowthisbasicprotocol.\nGreedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\nforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\nandSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent\nvariables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006\nBoltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\nmodelswillbedescribedinchapter.20",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "modelswillbedescribedinchapter.20\nAsdiscussedinsection,itisalsopossibletohavegreedylayer-wise 8.7.4\nsupervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\niseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\ncontexts(,). Erhanetal.2010\n1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk?\nOnmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\nimprovementsintesterrorforclassicationtasks.Thisobservationwasresponsible",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\n5 2 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nAl g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol.\nGiventhefollowing:Unsupervisedfeaturelearningalgorithm L,whichtakesa\ntrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw\ninputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftherst\nstageencoderonX.Inthecasewherene-tuningisperformed,weusealearner\nTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber\nofstagesis.m\nfIdentityfunction\nXX= \nf o r dok,...,m = 1\nf( ) k= (LX)\nff( ) kf\nXf( ) k(X)\ne nd f o r\ni fne-tuning t he n\nff,, T(XY)\ne nd i f\nRet ur nf\n2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,\nunsupervisedpretrainingeitherdoesnotconferabenetorevencausesnoticeable\nharm. ()studiedtheeectofpretrainingonmachinelearning Maetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\nslightlyharmful,butformanytaskswassignicantlyhelpful.Becauseunsupervised\npretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand\nwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\ntask.\nAttheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\ntogreedyunsupervisedpretraininginparticular.Thereareother,completely",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "dierentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\nsuchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13\ntrainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\nExamplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\nandBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\nobjectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly\nusingtheinput).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "usingtheinput).\nUnsupervisedpretrainingcombinestwodierentideas.First,itmakesuseof\n5 3 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\nasignicantregularizingeectonthemodel(and,toalesserextent,thatitcan\nimproveoptimization). Second,itmakesuseofthemoregeneralideathatlearning\nabouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto\noutputs.\nBothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral\npartsofthemachinelearningalgorithmthatarenotentirelyunderstood.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "Therstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\ncanhaveastrongregularizingeectonitsperformance, istheleastwellunderstood.\nAtthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\nmodelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\nanother.Today,localminimaarenolongerconsideredtobeaseriousproblem\nforneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork\ntrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\nbeinaccessibleforexample,aregionthatissurroundedbyareaswherethecost\nfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\naverynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\nHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\nverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\npretrainedparametersareretainedduringthesupervisedtrainingstageislimited.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\nlearningandsupervisedlearningratherthantwosequentialstages.Onemay\nalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe\nsupervisedlearningstagepreservesinformationfromtheunsupervisedlearning\nstagebysimplyfreezingtheparameters forthefeatureextractorsandusing\nsupervisedlearningonlytoaddaclassierontopofthelearnedfeatures.\nTheotheridea,thatalearningalgorithmcanuseinformationlearnedinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\nunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\ntaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\nagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout\nwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\ntherepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\nlearnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "soitisnotalwayspossibletopredictwhichtaskswillbenetfromunsupervised\nlearninginthisway.Manyaspectsofthisapproacharehighlydependenton\nthespecicmodelsused.Forexample,ifwewishtoaddalinearclassieron\n5 3 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\nseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\nisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\npreferabletheconstraintsimposedbytheoutputlayerarenaturallyincluded\nfromthestart.\nFromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\nwecanexpectunsupervisedpretrainingtobemoreeectivewhentheinitial",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "representationispoor.Onekeyexampleofthisistheuseofwordembeddings.\nWordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo\ndistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance\nof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2\ndistancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially\nusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps\nbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealow",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "qualitysimilaritymetric.\nFromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\nexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\nexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\npretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\ntoperformbestwhenthenumberofunlabeledexamples isverylarge.The\nadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\nunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\ncompetitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011\nnumberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\nofexamplesperclass).Theseeectswerealsodocumentedincarefullycontrolled\nexperimentsbyPaine2014etal.().\nOtherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\nislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningdiersfromregularizerslikeweightdecaybecauseitdoesnot\nbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering\nfeaturefunctionsthatareusefulfortheunsupervisedlearningtask.Ifthetrue\nunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput\ndistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\nThesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\npretrainingisknowntocauseanimprovement,andexplainwhatisknownabout",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused\ntoimproveclassiers,andisusuallymostinterestingfromthepointofviewof\n5 3 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n\u0000    \u0000    \u0000    \u0000                    \u0000   \u0000   \u0000          \n               \n                  \nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdierent\nneuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one\nmappingsfromparametervectorstofunctions),withdierentrandominitializations",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadierent\nneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisgureisadapted\nwithpermissionfrom ().Acoordinateinfunctionspaceisaninnite- Erhan e t a l .2010\ndimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010\nalinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspecicx\npoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\n(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\novertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to\npointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\nusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.\nIsomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\nhasreducedvariance.\n5 3 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nreducingtestseterror.However,unsupervisedpretrainingcanhelptasksother\nthanclassication,andcanacttoimproveoptimization ratherthanbeingmerely\naregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror\nfordeepautoencoders(HintonandSalakhutdinov2006,).\nErhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof\nunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "totesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe\nparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\ntrainingisnon-determinis tic,andconvergestoadierentfunctioneverytimeit\nisrun.Trainingmayhaltatapointwherethegradientbecomessmall,apoint\nwhereearlystoppingendstrainingtopreventovertting,oratapointwherethe\ngradientislargebutitisdiculttondadownhillstepduetoproblemssuchas\nstochasticityorpoorconditioningoftheHessian.Neuralnetworksthatreceive",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\nwhileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\ngureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1\nnetworksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\nestimationprocess,whichcaninturnreducetheriskofsevereover-tting.In\notherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\naregionthattheydonotescape,andtheresultsfollowingthisinitialization are",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\nErhan2010etal.()alsoprovidesomeanswersastopretrainingworks when\nbestthemeanandvarianceofthetesterrorweremostreducedbypretrainingfor\ndeepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\ninventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks\n(rectiedlinearunits,dropoutandbatchnormalization) solessisknownaboutthe\neectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.\nOnehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\nfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\nThisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\npretraining,andisdescribedfurtherinsection.15.3\nComparedtootherformsofunsupervisedlearning,unsupervisedpretraining\nhasthedisadvantagethatitoperateswithtwoseparatetrainingphases.Many",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\nstrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\nUnsupervisedpretrainingdoesnotoeraclearwaytoadjustthethestrength\noftheregularizationarisi ngfromtheunsupervisedstage.Instead,thereare\n5 3 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nverymanyhyperparameters ,whoseeectmaybemeasuredafterthefactbut\nisoftendiculttopredictaheadoftime.Whenweperformunsupervisedand\nsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there\nisasinglehyperparameter,usuallyacoecientattachedtotheunsupervised\ncost,thatdetermineshowstronglytheunsupervisedobjectivewillregularize\nthesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasingthiscoecient.Inthecaseofunsupervisedpretraining,thereisnota\nwayofexiblyadaptingthestrengthoftheregularizationeither thesupervised\nmodelisinitializedtopretrainedparameters,oritisnot.\nAnotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\nhasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\nbepredictedduringtherstphase,sothereisalongdelaybetweenproposing\nhyperparametersfortherstphaseandbeingabletoupdatethemusingfeedback",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "fromthesecondphase.Themostprincipledapproachistousevalidationseterror\ninthesupervisedphaseinordertoselectthehyperparameters ofthepretraining\nphase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009\nlikethenumberofpretrainingiterations,aremoreconvenientlysetduringthe\npretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis\nnotidealbutcomputationally muchcheaperthanusingthesupervisedobjective.\nToday,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "eldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\none-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\nsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\nonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\nwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\nthenusethisrepresentationorne-tuneitforasupervisedtaskforwhichthe\ntrainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal.\n()andremainsincommonusetoday. 2011a\nDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\norbatchnormalization, areabletoachievehuman-levelperformanceonverymany\ntasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-\nperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and\nMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "datasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform\nmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,\nthepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\npretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\n5 3 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nandcontinuestoinuencecontemporaryapproaches.Theideaofpretraininghas\nbeengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4\ncommonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\nispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\nnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparameters\nofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\n15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n\nTransferlearninganddomainadaptationrefertothesituationwherewhathasbeen\nlearnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization\ninanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe\nprevioussection,wherewetransferredrepresentationsbetweenanunsupervised\nlearningtaskandasupervisedlearningtask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "learningtaskandasupervisedlearningtask.\nIn t r ansf e r l e ar ni ng,thelearnermustperformtwoormoredierenttasks,\nbutweassumethatmanyofthefactorsthatexplainthevariationsinP 1are\nrelevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically\nunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\ntargetmaybeofadierentnature.Forexample,wemaylearnaboutonesetof\nvisualcategories,suchascatsanddogs,intherstsetting,thenlearnabouta",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "dierentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If\nthereissignicantlymoredataintherstsetting(sampledfromP 1),thenthat\nmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly\nveryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions\nofedgesandvisualshapes,theeectsofgeometricchanges,changesinlighting,\netc.Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7\nadaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatareusefulforthedierentsettingsortasks,correspondingtounderlying\nfactorsthatappearinmorethanonesetting.Thisisillustratedingure,with7.2\nsharedlowerlayersandtask-dependentupperlayers.\nHowever,sometimes,whatissharedamongthedierenttasksisnotthe\nsemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\nrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,but\ntheearlierlayersneartheinputmayneedtorecognizeverydierentversionsof",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesamephonemesorsub-phonemicvocalizationsdependingonwhichperson\nisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\n(neartheoutput)oftheneuralnetwork,andhaveatask-specicpreprocessing,as\n5 3 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nillustratedingure.15.2\nSe l e c t i onsw i t c h\nh(1)h(1)h(2)h(2)h(3)h(3)yy\nh(shared)h(shared)\nx(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )\nFigure15.2:Example architectureformulti-taskortransferlearningwhentheoutput\nvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadierent y x \nmeaning(andpossiblyevenadierentdimension)foreachtask(or,forexample,each\nuser),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "switch)aretask-specic,whiletheupperlevelsareshared.Thelowerlevelslearnto\ntranslatetheirtask-specicinputintoagenericsetoffeatures.\nIntherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-\noutputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\nisslightlydierent.Forexample,considerthetaskofsentimentanalysis,which\nconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\nCommentspostedonthewebcomefrommanycategories.Adomainadaptation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\nmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments\naboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine\nthatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\nneutralornegative,butofcoursethevocabularyandstylemayvaryfromone\ndomaintoanother,makingitmorediculttogeneralizeacrossdomains.Simple\nunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "successfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\nArelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform\noftransferlearningduetogradualchangesinthedatadistributionovertime.\nBothconceptdriftandtransferlearningcanbeviewedasparticularformsof\n5 3 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nmulti-tasklearning.Whilethephrasemulti-tasklearningtypicallyrefersto\nsupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\ntounsupervisedlearningandreinforcementlearningaswell.\nInallofthesecases,theobjectiveistotakeadvantageofdatafromtherst\nsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\ndirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\nsamerepresentationinbothsettingsallowstherepresentationtobenetfromthe\ntrainingdatathatisavailableforbothtasks.\nAsmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\nsuccessinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\netal.,).Intherstofthesecompetitions,theexperimentalsetupisthe 2011\nfollowing.Eachparticipantisrstgivenadatasetfromtherstsetting(from",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "distributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants\nmustusethistolearnagoodfeaturespace(mappingtherawinputtosome\nrepresentation),suchthatwhenweapplythislearnedtransformationtoinputs\nfromthetransfersetting(distributionP 2),alinearclassiercanbetrainedand\ngeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults\nfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand\ndeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "intherstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond\n(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled\nexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\ngeneralization performance.\nTwoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t\nl e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample\nofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "givenatallforthezero-shotlearningtask.\nOne-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\nlearnstocleanlyseparatetheunderlyingclassesduringtherststage.Duringthe\ntransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\npossibletestexamplesthatallclusteraroundthesamepointinrepresentation\nspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\ntheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "representationspace,andwehavesomehowlearnedwhichfactorsdoanddonot\nmatterwhendiscriminatingobjectsofcertaincategories.\nAsanexampleofazero-shotlearningsetting,considertheproblemofhaving\nalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\n5 3 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nItmaybepossibletorecognizeaspecicobjectclassevenwithouthavingseenan\nimageofthatobject,ifthetextdescribestheobjectwellenough.Forexample,\nhavingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\nguessthatanimageisacat,withouthavingseenacatbefore.\nZero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\netal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\nasincludingthreerandomvariables:thetraditionalinputsx,thetraditional\noutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.\nThemodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where\nTisadescriptionofthetaskwewishthemodeltoperform.Inourexampleof\nrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\nwithy= 1indicatingyesandy= 0indicatingno.ThetaskvariableTthen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "representsquestionstobeansweredsuchasIsthereacatinthisimage?Ifwe\nhaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\nsamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.\nInourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\nimportantthatwehavehadunlabeledtextdatacontainingsentencessuchascats\nhavefourlegsorcatshavepointyears.\nZero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "ofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan\nobjectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b\nofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\nwitheachcategory.\nAsimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;\nMikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and\ntherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "otherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith\nwordsintheother.Eventhoughwemaynothavelabeledexamplestranslating\nwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa\ntranslationforwordAbecausewehavelearnedadistributedrepresentationfor\nwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and\ncreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples\nconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "mostsuccessfulifallthreeingredients(thetworepresentationsandtherelations\nbetweenthem)arelearnedjointly.\nZero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\nexplainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation\n5 3 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh x = f x ( ) x\nx t e s t\ny t e s th y = f y ( ) y\ny  s pa ce\nR e l at i onshi p b e t w e e n  e m be dde d p oi n t s  w i t hi n one  o f  t h e  d o m a i n s\nMapsbe t w e e n  r e p r e s e n t at i onspac e s f x\nf y\nx  s pa ce\n( ) pa i r s i n t he t r a i ni ng s et x y ,\nf x : enco der f unctio n f o r x\nf y : enco der f unctio n f o r y\nFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand\nsimilarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions\nappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\napplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints\ninxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance\ninh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "ofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled\nexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way\nortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe\nrepresentationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning\nisthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno\nimageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "andimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween\nrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwerenever\npaired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach\nother.FigureinspiredfromsuggestionbyHrantKhachatrian.\n5 4 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ninonemodality,arepresentationintheother,andtherelationship(ingeneralajoint\ndistribution)betweenpairs (xy,)consistingofoneobservationxinonemodality\nandanotherobservationyintheothermodality(SrivastavaandSalakhutdino v,\n2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from\nytoitsrepresentation,andtherelationshipbetweenthetworepresentations),\nconceptsinonerepresentationareanchoredintheother,andvice-versa,allowing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "onetomeaningfullygeneralizetonewpairs.Theprocedureisillustratedin\ngure.15.3\n15. 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors\nAnimportantquestionaboutrepresentationlearningiswhatmakesonerepre-\nsentationbetterthananother?Onehypothesisisthatanidealrepresentation\nisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-\nlyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "spacecorrespondingtodierentcauses,sothattherepresentationdisentanglesthe\ncausesfromoneanother.Thishypothesismotivatesapproachesinwhichwerst\nseekagoodrepresentationforp(x).Sucharepresentationmayalsobeagood\nrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesof\nx.Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\nthe1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail.\nForotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006\nInotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\nwitharepresentationthatiseasytomodelforexample,onewhoseentriesare\nsparse,orindependentfromeachother.Arepresentationthatcleanlyseparates\ntheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.\nHowever,afurtherpartofthehypothesismotivatingsemi-supervisedlearning\nviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "propertiescoincide:once weareabletoobtaintheunderlyingexplanationsfor\nwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\ntheothers.Specically,ifarepresentationhrepresentsmanyoftheunderlying\ncausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,\nthenitiseasytopredictfrom.yh\nFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\nlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "wherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly,\nobservingatrainingsetofvaluesalonegivesusnoinformationabout. x p( )y x|\n5 4 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nxp x ( )y = 1 y = 2 y = 3\nFigure15.4:Exampleofadensityoverxthatisamixtureoverthreecomponents.\nThecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture\ncomponents(e.g.,naturalobjectclassesinimagedata)arestatisticallysalient,just\nmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor\ny.\nNext,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\nConsiderthesituationwhere xarisesfromamixture,withonemixturecomponent",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "pervalueofy,asillustratedingure.Ifthemixturecomponentsarewell- 15.4\nseparated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda\nsinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).\nButmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x\nIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and\np(yx|)willbestronglytied,andunsupervisedrepresentationlearningthat\ntriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "semi-supervisedlearningstrategy.\nConsidertheassumptionthatyisoneofthecausalfactorsofx,andlet\nhrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas\nstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\np,pp. (hx) = ( )xh|()h (15.1)\nAsaconsequence,thedatahasmarginalprobability\np() = x E hp. ( )xh| (15.2)\nFromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\nofx(fromageneralization pointofview)istheonethatuncoverstheabovetrue\n5 4 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.\nTheidealrepresentationlearningdiscussedaboveshouldthusrecovertheselatent\nfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe\nveryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe\nconditionaldistributionofygivenxistiedbyBayesruletothecomponentsin\ntheaboveequation:\np( ) = yx|pp ( )xy|()y\np()x. (15.3)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "p( ) = yx|pp ( )xy|()y\np()x. (15.3)\nThusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge\nofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\nsituationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\nperformance.\nAnimportantresearchproblemregardsthefactthatmostobservationsare\nformedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but\ntheunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\nsalientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking\niteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y\nInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible\ntocaptureallormostofthefactorsofvariationthatinuenceanobservation.\nForexample,inavisualscene,shouldtherepresentationalwaysencodeallof\nthesmallestobjectsinthebackground? Itisawell-documented psychological",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "phenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat\narenotimmediately relevanttothetasktheyareperformingsee,e.g.,Simons\nandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\ndetermining toencodeineachsituation.Currently,twoofthemainstrategies what\nfordealingwithalargenumberofunderlyingcausesaretouseasupervised\nlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\nmodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "largerrepresentationsifusingpurelyunsupervisedlearning.\nAnemergingstrategyforunsupervisedlearningistomodifythedenitionof\nwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\nmodelshavebeentrainedtooptimizeaxedcriterion,oftensimilartomean\nsquarederror.Thesexedcriteriadeterminewhichcausesareconsideredsalient.\nForexample,meansquarederrorappliedtothepixelsofanimageimplicitly\nspeciesthatanunderlyingcauseisonlysalientifitsignicantlychangesthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\ntosolveinvolvesinteractingwithsmallobjects.Seegureforanexample 15.5\n5 4 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInput Reconstruction\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\nfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits\nspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\narerelevanttotheroboticstask.Unfortunately,theautoencoderhaslimitedcapacity,\nandthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\nsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\npingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\nobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\nOtherdenitionsofsaliencearepossible.Forexample,ifagroupofpixels\nfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\nbrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\nOnewaytoimplementsuchadenitionofsalienceistousearecentlydeveloped",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "approachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c\nInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassier.\nThefeedforwardclassierattemptstorecognizeallsamplesfromthegenerative\nmodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis\nframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis\nhighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "insection.Forthepurposesofthepresentdiscussion,itissucientto 20.10.4\nunderstandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015\nshowedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect\ntogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully\ngeneratetheearswhentrainedwiththeadversarialframework.Becausethe\nearsarenotextremelybrightordarkcomparedtothesurroundingskin,they\narenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly\n5 4 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nGroundTruth MSE Adversarial\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\nlearningwhichfeaturesaresalient.Inthisexample,thepredictivegenerativenetwork\nhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspecic\nviewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould\nemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "squarederroralone.Becausetheearsdonotcauseanextremedierenceinbrightness\ncomparedtotheneighboringskin,theywerenotsucientlysalientforthemodeltolearn\ntorepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof\nmeansquarederrorandadversarialloss.Usingthislearnedcostfunction,theearsare\nsalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\nimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\ngraciouslyprovidedby (). Lotter e t a l .2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "graciouslyprovidedby (). Lotter e t a l .2015\nrecognizableshapeandconsistentpositionmeansthatafeedforwardnetwork\ncaneasilylearntodetectthem,makingthemhighlysalientunderthegenerative\nadversarialframework.Seegureforexampleimages.Generativeadversarial 15.6\nnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.\nWeexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich\nfactorstorepresent,anddevelopmechanismsforrepresentingdierentfactors\ndependingonthetask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "dependingonthetask.\nAbenetoflearningtheunderlyingcausalfactors,aspointedoutbySchlkopf\netal.(),isthatifthetruegenerativeprocesshas 2012 xasaneectandyas\nacause,thenmodelingp(x y|)isrobusttochangesinp(y).Ifthecause-eect\nrelationshipwasreversed,thiswouldnotbetrue,sincebyBayesrule,p(x y|)\nwouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin\ndistributionduetodierentdomains,temporalnon-stationarity,orchangesin\nthenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "universeareconstant)whilethemarginaldistributionovertheunderlyingcauses\ncanchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan\n5 4 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\nfactorsand. h p( )xh|\n15. 4 D i s t ri b u t ed R ep res en t at i on\nDistributedrepresentationsofconceptsrepresentationscomposedofmanyele-\nmentsthatcanbesetseparatelyfromeachotherareoneofthemostimportant\ntoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\ntheycanusenfeatureswithkvaluestodescribekndierentconcepts.Aswe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "haveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits\nandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof\ndistributedrepresentation.Wenowintroduceanadditionalobservation.Many\ndeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\ncanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\ndiscussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\nbecauseeachdirectioninrepresentationspacecancorrespondtothevalueofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "dierentunderlyingcongurationvariable.\nAnexampleofadistributedrepresentationisavectorofnbinaryfeatures,\nwhichcantake2ncongurations, eachpotentiallycorrespondingtoadierent\nregionininputspace,asillustratedingure.Thiscanbecomparedwith 15.7\nasymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\ncategory.Iftherearensymbolsinthedictionary,onecanimaginenfeature\ndetectors,eachcorrespondingtothedetectionofthepresenceoftheassociated",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "category.Inthatcaseonlyndierentcongurations oftherepresentationspace\narepossible,carvingndierentregionsininputspace,asillustratedingure.15.8\nSuchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\nbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone\nofthemcanbeactive).Asymbolicrepresentationisaspecicexampleofthe\nbroaderclassofnon-distributedrepresentations,whicharerepresentationsthat\nmaycontainmanyentriesbutwithoutsignicantmeaningfulseparatecontrolover",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "eachentry.\nExamplesoflearningalgorithmsbasedonnon-distributedrepresentations\ninclude:\nClusteringmethods,includingthek-meansalgorithm:eachinputpointis\nassignedtoexactlyonecluster.\nk-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\nareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple\n5 4 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh 1h 2 h 3\nh = [ 1 , , 1 1 ]\nh = [ 0 , , 1 1 ]h = [ 1 , , 0 1 ]h = [ 1 , , 1 0 ]\nh = [ 0 , , 1 0 ]h = [ 0 , , 0 1 ]h = [ 1 , , 0 0 ]\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\nbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\nh 1,h 2,andh 3.Eachfeatureisdenedbythresholdingtheoutputofalearned,linear\ntransformation.Eachfeaturedivides R2intotwohalf-planes.Leth+\nibethesetofinput",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ibethesetofinput\npointsforwhichh i=1andh\nibethesetofinputpointsforwhichh i=0.Inthis\nillustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding\narrowpointingtotheh+\nisideoftheboundary.Therepresentationasawholetakes\nonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\nrepresentationvalue[1,1,1]correspondstotheregionh+\n1h+\n2h+\n3.Comparethistothe\nnon-distributedrepresentationsingure.Inthegeneralcaseof 15.8 dinputdimensions,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "adistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes.\nThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)dierent\nregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly\nnregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\nmoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible\n(thereisnoh=0inthisexample)andthatalinearclassierontopofthedistributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "representationisnotabletoassigndierentclassidentitiestoeveryneighboringregion;\nevenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew\nisthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\nlayerandaweakclassierlayercanbeastrongregularizer;aclassiertryingtolearn\ntheconceptofpersonversusnotapersondoesnotneedtoassignadierentclassto\naninputrepresentedaswomanwithglassesthanitassignstoaninputrepresentedas",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "manwithoutglasses.Thiscapacityconstraintencourageseachclassiertofocusonfew\nh iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h\n5 4 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom\neachother,sothisdoesnotqualifyasatruedistributedrepresentation.\nDecisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\nactivatedwhenaninputisgiven.\nGaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\nexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest\nneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "thosevaluescannotreadilybecontrolledseparatelyfromeachother.\nKernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\nalthoughthedegreeofactivationofeachsupportvectorortemplateexample\nisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\nLanguageortranslationmodelsbasedonn-grams.Thesetofcontexts\n(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuxes.\nAleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Separateparametersareestimatedforeachleafofthetree(withsomesharing\nbeingpossible).\nForsomeofthesenon-distributedalgorithms,theoutputisnotconstantby\npartsbutinsteadinterpolatesbetweenneighboringregions.Therelationship\nbetweenthenumberofparameters(orexamples)andthenumberofregionsthey\ncandeneremainslinear.\nAnimportantrelatedconceptthatdistinguishesadistributedrepresentation\nfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "dierentconcepts.Aspuresymbols,catanddogareasfarfromeachother\nasanyothertwosymbols.However,ifoneassociatesthemwithameaningful\ndistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\ncangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation\nmaycontainentriessuchashas_furornumber_of_legsthathavethesame\nvaluefortheembeddingofbothcatanddog.Neurallanguagemodelsthat\noperateondistributedrepresentationsofwordsgeneralizemuchbetterthanother",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\nsection.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\nsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\nabsentfrompurelysymbolicrepresentations.\nWhenandwhycantherebeastatisticaladvantagefromusingadistributed\nrepresentationaspartofalearningalgorithm?D istributedrepresentationscan\n5 4 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintodierentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\nalgorithmbasedonanon-distributedrepresentation.Dierentnon-distributedalgorithms\nmayhavedierentgeometry,buttheytypicallybreaktheinputspaceintoregions,\nw i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "approachisthat,givenenoughparameters,itcantthetrainingsetwithoutsolvinga\ndicultoptimizationalgorithm,becauseitisstraightforwardtochooseadierentoutput\ni n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels\ngeneralizeonlylocallyviathesmoothnessprior,makingitdiculttolearnacomplicated\nfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\nthiswithadistributedrepresentation,gure.15.7\n5 4 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe\ncompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-\ndistributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,\nwhichstatesthatifuv,thenthetargetfunctionftobelearnedhasthe\npropertythatf(u)f(v),ingeneral.Therearemanywaysofformalizingsuchan\nassumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "knowthatf(x)y,thenwechooseanestimator fthatapproximatelysatises\ntheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby\ninputx+.Thisassumptionisclearlyveryuseful,butitsuersfromthecurseof\ndimensionality:inordertolearnatargetfunctionthatincreasesanddecreases\nmanytimesinmanydierentregions,1wemayneedanumberofexamplesthatis\natleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof\ntheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "eachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol\ntovalue.However,thisdoesnotallowustogeneralizetonewsymbolsfornew\nregions.\nIfwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\nsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean\nobjectregardlessofitslocationintheimage,eventhoughspatialtranslationof\ntheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\nLetusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\nbinaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces,as\nillustratedingure.Theexponentiallylargenumberofintersectionsof 15.7 n\nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\nrepresentationlearnercandistinguish.Howmanyregionsaregeneratedbyan\narrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe\nintersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\nd\nj = 0n\nj\n= (Ond). (15.4)\nTherefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\nthenumberofhiddenunits.\n1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "re g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i  e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e\nm i g h t wa n t t o d i  e r i n f 2dd i  e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s .\n5 5 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nThisprovidesageometricargumenttoexplainthegeneralization powerof\ndistributedrepresentation:withO(nd)parameters(fornlinear-threshold features\nin Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade\nnoassumptionatallaboutthedata,andusedarepresentationwithoneunique\nsymbolforeachregion,andseparateparametersforeachsymboltorecognizeits\ncorrespondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "examples.Moregenerally,theargumentinfavorofthedistributedrepresentation\ncouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\nusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\nthedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\ntransformationwithkparameterscanlearnaboutrregionsininputspace,with\nkr,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\nwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "settingwherewewouldneedO(r)examplestoobtainthesamefeaturesand\nassociatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto\nrepresentthemodelmeansthatwehavefewerparameterstot,andthusrequire\nfarfewertrainingexamplestogeneralizewell.\nAfurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\ntationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\ndistinctlyencodesomanydierentregions.Forexample,theVCdimensionofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber\nofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\nmanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode\nspace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\nhtotheoutputyusingalinearclassier.Theuseofadistributedrepresentation\ncombinedwithalinearclassierthusexpressesapriorbeliefthattheclassesto\nberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "capturedbyh.Wewilltypicallywanttolearncategoriessuchasthesetofall\nimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\nrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition\nthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\ngreencarsandredtrucksasanotherclass.\nTheideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\nvalidated. ()ndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery\nofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\nInpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\nthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\ntoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\n5 5 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n-+ =\nFigure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\ntheconceptofgenderfromtheconceptofwearingglasses.Ifwebeginwiththerepre-\nsentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\nconceptofamanwithoutglasses,andnallyaddthevectorrepresentingtheconcept\nofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\nwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\npermissionfrom (). Radford e t a l .2015\ncommonisthatonecouldimagine learningabouteachofthemwithouthavingto\nseeallthecongurationsofalltheothers. ()demonstratedthat Radfordetal.2015\nagenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\ndirectionsinrepresentationspacecapturingdierentunderlyingfactorsofvariation.\nFiguredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "towhetherthepersonismaleorfemale,whileanothercorrespondstowhether\nthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not\nxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiers:\ngradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\ninterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout\nthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\nglasses,withouthavingtocharacterizeallofthecongurations ofthen1other",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "featuresbyexamplescoveringallofthesecombinationsofvalues.Thisformof\nstatisticalseparabilityiswhatallowsonetogeneralizetonewcongurations ofa\npersonsfeaturesthathaveneverbeenseenduringtraining.\n5 5 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n15. 5 E x p on en t i al Gai n s f rom D ep t h\nWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\ntors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\nnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\nimprovedstatisticaleciency.Inthissection,wedescribehowsimilarresultsapply\nmoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\nInsection,wesawanexampleofagenerativemodelthatlearnedabout 15.4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "theexplanatoryfactorsunderlyingimagesoffaces,includingthepersonsgender\nandwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\nthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\nashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\nbetweentheseabstractexplanatoryfactorsandthepixelsintheimage.Inthis\nandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\neachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "high-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\ndemands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas\nfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\nthecompositionofmanynonlinearities.\nIthasbeenproveninmanydierentsettingsthatorganizingcomputation\nthroughthecompositionofmanynonlinearities andahierarchyofreusedfeatures\ncangiveanexponentialboosttostatisticaleciency,ontopoftheexponential",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\nwithsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with\nasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\nfamilythatisauniversalapproximator canapproximatealargeclassoffunctions\n(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough\nhiddenunits.However,therequirednumberofhiddenunitsmaybeverylarge.\nTheoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "therearefamiliesoffunctionsthatcanberepresentedecientlybyanarchitecture\nofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect\ntotheinputsize)withinsucientdepth(depth2ordepth).k1\nInsection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\napproximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\nhiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\nbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "andBengio20082010MontfarandAy2011Montfar2014Krause ,,; ,;,; etal.,\n2013).\n5 5 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInsection,wesawthatasucientlydeepfeedforwardnetworkcanhave 6.4.1\nanexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\nbeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\nmodelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These\nmodelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\nsetofrandomvariables. ()showedthatthereexist DelalleauandBengio2011",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\nneedinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014\nshowedthattherearesignicantdierencesbetweeneverytwonitedepthsof\nSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\ntheirrepresentationalpower.\nAnotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\npoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\ntoonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal.\n2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\ncasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\n15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es\nToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\nrepresentationbetterthananother?Oneanswer,rstintroducedinsection,is15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\nvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\napplications.Moststrategiesforrepresentationlearningarebasedonintroducing\ncluesthathelpthelearningtondtheseunderlyingfactorsofvariations.Theclues\ncanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\nlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "speciesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\ntomakeuseofabundantunlabeleddata,representationlearningmakesuseof\nother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof\nimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\nordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\nregularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis\nimpossibletondauniversallysuperiorregularizationstrategy,onegoalofdeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "learningistondasetoffairlygenericregularizationstrategiesthatareapplicable\ntoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\ntosolve.\n5 5 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nWeprovideherealistofthesegenericregularizationstrategies.Thelistis\nclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning\nalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying\nfactors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d\nbeenpartiallyexpandedhere.\nSmoothness:Thisistheassumptionthatf(x+d)f(x)forunitdand\nsmall.Thisassumptionallowsthelearnertogeneralizefromtraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "examplestonearbypointsininputspace.Manymachinelearningalgorithms\nleveragethisidea,butitisinsucienttoovercomethecurseofdimensionality.\nLinearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\nvariablesarelinear.Thisallowsthealgorithmtomakepredictionseven\nveryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\npredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\nsmoothnessassumptioninsteadmakethelinearityassumption.Theseare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "infactdierentassumptionslinearfunctionswithlargeweightsapplied\ntohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\n()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\nMultipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\nmotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\nexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\nofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearningviarepresentationlearning.Learningthestructureofp(x)\nrequireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|\nx)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\ndescribeshowthisviewmotivatestheuseofdistributedrepresentations,with\nseparatedirectionsinrepresentationspacecorrespondingtoseparatefactors\nofvariation.\nCausalfactors:themodelisconstructedinsuchawaythatittreatsthe\nfactorsofvariationdescribedbythelearnedrepresentationhasthecauses",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "oftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3\nisadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\nmorerobustwhenthedistributionovertheunderlyingcauseschangesor\nwhenweusethemodelforanewtask.\nDepthahierarchicalorganizationofexplanatoryfactors ,or :High-level,\nabstractconceptscanbedenedintermsofsimpleconcepts,forminga\nhierarchy.Fromanotherpoint ofview,theus eofadeeparchitecture\n5 5 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step\nprogram,with eachstepreferringbacktotheoutputoftheprocessing\naccomplishedviaprevioussteps.\nSharedfactorsacrosstasks:Inthecontextwherewehavemanytasks,\ncorrespondingtodierentyivariablessharingthesameinput xorwhere\neachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput\nx,theassumptionisthateachyiisassociatedwithadierentsubsetfroma",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "commonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning\nalltheP(yi|x)viaasharedintermediate representationP(h x|)allows\nsharingofstatisticalstrengthbetweenthetasks.\nManifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\ncentratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\ncase,theseregionscanbeapproximatedbylow-dimensional manifoldswith\namuchsmallerdimensionalitythantheoriginalspacewherethedatalives.\nManymachinelearningalgorithmsbehavesensiblyonlyonthismanifold",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b\nautoencoders,attempttoexplicitlylearnthestructureofthemanifold.\nNaturalclustering:Manymachinelearningalgorithmsassumethateach\nconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\ndatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\nwithineachoneofthese.Thisassumptionmotivatesavarietyoflearning\nalgorithms,includingtangentpropagation, doublebackprop,themanifold\ntangentclassierandadversarialtraining.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "tangentclassierandadversarialtraining.\nTemporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\nmaketheassumptionthatthemostimportantexplanatoryfactorschange\nslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\nexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\nSeesectionforfurtherdescriptionofthisapproach. 13.3\nSparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\ninputsthereisnoneedtouseafeaturethatdetectselephanttrunkswhen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "representinganimageofacat.Itisthereforereasonabletoimposeaprior\nthatanyfeaturethatcanbeinterpretedaspresentorabsentshouldbe\nabsentmostofthetime.\nSimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the\nfactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\n5 5 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\npossibleismarginalindependence,P(h) =\niP(h i),butlineardependencies\northosecapturedbyashallowautoencoderarealsoreasonableassumptions.\nThiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga\nlinearpredictororafactorizedpriorontopofalearnedrepresentation.\nTheconceptofrepresentationlearningtiestogetherallofthemanyforms\nofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep\nprobabilisticmodelsalllearnandexploitrepresentations.Learningthebest",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "possiblerepresentationremainsanexcitingavenueofresearch.\n5 5 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 6\nS t ru ct u r e d Probabilis t i c Mo d e l s\nf or D e e p L e ar n i n g\nDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto\nguidetheirdesigneortsanddescribetheiralgorithms.Oneoftheseformalisms\nistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed\nstructuredprobabilisticmodelsbrieyinsection.Thatbriefpresentationwas 3.14\nsucienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "describesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III\nmodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep\nlearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes\nstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended\ntobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction\nbeforecontinuingwiththischapter.\nAstructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\ninteractwitheachotherdirectly.Hereweusegraphinthegraphtheorysensea\nsetofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\nofthemodelisdenedbyagraph,thesemodelsareoftenalsoreferredtoas\ngraphicalmodels.\nThegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\ndierentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\nprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning\nresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,\nyoumaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert\n558",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nmaybenetfromreadingthenalsectionofthischapter,section,inwhichwe 16.7\nhighlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning\nalgorithms.Deeplearningpractitioners tendtouseverydierentmodelstructures,\nlearningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest\nofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese\ndierencesinpreferencesandexplainthereasonsforthem.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthischapterwerstdescribethechallengesofbuildinglarge-scaleproba-\nbilisticmodels.Next,wedescribehowtouseagraphtodescribethestructure\nofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany\nchallenges,itisnotwithoutitsowncomplications. Oneofthemajordicultiesin\ngraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract\ndirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem.We\noutlinetwoapproachestoresolvingthisdicultybylearningaboutthedependen-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5\ndeeplearningpractitioners placeonspecicapproachestographicalmodelingin\nsection.16.7\n16.1TheChallengeofUnstructuredModeling\nThegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\nneededtosolvearticialintelligence.Thismeansbeingabletounderstandhigh-\ndimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\nbeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "documentscontainingmultiplewordsandpunctuationcharacters.\nClassicationalgorithmscantakeaninputfromsucharichhigh-dimensional\ndistributionandsummarizeitwithacategoricallabelwhatobjectisinaphoto,\nwhatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\nofclassicationdiscardsmostoftheinformationintheinputandproducesa\nsingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\nclassierisalsooftenabletoignoremanypartsoftheinput.Forexample,when",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "recognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof\nthephoto.\nItispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\noftenmoreexpensivethanclassication.Someofthemrequireproducingmultiple\noutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\n1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "e n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c .\n5 5 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\nDensityestimation:givenaninput x,themachinelearningsystemreturns\nanestimateofthetruedensity p( x)underthedatageneratingdistribution.\nThisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-\ningoftheentireinput.Ifevenoneelementofthevectorisunusual,the\nsystemmustassignitalowprobability.\nDenoising:givenadamagedorincorrectlyobservedinput  x,themachine",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "learningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,\nthemachinelearningsystemmightbeaskedtoremovedustorscratches\nfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\nestimatedcleanexample x)andanunderstandingoftheentireinput(since\nevenonedamagedareawillstillrevealthenalestimateasbeingdamaged).\nMissingvalueimputation:giventheobservationsofsomeelementsof x,\nthemodelisaskedtoreturnestimatesoforaprobabilitydistributionover",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "someoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs.\nBecausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it\nmustunderstandtheentireinput.\nSampling:themodelgeneratesnewsamplesfromthedistribution p( x).\nApplicationsincludespeechsynthesis,i.e.producingnewwaveformsthat\nsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\ngoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\nfromthewrongdistribution,thenthesamplingprocessiswrong.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "Foranexampleofasamplingtaskusingsmallnaturalimages,seegure.16.1\nModelingarichdistributionoverthousandsormillionsofrandomvariablesisa\nchallengingtask,bothcomputationally andstatistically.Supposeweonlywanted\ntomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\nseemsoverwhelming.Forasmall, 3232 2 pixelcolor(RGB)image,thereare3 0 7 2\npossiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan\ntheestimatednumberofatomsintheuniverse.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "theestimatednumberofatomsintheuniverse.\nIngeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining\nndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof\nrepresenting P(x)bystoringalookuptablewithoneprobabilityvalueperpossible\noutcomerequires knparameters!\nThisisnotfeasibleforseveralreasons:\n5 6 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example3232pixelcolor\nimagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )\ndrawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\natthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\nspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\nadjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011\n5 6 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues\nof nand k,representingthedistributionasatablewillrequiretoomany\nvaluestostore.\n St a t i s t i c a l e  c i e nc y:Asthenumberofparametersinamodelincreases,\nsodoestheamountoftrainingdataneededtochoosethevaluesofthose\nparametersusingastatisticalestimator.Becausethetable-basedmodel",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "hasanastronomicalnumberofparameters,itwillrequireanastronomically\nlargetrainingsettotaccurately.Anysuchmodelwillovertthetraining\nsetverybadlyunlessadditionalassumptionsaremadelinkingthedierent\nentriesinthetable(forexample,likeinback-oorsmoothed n-grammodels,\nsection).12.4.1\n R u nt i m e :  t h e c o s t o f i nfe r e nc e:Supposewewanttoperformaninference\ntaskwhereweuseourmodelofthejointdistribution P(x)tocomputesome\notherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "distribution P(x 2|x 1).Computingthesedistributionswillrequiresumming\nacrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\nintractablememorycostofstoringthemodel.\n R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample\nfromthemodel.Thenaivewaytodothisistosamplesomevalueu U(0 ,1),\ntheniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\nexceed uandreturntheoutcomecorrespondingtothatpositioninthetable.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "Thisrequiresreadingthroughthewholetableintheworstcase,soithas\nthesameexponentialcostastheotheroperations.\nTheproblemwiththetable-basedapproachisthatweareexplicitlymodeling\neverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\nprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\nUsually,mostvariablesinuenceeachotheronlyindirectly.\nForexample,considermodelingthenishingtimesofateaminarelayrace.\nSupposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\nherlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\nlapandhandsthebatontoCarol,whorunsthenallap.Wecanmodeleachof\ntheirnishingtimesasacontinuousrandomvariable.Alicesnishingtimedoes\nnotdependonanyoneelses,sinceshegoesrst.Bobsnishingtimedepends\nonAlices,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\nhascompletedhers.IfAlicenishesfaster,Bobwillnishfaster,allelsebeing\n5 6 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nequal.Finally,Carolsnishingtimedependsonbothherteammates.IfAliceis\nslow,Bobwillprobablynishlatetoo.Asaconsequence,Carolwillhavequitea\nlatestartingtimeandthusislikelytohavealatenishingtimeaswell.However,\nCarolsnishingtimedependsonly i ndir e c t l yonAlicesnishingtimeviaBobs.\nIfwealreadyknowBobsnishingtime,wewillnotbeabletoestimateCarols\nnishingtimebetterbyndingoutwhatAlicesnishingtimewas.Thismeans",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "wecanmodeltherelayraceusingonlytwointeractions: AliceseectonBoband\nBobseectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\nandCarolfromourmodel.\nStructuredprobabilisticmodelsprovideaformalframeworkformodelingonly\ndirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\nsignicantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\nThesesmallermodelsalsohavedramatically reducedcomputational costinterms",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\nthemodel.\n16.2UsingGraphstoDescribeModelStructure\nStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseofnodesor\nverticesconnectedbyedges)torepresentinteractionsbetweenrandomvariables.\nEachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\nThesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\ninteractionsneedtobeexplicitlymodeled.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "interactionsneedtobeexplicitlymodeled.\nThereismorethanonewaytodescribetheinteractionsinaprobability\ndistributionusingagraph.Inthefollowingsectionswedescribesomeofthemost\npopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\ntwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\nundirectedgraphs.\n1 6 . 2 . 1 D i rect ed Mo d el s\nOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,\notherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,).",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Directedgraphicalmodelsarecalleddirectedbecausetheiredgesaredirected,\n2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm  B a y e s i a n n e t wo rk  wh e n o n e wis h e s t o  e m p h a s i z e\nt h e j u d g m e n t a l  n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y\nre p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s .\n5 6 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nt 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol\nFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alicesnishing\ntimet 0inuencesBobsnishingtimet 1,becauseBobdoesnotgettostartrunninguntil\nAlicenishes.Likewise,CarolonlygetstostartrunningafterBobnishes,soBobs\nnishingtimet 1directlyinuencesCarolsnishingtimet 2.\nthatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariables\nprobabilitydistributionisdenedintermsoftheothers.Drawinganarrowfrom\natobmeansthatwedenetheprobabilitydistributionoverbviaaconditional\ndistribution,withaasoneofthevariablesontherightsideoftheconditioning\nbar.Inotherwords,thedistributionoverbdependsonthevalueofa.\nContinuingwiththerelayraceexamplefromsection,supposewename 16.1\nAlicesnishingtimet 0,Bobsnishingtimet 1,andCarolsnishingtimet 2.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "Aswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends\ndirectlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected\ngraphicalmodel,illustratedingure.16.2\nFormally,adirectedgraphicalmodeldenedonvariables xisdenedbya\ndirectedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,\nandasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where\nP aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven\nby\np() = x i p(x i| P aG(x i)) . (16.1)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "by\np() = x i p(x i| P aG(x i)) . (16.1)\nInourrelayraceexample,thismeansthat,usingthegraphdrawningure,16.2\np(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2)\nThisisourrsttimeseeingastructuredprobabilisticmodelinaction.We\ncanexaminethecostofusingit,inordertoobservehowstructuredmodelinghas\nmanyadvantagesrelativetounstructuredmodeling.\nSupposewerepresentedtimebydiscretizingtimerangingfromminute0to\nminute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "variablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha\ntable,itwouldneedtostore999,999values(100valuesoft 0100valuesoft 1\n100valuesoft 2,minus1,sincetheprobabilityofoneofthecongurations ismade\n5 6 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\nonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe\ndistributionovert 0requires99values,thetabledeningt 1givent 0requires9900\nvalues,andsodoesthetabledeningt 2givent 1.Thiscomestoatotalof19,899\nvalues.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\nparametersbyafactorofmorethan50!",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "parametersbyafactorofmorethan50!\nIngeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe\nsingletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose\nwebuildadirectedgraphicalmodeloverthesevariables.If misthemaximum\nnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\nconditionalprobabilitydistribution,thenthecostofthetablesforthedirected\nmodelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we\ngetverydramaticsavings.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "getverydramaticsavings.\nInotherwords,solongaseachvariablehasfewparentsinthegraph,the\ndistributioncanberepresentedwithveryfewparameters.Somerestrictionson\nthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\noperationslikecomputingmarginalorconditionaldistributionsoversubsetsof\nvariablesareecient.\nItisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\nthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areconditionallyindependentfromeachother.Itisalsopossibletomakeother\nkindsofsimplifyingassumptions.Forexample,supposeweassumeBobalways\nrunsthesameregardlessofhowAliceperformed.(Inreality,Alicesperformance\nprobablyinuencesBobsperformancedependingonBobspersonality,ifAlice\nrunsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\nmatchherexceptionalperformance,oritmightmakehimovercondentandlazy).\nThentheonlyeectAlicehasonBobsnishingtimeisthatwemustaddAlices",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "nishingtimetothetotalamountoftimewethinkBobneedstorun.This\nobservationallowsustodeneamodelwith O( k)parametersinsteadof O( k2).\nHowever,notethatt 0andt 1arestilldirectlydependentwiththisassumption,\nbecauset 1representstheabsolutetimeatwhichBobnishes,notthetotaltime\nhehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom\nt 0tot 1.TheassumptionthatBobspersonalrunningtimeisindependentfrom\nallotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "encodethisinformationinthedenitionoftheconditionaldistributionitself.The\nconditionaldistributionisnolongera k k1elementtableindexedbyt 0andt 1\nbutisnowaslightlymorecomplicatedformulausingonly k1parameters.The\ndirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedene\n5 6 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nourconditionaldistributions.Itonlydeneswhichvariablestheyareallowedto\ntakeinasarguments.\n1 6 . 2 . 2 Un d i rec t ed Mo d el s\nDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\nticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise\nknownasMarkovrandomelds(MRFs)orMarkovnetworks(Kinder-\nmann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\nareundirected.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "areundirected.\nDirectedmodelsaremostnaturallyapplicabletosituationswherethereis\naclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\nsituationswhereweunderstandthecausalityandthecausalityonlyowsinone\ndirection.Onesuchsituationistherelayraceexample.Earlierrunnersaectthe\nnishingtimesoflaterrunners;laterrunnersdonotaectthenishingtimesof\nearlierrunners.\nNotallsituationswemightwanttomodelhavesuchacleardirectiontotheir",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto\noperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\nAsanexampleofsuchasituation,supposewewanttomodeladistribution\noverthreebinaryvariables:whetherornotyouaresick,whetherornotyour\ncoworkerissick,andwhetherornotyourroommateissick.Asintherelayrace\nexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\ntakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\ncolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\nit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\nthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\nacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe\ncoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\nroommate.\nInthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "itisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional\nnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\nAswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\nedge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\notherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\narrow,andisnotassociatedwithaconditionalprobabilitydistribution.\n5 6 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh r h r h y h y h c h c\nFigure16.3:Anundirectedgraphrepresentinghowyourroommateshealthh r,your\nhealthh y,andyourworkcolleagues healthh caecteachother.Youandyourroommate\nmightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,\nbutassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan\nonlyinfecteachotherindirectlyviayou.\nWedenotetherandomvariablerepresentingyourhealthash y,therandom",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "variablerepresentingyourroommateshealthash r,andtherandomvariable\nrepresentingyourcolleagueshealthash c.Seegureforadrawingofthe 16.3\ngraphrepresentingthisscenario.\nFormally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\ndenedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor (C)\n(alsocalledacliquepotential)measurestheanityofthevariablesinthatclique\nforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "non-negative.Togethertheydeneanunnormalizedprobabilitydistribution\n p() = x CG  .()C (16.3)\nTheunnormalized probabilitydistributionisecienttoworkwithsolongas\nallthecliquesaresmall.Itencodestheideathatstateswithhigheranityare\nmorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\ndenitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\ntogetherwillyieldavalidprobabilitydistribution.Seegureforanexample 16.4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "ofreadingfactorizationinformationfromanundirectedgraph.\nOurexampleofthecoldspreadingbetweenyou,yourroommate,andyour\ncolleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis\ncliquecanbedenedbyatable,andmighthavevaluesresemblingthese:\nh y= 0h y= 1\nh c= 021\nh c= 1110\n3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f\nt h e g ra p h .\n5 6 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\n(havingbeeninfectedwithacold).Bothofyouareusuallyhealthy,sothe\ncorrespondingstatehasthehighestanity.Thestatewhereonlyoneofyouis\nsickhasthelowestanity,becausethisisararestate.Thestatewherebothof\nyouaresick(becauseoneofyouhasinfectedtheother)isahigheranitystate,\nthoughstillnotascommonasthestatewherebotharehealthy.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "Tocompletethemodel,wewouldneedtoalsodeneasimilarfactorforthe\ncliquecontainingh yandh r.\n1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n\nWhiletheunnormalized probabilitydistributionisguaranteedtobenon-negative\neverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\nprobabilitydistribution,wemustusethecorrespondingnormalizedprobability\ndistribution:4\np() =x1\nZ p()x (16.4)\nwhere Zisthevaluethatresultsintheprobabilitydistributionsummingor\nintegratingto1:\nZ=\n p d . ()xx (16.5)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "integratingto1:\nZ=\n p d . ()xx (16.5)\nYoucanthinkof Zasaconstantwhenthe functionsareheldconstant.Note\nthatifthe functionshaveparameters,then Zisafunctionofthoseparameters.\nItiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace.\nThenormalizingconstant Zisknownasthepartitionfunction,atermborrowed\nfromstatisticalphysics.\nSince Zisanintegralorsumoverallpossiblejointassignmentsofthestatex\nitisoftenintractabletocompute.Inordertobeabletoobtainthenormalized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionofanundirectedmodel,themodelstructureandthe\ndenitionsofthe functionsmustbeconducivetocomputing Zeciently.In\nthecontextofdeeplearning, Zisusuallyintractable.Due totheintractability\nofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate\nalgorithmsarethetopicofchapter.18\nOneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\nisthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\n4A d i s t rib u t i o n d e  n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s\nd is t rib u t i on .\n5 6 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nof povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\nscalarvariablexwithasinglecliquepotential  R  x x () = 2.Inthiscase,\nZ=\nx2d x . (16.6)\nSincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\nthischoiceof ( x).Sometimes thechoiceofsomeparameterofthe functions\ndetermineswhethertheprobabilit ydistributionisdened.Forexample,for\n( x; ) =exp  x2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "( x; ) =exp  x2\n,the parameterdetermineswhether Zexists.Positive \nresultsinaGaussiandistributionoverxbutallothervaluesof make impossible\ntonormalize.\nOnekeydierencebetweendirectedmodelingandundirectedmodelingisthat\ndirectedmodelsaredeneddirectlyintermsofprobabilitydistributionsfrom\nthestart,whileundirectedmodelsaredenedmorelooselyby functionsthat\narethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone\nmustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\nhasdramaticeectonthekindofprobabilitydistributionthatagivensetof \nfunctionscorrespondsto.Forexample,consideran n-dimensionalvector-valued\nrandomvariable xandanundirectedmodelparametrized byavectorofbiases\nb.Supposewehaveonecliqueforeachelementofx, ( ) i(x i) =exp( b ix i).What\nkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\nnothaveenoughinformation,becausewehavenotyetspeciedthedomainofx.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "Ifx  Rn,thentheintegraldening Zdivergesandnoprobabilitydistribution\nexists.Ifx{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with\np(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors\n({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge\nvalueof b iactuallyreduces p(x j=1)for j= i.Often,itispossibletoleverage\ntheeectofacarefullychosendomainofavariableinordertoobtaincomplicated",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "behaviorfromarelativelysimplesetof functions.Wewillexploreapractical\napplicationofthisidealater,insection.20.6\n1 6 . 2 . 4 E n erg y-B a s ed Mo d el s\nManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\nsumptionthatx , p(x) >0.Aconvenientwaytoenforcethisconditionistouse\nan (EBM)where energy-basedmodel\n p E () = exp( x ())x (16.7)\n5 6 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas\n1\nZ a b ,(ab ,)  b c ,(bc ,)  a d ,(ad ,)  b e ,(be ,)  e f ,(ef ,)foranappropriatechoiceofthe func-\ntions.\nand E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall\nz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\nforanystatex.Beingcompletelyfree tochoosetheenergyfunctionmakes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\nconstrainedoptimization toarbitrarilyimposesomespecicminimalprobability\nvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\nTheprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\nbutneverreachit.\nAnydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\nmanndistribution.Forthisreason,manyenergy-basedmodelsarecalled",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Boltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,\n1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\namodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\ntermBoltzmannmachinewasrstintroducedtodescribeamodelwithexclusively\nbinaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\nBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann\nmachineswereoriginallydenedtoencompassbothmodelswithandwithoutla-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\nmodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\naremoreoftencalledMarkovrandomeldsorlog-linearmodels.\nCliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\nprobabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdierent\ncliquesintheundirectedgraphcorrespondtothedierenttermsoftheenergy\nfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "network:theexponentiationmakeseachtermintheenergyfunctioncorrespond\ntoafactorforadierentclique.Seegureforanexampleofhowtoreadthe 16.5\n5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z\n5 7 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure16.5:Thisgraphimpliesthat E(abcdef , , , , ,)canbewrittenas E a b ,(ab ,)+\nE b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique\nenergyfunctions.Notethatwecanobtainthe functionsingurebysettingeach 16.4 \ntotheexponentialofthecorrespondingnegativeenergy,e.g.,  a b ,(ab ,) =exp(())  Eab ,.\nformoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\nofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\nanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\nbethoughtofasanexpertthatdetermineswhetheraparticularsoftconstraint\nissatised.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\nalow-dimensionalprojectionoftherandomvariables,butwhencombinedby\nmultiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "dimensionalconstraint.\nOnepartofthedenitionofanenergy-basedmodelservesnofunctionalpurpose\nfromamachinelearningpointofview:thesigninequation.This16.7 sign\ncouldbeincorporatedintothedenitionof E.Formanychoicesofthefunction\nE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\nsignispresentprimarilytopreservecompatibilitybetweenthemachinelearning\nliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "wereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,\nphysicalenergyanddoesnothavearbitrarysign.Terminologysuchasenergy\nandpartitionfunctionremainsassociatedwiththesetechniques,eventhough\ntheirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey\nweredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986\nreferredtonegativeenergyasharmony)havechosentoemitthenegation,but\nthisisnotthestandardconvention.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thisisnotthestandardconvention.\nManyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute\np m o de l( x)butonly log  p m o de l( x).Forenergy-basedmodelswithlatentvariables h,\nthesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\n5 7 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b a s b\n(a) (b)\nFigure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis\nactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres\nisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\nthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.\ncalledthe :freeenergy\nF  () = x log\nhexp(( ))  E x h , . (16.8)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "F  () = x log\nhexp(( ))  E x h , . (16.8)\nInthisbook,weusuallypreferthemoregeneral log  p m o de l() xformulation.\n1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n\nTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\nneedtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions\ncanbeenabledordisabledbyobservingothervariables.Moreformally,wewould\nliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "other,giventhevaluesofothersubsetsofvariables.\nIdentifyingtheconditionalindependencesinagraphisverysimpleinthecase\nofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph\niscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat\nAisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath\ninvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\nseparated.Werefertopathsinvolvingonlyunobservedvariablesasactiveand\npathsincludinganobservedvariableasinactive.\nWhenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\nSeegureforadepictionofhowactiveandinactivepathsinanundirected 16.6\nmodellookwhendrawninthisway.Seegureforanexampleofreading 16.7\nseparationfromanundirectedgraph.\nSimilarconcepts applytodirectedmodels ,exceptthatinthecontextof",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "directedmodels,theseconceptsarereferredtoasd-separation.Thedstands\nfordependence.D-separati onfordirectedgraphsisdenedthesameasseparation\n5 7 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na\nb c\nd\nFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\nbisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom\natoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb\nalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.\nTherefore,aanddarenotseparatedgivenb.\nforundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies\nthatisindependentfromgiven. A B S\nAswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\ngraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\naredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch\npathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\nmorecomplicated. Seegureforaguidetoidentifyingactivepathsina 16.8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "directedmodel.Seegureforanexampleofreadingsomepropertiesfroma 16.9\ngraph.\nItisimportanttorememberthatseparationandd-separationtellusonly\naboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno\nrequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\nitisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\ntorepresentanydistribution.Infact,somedistributionscontainindependences",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "thatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\nspecicindependencesareindependencesthatarepresentdependentonthe\nvalueofsomevariablesinthenetwork.Forexample,consideramodelofthree\nbinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,\nbutwhenais1,bisdeterministicallyequaltoc.Encodingthebehaviorwhen\na= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb\nandcareindependentwhena.= 0\nIngeneral,agraphwillneverimplythatanindependenceexistswhenitdoes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "not.However,agraphmayfailtoencodeanindependence.\n5 7 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b\na s b\na\nsb a s ba s b\nc( a ) ( b )\n( c ) ( d )\nFigure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\nvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa.\nThiskindofpathbecomesblockedifsisobserved.Wehavealreadyseenthiskindof\npathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For\nexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaand",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "bmeasurethewindspeedattwodierentnearbyweathermonitoringoutposts.Ifwe\nobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This\nkindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we\nexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected\nwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing\nthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "pathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe\ncollidercase.TheV-structurecausesaandbtoberelatedbytheexplainingaway\neect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose\nsisavariableindicatingthatyourcolleagueisnotatwork.Thevariablearepresents\nherbeingsick,whilebrepresentsherbeingonvacation.Ifyouobservethatsheisnot\natwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "likelythatbothhavehappenedatthesametime.Ifyoundoutthatsheisonvacation,\nthisfactissucienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n\nsick.Theexplainingawayeecthappensevenifanydescendantof ( d ) sisobserved!For\nexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareport\nfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases\nyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "morelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha\nV-structureistoobservenoneofthedescendantsofthesharedchild.\n5 7 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nc\nd e\nFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\ninclude:\naandbared-separatedgiventheemptyset.\naandeared-separatedgivenc.\ndandeared-separatedgivenc.\nWecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\nvariables:\naandbarenotd-separatedgivenc.\naandbarenotd-separatedgivend.\n5 7 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s\nWeoftenrefertoaspecicmachinelearningmodelasbeingundirectedordirected.\nForexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\nThischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\nisinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\nDirectedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\nvantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\nweshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\ndependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\nuseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\ncapturethemostindependencesintheprobabilitydistributionorwhichapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "usesthefewestedgestodescribethedistribution.Thereareotherfactorsthat\ncanaectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle\nprobabilitydistribution,wemaysometimesswitchbetweendierentmodeling\nlanguages.Sometimesadierentlanguagebecomesmoreappropriateifweobserve\nacertainsubsetofvariables,orifwewishtoperformadierentcomputational\ntask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward\napproachtoecientlydrawsamplesfromthemodel(describedinsection)16.3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whiletheundirectedmodelformulationisoftenusefulforderivingapproximate\ninferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19\nmodelsishighlightedinequation).19.56\nEveryprobabilitydistributioncanberepresentedbyeitheradirectedmodel\norbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany\ndistributionbyusingacompletegraph.Inthecaseofadirectedmodel,the\ncompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingon",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "therandomvariables,andeachvariablehasallothervariablesthatprecedeitin\ntheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete\ngraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.\nSeegureforanexample. 16.10\nOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\nvariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\ndoesnotimplyanyindependences.\nWhenwerepresentaprobabilitydistributionwithagraph,wewanttochoose",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\nindependencesthatdonotactuallyexist.\nFromthispointofview,somedistributionscanberepresentedmoreeciently\n5 7 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\nHereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph.\nIntheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t )\nInthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\nvariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\nvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.\nusingdirectedmodels,whileotherdistributionscanberepresentedmoreeciently\nusingundirectedmodels.Inotherwords,directedmodelscanencodesome\nindependencesthatundirectedmodelscannotencode,andviceversa.\nDirectedmodelsareabletouseonespecickindofsubstructurethatundirected\nmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "Thestructureoccurswhentworandomvariablesaandbarebothparentsofa\nthirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither\ndirection.(Thenameimmoralitymayseemstrange;itwascoinedinthegraphical\nmodelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\nwithgraph Dintoanundirectedmodel,weneedtocreateanewgraph U.For\neverypairofvariablesxandy,weaddanundirectededgeconnectingxandyto\nUifthereisadirectededge(ineitherdirection)connectingxandyinDorifx",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "andyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa\nmoralizedgraph.Seegureforexamplesofconvertingdirectedmodelsto 16.11\nundirectedmodelsviamoralization.\nLikewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\ncanrepresentperfectly.Specically,adirectedgraphcannotcaptureallofthe D\nconditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop\noflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "asequenceofvariablesconnectedbyundirectededges,withthelastvariablein\nthesequenceconnectedbacktotherstvariableinthesequence.Achordisa\nconnectionbetweenanytwonon-consecutivevariablesinthesequencedeninga\nloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese\nloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\n5 7 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nFigure16.11:Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\n(bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted\ntoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\nresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "independences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )\ntoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely\nofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive\npathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude\nacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab.\n( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\nedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\ndirectdependences.\n5 7 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nd ca b\nd ca b\nd c\nFigure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected\nmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour\nwithnochords.Specically,theundirectedmodelencodestwodierentindependencesthat\nnodirectedmodelcancapturesimultaneously:acbd |{ ,}andbdac |{ ,}.To ( C e n t e r )\nconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "ensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither\naddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis\nexample,wechoosetoaddtheedgeconnectingaandc.Tonishtheconversion ( R i g h t )\nprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\ndirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\nandalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\nalphabeticalorder.\nthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.\nThegraphformedbyaddingchordstoUisknownasachordalortriangulated\ngraph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\nloops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign\ndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein\nD,ortheresultdoesnotdeneavaliddirectedprobabilisticmodel.Oneway",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "toassigndirectionstotheedgesinDistoimposeanorderingontherandom\nvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingto\nthenodethatcomeslaterintheordering.Seegureforademonstration. 16.12\n1 6 . 2 . 7 F a ct o r G ra p h s\nFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean\nambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\nanundirectedmodel,thescopeofevery functionmustbeaofsomeclique s u b s e t",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\nacorrespondingfactorwhosescopeencompassestheentirecliqueforexample,\nacliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\normaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\n5 7 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach \nfunction.Specically,afactorgraphisagraphicalrepresentationofanundirected\nmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\nascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected\nmodel.Therestofthenodesaredrawnassquares.Thesenodescorrespondto\nthefactors oftheunnormalized probabilitydistribution.Variablesandfactors",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "maybeconnectedwithundirectededges.Avariableandafactorareconnected\ninthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\ntheunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother\nfactorinthegraph,norcanavariablebeconnectedtoavariable.Seegure16.13\nforanexampleofhowfactorgraphscanresolveambiguityintheinterpretation of\nundirectednetworks.\na b\nca b\ncf 1 f 1a b\ncf 1 f 1f 2 f 2\nf 3 f 3\nFigure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "ofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:\na,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )\nfactorgraphhasonefactoroverallthreevariables.Anothervalidfactorgraph ( R i g h t )\nforthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo\nvariables.Representation,inference,andlearningareallasymptoticallycheaperinthis\nfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sameundirectedgraphtorepresent.\n16.3SamplingfromGraphicalModels\nGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\nOneadvantageofdirectedgraphicalmodelsisthatasimpleandecientproce-\ndurecalledancestralsamplingcanproduceasamplefromthejointdistribution\nrepresentedbythemodel.\nThebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,\nsothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables\n5 8 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ncanthenbesampledinthisorder.Inotherwords,werstsamplex 1 P(x 1),\nthensample P(x 2| P aG(x 2)),andsoon,untilnallywesample P(x n| P aG(x n)).\nSolongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,\nthenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\nguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\nsamplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "sampleavariablebeforeitsparentsareavailable.\nForsomegraphs,morethanonetopologicalorderingispossible.Ancestral\nsamplingmaybeusedwithanyofthesetopologicalorderings.\nAncestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\ntionaliseasy)andconvenient.\nOnedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\nmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\noperation.Whenwewishtosamplefromasubsetofthevariablesinadirected",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\ningvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\nInthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\nspeciedbythemodeldistribution.Otherwise,theconditionaldistributionswe\nneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.\nTheseposteriordistributionsareusuallynotexplicitlyspeciedandparametrized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\nthisisthecase,ancestralsamplingisnolongerecient.\nUnfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\ncansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\noftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\ndistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\nsomanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "fromanundirectedmodelwithoutrstconvertingittoadirectedmodelseemsto\nrequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\nvariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\ndrawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass\nprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewe\nhaveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We\niterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "variables,from p(x i|x i).Duetotheseparationpropertiesofthegraphical\nmodel,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,\nafterwehavemadeonepassthroughthegraphicalmodelandsampledall n\nvariables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe\n5 8 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nprocessandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors.\nAsymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\nthecorrectdistribution.Itcanbediculttodeterminewhenthesampleshave\nreachedasucientlyaccurateapproximationofthedesireddistribution.Sampling\ntechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\nchapter.17\n16.4AdvantagesofStructuredModeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "chapter.17\n16.4AdvantagesofStructuredModeling\nTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\nustodramatically reducethecostofrepresentingprobabilitydistributionsaswell\naslearningandinference.Samplingisalsoacceleratedinthecaseofdirected\nmodels,whilethesituationcanbecomplicatedwithundirectedmodels.The\nprimarymechanismthatallowsalloftheseoperationstouselessruntimeand\nmemoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "informationbyleavingedgesout.Anywherethereisnotanedge,themodel\nspeciestheassumptionthatwedonotneedtomodeladirectinteraction.\nAlessquantiablebenetofusingstructuredprobabilisticmodelsisthat\ntheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\nknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\ndevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\ninferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\ndata.Wecanthencombinethesedierentalgorithmsandstructuresandobtain\naCartesianproductofdierentpossibilities.Itwouldbemuchmoredicultto\ndesignend-to-endalgorithmsforeverypossiblesituation.\n16.5LearningaboutDependencies\nAgoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\nobservedorvisiblevariables v.Oftenthedierentelementsofvarehighly\ndependentoneachother.Inthecontextofdeeplearning,theapproachmost",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedtomodelthesedependenciesistointroduceseverallatentor\nhiddenvariables,h.Themodelcanthencapturedependenciesbetweenanypair\nofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and\ndirectdependenciesbetweenandv h j.\nAgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto\n5 8 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\ncliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis\ncostlybothinacomputational sense,becausethenumberofparametersthat\nmustbestoredinmemoryscalesexponentiallywiththenumberofmembersina\nclique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\nrequiresawealthofdatatoestimateaccurately.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "requiresawealthofdatatoestimateaccurately.\nWhenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\nwithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\ngraphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\nomitedgesbetweenothervariables.Anentireeldofmachinelearningcalled\nstructurelearningisdevotedtothisproblemForagoodreferenceonstructure\nlearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "aformofgreedysearch.Astructureisproposed,amodelwiththatstructure\nistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\npenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\naddedorremovedarethenproposedasthenextstepofthesearch.Thesearch\nproceedstoanewstructurethatisexpectedtoincreasethescore.\nUsinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\ndiscretesearchesandmultipleroundsoftraining.Axedstructureovervisible",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "andhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits\ntoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter\nlearningtechniqueswecanlearnamodelwithaxedstructurethatimputesthe\nrightstructureonthemarginal . p()v\nLatentvariableshaveadvantagesbeyondtheirroleinecientlycapturing p(v).\nThenewvariables halsoprovideanalternativerepresentationforv.Forexample,\nasdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This\nmeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo\nclassication.Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14\ncodinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassier,\norascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,\nbutdeepermodelsandmodelswithdierentkindsofinteractionscancreateeven\nricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "bylearninglatentvariables.Often,givensomemodelofvandh,experimental\nobservationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor\nv.\n5 8 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n16.6InferenceandApproximateInference\nOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\nhowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\nwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\nextractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed\ntosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\nusingtheprincipleofmaximumlikelihood.Because",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "usingtheprincipleofmaximumlikelihood.Because\nlog()= p v E h h p (| v )[log( )log( )] p h v , p h v| ,(16.9)\nweoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof\ntheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof\nsomevariablesgivenothervariables,orpredicttheprobabilitydistributionover\nsomevariablesgiventhevalueofothervariables.\nUnfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\nintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\nwithareasonablenumberofparameters,butthegraphsusedfordeeplearningare\nusuallynotrestrictiveenoughtoalsoallowecientinference.\nItisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\ngraphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe\ncomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\nhasasolutionandndingasolutionifoneexists.Problemsin#Prequirecounting",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\nwedeneagraphicalmodeloverthebinaryvariablesina3-SATproblem.We\ncanimposeauniformdistributionoverthesevariables.Wecanthenaddone\nbinarylatentvariableperclausethatindicateswhethereachclauseissatised.\nWecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare\nsatised.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\ntreeoflatentvariables,witheachnodeinthetreereportingwhethertwoother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "variablesaresatised.Theleavesofthistreearethevariablesforeachclause.\nTherootofthetreereportswhethertheentireproblemissatised.Duetothe\nuniformdistributionovertheliterals,themarginaldistributionovertherootofthe\nreductiontreespecieswhatfractionofassignmentssatisfytheproblem.While\nthisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical\nreal-worldscenarios.\nThismotivatestheuseofapproximate inference.Inthecontextofdeep",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "learning,thisusuallyreferstovariationalinference,inwhichweapproximate the\n5 8 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntruedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas\nclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\ninchapter.19\n16.7TheDeepLearningApproachtoStructuredProb-\nabilisticModels\nDeeplearningpractitioners generallyusethesamebasiccomputational toolsas\nothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\nHowever,inthecontextofdeeplearning,weusuallymakedierentdesigndecisions",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\nhaveaverydierentavorfrommoretraditionalgraphicalmodels.\nDeeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\ncontextofgraphicalmodels,wecandenethedepthofamodelintermsofthe\ngraphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa\nlatentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved\nvariableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "depthofanysuch h i.Thiskindofdepthisdierentfromthedepthinducedby\nthecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno\nlatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational\ngraphstodenetheconditionaldistributionswithinamodel.\nDeeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\ntations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\nshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "haveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave\nmorelatentvariablesthanobservedvariables.Complicated nonlinearinteractions\nbetweenvariablesareaccomplishedviaindirectconnectionsthatowthrough\nmultiplelatentvariables.\nBycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\nareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\nrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\nvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.\nThewaythatlatentvariablesaredesignedalsodiersindeeplearning.The\ndeeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\ntakeonanyspecicsemanticsaheadoftimethetrainingalgorithmisfreeto\ninventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\n5 8 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\ntechniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When\nlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\noftendesignedwithsomespecicsemanticsinmindthetopicofadocument,\ntheintelligenceofastudent,thediseasecausingapatientssymptoms,etc.These\nmodelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "moretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare\nnotreusableinasmanydierentcontextsasdeepmodels.\nAnotherobviousdierenceisthekindofconnectivitytypicallyusedinthe\ndeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits\nthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween\ntwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels\nhaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "individuallydesigned.Thedesignofthemodelstructureistightlylinkedwith\nthechoiceofinferencealgorithm.Traditionalapproachestographicalmodels\ntypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint\nistoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled\nloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery\nsparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto\nconnecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "distributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo).\nDistributedrepresentationshavemanyadvantages,butfromthepointofview\nofgraphicalmodelsandcomputational complexity,distributedrepresentations\nhavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor\nthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe\nrelevant.Asaconsequence,oneofthemoststrikingdierencesbetweenthelarger\ngraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "loopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels\nareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms\necient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge\nnumberoflatentvariables,makingecientnumericalcodeessential.Thisprovides\nanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for\ngroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween\ntwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "withecientmatrixproductoperations,orsparselyconnectedgeneralizations ,like\nblockdiagonalmatrixproductsorconvolutions.\nFinally,thedeeplearningapproachtographicalmodelingischaracterizedby\namarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\nallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\n5 8 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\nwhosemarginaldistributionscannotbecomputed,andaresatisedsimplytodraw\napproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\nobjectivefunctionthatwecannotevenapproximate inareasonableamountof\ntime,butwearestillabletoapproximately trainthemodelifwecaneciently\nobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isoftentogureoutwhattheminimumamountofinformationweabsolutely\nneedis,andthentogureouthowtogetareasonableapproximation ofthat\ninformationasquicklyaspossible.\n1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e\nTherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium\nisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.\nTheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "thatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20\nseehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe\nRBMexempliesmanyofthepracticesusedinawidevarietyofdeepgraphical\nmodels:itsunitsareorganizedintolargegroupscalledlayers,theconnectivity\nbetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the\nmodelisdesignedtoallowecientGibbssampling,andtheemphasisofthemodel\ndesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "werenotspeciedbythedesigner.Later,insection,wewillrevisittheRBM 20.2\ninmoredetail.\nThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\nunits.Itsenergyfunctionis\nE ,( v h b ) = v ch vW h , (16.10)\nwhere b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan\nseethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction\nbetweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically\ningure.Asthisguremakesclear,animportantaspectofthismodelis 16.14",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany\ntwohiddenunits(hencetherestricted,ageneralBoltzmannmachinemayhave\narbitraryconnections).\nTherestrictionsontheRBMstructureyieldtheniceproperties\np( ) =  hv| i p(h i|v) (16.11)\n5 8 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\nFigure16.14:AnRBMdrawnasaMarkovnetwork.\nand\np( ) =  vh| i p(v i|h) . (16.12)\nTheindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\nweobtain:\nP(h i= 1 ) = |v \nvW : , i+ b i\n, (16.13)\nP(h i= 0 ) = 1 |v  \nvW : , i+ b i\n. (16.14)\nTogetherthesepropertiesallowforecientblockGibbssampling,whichalter-\nnatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\ngure.16.15\nSincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\neasytotakeitsderivatives.Forexample,\n\n W i , jE ,(vh) = v ih j . (16.15)\nThesetwopropertiesecientGibbssamplingandecientderivativesmake\ntrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\ntrainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\nTrainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "E h h p (| v )[] hasasetoffeaturestodescribe. v\nOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\nicalmodels:representationlearningaccomplishedvialayersoflatentvariables,\ncombinedwithecientinteractionsbetweenlayersparametrized bymatrices.\nThelanguageofgraphicalmodelsprovidesanelegant,exibleandclearlanguage\nfordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\namongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\n5 8 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith\npermissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn\nusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow\nrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare\nhighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "thistothesamplesandweightsofalinearfactormodel,showningure.Thesamples 13.2\nherearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The\nRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,\ntheRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|\nsothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable\ntohavebothanon-factorialandanon-factorial. p() h p( ) h v|\n5 8 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Acknowledgments\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\nWewouldliketothankthosewhocommentedonourproposalforthebook\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\nalarGlehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\nRohe.\nWewouldliketothankthepeoplewhooeredfeedbackonthecontentofthe\nbookitself.Someoeredfeedbackonmanychapters:MartnAbadi,Guillaume\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiici,Matko",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Bonjak,JohnBoersma,GregBrockman,AlexandredeBrbisson,PierreLuc\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrdricFrancis,\nNandodeFreitas,alarGlehre,JurgenV anGael,JavierAlonsoGarca,\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\nMathey,MatasMattamala,AbhinavMaurya,KevinMurphy,OlegMrk,Roman",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Novak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\nHalisSak,CsarSalgado,GrigorySapunov,YoshinoriSasaki,MikeSchuster,\nJulianSerban,NirShabat,KenShirri,AndreSimpelo,ScottStanley,David\nSussillo,IlyaSutskever,CarlesGeladaSez,GrahamTaylor,ValentinTolmer,\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Webb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZajcandOzanalayan.\nWewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\nindividualchapters:\nNotation:ZhangYuanhang.\nChapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\nviii",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPrvulescu\nandAlfredoSolano.\nChapter, :AmjadAlmahairi,NikolaBani,KevinBennett, 2LinearAlgebra\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\nSergeyOreshkov,IstvnPetrs,DennisPrangle,ThomasRohe,Gitanjali\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\nChapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "AnttiRasmus,AlexeySurkovandVolkerTresp.\nChapter, :TranLamAnIanFischerandHu 4NumericalComputation\nYuhuang.\nChapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.\nChapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\nElizabethBurl,IshanDurugkar,JeHlywa,JongWookKim,DavidKrueger\nandAdityaKumarPraharaj.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "andAdityaKumarPraharaj.\nChapter, :MortenKolbk,KshitijLauria, 7RegularizationforDeepLearning\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.\nChapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\nKashifRasul,KlausStroblandNicholasTurner.\nChapter,9ConvolutionalNetworks:MartnArjovsky,EugeneBrevdo,Kon-\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\nStoutandWentaoWu.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "StoutandWentaoWu.\nChapter,10SequenceModeling:RecurrentandRecursiveNets:Gken\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang.\nChapter, :DanielBeckstein. 11PracticalMethodology\nChapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\nRoscher.\nChapter,13LinearFactorModels:JayanthKoushik.\ni x",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nChapter, :KunalGhosh. 15RepresentationLearning\nChapter, :MinhL 16StructuredProbabilisticModelsforDeepLearning\nandAntonVarfolom.\nChapter,18ConfrontingthePartitionFunction:SamBowman.\nChapter, :YujiaBao. 19ApproximateInference\nChapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\nWenmingMa,FadyMedhat,ShakirMohamedandGrgoireMontavon.\nBibliography:LukasMichelbacherandLeslieN.Smith.\nWealsowanttothankthosewhoallowedustoreproduceimages,guresor",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "datafromtheirpublications.Weindicatetheircontributionsinthegurecaptions\nthroughoutthetext.\nWewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\nmakethewebversionofthebook,andforoeringsupporttoimprovethequality\noftheresultingHTML.\nWewouldliketothankIanswifeDanielaFloriGoodfellowforpatiently\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\nWewouldliketothanktheGoogleBrainteamforprovidinganintellectual",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "environmentwhereIancoulddevoteatremendousamountoftimetowritingthis\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\ntothankIansformermanager,GregCorrado,andhiscurrentmanager,Samy\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeorey\nHintonforencouragement whenwritingwasdicult.\nx",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 4\nNumericalComputation\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\nlution.Commonoperationsincludeoptimization (ndingthevalueofanargument\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Evenjustevaluatingamathematical functiononadigitalcomputercanbedicult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusinganiteamountofmemory.\n4. 1 O v er o w an d Un d er o w\nThefundamentaldicultyinperformingcontinuousmathonadigitalcomputer\nisthatweneedtorepresentinnitelymanyrealnumberswithanitenumber\nofbitpatterns.Thismeansthatforalmostallrealnumbers,weincursome\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "cases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\nroundingerror.\nOneformofroundingerrorthatisparticularlydevastatingis under o w.\nUnderowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\nbehavequalitativelydierentlywhentheirargumentiszeroratherthanasmall\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\n80",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\nisusuallytreatedas,whichthenbecomesnot-a-numberifitisusedformany\nfurtherarithmeticoperations).\nAnotherhighlydamagingformofnumericalerroris o v e r o w.Overowoccurs\nwhennumberswithlargemagnitudeareapproximatedasor.Further\narithmeticwillusuallychangetheseinnitevaluesintonot-a-numbervalues.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "Oneexampleofafunctionthatmustbestabilizedagainstunderowand\noverowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\ndenedtobe\nsoftmax() x i=exp( x i)n\nj = 1exp( x j). (4.1)\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\nwecanseethatalloftheoutputsshouldbeequalto1\nn.Numerically,thismay\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "underow.Thismeansthedenominator ofthesoftmaxwillbecome0,sothenal\nresultisundened.When cisverylargeandpositive,exp( c)willoverow,again\nresultingintheexpressionasawholebeingundened.Bothofthesediculties\ncanberesolvedbyinsteadevaluating softmax( z)where z= xmax i x i.Simple\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverow.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunderowinthedenominator leadingtoadivisionbyzero.\nThereisstillonesmallproblem.Underowinthenumeratorcanstillcause\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\nlogsoftmax( x)byrstrunningthesoftmaxsubroutinethenpassingtheresultto\nthelogfunction,wecoulderroneouslyobtain .Instead,wemustimplement\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "logsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\nthefunction. softmax\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "toimplementanewalgorithmandhavethenewimplementation automatically\n8 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nstabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012\nofasoftwarepackagethatautomatically detectsandstabilizesmanycommon\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\n4. 2 P o or C on d i t i o n i n g\nConditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges\ninitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\ncanbeproblematicforscienticcomputationbecauseroundingerrorsintheinputs",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "canresultinlargechangesintheoutput.\nConsiderthefunction f( x)= A 1x.When A Rn n hasaneigenvalue\ndecomposition,its c o ndi t i o n num beris\nmax\ni , j i\n j. (4.2)\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\nThissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\nofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\n4. 3 Gradi en t - Bas e d O p t i m i z a t i o n\nMostdeeplearningalgorithmsinvolveoptimization ofsomesort.Optimization\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\nx.Weusuallyphrasemostoptimization problemsintermsofminimizing f( x).\nMaximization maybeaccomplishedviaaminimization algorithmbyminimizing\n f() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": " f() x.\nThefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -\nt i o nor c r i t e r i o n.Whenweareminimizingit,wemayalsocallitthe c o st\nf unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n.Inthisbook,weusetheseterms\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\ntosomeoftheseterms.\nWeoftendenotethevaluethatminimizesormaximizesafunctionwitha\nsuperscript.Forexample,wemightsay  x= argmin() f x.\n8 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n    20. 15. 10. 05 00 05 10 15 20 ......\nx20.15.10.05.00.05.10.15.20.\nGlobalminimumat= 0.x\nSincef() = 0,gradient x\ndescent haltshere.\nFor 0,wehave x< f() 0,x<\nsowecandecreasebyf\nmoving rightward.For 0,wehave x> f() 0,x>\nsowecandecreasebyf\nmoving leftward.\nf x() =1\n2x2\nf() = x x\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here.\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\nThe der i v at i v eofthisfunctionisdenotedas f( x)orasd y\nd x.Thederivative f( x)\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspecieshowtoscale\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\noutput: f x  f x  f (+) ()+() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "output: f x  f x  f (+) ()+() x.\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\nweknowthat f( x sign( f( x)))islessthan f( x)forsmallenough .Wecan\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seegureforan4.1\nexampleofthistechnique.\nWhen f( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "tomove.Pointswhere f( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\npoints,soitisnolongerpossibletodecrease f( x)bymakinginnitesimalsteps.\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\n8 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nMinimum Maximum Saddlepoint\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\nsoitisnotpossibletoincrease f( x)bymakinginnitesimalsteps.Somecritical",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "pointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\ngureforexamplesofeachtypeofcriticalpoint. 4.2\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "veryatregions.Allofthismakesoptimization verydicult,especiallywhenthe\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforndinga\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\ngureforanexample.4.3\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn R.Forthe\nconceptofminimizationto makesense,theremuststillbeonlyone(scalar)\noutput.\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\nder i v at i v e s.Thepartialderivative",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "der i v at i v e s.Thepartialderivative\n x if( x)measureshow fchangesasonlythe\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\nvectorcontainingallofthepartialderivatives,denoted  x f( x).Element iofthe\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\n8 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()\nIdeally,wewouldlike\ntoarriveattheglobal\nminimum, butthis\nmight notbepossible.Thislocalminimum\nperformsnearlyaswellas\ntheglobalone,\nsoitisanacceptable\nhaltingpoint.\nThislocalminimumperforms\npoorlyandshouldbeavoided.\nFigure4.3:Optimizationalgorithmsmayfailtondaglobalminimumwhenthereare\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "tosignicantlylowvaluesofthecostfunction.\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\nofthefunction f( x+  u)withrespectto ,evaluatedat = 0.Usingthechain\nrule,wecanseethat\n f  (+ x u)evaluatesto u x f  () xwhen = 0.\nTominimize f,wewouldliketondthedirectioninwhich fdecreasesthe",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "fastest.Wecandothisusingthedirectionalderivative:\nmin\nu u , u = 1u x f() x (4.3)\n=min\nu u , u = 1|||| u 2|| x f() x|| 2cos  (4.4)\nwhere istheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\nignoringfactorsthatdonotdependon u,thissimpliestomin ucos .Thisis\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "Thisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\nSteepestdescentproposesanewpoint\nx= x  x f() x (4.5)\n8 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nwhere isthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\nWecanchoose inseveraldierentways.Apopularapproachistoset toasmall\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\nderivativevanish.Anotherapproachistoevaluate f  ( x x f()) xforseveral\nvaluesof andchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\nThislaststrategyiscalleda l i ne se ar c h.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Thislaststrategyiscalleda l i ne se ar c h.\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\nequation  x f() = 0 xfor. x\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\nsmallmove)towardsbettercongurations canbegeneralizedtodiscretespaces.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\n( ,). RusselandNorvig2003\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\nSometimesweneedtondallofthepartialderivativesofafunctionwhoseinput\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\nknownasa J ac o bi an m at r i x.Specically,ifwehaveafunction f: Rm Rn,\nthentheJacobianmatrix J Rn m ofisdenedsuchthat f J i , j=\n x jf() x i.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": " x jf() x i.\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn R,thederivative\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas2\n x i  x jf.\nInasingledimension,wecandenoted2\nd x2 fby f ( x).Thesecondderivativetells\nushowtherstderivativewillchangeaswevarytheinput.Thisisimportant\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\nthenthereisnocurvature.Itisaperfectlyatline,anditsvaluecanbepredicted\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \nalongthenegativegradient,andthecostfunctionwilldecreaseby .Ifthesecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\nactuallydecreasebymorethan .Finally,ifthesecondderivativeispositive,the\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan .See\n8 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()N e g a t i v e c u r v a t u r e\nxf x()N o c u r v a t u r e\nxf x()P o s i t i v e c u r v a t u r e\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "thanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\nfunctioninadvertently.\nguretoseehowdierentformsofcurvatureaecttherelationshipbetween 4.4\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\nHessian m at r i x.TheHessianmatrix isdenedsuchthat H x()( f)\nH x()( f) i , j=2\n x i  x jf .() x (4.6)\nEquivalently,theHessianistheJacobianofthegradient.\nAnywherethatthesecondpartialderivativesarecontinuous,thedierential\noperatorsarecommutative,i.e.theirordercanbeswapped:\n2\n x i  x jf() = x2\n x j  x if .() x (4.7)\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere.Because theHessianmatrixisrealandsymmetric,\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n8 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\neigenvectors.Thesecondderivativeinaspecicdirectionrepresentedbyaunit\nvector disgivenby dH d.When disaneigenvectorof H,thesecondderivative\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "derivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\ntothefunction aroundthecurrentpoint f() x x( 0 ):\nf f () x( x( 0 ))+( x x( 0 ))g+1\n2( x x( 0 ))H x x (( 0 )) .(4.8)\nwhere gisthegradientand HistheHessianat x( 0 ).Ifweusealearningrate\nof ,thenthenewpoint xwillbegivenby x( 0 )  g.Substitutingthisintoour\napproximation,weobtain",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "approximation,weobtain\nf( x( 0 )   g) f( x( 0 ))  gg+1\n22gH g . (4.9)\nTherearethreetermshere:theoriginalvalueofthefunction,the expected\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\ngradientdescentstepcanactuallymoveuphill.When gH giszeroornegative,\ntheTaylorseriesapproximationpredictsthatincreasing foreverwilldecrease f\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ,so",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "onemustresorttomoreheuristicchoicesof inthiscase.When gH gispositive,\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\nthefunctionthemostyields\n=gg\ngH g. (4.10)\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\nmaximaleigenvalue  m a x,thenthisoptimalstepsizeisgivenby1\nmax.Tothe\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\nrate.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "rate.\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\npoint, f( x) = 0.Whenthesecondderivative f ( x) >0,therstderivative f( x)\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n8 8",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nf( x ) <0and f( x+ ) >0forsmallenough .Inotherwords,aswemove\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\nbeginstopointuphilltotheleft.Thus,when f( x)=0and f ( x) >0,wecan\nconcludethat xisalocalminimum.Similarly,when f( x) = 0and f ( x) <0,we\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\nt e st.Unfortunately,when f ( x) = 0,thetestisinconclusive.Inthiscase xmay",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "beasaddlepoint,orapartofaatregion.\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\n x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\nHessianispositivedenite(allitseigenvaluesarepositive),thepointisalocal",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\nderivativetest.Likewise,whentheHessianisnegativedenite(allitseigenvalues\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\npossibletondpositiveevidenceofsaddlepointsinsomecases.Whenatleast\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "crosssection.Seegureforanexample.Finally,themultidimensionalsecond 4.5\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\nInmultipledimensions,thereisadierentsecondderivativeforeachdirection\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "howmuchthesecondderivativesdierfromeachother.WhentheHessianhasa\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\nremainsnegativeforlonger.Italsomakesitdiculttochooseagoodstepsize.\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "uphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\nstepsizeistoosmalltomakesignicantprogressinotherdirectionswithless\ncurvature.Seegureforanexample.4.6\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n8 9",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n\u0000   \u0000   \n\u0000    \nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\ninthisexampleis f( x)= x2\n1 x2\n2.Alongtheaxiscorrespondingto x 1,thefunction\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\neigenvectoroftheHessianwithnegativeeigenvalue.Thenamesaddlepointderivesfrom",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\n9 0",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\n   3 0 2 0 1 0 0 1 0 2 0\nx 1 3 0 2 0 1 001 02 0x 2\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\nhasvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\ncurvatureisinthedirection[1 ,1]andtheleastcurvatureisinthedirection[1 ,1].The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "redlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\ndirectioninthiscontext.\n9 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n s m e t ho d.\nNewtonsmethodisbasedonusingasecond-orderTaylorseriesexpansionto\napproximatenearsomepoint f() x x( 0 ):\nf f () x( x( 0 ))+( x x( 0 )) x f( x( 0 ))+1\n2( x x( 0 ))H x()( f( 0 ))( x x( 0 )) .(4.11)\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\nx= x( 0 ) H x()( f( 0 )) 1 x f( x( 0 )) . (4.12)\nWhen fisapositivedenitequadraticfunction,Newtonsmethodconsistsof",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "applyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\ndenitequadratic,Newtonsmethodconsistsofapplyingequationmultiple4.12\ntimes.Iterativelyupdatingtheapproximation andjumpingtotheminimumof\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\npropertynearasaddlepoint.Asdiscussedinsection,Newtonsmethodis 8.2.3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\npointsunlessthegradientpointstowardthem.\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\narecalled r st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\nalsousetheHessianmatrix,suchasNewtonsmethod,arecalled se c o nd-or d e r\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "Theoptimization algorithmsemployedinmostcontextsinthisbookare\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\nusedindeeplearningisquitecomplicated.Inmanyotherelds,thedominant\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\noffunctions.\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "ingourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\nofchangeisboundedbya L i psc hi t z c o nst antL:\n|  |L|||| x , y , f() x f() y x y 2 . (4.13)\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n9 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\nwithrelativelyminormodications.\nPerhapsthemostsuccessfuleldofspecializedoptimization is c o n v e x o p-\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\napplicableonlytoconvexfunctionsfunctionsforwhichtheHessianispositive",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "semideniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\nproblemsindeeplearningarediculttoexpressintermsofconvexoptimization.\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "moreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\norRockafellar1997().\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\npossiblevaluesof x.Insteadwemaywishtondthemaximalorminimal\nvalueof f( x)forvaluesof xinsomeset S.Thisisknownas c o nst r ai n e d\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\nconstrainedoptimization terminology.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "constrainedoptimization terminology.\nWeoftenwishtondasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas. |||| x 1\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ,\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\nalinesearch,wecansearchonlyoverstepsizes thatyieldnew xpointsthatare",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\nWhenpossible,thismethodcanbemademoreecientbyprojectingthegradient\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\nthelinesearch(,).Rosen1960\nAmoresophisticatedapproachistodesignadierent,unconstrainedopti-\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\n9 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nx R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\ng( ) = f([cossin  , ])withrespectto ,thenreturn[cossin  , ]asthesolution\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\nbetweenoptimization problemsmustbedesignedspecicallyforeachcasewe\nencounter.\nThe K ar ush K u h n  T uc k e r(KKT)approach1providesaverygeneralso-\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "newfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n.\nTodenetheLagrangian,werstneedtodescribe Sintermsofequations\nandinequalities.W ewantadescriptionof Sintermsof mfunctions g( ) iand n\nfunctions h( ) jsothat S={| x i , g( ) i( x) = 0and j , h( ) j( x)0}.Theequations\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Weintroducenewvariables  iand  jforeachconstraint,thesearecalledthe\nKKTmultipliers.ThegeneralizedLagrangianisthendenedas\nL , , f ( x  ) = ()+ x\ni i g( ) i()+ x\nj j h( ) j() x .(4.14)\nWecannowsolveaconstrainedminimization problemusingunconstrained\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x \nmin\nxmax\nmax\n  ,  0L , , . ( x  ) (4.15)\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "min\nx  Sf .() x (4.16)\nThisfollowsbecauseanytimetheconstraintsaresatised,\nmax\nmax\n  ,  0L , , f , ( x  ) = () x (4.17)\nwhileanytimeaconstraintisviolated,\nmax\nmax\n  ,  0L , , . ( x  ) =  (4.18)\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\n9 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged.\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\ngrangefunctionof,whichleadstothisoptimization problem:  f() x\nmin\nxmax\nmax\n  ,  0 f()+ x\ni i g( ) i()+ x\nj j h( ) j() x .(4.19)\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\nmax\nxmin\nmin\n  ,  0f()+ x\ni i g( ) i() x\nj j h( ) j() x .(4.20)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "i i g( ) i() x\nj j h( ) j() x .(4.20)\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeneit\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\nanysignforeach  i.\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\nh( ) i( x)is ac t i v eif h( ) i( x) = 0.Ifaconstraintisnotactive,thenthesolutionto\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "othersolutions.Forexample,aconvexproblemwithanentireregionofglobally\noptimalpoints(awide,at,regionofequalcost)couldhaveasubsetofthis\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\nthesolutiontomin xmax max   ,  0 L( x   , ,)willhave  i=0.Wecanthus",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "observethatatthesolution,  h( x)= 0.Inotherwords,forall i,weknow\nthatatleastoneoftheconstraints  i0and h( ) i( x)0mustbeactiveatthe\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\ntoinuencethesolutionto x,ortheinequalityhasnoinuenceonthesolution\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\nbutnotalwayssucientconditions,forapointtobeoptimal.Theconditionsare:\nThegradientofthegeneralizedLagrangianiszero.\nAllconstraintsonbothandtheKKTmultipliersaresatised. x\n9 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nTheinequalityconstraintsexhibitcomplementary slackness:  h( x) = 0.\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\n4. 5 E x am p l e: L i n ear L eas t S q u are s\nSupposewewanttondthevalueofthatminimizes x\nf() = x1\n2|||| A x b2\n2 . (4.21)\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeciently.\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "asimpleexampleofhowthesetechniqueswork.\nFirst,weneedtoobtainthegradient:\n x f() = x A( ) = A x b AA x Ab . (4.22)\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\nfordetails.\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\n2|||| A x b2\n2withrespectto x\nusinggradientdescent,startingfromanarbitraryvalueof. x\nSetthestepsize()andtolerance()tosmall,positivenumbers.  \nwhi l e|| AA x Ab|| 2 >  do\nx x  \nAA x Ab\ne nd whi l e",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "x x  \nAA x Ab\ne nd whi l e\nOnecanalsosolvethisproblemusingNewtonsmethod.Inthiscase,because\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewtons\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\nstep.\nNowsupposewewishtominimizethesamefunction,butsubjecttothe\nconstraint xx1.Todoso,weintroducetheLagrangian\nL ,  f  ( x) = ()+ x\nxx1\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\n ,   0L ,  . ( x) (4.24)\n9 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustnda\nsolutionwheretheconstraintisactive.Bydierentiating theLagrangianwith\nrespectto,weobtaintheequation x\nAA x Ab x+2 = 0 . (4.25)\nThistellsusthatthesolutionwilltaketheform\nx A= (A I+2 ) 1Ab . (4.26)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "x A= (A I+2 ) 1Ab . (4.26)\nThemagnitudeof mustbechosensuchthattheresultobeystheconstraint.We\ncanndthisvaluebyperforminggradientascenton.Todoso,observe \n\n L , ( x) = xx1 . (4.27)\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\nuphillandincreasetheLagrangianwithrespectto ,weincrease .Becausethe\ncoecientonthe xxpenaltyhasincreased,solvingthelinearequationfor xwill\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "andadjusting continuesuntil xhasthecorrectnormandthederivativeon is\n0.\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-edged\nlearningsystems.\n9 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I\nD e e p N e t w orks: Mo d e rn\nPractices\n166",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\nusedtosolvepracticalapplications.\nDeeplearninghasalonghistoryandmanyaspirations.Severalapproaches\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\nthenalpartofthebook.\nThispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\nnologiesthatarealreadyusedheavilyinindustry.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "nologiesthatarealreadyusedheavilyinindustry.\nModerndeeplearningprovidesaverypowerfulframeworkforsupervised\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\nbeaccomplishedviadeeplearning,givensucientlylargemodelsandsuciently\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "asassociatingonevectortoanother,orthataredicultenoughthataperson\nwouldrequiretimetothinkandreectinordertoaccomplishthetask,remain\nbeyondthescopeofdeeplearningfornow.\nThispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\nWebeginbydescribingthefeedforwarddeepnetworkmodelthatisusedto\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "andoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\nforthepracticalmethodologyinvolvedindesigning,building,andconguringan\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\nlearning.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "learning.\nThesechaptersarethemostimportantforapractitionersomeone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday.\n1 6 7",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 1\nPractical Methodology\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "whethertogathermoredata,increaseordecreasemodelcapacity,addorremove\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.\nMostofthisbookisaboutdierentmachinelearningmodels,trainingalgo-\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "importantingredienttobeingamachinelearningexpertisknowingawidevariety\nofmachinelearningtechniquesandbeinggoodatdierentkindsofmath.Inprac-\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\nrecommendations inthischapterareadaptedfrom().Ng2015\nWerecommendthefollowingpracticaldesignprocess:",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Werecommendthefollowingpracticaldesignprocess:\nDetermineyourgoalswhaterrormetrictouse,andyourtargetvaluefor\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\nproblemthattheapplicationisintendedtosolve.\nEstablishaworkingend-to-endpipelineassoonaspossible,includingthe\n421",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics.\nInstrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\nisduetoovertting,undertting, oradefectinthedataorsoftware.\nRepeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\nhyperparameters,orchangingalgorithms,basedonspecicndingsfrom\nyourinstrumentation.\nAsarunningexample,wewilluseStreetViewaddressnumbertranscription",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "system( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\nmethodologyweadvocate.\nWenowdescribeeachofthestepsinthisprocess.\n11.1PerformanceMetrics",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "11.1PerformanceMetrics\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryrst\nstepbecauseyourerrormetricwillguideallofyourfutureactions.Youshould\nalsohaveanideaofwhatlevelofperformanceyoudesire.\nKeepinmindthatformostapplications,itisimpossibletoachieveabsolute\nzeroerror.TheBayeserrordenestheminimumerrorratethatyoucanhopeto\nachieve,evenifyouhaveinnitetrainingdataandcanrecoverthetrueprobability\ndistribution.Thisisbecauseyourinputfeaturesmaynotcontaincomplete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\nstochastic.Youwillalsobelimitedbyhavinganiteamountoftrainingdata.\nTheamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\nmoney,orhumansuering(forexample,ifyourdatacollectionprocessinvolves",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "performinginvasivemedicaltests).Whenyourgoalistoanswerascienticquestion\naboutwhichalgorithmperformsbetteronaxedbenchmark,thebenchmark\n4 2 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nspecicationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\nmoredata.\nHowcanonedetermineareasonablelevelofperformancetoexpect?Typically,\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\ncost-eective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\nAnotherimportantconsiderationbesidesthetargetvalueoftheperformance\nmetricisthechoiceofwhichmetrictouse.Severaldierentperformancemetrics\nmaybeusedtomeasuretheeectivenessofacompleteapplicationthatincludes\nmachinelearningcomponents.Theseperformancemetricsareusuallydierent\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "However,manyapplicationsrequiremoreadvancedmetrics.\nSometimesitismuchmorecostlytomakeonekindofamistakethananother.\nForexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\nmeasuringtheerrorrateofaspamclassier,wemaywishtomeasuresomeform",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\nofallowingspammessages.\nSometimeswewishtotrainabinaryclassierthatisintendedtodetectsome\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassier\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "toinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "withprecisiononthe y-axisandrecallonthe x-axis.Theclassiergeneratesascore\nthatishigheriftheeventtobedetectedoccurred.Forexample,afeedforward\n4 2 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnetworkdesignedtodetectadiseaseoutputs  y= P( y=1| x),estimatingthe\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\nthreshold.Byvaryingthethreshold,wecantradeprecisionforrecall.Inmany\ncases,wewishtosummarizetheperformanceoftheclassierwithasinglenumber\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\nF-scor egivenby\nF=2 pr\np r+. (11.1)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "F-scor egivenby\nF=2 pr\np r+. (11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\nInsomeapplications,itispossibleforthemachinelearningsystemtorefuseto\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\nhowcondentitshouldbeaboutadecision,especiallyifawrongdecisioncan\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "transcribetheaddressnumberfromaphotographinordertoassociatethelocation\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.\nOfcourse,themachinelearningsystemisonlyusefulifitisabletodramatically",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "reducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.\nItispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\nis98%accuracy.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "is98%accuracy.\nManyothermetricsarepossible.Wecanforexample,measureclick-through\nrates,collectusersatisfactionsurveys,andsoon.Manyspecializedapplication\nareashaveapplication-speciccriteriaaswell.\nWhatisimportantistodeterminewhichperformancemetrictoimproveahead\noftime,thenconcentrateonimprovingthismetric.Withoutclearlydenedgoals,\nitcanbediculttotellwhetherchangestoamachinelearningsystemmake\nprogressornot.\n4 2 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n11.2DefaultBaselineModels\nAfterchoosingperformancemetricsandgoals,thenextstepinanypractical\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\nthissection,weproviderecommendations forwhichalgorithmstouseastherst\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\nafterthiswriting.\nDependingonthecomplexityofyourproblem,youmayevenwanttobegin",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\nstatisticalmodellikelogisticregression.\nIfyouknowthatyourproblemfallsintoanAI-completecategorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel.\nFirst,choosethegeneralcategoryofmodelbasedonthestructureofyour",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "data.Ifyouwanttoperformsupervisedlearningwithxed-sizevectorsasinput,\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\nAreasonablechoiceofoptimization algorithmisSGDwithmomentumwitha",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "decayinglearningrate(populardecayschemesthatperformbetterorworseon\ndierentproblemsincludedecayinglinearlyuntilreachingaxedminimumlearning\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.\nBatchnormalization canhaveadramaticeectonoptimization performance,\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\nWhileitisreasonabletoomitbatchnormalization fromtheveryrstbaseline,it",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "shouldbeintroducedquicklyifoptimization appearstobeproblematic.\nUnlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\neachvariable.\n4 2 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\nwillprobablydowellbyrstcopyingthemodelandalgorithmthatisalready\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\ntasks( ,). Girshicketal.2015\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "scribedfurtherinpart.Thisissomewhatdomainspecic.Somedomains,such III\nasnaturallanguageprocessing,areknowntobenettremendouslyfromunsuper-\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\nnotbringabenet,exceptinthesemi-supervisedsetting,whenthenumberof\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thenincludeitinyourrstend-to-endbaseline.Otherwise,onlyuseunsupervised\nlearninginyourrstattemptifthetaskyouwanttosolveisunsupervised.You\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\nbaselineoverts.\n11.3DeterminingWhethertoGatherMoreData\nAftertherstend-to-endsystemisestablished,itistimetomeasuretheperfor-\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\nnovicesaretemptedtomakeimprovementsbytryingoutmanydierentalgorithms.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "However,itisoftenmuchbettertogathermoredatathantoimprovethelearning\nalgorithm.\nHowdoesonedecidewhethertogathermoredata?First,determinewhether\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\nAlso,tryimprovingthelearningalgorithm,forexamplebytuningthelearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "ratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\nrichersetoffeatures.\nIftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n4 2 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\ntrainingsetperformance,thengatheringmoredataisoneofthemosteective\nsolutions.Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "signicantly.Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "regularization, byadjustinghyperparameters suchasweightdecaycoecients,\norbyaddingregularizationstrategiessuchasdropout.Ifyoundthatthegap\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\nregularizationhyperparameters ,thengatheringmoredataisadvisable.\nWhendecidingwhethertogathermoredata,itisalsonecessarytodecide\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\ntrainingsetsizeandgeneralization error,likeingure.Byextrapolatingsuch 5.4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.\nIfgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "domainofresearchandnotthedomainofadviceforappliedpractitioners.\n11.4SelectingHyperparameters\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\naspectsofthealgorithmsbehavior.Someofthesehyperparametersaectthetime\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters aect\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\ncorrectresultswhendeployedonnewinputs.\nTherearetwobasicapproachestochoosingthesehyperparameters :choosing",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "themmanuallyandchoosingthemautomatically .Choosingthehyperparameters\n4 2 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly.\n1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\nhyperparameters,trainingerror,generalization errorandcomputational resources",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\ndamentalideasconcerningtheeectivecapacityofalearningalgorithmfrom\nchapter.5\nThegoalofmanualhyperparametersearchisusuallytondthelowestgeneral-\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\nbecausethisishighlyplatform-dependent.\nTheprimarygoalofmanualhyperparametersearchistoadjusttheeective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "capacityofthemodeltomatchthecomplexityofthetask.Eectivecapacity\nisconstrainedbythreefactors:therepresentationalcapacityofthemodel,the\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\nhigherrepresentationalcapacityitiscapableofrepresentingmorecomplicated\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "thetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\nsomeofthesefunctions.\nThegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asingure.Atoneextreme,the 5.3\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\nbecausetrainingerrorishigh.Thisistheunderttingregime.Attheotherextreme,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "thehyperparameter valuecorrespondstohighcapacity,andthegeneralization\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\noftrainingerror.\nForsomehyperparameters,overttingoccurswhenthevalueofthehyper-\nparameterislarge.Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\nForsomehyperparameters ,overttingoccurswhenthevalueofthehyperparame-\nterissmall.Forexample,thesmallestallowableweightdecaycoecientofzero\ncorrespondstothegreatesteectivecapacityofthelearningalgorithm.\nNoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\nManyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\nareswitchesthatspecifywhetherornottousesomeoptionalcomponentof\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "partofthecurve.Forexample,theminimumweightdecaycoecientiszero.This\nmeansthatifthemodelisunderttingwhenweightdecayiszero,wecannotenter\ntheoverttingregionbymodifyingtheweightdecaycoecient.Inotherwords,\nsomehyperparameters canonlysubtractcapacity.\nThelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhavetimetotuneonlyonehyperparameter, tunethelearningrate. Itcon-\ntrolstheeectivecapacityofthemodelinamorecomplicatedwaythanother",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "hyperparameterstheeectivecapacityofthemodelishighestwhenthelearning\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\nillustratedingure.Whenthelearningrateistoolarge,gradientdescent 11.1\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.\nThiseectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\nTuningtheparametersotherthanthelearningraterequiresmonitoringboth\ntrainingandtesterrortodiagnosewhetheryourmodelisoverttingorundertting,\nthenadjustingitscapacityappropriately .\nIfyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\ncondentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\nincreasesthecomputational costsassociatedwiththemodel.\nIfyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\n4 2 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n1 0 21 0 11 00\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaxed\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\nfactorproportionaltothelearningratereduction.Generalizationerrorcanfollowthis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "curveorbecomplicatedbyregularizationeectsarisingoutofhavingatoolargeor\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\novertting,andevenpointswithequivalenttrainingerrorcanhavedierentgeneralization\nerror.\nnowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\nothesequantities.Neuralnetworkstypicallyperformbestwhenthetraining",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\ndrivenbythegapbetweentrainandtesterror.Yourgoalistoreducethisgap\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\nchangeregularizationhyperparameters toreduceeectivemodelcapacity,suchas\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\nlargemodelthatisregularizedwell,forexamplebyusingdropout.\nMosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasemodelcapacity.SomeexamplesareincludedinTable.11.1\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n4 3 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nHyperparameterIncreases\ncapacity\nwhen...Reason Caveats\nNumberofhid-\ndenunitsincreasedIncreasingthenumberof\nhiddenunitsincreasesthe\nrepresentationalcapacity\nofthemodel.Increasingthenumber\nofhiddenunitsincreases\nboththetimeandmemory\ncostofessentiallyeveryop-\nerationonthemodel.\nLearningratetunedop-\ntimallyAnimproperlearningrate,\nwhethertoohighortoo\nlow,resultsinamodel\nwithloweectivecapacity\nduetooptimizationfailure\nConvolutionker-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "duetooptimizationfailure\nConvolutionker-\nnelwidthincreasedIncreasingthekernelwidth\nincreasesthenumberofpa-\nrametersinthemodelAwiderkernelresultsin\nanarroweroutputdimen-\nsion,reducingmodelca-\npacityunlessyouuseim-\nplicitzeropaddingtore-\nducethiseect.Wider\nkernelsrequiremoremem-\noryforparameterstorage\nandincreaseruntime,but\nanarroweroutputreduces\nmemorycost.\nImplicitzero\npaddingincreasedAddingimplicitzerosbe-\nforeconvolutionkeepsthe\nrepresentationsizelargeIncreasedtimeandmem-\norycostofmostopera-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "orycostofmostopera-\ntions.\nWeightdecayco-\necientdecreasedDecreasingtheweightde-\ncaycoecientfreesthe\nmodelparameterstobe-\ncomelarger\nDropoutratedecreasedDroppingunitslessoften\ngivestheunitsmoreoppor-\ntunitiestoconspirewith\neachothertotthetrain-\ningset\nTable11.1:Theeectofvarioushyperparametersonmodelcapacity.\n4 3 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nprinciple,thisapproachcouldfailduetooptimization diculties,butformany\nproblemsoptimization doesnotseemtobeasignicantbarrier,providedthatthe\nmodelischosenappropriately .\n1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\noftenbenetsignicantlyfromtuningoffortyormorehyperparameters .Manual\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "hyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\nautomatedalgorithmscanndusefulvaluesofthehyperparameters .\nIfwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\nwearetryingtondavalueofthehyperparametersthatoptimizesanobjective\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "fortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\ntodevelop h y p e r par am e t e r  o p t i m i z a t i o nalgorithmsthatwrapalearnin g\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\nshouldbeexploredforeachofthelearningalgorithmshyperparameters .However,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\nsecondaryhyperparameters foralltasks.\n1 1 . 4 . 3 G ri d S ea rch\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\ng r i d se ar c h.Foreachhyperparameter,the userselectsasmallnitesetof\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\nspecicationofhyperparametervaluesintheCartesianproductofthesetofvalues",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n4 3 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nGrid Random\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "hyperparametercongurations.Usuallymostofthesehyperparametersareindependent\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\nhyperparametercongurationsandrunstrainingwitheachofthem.Bothgridsearch\nandrandomsearchevaluatethevalidationseterrorandreturnthebestconguration.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Thegureillustratesthetypicalcasewhereonlysomehyperparametershaveasignicant\ninuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\nhasasignicanteect.Gridsearchwastesanamountofcomputationthatisexponential\ninthenumberofnon-inuentialhyperparameters,whilerandomsearchtestsaunique\nvalueofeveryinuentialhyperparameteronnearlyeverytrial.Figurereproducedwith\npermissionfrom (). BergstraandBengio2012\n4 3 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\ngureforanillustrationofagridofhyperparameter values. 11.2\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "searchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\nratetakenwithintheset{ .1 , .01 ,103,104,105},oranumberofhiddenunits\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter usingvaluesof{1 ,0 ,1}.\nIfthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 \nliesandweshouldshiftthegridandrunanothersearchwith in,forexample,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "{1 ,2 ,3}.Ifwendthatthebestvalueof is,thenwemaywishtoreneour 0\nestimatebyzoominginandrunningagridsearchover. { } . , , .101\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\nparallelism(withalmostnoneedforcommunication betweendierentmachines",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "carryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\nevenparallelization maynotprovideasatisfactorysizeofsearch.\n1 1 . 4 . 4 Ra n d o m S ea rch\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\nrandomsearch( ,). BergstraandBengio2012\nArandomsearchproceedsasfollows.Firstwedeneamarginaldistribution\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\nhyperparameters.Forexample,\nl o g l e a r n i n g r a t e __  u(1 ,5) (11.2)\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b).\nSimilarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\nlog(2000) ).\n4 3 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\nnotincuradditionalcomputational cost.Infact,asillustratedingure,a11.2\nrandomsearchcanbeexponentiallymoreecientthanagridsearch,whenthere\nareseveralhyperparametersthatdonotstronglyaecttheperformancemeasure.\nThisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "searchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\nthenumberoftrialsrunbyeachmethod.\nAswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,torenethesearchbasedontheresultsoftherstrun.\nThemainreasonwhyrandomsearchndsgoodsolutionsfasterthangridsearch\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\nwouldusuallyhavedierentvalues.Henceifthechangebetweenthesetwovalues\ndoesnotmarginallymakemuchdierenceintermsofvalidationseterror,grid\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters .\n1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\nsimpliedsettingswhereitisfeasibletocomputethegradientofsomedierentiable\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal.\n2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "intrinsicallynon-dierentiable interactionswiththevalidationseterror,asinthe\ncaseofdiscrete-valuedhyperparameters .\nTocompensateforthislackofagradient,wecanbuildamodelofthevalidation\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "mizationthusinvolvesatradeobetweenexploration(proposinghyperparameters\n4 3 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\niscondentwillperformaswellasanyhyperparameters ithasseensofarusually\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\nTPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "Currently,wecannotunambiguously recommendBayesianhyperparameter\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\nforobtainingthoseresultswithlesseort.Bayesianhyperparameteroptimization\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\naparticularproblembutisnotyetsucientlymatureorreliable.Thatbeing\nsaid,hyperparameter optimization isanimportanteldofresearchthat,while",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "oftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobenet\nnotonlytheentireeldofmachinelearningbutthedisciplineofengineeringin\ngeneral.\nOnedrawbackcommontomosthyperparameter optimization algorithmswith\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\nfromtheexperiment.Thisismuchlessecient,inthesenseofhowmuchinfor-\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\nexperiment,tofreezearunningexperimentthatisnotpromising,ortothaw\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\nmoreinformation.\n11.5DebuggingStrategies",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "moreinformation.\n11.5DebuggingStrategies\nWhenamachinelearningsystemperformspoorly,itisusuallydiculttotell\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\nisabugintheimplementation ofthealgorithm.Machine learningsystemsare\ndiculttodebugforavarietyofreasons.\nInmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n4 3 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nneuralnetworkonaclassicationtaskanditachieves5%testerror,wehave new\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\nbehavior.\nAfurtherdicultyisthatmostmachinelearningmodelshavemultipleparts\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "furtherthatwehavemanuallyimplemented thegradientdescentruleforeach\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\nb b  (11.4)\nwhere isthelearningrate.Thiserroneousupdatedoesnotusethegradientat\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\nDependingonthedistributionoftheinput,theweightsmaybeabletoadaptto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "compensateforthenegativebiases.\nMostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\nbothofthesetwodiculties.Eitherwedesignacasethatissosimplethatthe\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\npartoftheneuralnetimplementationinisolation.\nSomeimportantdebuggingtestsinclude:\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "superimposedontheimage.Whentrainingagenerativemodelofspeech,listento\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performingwellwhenitisnot.\nVisualizetheworstmistakes:Mostmodelsareabletooutputsomesortof\ncondencemeasureforthetasktheyperform.Forexample,classiersbasedona\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\ntothemostlikelyclassthusgivesanestimateofthecondencethemodelhasin\nitsclassicationdecision.Typically,maximumlikelihoodtrainingresultsinthese\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n4 3 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.\nForexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "someofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\ncondentmistakesshowedthattherewasasystematicproblemwiththecropping.\nModifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\nnumbers.\nReasoningaboutsoftwareusingtrainandtesterror:Itisoftendicultto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "determinewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\nmodelisoverttingforfundamentalalgorithmicreasons.Analternativepossibility\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\nwasprepareddierentlyfromthetrainingdata.Ifbothtrainandtesterrorare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "high,thenitisdiculttodeterminewhetherthereisasoftwaredefectorwhether\nthemodelisunderttingduetofundamentalalgorithmicreasons.Thisscenario\nrequiresfurthertests,describednext.\nFitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\nitisduetogenuineunderttingorduetoasoftwaredefect.Usuallyevensmall\nmodelscanbeguaranteedtobeabletasucientlysmalldataset.Forexample,\naclassicationdatasetwithonlyoneexamplecanbetjustbysettingthebiases",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "oftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiertocorrectly\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\nwithhighdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\nCompareback-propagatedderivativestonumericalderivatives:Ifyouareusing\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "putations,orifyouareaddinganewoperationtoadierentiation libraryand\nmustdeneitsbpropmethod,thenacommonsourceoferrorisimplementingthis\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n4 3 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nistocomparethederivativescomputedbyyourimplementation ofautomatic\ndierentiationtothederivativescomputedbya .Because ni t e di  e r e nc e s\nf() =lim x\n 0f x  f x (+)()\n, (11.5)\nwecanapproximate thederivativebyusingasmall,nite: \nf() xf x  f x (+)()\n. (11.6)\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di  e r -\ne nc e:\nf() xf x(+1\n2 f x )(1\n2 )\n. (11.7)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "e nc e:\nf() xf x(+1\n2 f x )(1\n2 )\n. (11.7)\nTheperturbationsize mustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchbynite-precisionnumericalcomputations.\nUsually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\ng: Rm Rn.Unfortunately,nitedierencingonlyallowsustotakeasingle\nderivativeatatime.Wecaneitherrunnitedierencing m ntimestoevaluateall\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "randomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\nwhere uand varerandomlychosenvectors.Computing f( x)correctlyrequires\nbeingabletoback-propagatethrough gcorrectly,yetisecienttodowithnite\ndierencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\nprojection.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "projection.\nIfonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\naveryecientwaytonumericallyestimatethegradientbyusingcomplexnumbers\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\nobservationthat\nf x i  f x i  f (+) = ()+()+( x O 2) (11.8)\nreal((+)) = ()+( f x i  f x O 2)imag( ,f x i  (+)\n) = f()+( x O 2) ,(11.9)\nwhere i=\n1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeect\nduetotakingthedierencebetweenthevalueof fatdierentpoints.Thisallows",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "theuseoftinyvaluesof like = 10150,whichmakethe O( 2)errorinsignicant\nforallpracticalpurposes.\n4 3 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiers,\nhowoftenaretheyo?Arethereunitsthatarealwayso?Fortanhunits,\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\nAssuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "stalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\nevolution.\nFinally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\nproblems.Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\nSomeguaranteesthatsomeoptimizationalgorithmsoerincludethattheobjective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\nUsuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter.\n11.6Example:Multi-DigitNumberRecognition\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\ndatabaseinfrastructure,andsoon,wereofparamountimportance.\nFromthepointofviewofthemachinelearningtask,theprocessbeganwith\ndatacollection.The carscollectedtherawdataandhumanoperatorsprovided\nlabels.Thetranscriptiontaskwasprecededbyasignicantamountofdataset",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "curation,includingusingothermachinelearningtechniquestodetectthehouse\n4 4 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnumberspriortotranscribingthem.\nThetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics.Animportantgeneralprincipleistotailorthe\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\nforthisproject.Specically,thegoalwastoobtainhuman-level,98%accuracy.\nThislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "thislevelofaccuracy,theStreetViewtranscriptionsystemsacricescoverage.\nCoveragethusbecamethemainperformancemetricoptimizedduringtheproject,\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\npossibletoreducethecondencethresholdbelowwhichthenetworkrefusesto\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.\nAfterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworkwithrectiedlinearunits.Thetranscriptionprojectbegan\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\nbaseline,therstimplementation oftheoutputlayerofthemodelconsistedof n\ndierentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\nweretrainedexactlythesameasifthetaskwereclassication,witheachsoftmax\nunittrainedindependently.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "unittrainedindependently.\nOurrecommendedmethodologyistoiterativelyrenethebaselineandtest\nwhethereachchangemakesanimprovement.TherstchangetotheStreetView\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\nmetricandthestructureofthedata.Specically,thenetworkrefusestoclassify\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\nsomethreshold t.Initially,thedenitionof p( y x|)wasad-hoc,basedonsimply",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "multiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\nmuchmoreeectively.\nAtthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "isunderttingorovertting.Inthiscase,trainandtestseterrorwerenearly\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\n4 4 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\ntounderttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\nstrategieswerecommendistovisualizethemodelsworsterrors.Inthiscase,that\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\nhighestcondence.Theseprovedtomostlyconsistofexampleswheretheinput\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\nremovedbythecroppingoperation.Forexample,aphotoofanaddress1849",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "mightbecroppedtootightly,withonlythe849remainingvisible.Thisproblem\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\nsystemscoverage.\nFinally,thelastfewpercentagepointsofperformancecamefromadjusting",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\nremainedroughlyequal,itwasalwaysclearthatanyperformancedecitsweredue\ntoundertting, aswellasduetoafewremainingproblemswiththedatasetitself.\nOverall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\nhavebeenpossibleviahumaneort.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "havebeenpossibleviahumaneort.\nWehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\nothersimilarsuccesses.\n4 4 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\nnewuncertainstatements.Inarticialintelligenceapplications,weuseprobability\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "expressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\nunderstandthematerialinthisbook.\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\nofuncertaintyinaprobabilitydistribution.\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\nbesucienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "suggestthatyouconsultanadditionalresource,suchasJaynes2003().\n53",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability?\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\nexecuteeachmachineinstructionawlessly.Errorsinhardwaredooccur,butare\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "makesheavyuseofprobabilitytheory.\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\nbyPearl1988().\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Infact,beyondmathematical statementsthataretruebydenition,itisdicult\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\nguaranteedtooccur.\nTherearethreepossiblesourcesofuncertainty:\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "whereweassumethatthecardsaretrulyshuedintoarandomorder.\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.Theoutcome\ngiventhecontestantschoiceisdeterministic,butfromthecontestantspoint\nofview,theoutcomeisuncertain.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "ofview,theoutcomeisuncertain.\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\ntheinformation wehaveobserved,thediscardedi nformationresultsin\nuncertaintyinthemodelspredictions. Forexample,supposewebuilda\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n54",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\ntheprecisepositionofobjects:eachobjectcouldbeanywherewithinthe\ndiscretecellthatitwasobservedtooccupy.\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\nmodelingsystemhasthedelitytoaccommodateacomplexrule.Forexample,the",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "simpleruleMostbirdsyischeaptodevelopandisbroadlyuseful,whilearule\noftheform,Birdsy,exceptforveryyoungbirdsthathavenotyetlearnedto\ny,sickorinjuredbirdsthathavelosttheabilitytoy,ightlessspeciesofbirds\nincludingthecassowary,ostrichandkiwi...isexpensivetodevelop,maintainand\ncommunicate,andafterallofthiseortisstillverybrittleandpronetofailure.\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "allofthetoolswewantforarticialintelligenceapplications.Probabilitytheory\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.Whenwe\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\ntheexperiment(e.g.,drawahandofcards)innitelymanytimes,thenproportion\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheu,\nthismeanssomethingverydierentwecannotmakeinnitelymanyreplicasof\nthepatient,noristhereanyreasontobelievethatdierentreplicasofthepatient\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheu",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "and0indicatingabsolutecertaintythatthepatientdoesnothavetheu.The\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\nofcertainty,isknownas B ay e si an pr o babili t y.\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n55",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\nsee(). Ramsey1926\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\n3.2RandomVariables\nA r andom v ar i abl eisavariablethatcantakeondierentvaluesrandomly.We\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\nmustbecoupledwithaprobabilitydistributionthatspecieshowlikelyeachof\nthesestatesare.\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\nisonethathasaniteorcountablyinnitenumberofstates.Notethatthese\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\nassociatedwitharealvalue.\n3.3ProbabilityDistributions",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "3.3ProbabilityDistributions\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\ncontinuous.\n3.3.1DiscreteVariablesandProbabilityMassFunctions\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "acapitalP.Oftenweassociateeachrandomvariablewithadierentprobability\n56",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\nP P ()xisusuallynotthesameas()y.\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\nexplicitly:P(x=x).Sometimeswedeneavariablerst,thenusenotationto\nspecifywhichdistributionitfollowslater:xx. P()\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "satisfythefollowingproperties:\nThedomainofmustbethesetofallpossiblestatesofx. P\nxx,0P(x)1.Animpossibleeventhasprobabilityandnostatecan 0 \nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\n\nx  xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\ntheprobabilityofoneofmanyeventsoccurring.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "theprobabilityofoneofmanyeventsoccurring.\nForexample,considerasinglediscreterandomvariablexwithkdierent\nstates.Wecanplacea uni f o r m di st r i but i o nonxthatis,makeeachofits\nstatesequallylikelybysettingitsprobabilitymassfunctionto\nPx (= x i) =1\nk(3.1)\nforalli.Wecanseethatthiststherequirementsforaprobabilitymassfunction.\nThevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\n\niPx (= x i) =\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized.\n57",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\nfollowingproperties:\nThedomainofmustbethesetofallpossiblestatesofx. p\n   xx,px() 0 () . p Notethatwedonotrequirex 1.\n\npxdx()= 1.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "\npxdx()= 1.\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspecic\nstatedirectly,insteadtheprobabilityoflandinginsideaninnitesimalregionwith\nvolumeisgivenby. x pxx()\nWecanintegratethedensityfunctiontondtheactualprobabilitymassofa\nsetofpoints.Specically,theprobabilitythatxliesinsomeset Sisgivenbythe\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\nliesintheintervalisgivenby []a,b\n[ ] a , bpxdx().",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "[ ] a , bpxdx().\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspecic\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\nwhereaandbaretheendpointsoftheinterval,withb>a.The;notationmeans\nparametrized by;weconsiderxtobetheargumentofthefunction,whileaand\nbareparametersthatdenethefunction.Toensurethatthereisnoprobability\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx[a,b] [.Withina,b],",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "uxa,b (;) =1\nb a .Wecanseethatthisisnonnegativeeverywhere.Additionally,it\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\nbywritingx. Ua,b()\n3.4MarginalProbability\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\nForexample,supposewehavediscreterandomvariablesxandy,andweknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "P,(xy.Wecanndxwiththe : ) P() sum r ul e\nxxx,P(= ) =x\nyPx,y. (= xy= ) (3.3)\n58",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThenamemarginalprobabilitycomesfromtheprocessofcomputingmarginal\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\ndierentvaluesofxinrowsanddierentvaluesofyincolumns,itisnaturalto\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\ntherightoftherow.\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\npx() =\npx,ydy. () (3.4)\n3.5ConditionalProbability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "px,ydy. () (3.4)\n3.5ConditionalProbability\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\nconditionalprobabilitycanbecomputedwiththeformula\nPyx (= y |x= ) =Py,x (= yx= )\nPx (= x ). (3.5)\nTheconditionalprobabilityisonlydenedwhenP(x=x)>0.Wecannotcompute\ntheconditionalprobabilityconditionedonaneventthatneverhappens.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\nwhichwedonotexploreinthisbook.\n3.6TheChainRuleofConditionalProbabilities",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "3.6TheChainRuleofConditionalProbabilities\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\nintoconditionaldistributionsoveronlyonevariable:\nP(x( 1 ),...,x( ) n) = (Px( 1 ))n\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i ).(3.6)\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\nItfollowsimmediatelyfromthedenitionofconditionalprobabilityinequation.3.5\n59",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nForexample,applyingthedenitiontwice,weget\nP,,P,P, (abc)= (ab|c)(bc)\nP,PP (bc)= ( )bc| ()c\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\n3.7IndependenceandConditionalIndependence\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\nonlyy:\n xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "variableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\nwayforeveryvalueofz:\n   | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\n(3.8)\nWecandenoteindependenceandconditionalindependencewith compact\nnotation:xymeansthatxandyareindependent,whilexyz |meansthatx\nandyareconditionallyindependentgivenz.\n3.8Expectation,VarianceandCovariance\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\nE x  P[()] =fx\nxPxfx, ()() (3.9)\nwhileforcontinuousvariables,itiscomputedwithanintegral:\nE x  p[()] =fx\npxfxdx. ()() (3.10)\n60",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[]averagesover\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\nnoambiguity,wemayomitthesquarebrackets.\nExpectationsarelinear,forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Expectationsarelinear,forexample,\nE x[()+ ()] = fxgx E x[()]+fx E x[()]gx, (3.11)\nwhenandarenotdependenton.  x\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\nvariablexvaryaswesampledierentvaluesofxfromitsprobabilitydistribution:\nVar(()) = fx E\n(() [()]) fx Efx2\n. (3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "The c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\ntoeachother,aswellasthescaleofthesevariables:\nCov(()()) = [(() [()])(() [()])] fx,gy Efx Efxgy Egy.(3.13)\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\nrelated,ratherthanalsobeingaectedbythescaleoftheseparatevariables.\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ever,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\nzerocovariance.Forexample,supposewerstsamplearealnumberxfroma\nuniformdistributionovertheinterval[1,1].Wenextsamplearandomvariable\n61",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ns.Withprobability1\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\nthevalueofstobe1.Wecanthengeneratearandomvariableybyassigning\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\nthemagnitudeof.However,y Cov() = 0x,y.\nThe c o v ar i anc e m at r i xofarandomvector x Rnisannnmatrix,such\nthat\nCov() x i , j= Cov(x i,x j). (3.14)\nThediagonalelementsofthecovariancegivethevariance:\nCov(x i,x i) = Var(x i). (3.15)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "Cov(x i,x i) = Var(x i). (3.15)\n3.9CommonProbabilityDistributions\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\nlearning.\n3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\nItiscontrolledbyasingleparameter[0,1],whichgivestheprobabilityofthe\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\nP  (= 1) = x (3.16)\nP  (= 0) = 1x  (3.17)\nPx (= x ) = x(1 )1  x(3.18)\nE x[] = x (3.19)\nVar x() = (1 )x (3.20)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "E x[] = x (3.19)\nVar x() = (1 )x (3.20)\n3.9.2MultinoulliDistribution\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\nvariablewithkdierentstates,wherekisnite.1Themultinoullidistributionis\n1MultinoulliisatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "timeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\nManytextsusethetermmultinomialtorefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase. n= 1\n62",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nparametrized byavector p[0,1]k  1,wherep igivestheprobabilityofthei-th\nstate.Thenal,k-thstatesprobabilityisgivenby1 1p.Notethatwemust\nconstrain 1p1.Multinoullidistributionsareoftenusedtorefertodistributions\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\norvarianceofmultinoulli-dis tributedrandomvariables.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "TheBernoulliandmultinoullidistributionsaresucienttodescribeanydistri-\nbutionovertheirdomain.They areabletodescribeanydistributionovertheir\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\nparametersmustimposestrictlimitsonthedistribution.\n3.9.3GaussianDistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "3.9.3GaussianDistribution\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\nt i o n,alsoknownasthe : G aussian di st r i but i o n\nN(;x,2) =\n1\n22exp\n1\n22( )x2\n.(3.21)\nSeegureforaplotofthedensityfunction. 3.1\nThetwoparameters  Rand(0,)controlthenormaldistribution.\nTheparametergivesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =.Thestandarddeviationofthedistributionisgivenby\n,andthevarianceby2.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": ",andthevarianceby2.\nWhenweevaluatethePDF,weneedtosquareandinvert.Whenweneedto\nfrequentlyevaluatethePDFwithdierentparametervalues,amoreecientway\nofparametrizing thedistributionistouseaparameter(0,)tocontrolthe\npr e c i si o norinversevarianceofthedistribution:\nN(;x, 1) =\n\n2exp\n1\n2x ()2\n. (3.22)\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "take,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\n63",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n    20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x \nInectionpointsat\nx   = \nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;,2)exhibits\naclassicbellcurveshape,withthexcoordinateofitscentralpeakgivenby,and\nthewidthofitspeakcontrolledby.Inthisexample,wedepictthestandardnormal\ndistribution,withand. = 0= 1\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "distributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\nstructuredbehavior.\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\nsection.19.4.2\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\ndenitesymmetricmatrix: \nN(; ) = x , \n1\n(2)ndet() exp\n1\n2( ) x  1( ) x \n.(3.23)\n64",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nTheparameter stillgivesthemeanofthedistribution,thoughnowitis\nvector-valued.Theparameter givesthecovariancematrixofthedistribution.\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\nmanydierentvaluesoftheparameters,thecovarianceisnotacomputationally\necientwaytoparametrizethedistribution,sinceweneedtoinvert toevaluate\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x \nN(; x  , 1) =\ndet() \n(2)nexp\n1\n2( ) x  x  ()",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "det() \n(2)nexp\n1\n2( ) x  x  ()\n.(3.24)\nWeoftenxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\ntimestheidentitymatrix.\n3.9.4ExponentialandLaplaceDistributions\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\ndi st r i but i o n:\npx (;) = 1 x  0exp( )x. (3.25)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "px (;) = 1 x  0exp( )x. (3.25)\nTheexponentialdistributionusestheindicatorfunction 1 x  0toassignprobability\nzerotoallnegativevaluesof.x\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\nofprobabilitymassatanarbitrarypointisthe L apl ac e di st r i but i o n\nLaplace(;) =x,1\n2exp\n||x\n\n. (3.26)\n3.9.5TheDiracDistributionandEmpiricalDistribution\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "clustersaroundasinglepoint.ThiscanbeaccomplishedbydeningaPDFusing\ntheDiracdeltafunction,:x()\npxx. () = () (3.27)\nTheDiracdeltafunctionisdenedsuchthatitiszero-valuedeverywhereexcept\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\nassociateseachvaluexwithareal-valuedoutput,insteaditisadierentkindof\n65",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdenedintermsofits\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\nthanzero.\nBydeningp(x)tobeshiftedbyweobtainaninnitelynarrowand\ninnitelyhighpeakofprobabilitymasswhere.x= \nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\ndi st r i but i o n,\np() = x1\nmm",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "di st r i but i o n,\np() = x1\nmm\ni = 1( x x( ) i) (3.28)\nwhichputsprobabilitymass1\nmoneachofthempoints x( 1 ),..., x( ) mforminga\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\ntodenetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset.Anotherimportantperspectiveontheempiricaldistributionis\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n(seesection).5.5\n3.9.6MixturesofDistributions\nItisalsocommontodeneprobabilitydistributionsbycombiningothersimpler",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributions.Onecommonwayofcombiningdistributionsisto\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\nmultinoullidistribution:\nP() =x\niPiPi (= c )( = xc| ) (3.29)\nwherecisthemultinoullidistributionovercomponentidentities. P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample.\n66",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\nprobabilitydistributionsfromsimpleonesinmoredetail.\nThemixturemodelallowsustobrieyglimpseaconceptthatwillbeof\nparamountimportancelaterthe l at e n t v ar i abl e.Alatentvariableisarandom\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\nvariablesarediscussedfurtherinsection.16.5\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "model,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\naseparatelyparametrized mean ( ) iandcovariance ( ) i.Somemixturescanhave\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\nviatheconstraint ( ) i= ,i.AswithasingleGaussiandistribution,themixture\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\ndiagonalorisotropic.\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "specifythe pr i o r pr o babili t y i=P(c=i) giventoeachcomponenti.Theword\npriorindicatesthatitexpressesthemodelsbeliefsaboutc b e f o r eithasobserved\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\nspecic,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\ncomponents.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "components.\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\n3.10UsefulPropertiesofCommonFunctions\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\ntheprobabilitydistributionsusedindeeplearningmodels.\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\nx() =1\n1+exp()x. (3.30)\nThelogisticsigmoidiscommonlyusedtoproducetheparameterofaBernoulli\n67",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx 1x 2\nFigure3.2:SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\ncomponents.Fromlefttoright,therstcomponenthasanisotropiccovariancematrix,\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "separatelyalonganarbitrarybasisofdirections.\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\nfortheparameter.Seegureforagraphofthesigmoidfunction.The 3.3\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\nmeaningthatthefunctionbecomesveryatandinsensitivetosmallchangesinits\ninput.\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\n2001):\nx x. () = log(1+exp()) (3.31)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "2001):\nx x. () = log(1+exp()) (3.31)\nThesoftplusfunctioncanbeusefulforproducingtheorparameterofanormal\ndistributionbecauseitsrangeis(0,).Italsoarisescommonlywhenmanipulating\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\nfactthatitisasmoothedorsoftenedversionof\nx+= max(0),x. (3.32)\nSeegureforagraphofthesoftplusfunction. 3.4\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\nthem:\n68",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n  1 0 5 0 5 1 0\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 . x ( )\nFigure3.3:Thelogisticsigmoidfunction.\n  1 0 5 0 5 1 0\nx024681 0 x ( )\nFigure3.4:Thesoftplusfunction.\n69",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx() =exp()x\nexp()+exp(0)x(3.33)\nd\ndxxxx () = ()(1()) (3.34)\n1 () = () xx (3.35)\nlog() = () x x (3.36)\nd\ndxxx () = () (3.37)\nx(01),, 1() = logxx\n1x\n(3.38)\nx>,0 1() = log(exp()1) x x (3.39)\nx() =x\n ydy() (3.40)\nxxx ()() = (3.41)\nThefunction 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\nusedinmachinelearning.\nEquationprovidesextrajusticationforthenamesoftplus.Thesoftplus 3.41",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\nfunction,x=max{0,x}.Toobtainasmoothfunctionthatisanalogoustothe\nnegativepart,onecanuse(x).Justasxcanberecoveredfromitspositivepart\nandnegativepartviatheidentityx+x=x,itisalsopossibletorecoverx\nusingthesamerelationshipbetweenand,asshowninequation. x()x() 3.41\n3.11BayesRule\nWeoftenndourselvesinasituationwhereweknowP(yx|)andneedtoknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "P(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\nusing B a y e s r ul e:\nP( ) =xy|PP()x( )yx|\nP()y. (3.42)\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\nP() =y\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\n70",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nBayesruleisstraightforwardtoderivefromthedenitionofconditional\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whorst\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\nindependentlydiscoveredbyPierre-SimonLaplace.\n3.12TechnicalDetailsofContinuousVariables\nAproperformalunderstandingofcontinuousrandomvariablesandprobability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\nthistextbook,butwecanbrieysketchsomeoftheissuesthatmeasuretheoryis\nemployedtoresolve.\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "S 1and S 2suchthatp( x S 1) +p( x S 2)>1but S 1 S 2=.Thesesets\naregenerallyconstructedmakingveryheavyuseoftheinniteprecisionofreal\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredenedby\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\nprobabilityofwithoutencounteringparadoxes.Inthisbook,weonlyintegrate\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "becomesarelevantconcern.\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\nasetissaidtohave m e asur e z e r o.Wedonotformallydenethisconceptinthis\ntextbook.Forourpurposes,itissucienttounderstandtheintuitionthataset\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "within R2,alinehasmeasurezero,whilealledpolygonhaspositivemeasure.\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\nnumbershasmeasurezero,forinstance).\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\n71",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\ntheoryholdforalldiscretevaluesbutonlyholdalmosteverywhereforcontinuous\nvalues.\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "tinuous,dierentiabletransformation.Onemightexpectthatp y( y) =p x(g 1( y)).\nThisisactuallynotthecase.\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\ny=x\n2andxU(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\neverywhereexcepttheinterval[0,1\n2] 1 ,anditwillbeonthisinterval.Thismeans\n\np y()=ydy1\n2, (3.43)\nwhichviolatesthedenitionofaprobabilitydistribution.Thisisacommonmistake.\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "spaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\ninnitesimallysmallregionwithvolume xisgivenbyp( x) x.Sincegcanexpand\norcontractspace,theinnitesimalvolumesurrounding xin xspacemayhave\ndierentvolumeinspace. y\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\npreservetheproperty\n|p y(())= gxdy||p x()xdx.| (3.44)\nSolvingfromthis,weobtain\np y() = yp x(g 1())yx\ny(3.45)\norequivalently\np x() = xp y(())gxgx()\nx. (3.46)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "p x() = xp y(())gxgx()\nx. (3.46)\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\nm at r i xthematrixwithJ i , j= x i\n y j.Thus,forreal-valuedvectorsand, x y\np x() = xp y(())g xdetg() x\n x . (3.47)\n72",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.13InformationTheory\nInformationtheoryisabranchofappliedmathematicsthatrevolvesaround\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\nspecicprobabilitydistributionsusingvariousencodingschemes.Inthecontextof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\nwheresomeofthesemessagelengthinterpretations donotapply.Thiseldis\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\n().2003\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "eventhasoccurredismoreinformativethanlearningthatalikelyeventhas\noccurred.Amessagesayingthesunrosethismorningissouninformative as\ntobeunnecessarytosend,butamessagesayingtherewasasolareclipsethis\nmorningisveryinformative.\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\nSpecically,\nLikelyeventsshouldhavelowinformationcontent,andintheextremecase,\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\nwhatsoever.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whatsoever.\nLesslikelyeventsshouldhavehigherinformationcontent.\nIndependenteventsshouldhaveadditiveinformation. Forexample,nding\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\nmuchinformationasndingoutthatatossedcoinhascomeupasheads\nonce.\nInordertosatisfyallthreeoftheseproperties,wedenethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx. () = log () (3.48)\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "denitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\n73",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ninformationgainedbyobservinganeventofprobability1\ne.Othertextsusebase-2\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\njustarescalingofinformationmeasuredinnats.\nWhenxiscontinuous,weusethesamedenitionofinformationbyanalogy,\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\nguaranteedtooccur.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "guaranteedtooccur.\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x  P[()] = Ix  E x  P[log()]Px. (3.49)\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\naredierent)neededonaveragetoencodesymbolsdrawnfromadistributionP.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\ngureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\nknownasthe di  e r e n t i al e nt r o p y.\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\nrandomvariablex,wecanmeasurehowdierentthesetwodistributionsareusing\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\nD K L( ) = PQ E x  P\nlogPx()\nQx()",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "D K L( ) = PQ E x  P\nlogPx()\nQx()\n= E x  P[log()log()] PxQx.(3.50)\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\nthecaseofdiscretevariables,orequalalmosteverywhereinthecaseofcontinuous\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthedierence\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\nbecauseitisnotsymmetric:D K L(PQ)=D K L(QP)forsomePandQ.This\n74",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\nto.Theentropyisgivenby 1 (p1)log(1p)pplog.Whenpisnear0,thedistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "isnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\noutcomes.\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\ntouseD K L( )PQorD K L( )QP.Seegureformoredetail.3.6\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "H(P,Q) =H(P)+D K L(PQ),whichissimilartotheKLdivergencebutlacking\nthetermontheleft:\nHP,Q( ) =  E x  Plog()Qx. (3.51)\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\ntreattheseexpressionsaslim x  0xxlog= 0.\n3.14StructuredProbabilisticModels",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "3.14StructuredProbabilisticModels\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n75",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nxProbability Densityq= argminq D K L() p q \np x()\nq() x\nxProbability Densityq= argminq D K L() q p \np() x\nq() x\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\neitherD KL(pq)orD KL(qp).Weillustratetheeectofthischoiceusingamixtureof\ntwoGaussiansforp,andasingleGaussianforq.Thechoiceofwhichdirectionofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "KLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\ndirectionoftheKLdivergencereectswhichoftheseconsiderationstakespriorityforeach\napplication. ( L e f t )TheeectofminimizingD KL(pq).Inthiscase,weselectaqthathas",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "highprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\neectofminimizingD KL(qp).Inthiscase,weselectaqthathaslowprobabilitywhere\nphaslowprobability.Whenphasmultiplemodesthataresucientlywidelyseparated,\nasinthisgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "illustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\narenotseparatedbyasucientlystronglowprobabilityregion,thenthisdirectionofthe\nKLdivergencecanstillchoosetoblurthemodes.\n76",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ndescribetheentirejointprobabilitydistributioncanbeveryinecient(both\ncomputationally andstatistically).\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\nainuencesthevalueofbandbinuencesthevalueofc,butthataandcare\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "variablesasaproductofprobabilitydistributionsovertwovariables:\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\nThesefactorizationscangreatlyreducethenumberofparametersneeded\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\nreducethecostofrepresentingadistributionifweareabletondafactorization\nintodistributionsoverfewervariables.\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "graphinthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\ninthegraphcorrespondstoarandomvariable,and anedgeconnectingtwo\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "interactionsbetweenthosetworandomvariables.\nD i r e c t e dmodelsusegraphswithdirectededges,andtheyrepresentfac-\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\nSpecically,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\ngiventheparentsofx i,denotedPa G(x i):\np() = x\nip(x i|Pa G(x i)). (3.53)\nSeegureforanexampleofadirectedgraphandthefactorizationofprobability 3.7",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "distributionsitrepresents.\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n77",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "connectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\nmodelisassociatedwithafactor( ) i(C( ) i).Thesefactorsarejustfunctions,not\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\ndistribution.\nTheprobabilityofacongurationofrandomvariablesis pr o p o r t i o naltothe\nproductofallofthesefactorsassignmentsthatresultinlargerfactorvaluesare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\nthereforedividebyanormalizingconstantZ,denedtobethesumorintegral\noverallstatesoftheproductofthefunctions,inordertoobtainanormalized\nprobabilitydistribution:\np() = x1\nZ\ni( ) i\nC( ) i\n. (3.55)\nSeegureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents.\nKeepinmindthatthesegraphicalrepresentationsoffactorizations area",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\n78",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,, (abcde) =1\nZ( 1 )( )abc,,( 2 )()bd,( 3 )()ce,. (3.56)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ways.\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\ndierentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\ndetail.\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "mostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods.\n79",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 8\nOptimizationforTrainingDeep\nModels\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\ndicultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "networktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral.\nThischapterfocusesononeparticularcaseofoptimization: ndingtheparam-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "etersofaneuralnetworkthatsignicantlyreduceacostfunction J(),which\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\nwellasadditionalregularizationterms.\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdiersfrompureoptimization. Next,wepresentseveral\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdicult.We\nthendeneseveralpracticalalgorithms,includingbothoptimization algorithms",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\n274",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\nalgorithmsintohigher-levelprocedures.\n8.1HowLearningDiersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdierfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "P,thatisdenedwithrespecttothetestsetandmayalsobeintractable.We\nthereforeoptimize Ponlyindirectly.Wereduceadierentcostfunction J()in\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\ndeepmodelsalsotypicallyincludesomespecializationonthespecicstructureof\nmachinelearningobjectivefunctions.\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\nsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "suchas\nJ() =  E ( )  x ,y  pdataL f , y , ((;)x) (8.1)\nwhere Listheper-examplelossfunction, f(x;)isthepredictedoutputwhen\ntheinputisx, p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\nsupervisedcase,wheretheargumentsto Lare f(x;)and y.However,itistrivial\ntoextendthisdevelopment,forexample,toincludeorxasarguments,orto\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\nunsupervisedlearning.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedlearning.\nEquationdenesanobjectivefunctionwithrespecttothetrainingset.We 8.1\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\noverthenitetrainingset:\nJ() =  E ( ) x ,y  pdataL f , y . ((;)x) (8.2)\n8.1.1EmpiricalRiskMinimization\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "thattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\n2 7 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistribution p(x , y)\ndenedbythetrainingset.Wenowminimizetheempiricalrisk\nE x ,y   pdata ( ) x , y[((;))] = L fx , y1\nmm ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "mm \ni = 1L f((x( ) i;) , y( ) i)(8.3)\nwhereisthenumberoftrainingexamples. m\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessignicantlyas\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\ncanbeexpectedtodecreasebyvariousamounts.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "canbeexpectedtodecreasebyvariousamounts.\nHowever,empiricalriskminimization ispronetoovertting.Modelswith\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\nriskminimization isnotreallyfeasible.Themosteectivemodernoptimization\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundened\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "rarelyuseempiricalriskminimization. Instead,wemustuseaslightlydierent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmoredierent\nfromthequantitythatwetrulywanttooptimize.\n8.1.2SurrogateLossFunctionsandEarlyStopping\nSometimes,thelossfunctionweactuallycareabout(sayclassicationerror)isnot\nonethatcanbeoptimizedeciently.Forexample,exactlyminimizingexpected0-1\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "classier(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassicationerrorin\nexpectation.\n2 7 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\nonecanimprovetherobustnessoftheclassierbyfurtherpushingtheclassesapart\nfromeachother,obtainingamorecondentandreliableclassier,thusextracting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\nminimizingtheaverage0-1lossonthetrainingset.\nAveryimportantdierencebetweenoptimization ingeneralandoptimization\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\nstopping(section)issatised.Typicallytheearlystoppingcriterionisbased 7.8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\nandisdesignedtocausethealgorithmtohaltwheneveroverttingbeginstooccur.\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\nwhichisverydierentfromthepureoptimization setting,whereanoptimization\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\n8.1.3BatchandMinibatchAlgorithms\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\nspace,decomposeintoasumovereachexample:\n M L= argmax\nm \ni = 1log p m o de l(x( ) i, y( ) i;) . (8.4)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "i = 1log p m o de l(x( ) i, y( ) i;) . (8.4)\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\nempiricaldistributiondenedbythetrainingset:\nJ() =  E x ,y   pdatalog p m o de l(;)x , y . (8.5)\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n2 7 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmostcommonlyusedpropertyisthegradient:\n  J() =  E x ,y   pdata log p m o de l(;)x , y . (8.6)\nComputingthis expectationexactlyisveryexpensivebecauseitrequires\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "samplesisgivenby  /n ,where isthetruestandarddeviationofthevalueof\nthesamples.Thedenominator ofnshowsthattherearelessthanlinearreturns\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "numberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\nofthegradientratherthanslowlycomputingtheexactgradient.\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "areunlikelytotrulyencounterthisworst-casesituation,butwemayndlarge\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\nbecausethewordbatchisalsooftenusedtodescribetheminibatchusedby\nminibatchstochasticgradientdescent.Typicallythetermbatchgradientdescent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "impliestheuseofthefulltrainingset,whiletheuseofthetermbatchtodescribe\nagroupofexamplesdoesnot.Forexample,itisverycommontousetheterm\nbatchsizetodescribethesizeofaminibatch.\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\ncreatedexamplesratherthanfromaxed-sizetrainingsetoverwhichseveral\npassesaremade.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "passesaremade.\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n2 7 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\ncallthemstochasticmethods.\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\npresentedindetailinsection.8.3.1\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\nLargerbatchesprovideamoreaccurateestimateofthegradient,butwith\nlessthanlinearreturns.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "lessthanlinearreturns.\nMulticorearchitectures areusuallyunderutilized byextremelysmallbatches.\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\nisnoreductioninthetimetoprocessaminibatch.\nIfallexamplesinthebatcharetobeprocessedinparallel(asistypically\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\nhardwaresetupsthisisthelimitingfactorinbatchsize.\nSomekindsofhardwareachievebetterruntimewithspecicsizesofarrays.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooer\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\nsometimesbeingattemptedforlargemodels.\nSmallbatchescanoeraregularizingeect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1.Trainingwithsuchasmallbatch\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "duetotheneedtomakemoresteps,bothbecauseofthereducedlearning\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\nDierentkindsofalgorithmsusedierentkindsofinformationfromthemini-\nbatchindierentways.Somealgorithmsaremoresensitivetosamplingerrorthan\nothers,eitherbecausetheyuseinformationthatisdiculttoestimateaccurately\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "usuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\nH 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\nsizesarerequiredtominimizeuctuationsintheestimatesofH 1g.Suppose\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\n2 7 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nHoritsinverseampliespre-existingerrors,inthiscase,estimationerrorsing.\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\nH 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\napproximately,sotheupdateH 1gwillcontainevenmoreerrorthanwewould\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\nlistmightbearrangedsothatrstwehavevebloodsamplestakenatdierent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "timesfromtherstpatient,thenwehavethreebloodsamplestakenfromthe\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\nholdssomesignicance,itisnecessarytoshuetheexamplesbeforeselecting\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\nitisusuallysucienttoshuetheorderofthedatasetonceandthenstoreitin\nshuedfashion.Thiswillimposeaxedsetofpossibleminibatchesofconsecutive\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "signicantdetrimentaleect.Failingtoevershuetheexamplesinanywaycan\nseriouslyreducetheeectivenessofthealgorithm.\nManyoptimization problemsinmachinelearningdecomposeoverexamples\nwellenoughthatwecancomputeentireseparateupdatesoverdierentexamples\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\ndiscussedfurtherinsection.12.1.3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "discussedfurtherinsection.12.1.3\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\n2 8 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ndescentshuethedatasetonceandthenpassthroughitmultipletimes.Onthe\nrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\nnewfairsamplesfromthedatageneratingdistribution.\nThefactthatstochasticgradientdescentminimizesgeneralization erroris",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "easiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\nfromastreamofdata.Inotherwords,insteadofreceivingaxed-sizetraining\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\nfrom p da t a.\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete.Inthis\ncase,thegeneralization error(equation)canbewrittenasasum 8.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "J() =\nx\nyp da t a()((;)) x , y L fx , y , (8.7)\nwiththeexactgradient\ng=   J() =\nx\nyp da t a()x , y  L f , y . ((;)x)(8.8)\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\nundermildassumptionsregarding p da t aand. L\nHence,wecanobtainanunbiasedestimatoroftheexactgradientofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "generalization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\ng=1\nm \niL f((x( ) i;) , y( ) i) . (8.9)\nUpdatinginthedirectionof  gperformsSGDonthegeneralization error.\nOfcourse,thisinterpretation onlyapplies whenexamplesarenotreused.\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "unlessthetrainingsetisextremelylarge.When multiplesuchepochsareused,\nonlytherstepochfollowstheunbiasedgradientofthegeneralization error,but\n2 8 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofcourse,theadditionalepochsusuallyprovideenoughbenetduetodecreased\ntrainingerrortoosettheharmtheycausebyincreasingthegapbetweentraining\nerrorandtesterror.\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\nset.Whenusinganextremelylargetrainingset,overttingisnotanissue,so",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "underttingandcomputational eciencybecomethepredominant concerns.See\nalso ()foradiscussionoftheeectofcomputational BottouandBousquet2008\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\n8.2ChallengesinNeuralNetworkOptimization\nOptimization ingeneralisanextremelydiculttask.Traditionally,machine\nlearninghasavoidedthedicultyofgeneraloptimization bycarefullydesigning\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels.\n8.2.1Ill-Conditioning\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\nmoredetailinsection.4.3.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "moredetailinsection.4.3.1\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\nstuckinthesensethatevenverysmallstepsincreasethecostfunction.\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\ncostfunctionpredictsthatagradientdescentstepofwilladd  g\n1\n22gHgg g (8.10)\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n22gHg",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "22gHg\nexceeds gg.Todeterminewhetherill-conditioning isdetrimentaltoaneural\nnetworktraining task,onecanmonitorthesquaredgradientnormggand\n2 8 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n50050100150200250\nTrainingtime(epochs)20246810121416Gradient norm\n0 50100150200250\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classicationerrorrate\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassication\nerrordecreasestoalowlevel.\nthegHgterm.Inmanycases,thegradientnormdoesnotshrinksignicantly",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "throughoutlearning,butthegHgtermgrowsbymorethananorderofmagnitude.\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\ncurvature.Figureshowsanexampleofthegradientincreasingsignicantly 8.1\nduringthesuccessfultrainingofaneuralnetwork.\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\ntraining,someofthetechniquesusedtocombatitinothercontextsareless",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "applicabletoneuralnetworks.Forexample,Newtonsmethodisanexcellenttool\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\nthesubsequentsectionswewillarguethatNewtonsmethodrequiressignicant\nmodicationbeforeitcanbeappliedtoneuralnetworks.\n8.2.2LocalMinima\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\ncanbereducedtotheproblemofndingalocalminimum.Anylocalminimumis\n2 8 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaatregionat\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\naatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\nknowthatwehavereachedagoodsolutionifwendacriticalpointofanykind.\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "anextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\nnecessarilyamajorproblem.\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiability\nproblem.Amodelissaidtobeidentiableifasucientlylargetrainingsetcan\nruleoutallbutonesettingofthemodelsparameters.Modelswithlatentvariables\nareoftennotidentiablebecausewecanobtainequivalentmodelsbyexchanging",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\nunits.Thiskindofnon-identiabilit yisknownasweightspacesymmetry.\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\nadditionalcausesofnon-identiabilit y.Forexample,inanyrectiedlinearor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "maxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\nifwealsoscaleallofitsoutgoingweightsby1\n.Thismeansthatifthecost\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\nweightsratherthanthemodelsoutputseverylocalminimumofarectiedlinear\normaxoutnetworkliesonan( m n)-dimensionalhyperbolaofequivalentlocal\nminima.\nThesemodelidentiabilityissuesmeanthattherecanbeanextremelylarge\norevenuncountablyinniteamountoflocalminimainaneuralnetworkcost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "function.However,alloftheselocalminimaarisingfromnon-identiabilit yare\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\nnotaproblematicformofnon-convexity.\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\noptimization algorithms.\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\n2 8 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\nnowsuspectthat,forsucientlylargeneuralnetworks,mostlocalminimahavea\nlowcostfunctionvalue,andthatitisnotimportanttondatrueglobalminimum",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "ratherthantondapointinparameterspacethathaslowbutnotminimalcost\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\netal.,).2014\nManypractitioners attributenearlyalldicultywithneuralnetworkoptimiza-\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspecic\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\ninsignicantsize,theproblemisneitherlocalminimanoranyotherkindofcritical",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "point.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\nspaces,itcanbeverydiculttopositivelyestablishthatlocalminimaarethe\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "whileothershavealowercost.Atasaddlepoint,theHessianmatrixhasboth\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\nmaximumalonganothercross-section.Seegureforanillustration. 4.5\nManyclassesofrandomfunctionsexhibitthefollowingbehavior:inlow-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn Rof\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.The\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\nImaginethatthesignofeacheigenvalueisgeneratedbyippingacoin.Inasingle",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\n2 8 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.In\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\nheads ntimesifweareatacriticalpointwithlowcost.Thismeansthatlocal\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\nhighcostaremorelikelytobelocalmaxima.\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "proofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "deepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\nfunctionsrelatedtoneuralnetworksdoessoaswell.\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\nrithms?Forrst-orderoptimization algorithmsthatuseonlygradientinformation,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\ngiveningure.Thesevisualizationsshowaatteningofthecostfunctionnear 8.2\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\nmaybedierentformorerealisticusesofgradientdescent.\nForNewtonsmethod,itisclearthatsaddlepointsconstituteaproblem.\n2 8 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nP r o j e c t i o n2o f \nP r o j e c t i o n 1 o f J(\n)\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\nwithpermissionfromGoodfellow2015 e t a l .().Thesevisualizationsappearsimilarfor\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\nvisualizationsusuallydonotshowmanyconspicuousobstacles.Priortothesuccessof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "stochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\nstructurethanisrevealedbytheseprojections.Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\nMostoftrainingtimeisspenttraversingtherelativelyatvalleyofthecostfunction,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\ninthisregion,orsimplytheneedtocircumnavigatethetallmountainvisibleinthe\ngureviaanindirectarcingpath.\n2 8 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nGradientdescentisdesignedtomovedownhillandisnotexplicitlydesigned\ntoseekacriticalpoint.Newtonsmethod,however,isdesignedtosolvefora\npointwherethegradientiszero.Withoutappropriatemodication,itcanjump\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "saddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\nimprovessignicantlyoverthetraditionalversion.Second-order methodsremain\ndiculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\npromiseifitcouldbescaled.\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\npoints.Therearealsomaxima,whic haremuchlikesaddlepointsfromthe\nperspectiveofoptimizationmany algorithmsarenotattractedtothem,but\nunmodiedNewtonsmethodis.Maximaofmanyclassesofrandomfunctions",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "becomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\nTheremayalsobewide,atregionsofconstantvalue.Intheselocations,the\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\natregionmustconsistentirelyofglobalminima,butinageneraloptimization\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\n8.2.4ClisandExplodingGradients",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "8.2.4ClisandExplodingGradients\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\nclis,asillustratedingure.Theseresultfromthemultiplicationofseveral 8.3\nlargeweightstogether.Onthefaceofanextremelysteepclistructure,the\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingo\noftheclistructurealtogether.\n2 8 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\n \n\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliregion,a\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "optimizationworkthathadbeendone.FigureadaptedwithpermissionfromPascanu\ne t a l .().2013\nTheclicanbedangerouswhetherweapproachitfromaboveorfrombelow,\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\nwithinaninnitesimalregion.Whenthetraditionalgradientdescentalgorithm",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "proposestomakeaverylargestep,thegradientclippingheuristicintervenesto\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cli\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\nmultiplication.\n8.2.5Long-TermDependencies",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "multiplication.\n8.2.5Long-TermDependencies\nAnotherdicultythatneuralnetworkoptimization algorithmsmustovercome\nariseswhenthecomputationalgra phbecomesextremelydeep.Feedforward\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\n2 8 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\npronounceddiculties.\nForexample,supposethatacomputational graphcontainsapaththatconsists\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag()V 1.\nInthissimplecase,itisstraightforwardtoseethat\nWt=\nVVdiag() 1t= ()VdiagtV 1. (8.11)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Wt=\nVVdiag() 1t= ()VdiagtV 1. (8.11)\nAnyeigenvalues  ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\nvanishingandexplodinggradientproblemreferstothefactthatgradients\nthroughsuchagrapharealsoscaledaccordingtodiag()t.Vanishinggradients\nmakeitdiculttoknowwhichdirectiontheparametersshouldmovetoimprove\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecli",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "structuresdescribedearlierthatmotivategradientclippingareanexampleofthe\nexplodinggradientphenomenon.\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\nsimilartothepowermethodalgorithmusedtondthelargesteigenvalueof\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\nnotsurprisingthatxWtwilleventuallydiscardallcomponentsofxthatare\northogonaltotheprincipaleigenvectorof.W\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\nvanishingandexplodinggradientproblem(,). Sussillo2014\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n8.2.6InexactGradients\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\nanoisyorevenbiasedestimateofthesequantities.Nearlyeverydeeplearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "algorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient.\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\n2 9 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\nofaBoltzmannmachine.\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\nlossfunctionatasinglepointitcanbediculttomakeasinglestepif J()is\npoorlyconditionedatthecurrentpoint,orifliesonacli,orifisasaddle\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\nnotpointtowarddistantregionsofmuchlowercost.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "notpointtowarddistantregionsofmuchlowercost.\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\nmountain-shapedstructure.\nMuchofresearchintothedicultiesofoptimization hasfocusedonwhether\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\nlog p( y|x;)canlackaglobalminimumpointandinsteadasymptotically\napproachsomevalueasthemodelbecomesmorecondent.Foraclassierwith\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "zero.Likewise,amodelofrealvalues p( y|x) =N( y; f() ,  1)canhavenegative\nlog-likelihoodthatasymptotestonegativeinnityif f()isabletocorrectly\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\nwithoutbound.Seegureforanexampleofafailureoflocaloptimization to 8.4\nndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\npoints.\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "inuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n2 9 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nJ ( ) \nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdicultyin\nthiscaseisbeinginitializedonthewrongsideofthemountainandnotbeingableto\ntraverseit.Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "suchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\nexcessivetrainingtime,asillustratedingure.8.2\noftheprocess.\nManyexistingresearchdirectionsareaimedatndinggoodinitialpointsfor\nproblemsthathavedicultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves.\nGradientdescentandessentiallyalllearningalgorithmsthatareeectivefor\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "canbediculttocompute.Wemaybeabletocomputesomepropertiesofthe\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\nnotdeneareasonablyshortpathtoavalidsolution,butwearenotactually\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thesecases,localdescentwithstepsofsize maydeneareasonablyshortpath\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\nstepsofsize  .Inthesecases,localdescentmayormaynotdeneapath\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\n2 9 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\nthefunctionhasawideatregion,orifwemanagetolandexactlyonacritical\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\nforcriticalpoints,suchasNewtonsmethod).Inthesecases,localdescentdoes\nnotdeneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "gure,oralonganunnecessarilylongtrajectorytothesolution,asingure. 8.4 8.2\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\nmakingneuralnetworkoptimization dicult,andthisisanactiveareaofresearch.\nRegardlessofwhichoftheseproblemsaremostsignicant,allofthemmightbe\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\nwithinthatwell-behavedregion.Thislastviewsuggestsresearchintochoosing",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "goodinitialpointsfortraditionaloptimization algorithmstouse.\n8.2.8TheoreticalLimitsofOptimization\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\nlittlebearingontheuseofneuralnetworksinpractice.\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\nnetworkoutputdiscretevalues.However,mostneuralnetworkunitsoutput",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "smoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\nitcanbediculttotellwhetheraparticularproblemfallsintothatclass.Other\nresultsshowthatndingasolutionforanetworkofagivensizeisintractable,but\ninpracticewecanndasolutioneasilybyusingalargernetworkforwhichmany\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\ncontextofneuralnetworktraining,weusuallydonotcareaboutndingtheexact",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "minimumofafunction,butseekonlytoreduceitsvaluesucientlytoobtaingood\ngeneralization error.Theoretical analysisofwhetheranoptimization algorithm\ncanaccomplishthisgoalisextremelydicult.Developingmorerealisticbounds\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\ngoalformachinelearningresearch.\n2 9 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.3BasicAlgorithms\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n8.3.1StochasticGradientDescent\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsformachinelearningingeneralandfordeeplearning\ninparticular.As discussedinsection,itispossibletoobtainanunbiased 8.1.3\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\nexamplesdrawni.i.dfromthedatageneratingdistribution.\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\nRequire:Learningrate  k.\nRequire:Initialparameter\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate: g+1\nm \ni L f((x( ) i;) ,y( ) i)\nApplyupdate:  g\nendwhile\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\nhavedescribedSGDasusingaxedlearningrate .Inpractice,itisnecessaryto\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\natiterationas k  k.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "atiterationas k  k.\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\ndescent,sobatchgradientdescentcanuseaxedlearningrate.Asucient\nconditiontoguaranteeconvergenceofSGDisthat\n\nk = 1 k= and  , (8.12)\n2 9 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\nk = 12\nk < . (8.13)\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: \n k= (1 )   0+    (8.14)\nwith =k\n.Afteriteration,itiscommontoleaveconstant.  \nThelearningratemaybechosenbytrialanderror,butitisusuallybest\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "theparameterstochooseare  0,  ,and .Usually maybesettothenumberof\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\n shouldbesettoroughlythevalueof 1%  0.Themainquestionishowtoset  0.\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\nfunctionoftenincreasingsignicantly.Gentleoscillationsarene,especiallyif\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\nnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\naftertherst100iterationsorso.Therefore,itisusuallybesttomonitortherst\nseveraliterationsandusealearningratethatishigherthanthebest-performing\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\nconvergetowithinsomexedtoleranceofitsnaltestseterrorbeforeithas\nprocessedtheentiretrainingset.\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\nmeasuretheexcesserror J()min  J(),whichistheamountthatthecurrent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "costfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\nproblem,theexcesserroris O(1\nk)after kiterations,whileinthestronglyconvex\ncaseitis O(1\nk).Theseboundscannotbeimprovedunlessextraconditionsare\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\ngradientdescentintheory.However,theCramr-Raobound(,;, Cramr1946Rao\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\nk).Bottou\n2 9 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\nanoptimization algorithmthatconvergesfasterthan O(1\nk)formachinelearning\ntasksfasterconvergencepresumablycorrespondstoovertting.Moreover,the\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\ntheremainderofthischapterachievebenetsthatmatterinpracticebutarelost\nintheconstantfactorsobscuredbythe O(1\nk)asymptoticanalysis.Onecanalso\ntradeothebenetsofbothbatchandstochasticgradientdescentbygradually\nincreasingtheminibatchsizeduringthecourseoflearning.\nFormoreinformationonSGD,see(). Bottou1998\n8.3.2Momentum\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\nintheirdirection.Theeectofmomentumisillustratedingure.8.5\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\nofvelocityitisthedirectionandspeedatwhichtheparametersmovethrough",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\naccordingtoNewtonslawsofmotion.Momentuminphysicsismasstimesvelocity.\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter [0 ,1)\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "Theupdateruleisgivenby:\nvv   \n1\nmm \ni = 1L((fx( ) i;) ,y( ) i)\n, (8.15)\nv  + . (8.16)\nThevelocityvaccumulatesthegradientelements 1\nmm\ni = 1 L((fx( ) i;) ,y( ) i)\n.\nThelarger isrelativeto ,themorepreviousgradientsaectthecurrentdirection.\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\n2 9 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n   3 0 2 0 1 0 0 1 0 2 0 3 0 2 0 1 001 02 0\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\novercomestherstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\nnarrowaxisofthecanyon.Comparealsogure,whichshowsthebehaviorofgradient 4.6\ndescentwithoutmomentum.\n2 9 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\nobservesgradientg,thenitwillaccelerateinthedirectionofg,untilreachinga\nterminalvelocitywherethesizeofeachstepis\n||||g\n1 . (8.17)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "||||g\n1 . (8.17)\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\n1  .For\nexample, = .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\nthegradientdescentalgorithm.\nCommonvaluesof usedinpracticeinclude .5, .9,and .99.Likethelearning\nrate, mayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\nislaterraised.Itislessimportanttoadapt overtimethantoshrink overtime.\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter.  ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Require:Learningrate,momentumparameter.  \nRequire:Initialparameter,initialvelocity.  v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate:g1\nm \ni L f((x( ) i;) ,y( ) i)\nComputevelocityupdate:vvg   \nApplyupdate:v  +\nendwhile\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "intuitionforhowthemomentumandgradientdescentalgorithmsbehave.\nThepositionoftheparticleatanypointintimeisgivenby( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = t2\n t2() t . (8.18)\nRatherthanviewingthisasasecond-orderdierentialequationoftheposition,\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\ntandrewritetheNewtoniandynamicsasarst-orderdierentialequation:\nv() = t\n t() t , (8.19)\n2 9 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nf() = t\n tv() t . (8.20)\nThemomentumalgorithmthenconsistsofsolvingthedierentialequationsvia\nnumericalsimulation.Asimplenumericalmethodforsolvingdierentialequations\nisEulersmethod,whichsimplyconsistsofsimulatingthedynamicsdenedby\ntheequationbytakingsmall,nitestepsinthedirectionofeachgradient.\nThisexplainsthebasicformofthemomentumupdate,butwhatspecicallyare\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "  J().Thisforcepushestheparticledownhillalongthecostfunctionsurface.\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\nuntilitbeginstogouphillagain.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "untilitbeginstogouphillagain.\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\notherforce,proportionaltov( t).Inphysicsterminology,thisforcecorresponds\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\nconvergetoalocalminimum.\nWhydoweusev( t)andviscousdraginparticular?Partofthereasonto\nusev( t)ismathematical convenienceanintegerpowerofthevelocityiseasy\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "velocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\nalocalminimum.Viscousdragavoidsbothoftheseproblemsitisweakenough\n2 9 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\n8.3.3NesterovMomentum\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\ninspiredbyNesterovsacceleratedgradientmethod(,,).The Nesterov19832004\nupdaterulesinthiscasearegivenby:\nvv   \n1\nmm \ni = 1L\nfx(( ) i;+ ) v ,y( ) i\n,(8.21)\nv  + , (8.22)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": ",(8.21)\nv  + , (8.22)\nwheretheparameters and playasimilarroleasinthestandardmomentum\nmethod.ThedierencebetweenNesterovmomentumandstandardmomentumis\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "convergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\nbyNesterov1983().Unfortunately,inthestochasticgradientcase,Nesterov\nmomentumdoesnotimprovetherateofconvergence.\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\nRequire:Learningrate,momentumparameter.  \nRequire:Initialparameter,initialvelocity.  v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondinglabelsy( ) i.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "correspondinglabelsy( ) i.\nApplyinterimupdate: v  + \nComputegradient(atinterimpoint):g1\nm  \ni L f((x( ) i;y) ,( ) i)\nComputevelocityupdate:vvg   \nApplyupdate:v  +\nendwhile\n3 0 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.4ParameterInitializationStrategies\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "learningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\nmodelsisasucientlydiculttaskthatmostalgorithmsarestronglyaectedby\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\nencountersnumericaldicultiesandfailsaltogether.Whenlearningdoesconverge,\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "convergestoapointwithhighorlowcost.Also,pointsofcomparablecost\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaectthe\ngeneralization aswell.\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\ninitialization strategiesisadiculttaskbecauseneuralnetworkoptimization is\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\nafterlearningbeginstoproceed.Afurtherdicultyisthatsomeinitialpoints\nmaybebenecialfromtheviewpointofoptimization butdetrimentalfromthe\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaects\ngeneralization isespeciallyprimitive,oeringlittletonoguidanceforhowtoselect\ntheinitialpoint.\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "parametersneedtobreaksymmetrybetweendierentunits.Iftwohidden\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\ntheseunitsmusthavedierentinitialparameters.Iftheyhavethesameinitial\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputedierent\nupdatesfordierentunits(forexample,ifonetrainswithdropout),itisusually",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "besttoinitializeeachunittocomputeadierentfunctionfromalloftheother\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\nofback-propagation. Thegoalofhavingeachunitcomputeadierentfunction\n3 0 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\nforalargesetofbasisfunctionsthatareallmutuallydierentfromeachother,\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\ndierentfunctionfromeachotherunit.Randominitialization fromahigh-entropy",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "distributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\ntoassignanyunitstocomputethesamefunctionaseachother.\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\nchosenconstantsmuchlikethebiasesare.\nWealmostalwaysinitializealltheweightsinthemodeltovaluesdrawn",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "randomlyfromaGaussianoruniformdistribution.ThechoiceofGaussian\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\nlargeeectonboththeoutcomeoftheoptimization procedureandontheability\nofthenetworktogeneralize.\nLargerinitialweightswillyieldastrongersymmetrybreakingeect,helping\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "back-propagationthroughthelinearcomponentofeachlayerlargervaluesinthe\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\nofthedeterministicforwardpropagationprocedureappearsrandom).Tosome\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\nTheperspectivesofregularizationandoptimization cangiveverydierent\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\n3 0 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nduetotriggeringsomeearlystoppingcriterionbasedonovertting)expressesa\npriorthatthenalparametersshouldbeclosetotheinitialparameters.Recall\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\ntheeectofinitialization. Wecanthinkofinitializingtheparametersto 0as",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "beingsimilartoimposingaGaussianprior p()withmean 0.Fromthispoint\nofview,itmakessensetochoose 0tobenear0.Thispriorsaysthatitismore\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\npreferenceforthemtointeract.Ontheotherhand,ifweinitialize 0tolarge\nvalues,thenourpriorspecieswhichunitsshouldinteractwitheachother,and\nhowtheyshouldinteract.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "howtheyshouldinteract.\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\nnoutputsbysamplingeachweightfrom U(1m,1m),whileGlorotandBengio\n()suggestusingthe 2010 normalizedinitialization\nW i , j U\n\n6\nm n+,\n6\nm n+\n. (8.23)\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "layerstohavethesamegradientvariance.Theformulaisderivedusingthe\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\nnonlinearcounterparts.\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ateachlayer.Theyderivespecicvaluesofthescalingfactorfordierenttypesof\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "gradientsincreaseinnormastheypropagatebackward. ()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissucienttotrainnetworksasdeepas\n3 0 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations.A keyinsightof\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\nbehavior.Thisisbecausefeedforwardnetworksuseadierentweightmatrixat\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "ariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\noptimalperformance.Thismaybeforthreedierentreasons.First,wemay\nbeusingthewrongcriteriaitmaynotactuallybebenecialtopreservethe\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "increasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\nnotexactlyequaltothetheoreticalpredictions.\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\nsamestandarddeviation,suchas1m,isthateveryindividualweightbecomes\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "initializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\nvalues.Becauseittakesalongtimeforgradientdescenttoshrinkincorrectlarge",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "values,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\nthathaveseveralltersthatmustbecarefullycoordinatedwitheachother.\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\nByrepeatedlyidentifyingtherstlayerwithunacceptably smallactivationsand\n3 0 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\nactivations.Thisprocedurecaninprinciplebeautomatedandisgenerallyless\ncomputationally costlythanhyperparameter optimization basedonvalidationset\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciedmore\nformallyandstudiedby (). MishkinandMatas2015\nSofarwehavefocusedontheinitializationoftheweights.Fortunately,\ninitialization ofotherparametersistypicallyeasier.\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "initialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\nnon-zerovalues:\nIfabiasisforanoutputunit,thenitisoftenbenecialtoinitializethebiasto\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\nonlybythebias.Thisjustiessettingthebiastotheinverseoftheactivation\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\nForexample,iftheoutputisadistributionoverclassesandthisdistribution",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiersbutalsoto\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\nmatchthemarginaldistributionover.x\nSometimeswemaywanttochoosethebiastoavoidcausingtoomuch",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "saturationatinitialization. Forexample,wemaysetthebiasofaReLU\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample,itisnot\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\nSometimesaunitcontrolswhetherotherunitsareabletoparticipateina\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "h[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h.We\n3 0 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h uor u h0.Inthese\nsituations,wewanttosetthebiasfor hsothat h1mostofthetimeat\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\ntheLSTMmodel,describedinsection.10.10\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\nusingthemodel",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "usingthemodel\np y y (| Nx) = (|wTx+1) b , / (8.24)\nwhere isaprecisionparameter.Wecanusuallyinitializevarianceorprecision\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\nenoughtozerothatthebiasesmaybesetwhileignoringtheeectoftheweights,\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\noersfasterconvergencethanarandominitialization. Someoftheseinitialization\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "encodeinformationaboutthedistributionintheinitialparametersofthemodel.\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdierentunitstocomputedierentfunctionsfromeachother.\n8.5AlgorithmswithAdaptiveLearningRates\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\nofthehyperparameters thatisthemostdiculttosetbecauseithasasignicant\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\n3 0 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\nthecourseoflearning.\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "thenthelearningrateshoulddecrease.Ofcourse,thiskindofrulecanonlybe\nappliedtofullbatchoptimization.\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrieyreviewafewofthesealgorithms.\n8.5.1AdaGrad\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "parameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\nhavearelativelysmalldecreaseintheirlearningrate.Theneteectisgreater\nprogressinthemoregentlyslopeddirectionsofparameterspace.\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthatfor\ntrainingdeepneuralnetworkmodelstheaccumulation ofsquaredgradientsfrom",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "thebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\neectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\nmodels.\n8.5.2RMSProp\nTheRMSPropalgorithm(,)modiesAdaGradtoperformbetterin Hinton2012\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\ntoaconvexfunction.When appliedtoanon-convexfunctiontotrainaneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "network,thelearningtrajectorymaypassthroughmanydierentstructuresand\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\n3 0 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.4TheAdaGradalgorithm\nRequire:Globallearningrate \nRequire:Initialparameter\nRequire:Smallconstant,perhaps  10 7,fornumericalstability\nInitializegradientaccumulationvariabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g1\nm \ni L f((x( ) i;) ,y( ) i)\nAccumulatesquaredgradient:rrgg +\nComputeupdate: ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "Computeupdate: \n +rg.(Divisionandsquarerootapplied\nelement-wise)\nApplyupdate:  +\nendwhile\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\nextremepastsothatitcanconvergerapidlyafterndingaconvexbowl,asifit\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "movingaverageintroducesanewhyperparameter, ,thatcontrolsthelengthscale\nofthemovingaverage.\nEmpirically,RMSProphasbeenshowntobeaneectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\n8.5.3Adam\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\nalgorithmandispresentedinalgorithm .ThenameAdamderivesfrom 8.7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "thephraseadaptivemoments.Inthecontextoftheearlieralgorithms,itis\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\ndirectlyasanestimateoftherstordermoment(withexponentialweighting)of\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n3 0 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.5TheRMSPropalgorithm\nRequire:Globallearningrate,decayrate.  \nRequire:Initialparameter\nRequire:Smallconstant ,usually 10 6,usedtostabilizedivisionbysmall\nnumbers.\nInitializeaccumulation variablesr= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g1\nm \ni L f((x( ) i;) ,y( ) i)\nAccumulatesquaredgradient:rrgg  +(1 ) ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "Accumulatesquaredgradient:rrgg  +(1 ) \nComputeparameterupdate: =\n + rg.(1\n + rappliedelement-wise)\nApplyupdate:  +\nendwhile\nbiascorrectionstotheestimatesofboththerst-ordermoments(themomentum\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\nthesuggesteddefault.\n8.5.4ChoosingtheRightOptimizationAlgorithm\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\nchoose?\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "presentedavaluablecomparisonofalargenumberofoptimization algorithms\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n3 0 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\nRequire:Globallearningrate,decayrate,momentumcoecient.   \nRequire:Initialparameter,initialvelocity.  v\nInitializeaccumulation variabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputeinterimupdate: v  + \nComputegradient:g1\nm  \ni L f((x( ) i;y) ,( ) i)\nAccumulategradient:rrgg  +(1 ) ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "Accumulategradient:rrgg  +(1 ) \nComputevelocityupdate:vv rg.(1rappliedelement-wise)\nApplyupdate:v  +\nendwhile\nlargelyontheusersfamiliaritywiththealgorithm(foreaseofhyperparameter\ntuning).\n8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\nrisk:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "risk:\nJ() =  E x ,y   pdata ( ) x , y[((;))] = L fx , y1\nmm \ni = 1L f((x( ) i;) , y( ) i) .(8.25)\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\ndiscussedinchapter.7\n8.6.1NewtonsMethod\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttorst- 4.3\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\noptimization. Themostwidelyusedsecond-ordermethodisNewtonsmethod.We",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "nowdescribeNewtonsmethodinmoredetail,withemphasisonitsapplicationto\nneuralnetworktraining.\n3 1 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: )  0001 .\nRequire:Exponentialdecayratesformomentestimates,  1and  2in[0 ,1).\n(Suggesteddefaults:andrespectively) 09 . 0999 .\nRequire:Smallconstant usedfornumericalstabilization.(Suggesteddefault:\n10 8)\nRequire:Initialparameters\nInitialize1stand2ndmomentvariables ,s= 0r= 0\nInitializetimestep t= 0\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:g1\nm \ni L f((x( ) i;) ,y( ) i)\nt t+1\nUpdatebiasedrstmomentestimate:s  1s+(1  1)g\nUpdatebiasedsecondmomentestimate:r  2r+(1  2)gg\nCorrectbiasinrstmoment:ss\n1  t\n1\nCorrectbiasinsecondmoment:rr\n1  t\n2\nComputeupdate: =  s\n r + (operationsappliedelement-wise)\nApplyupdate:  +\nendwhile",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "Applyupdate:  +\nendwhile\nNewtonsmethodisanoptimization schemebasedonusingasecond-orderTay-\nlorseriesexpansiontoapproximate J()nearsomepoint 0,ignoringderivatives\nofhigherorder:\nJ J () ( 0)+( 0)  J( 0)+1\n2( 0)H ( 0) ,(8.26)\nwhereHistheHessianof Jwithrespecttoevaluatedat 0.Ifwethensolvefor\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\n=  0H 1  J( 0) (8.27)\nThusforalocallyquadraticfunction(withpositivedeniteH),byrescaling",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "thegradientbyH 1,Newtonsmethodjumpsdirectlytotheminimum.If the\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewtons\nmethod,giveninalgorithm .8.8\n3 1 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.8Newtonsmethodwithobjective J() =\n1\nmm\ni = 1 L f((x( ) i;) , y( ) i).\nRequire:Initialparameter 0\nRequire:Trainingsetofexamples m\nwhile do stoppingcriterionnotmet\nComputegradient:g1\nm \ni L f((x( ) i;) ,y( ) i)\nComputeHessian:H1\nm2\n\ni L f((x( ) i;) ,y( ) i)\nComputeHessianinverse:H 1\nComputeupdate: = H 1g\nApplyupdate: = +\nendwhile\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "denite,Newtonsmethodcanbeappliediteratively.Thisimpliesatwo-step\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\ningthequadraticapproximation).Second, updatetheparametersaccordingto\nequation.8.27\nInsection,wediscussedhowNewtonsmethodisappropriateonlywhen 8.2.3\ntheHessianispositivedenite.Indeeplearning,thesurfaceoftheobjective\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\nareproblematicforNewtonsmethod.IftheeigenvaluesoftheHessianarenot",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "allpositive,forexample,nearasaddlepoint,thenNewtonsmethodcanactually\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes \n=  0[(( H f 0))+ ] I 1  f( 0) . (8.28)\nThisregularizationstrategyisusedinapproximations toNewtonsmethod,such\nastheLevenbergMarquardt algorithm(Levenberg1944Marquardt1963 ,;,),and",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\nvalueof wouldhavetobesucientlylargetoosetthenegativeeigenvalues.\nHowever,as increasesinsize,theHessianbecomesdominatedbythe Idiagonal\nandthedirectionchosenbyNewtonsmethodconvergestothestandardgradient\ndividedby .Whenstrongnegativecurvatureispresent, mayneedtobeso\nlargethatNewtonsmethodwouldmakesmallerstepsthangradientdescentwith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "aproperlychosenlearningrate.\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n3 1 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsuchassaddlepoints,theapplicationofNewtonsmethodfortraininglargeneural\nnetworksislimitedbythesignicantcomputational burdenitimposes.The\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\nkcanbeinthemillions),Newtonsmethodwouldrequiretheinversionofa k k\nmatrixwith computational complexityof O( k3).Also,sincetheparameterswill",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "changewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\ncanbepracticallytrainedviaNewtonsmethod.Intheremainderofthissection,\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewtons\nmethodwhileside-steppingthecomputational hurdles.\n8.6.2ConjugateGradients\nConjugategradientsisamethodtoecientlyavoidthecalculationoftheinverse\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineective\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\nsearchdirection.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "searchdirection.\nLettheprevioussearchdirectionbed t  1.Attheminimum,wheretheline\nsearchterminates,thedirectionalderivativeiszeroindirectiond t  1:  J()\nd t  1=0.Sincethegradientatthispointdenesthecurrentsearchdirection,\nd t=  J() willhavenocontributioninthedirectiond t  1.Thusd tisorthogonal\ntod t  1.Thisrelationshipbetweend t  1andd tisillustratedingurefor8.6\nmultipleiterationsofsteepestdescent.Asdemonstratedinthegure,thechoiceof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\nseekstoaddressthisproblem.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "seekstoaddressthisproblem.\nInthemethodofconjugategradients,weseektondasearchdirectionthat\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\n3 1 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u0000  \u0000  \u0000      \u0000 \u0000 \u0000   \nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\nmethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\ndenedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\nseenwithusingaxedlearningrateingure,butevenwiththeoptimalstepsize 4.6\nthealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydenition,at",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "theminimumoftheobjectivealongagivendirection,thegradientatthenalpointis\northogonaltothatdirection.\ntheform:\nd t=   J  ()+ td t  1 (8.29)\nwhere  tisacoecientwhosemagnitudecontrolshowmuchofthedirection,d t  1,\nweshouldaddbacktothecurrentsearchdirection.\nTwodirections,d tandd t  1,aredenedasconjugateifd\ntHd t  1= 0,where\nHistheHessianmatrix.\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\neigenvectorsofHtochoose  t,whichwouldnotsatisfyourgoalofdeveloping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "amethodthatismorecomputationally viablethanNewtonsmethodforlarge\nproblems.Canwecalculatetheconjugatedirectionswithoutresortingtothese\ncalculations?Fortunatelytheanswertothatisyes.\nTwopopularmethodsforcomputingthe  tare:\n1.Fletcher-Reeves:\n t=  J( t)  J( t)\n  J( t  1)  J( t  1)(8.30)\n3 1 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n2.Polak-Ribire:\n t=(  J( t)  J( t  1))  J( t)\n  J( t  1)  J( t  1)(8.31)\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\nthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\nminimumalongthepreviousdirections.Asaconsequence,ina k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "Algorithm8.9Theconjugategradientmethod\nRequire:Initialparameters 0\nRequire:Trainingsetofexamples m\nInitialize 0= 0\nInitialize g 0= 0\nInitialize t= 1\nwhile do stoppingcriterionnotmet\nInitializethegradientg t= 0\nComputegradient:g t1\nm \ni L f((x( ) i;) ,y( ) i)\nCompute  t=( g t  g t 1 )g t\ng\nt 1g t 1(Polak-Ribire)\n(Nonlinearconjugategradient:optionallyreset  ttozero,forexampleif tis\namultipleofsomeconstant,suchas) k k= 5\nComputesearchdirection: t= g t+  t t  1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "Computesearchdirection: t= g t+  t t  1\nPerformlinesearchtond: = argmin 1\nmm\ni = 1 L f((x( ) i; t+  t) ,y( ) i)\n(Onatrulyquadraticcostfunction,analyticallysolvefor ratherthan\nexplicitlysearchingforit)\nApplyupdate: t + 1=  t+  t\nt t+1\nendwhile\nNonlinearConjugateGradients:Sofarwehavediscussedthemethodof\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.Ofcourse,\nourprimaryinterestinthischapteristoexploreoptimization methodsfortraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\nobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodication.\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n3 1 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\nsearchalongtheunalteredgradient.\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbenecialto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\n2011).Adaptationsofconjugategradientsspecicallyforneuralnetworkshave\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\n1993).\n8.6.3BFGS",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "1993).\n8.6.3BFGS\nTheBroydenFletcherGoldfarbShanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewtonsmethodwithoutthecomputational\nburden.Inthatrespect,BFGSissimilartotheconjugategradientmethod.\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewtons\nupdate.RecallthatNewtonsupdateisgivenby\n=  0H 1  J( 0) , (8.32)\nwhereHistheHessianof Jwithrespecttoevaluatedat 0.Theprimary\ncomputational dicultyinapplyingNewtonsupdateisthecalculationofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "inverseHessianH 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\namatrixM tthatisiterativelyrenedbylowrankupdatestobecomeabetter\napproximationofH 1.\nThespecicationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984().\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\n tisdeterminedby t=M tg t.Alinesearchisperformedinthisdirectionto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "determinethesizeofthestep, ,takeninthisdirection.Thenalupdatetothe\nparametersisgivenby:\n t + 1=  t+  t . (8.33)\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\n3 1 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\nonthelinesearchndingapointveryclosetothetrueminimumalongtheline.\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\nlesstimereningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\nparameters.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nLimitedMemoryBFGS(orL-BFGS)Thememory costsoftheBFGS\nalgorithmcanbesignicantlydecreasedbyavoidingstoringthecompleteinverse\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\nthatM( 1 ) t istheidentitymatrix,ratherthanstoringtheapproximation fromone\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdenedbyL-BFGS\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "procedureremainswellbehavedwhentheminimumofthelinesearchisreached\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n8.7OptimizationStrategiesandMeta-Algorithms\nManyoptimization techniquesarenotexactlyalgorithms,butrathergeneral\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\nincorporatedintomanydierentalgorithms.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "incorporatedintomanydierentalgorithms.\n8.7.1BatchNormalization\nBatchnormalization ( ,)isoneofthemostexcitingrecent IoeandSzegedy2015\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\nbythedicultyoftrainingverydeepmodels.\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "layersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\n3 1 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\nanddoesnotuseanactivationfunctionateachhiddenlayer: y= x w 1 w 2 w 3 . . . w l.\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i  1 w i.\nTheoutput  yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\nweights w i.Supposeourcostfunctionhasputagradientofon1  y,sowewishto\ndecrease yslightly.Theback-propagationalgorithmcanthencomputeagradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "g= w y.Considerwhathappenswhenwemakeanupdatewwg   .The\nrst-orderTaylorseriesapproximation of ypredictsthatthevalueof ywilldecrease\nby gg.Ifwewantedtodecrease yby .1,thisrst-orderinformationavailablein\nthegradientsuggestswecouldsetthelearningrate to. 1\ngg.However,theactual\nupdatewillincludesecond-orderandthird-ordereects,onuptoeectsoforder l.\nThenewvalueof yisgivenby\nx w( 1  g 1)( w 2  g 2)( . . . w l  g l) . (8.34)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "Anexampleofonesecond-ordertermarisingfromthisupdateis 2g 1 g 2l\ni = 3 w i.\nThistermmightbenegligibleifl\ni = 3 w iissmall,ormightbeexponentiallylarge\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\ntochooseanappropriatelearningrate,becausetheeectsofanupdatetothe\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\nevenhigher-orderinteractionscanbesignicant.Evensecond-orderoptimization\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\nthemfromtrulyaccountingforallsignicantsecond-orderinteractions. Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead?\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "network.Thereparametrization signicantlyreducestheproblemofcoordinating\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\nH=H\n, (8.35)\nwhereisavectorcontainingthemeanofeachunitandisavectorcontaining\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "thevectorandthevectortobeappliedtoeveryrowofthematrixH.Within\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting  j\n3 1 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nanddividingby  j.TherestofthenetworkthenoperatesonHinexactlythe\nsamewaythattheoriginalnetworkoperatedon.H\nAttrainingtime,\n=1\nm\niH i , : (8.36)\nand\n=\n+1\nm\ni( )H2\ni , (8.37)\nwhere isasmallpositivevaluesuchas10 8imposedtoavoidencountering\ntheundenedgradientofzat z=0.Crucially,weback-propagatethrough\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "anoperationthat actssimplytoincreasethestandarddeviationormeanof\nh i;thenormalization operationsremovetheeectofsuchanactionandzero\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\nnormalization approach.Previous approacheshadinvolvedaddingpenaltiesto\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "usuallyresultedinsignicantwastedtimeasthelearningalgorithmrepeatedly\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\nunitsalwaysbestandardizedbydenition,deftlysidesteppingbothproblems.\nAttesttime,andmaybereplacedbyrunningaveragesthatwerecollected\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\nwithoutneedingtousedenitionsofandthatdependonanentireminibatch.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "Revisitingthe y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\ndicultiesinlearningthismodelbynormalizing h l  1.Supposethat xisdrawn\nfromaunitGaussian.Then h l  1willalsocomefromaGaussian,becausethe\ntransformationfrom xto h lislinear.However, h l  1willnolongerhavezeromean\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\nh l  1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "updatetothelowerlayers,h l  1willremainaunitGaussian.Theoutput  ymay\nthenbelearnedasasimplelinearfunction  y= w l h l  1.Learninginthismodelis\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\neectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.In\nsomecornercases,thelowerlayerscanhaveaneect.Changingoneofthelower\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\n3 1 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofoneofthelowerweightscaniptherelationshipbetween h l  1and y.These\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\nanextremeeectonthestatisticsof h l  1.Batchnormalization hasthusmade\nthismodelsignicantlyeasiertolearn.Inthisexample,theeaseoflearningof\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\nthelowerlayersnolongerhaveanyharmfuleect,buttheyalsonolongerhave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "anybenecialeect.Thisisbecausewehavenormalizedouttherstandsecond\norderstatistics,whichisallthatalinearnetworkcaninuence.Inadeepneural\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\nunittochange.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "unittochange.\nBecausethenallayerofthenetworkisabletolearnalineartransformation,\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\ntheinspirationforbatchnormalization. Unfortunately,eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "practicalapproach.\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\npoweroftheneuralnetworkcontainingthatunit.Inordertomaintainthe\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\nactivationsHwithH+ratherthansimplythenormalizedH.Thevariables\nandarelearnedparametersthatallowthenewvariabletohaveanymean\nandstandarddeviation.Atrstglance,thismayseemuselesswhydidweset\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "anyarbitraryvalue?Theansweristhatthenewparametrization canrepresent\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\nparametrization hasdierentlearningdynamics.Intheoldparametrization, the\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\ninthelayersbelowH.Inthenewparametrization, themeanofH+is\ndeterminedsolelyby.Thenewparametrization ismucheasiertolearnwith\ngradientdescent.\nMostneuralnetworklayerstaketheformof (XW+b)where issome",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "xednonlinearactivationfunctionsuchastherectiedlineartransformation.It\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\nX,ortothetransformedvalueXW+b. ()recommend IoeandSzegedy2015\n3 2 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthelatter.Morespecically,XW+bshouldbereplacedbyanormalizedversion\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\nthe parameterappliedbythebatchnormalization reparametrization. Theinput\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\nrectiedlinearfunctioninapreviouslayer.Thestatisticsoftheinputarethus\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\nsamenormalizing and ateveryspatiallocationwithinafeaturemap,sothat\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\n8.7.2CoordinateDescent\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\nvariable x i,thenminimizeit withrespecttoanother variable x jandsoon,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\nonecoordinateatatime.Moregenerally,blockcoordinatedescentrefersto\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\ncoordinatedescentisoftenusedtorefertoblockcoordinatedescentaswellas\nthestrictlyindividualcoordinatedescent.\nCoordinatedescentmakesthemostsensewhenthedierentvariablesinthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "optimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\nsignicantlymoreecientthanoptimization withrespecttoallofthevariables.\nForexample,considerthecostfunction\nJ ,(HW) =\ni , j| H i , j|+\ni , j\nXWH2\ni , j.(8.38)\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\ntondaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "HtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\nThefunction Jisnotconvex.However,wecandividetheinputstothe\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n3 2 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nusanoptimization strategythatallowsustouseecientconvexoptimization\nalgorithms,byalternatingbetweenoptimizingWwithHxed,thenoptimizing\nHWwithxed.\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\nstronglyinuencestheoptimalvalueofanothervariable,asinthefunction f(x) =\n( x 1 x 2)2+ \nx2\n1+ x2\n2\nwhere isapositiveconstant.Thersttermencourages\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "tobenearzero.Thesolutionistosetbothtozero.Newtonsmethodcansolve\ntheprobleminasinglestepbecauseitisapositivedenitequadraticproblem.\nHowever,forsmall ,coordinatedescentwillmakeveryslowprogressbecausethe\nrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiers\nsignicantlyfromthecurrentvalueoftheothervariable.\n8.7.3PolyakAveraging\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\npointsinthetrajectorythrough parameterspacevisitedbyanoptimization",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "algorithm.If titerationsofgradientdescentvisitpoints( 1 ), . . . ,( ) t,thenthe\noutputofthePolyakaveragingalgorithmis( ) t=1\nt\ni( ) i.Onsomeproblem\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustication\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "thelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\nverycomplicatedandvisitmanydierentregions.Includingpointsinparameter\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\nexponentiallydecayingrunningaverage:\n( ) t= ( 1 ) t +(1 ) ( ) t. (8.39)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "( ) t= ( 1 ) t +(1 ) ( ) t. (8.39)\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\netal.()forarecentexample. 2015\n3 2 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.4SupervisedPretraining\nSometimes,directlytrainingamodeltosolveaspecictaskcanbetooambitious\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydicult.Itis\nsometimesmoreeectivetotrainasimplermodeltosolvethetask,thenmake\nthemodelmorecomplex.Itcanalsobemoreeectivetotrainthemodeltosolve\nasimplertask,thenmoveontoconfrontthenaltask.Thesestrategiesthat\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\npretraining.\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "ne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\nnds.\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\ndeeplearning.Inthissection,wedescribespecicallythosepretrainingalgorithms\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\nproblems.Thisapproachisknownas . greedysupervisedpretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\nthelayersinthenalneuralnetwork.Anexampleofgreedysupervisedpretraining\nisillustratedingure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "therstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\nWhywouldgreedysup ervisedpretraining help?Thehypothesis initially\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n3 2 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ny y\nh( 1 )h( 1 )\nx x\n( a )U( 1 )U( 1 )\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\nx x\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\ny yh( 1 )h( 1 )\nx x\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\ny yh( 1 )h( 1 )\nx x\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\nU( 2 )U( 2 )\nW( 2 )W( 2 )\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\n( a )Westartbytrainingasucientlyshallowarchitecture.Anotherdrawingofthe ( b )",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\ndiscardthehidden-to-outputlayer.Wesendtheoutputofthersthiddenlayerasinput\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\nastherstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\nTofurtherimprovetheoptimization,wecanjointlyne-tuneallthelayers,eitheronlyat\ntheendorateachstageofthisprocess.\n3 2 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization.\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\nandtheninitializeasame-sizenetworkwiththerst klayersoftherstnet.All",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "thelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\nthenjointlytrainedtoperformadierentsetoftasks(anothersubsetofthe1000\nImageNetobjectcategories),withfewertrainingexamplesthanfortherstsetof\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\nsection.15.2\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "becomesateacherforasecondnetwork,designatedthestudent.Thestudent\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\ndiculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "parametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\nthenalclassicationtarget,theobjectiveistopredictthemiddlehiddenlayer\noftheteachernetwork.Thelowerlayersofthestudentnetworksthushavetwo\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\nanddeepnetworkappearstobemorediculttotrainthanawideandshallow",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdicultto\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\nsolvetheproblem.\n3 2 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.5DesigningModelstoAidOptimization\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\ncomefromdesigningthemodelstobeeasiertooptimize.\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\ndicult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\nSpecically,modernneuralnetworksreectadesignchoicetouselineartrans-\nformationsbetweenlayersandactivationfunctionsthataredierentiable almost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "everywhereandhavesignicantslopeinlargeportionsoftheirdomain.Inpar-\nticular,modelinnovationsliketheLSTM,rectiedlinearunitsandmaxoutunits\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\noptimization easier.Thegradientowsthroughmanylayersprovidedthatthe\nJacobianofthelineartransformationhasreasonablesingularvalues.Moreover,\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodels",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\ncorrespondsreasonablywelltomovingtowardadistantsolution.\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\ntheshortestpathfromthelowerlayersparameterstotheoutput,and thus",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\nanddeeply-supervisednets(,).Theseauxiliaryheadsaretrained Leeetal.2014\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\ntheauxiliaryheadsmaybediscarded.Thisisanalternativetothepretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "strategies,whichwereintroducedintheprevioussection.Inthisway,onecan\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n3 2 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\n8.7.6ContinuationMethodsandCurriculumLearning\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\nproblemistoattempttoinitializetheparametersinaregionthatisconnected",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "tothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\ndiscover.\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\nminimizeacostfunction J(),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "Thesecostfunctionsaredesignedtobeincreasinglydicult,with J( 0 )beingfairly\neasytominimize,and J( ) n,themostdicult,being J(),thetruecostfunction\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\nmeanthatitiswellbehavedovermoreofspace.Arandominitialization ismore\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "solvinganeasyproblemthenrenethesolutiontosolveincrementally harder\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\nwhichaddsnoisetotheparameters(Kirkpatrick1983etal.,).Continuation",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\novercomingthechallengeoflocalminima.Specically,theyweredesignedto\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\nthesecontinuationmethodswouldconstructeasiercostfunctionsbyblurringthe\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "J( ) i() =  E N ( ;  , ()2 i) J() (8.40)\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\n3 2 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\nenoughinformationaboutthelocationofaglobalminimumthatwecanndthe\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\napproachcanbreakdowninthreedierentways.First,itmightsuccessfullydene\naseriesofcostfunctionswheretherstisconvexandtheoptimumtracksfrom\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\nnomatterhowmuchitisblurred.Considerforexamplethefunction J() =.\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "originalcostfunction.\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\neliminateatregions,decreasevarianceingradientestimates,improveconditioning\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\nandprogresstowardaglobalsolution.\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "training(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\nlearning(,;,;,). () Solomono1989Elman1993Sanger1994Bengioetal.2009\njustiedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\nincreasingtheinuenceofsimplerexamples(eitherbyassigningtheircontributions\ntothecostfunctionlargercoecients,orbysamplingthemmorefrequently),and\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\ntasks.Curriculumlearningwasalsoveriedasbeingconsistentwiththewayin\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\n3 2 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmoreprototypicalexamplesandthenhelpthelearnerrenethedecisionsurface\nwiththelessobviouscases.Curriculum-based strategiesaremoreeectivefor\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\nalsoincreasetheeectivenessofotherteachingstrategies( , BasuandChristensen\n2013).\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\nstochasticcurriculum,inwhicharandommixofeasyanddicultexamplesisalways\npresentedtothelearner,butwheretheaverageproportionofthemoredicult\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\nfromthefulltrainingset)wasobserved.\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\nlittleornomodication.\n3 2 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 9\nC on v ol u t i on al N e t w orks\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\ntremendouslysuccessfulinpracticalapplications.Thenameconvolutionalneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "networkindicatesthatthenetworkemploysamathematical operationcalled\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\nmultiplicationinatleastoneoftheirlayers.\nInthischapter,wewillrstdescribewhatconvolutionis.Next,wewill\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "employ.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\ncorrespondpreciselytothedenitionofconvolutionasusedinothereldssuch\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\nwillalsoshowhowconvolutionmaybeappliedtomanykindsofdata,with\ndierentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\nmoreecient.Convolutionalnetworksstandoutasanexampleofneuroscientic",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "principlesinuencingdeeplearning.Wewilldiscusstheseneuroscienticprinciples,\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\n330",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\nhere.\n9.1TheConvolutionOperation\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "valuedargument.Tomotivatethedenitionofconvolution,westartwithexamples\noftwofunctionswemightuse.\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\nt.Both xand tarereal-valued,i.e.,wecangetadierentreadingfromthelaser\nsensoratanyinstantintime.\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\nestimateofthespaceshipsposition,wewouldliketoaveragetogetherseveral",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "measurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\ns t() =\nx a w t a d a ()( ) (9.1)\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\ndenotedwithanasterisk:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "denotedwithanasterisk:\ns t x w t () = ( )() (9.2)\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdened\nforanyfunctionsforwhichtheaboveintegralisdened,andmaybeusedforother\npurposesbesidestakingweightedaverages.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "purposesbesidestakingweightedaverages.\nInconvolutionalnetworkterminology,therstargument(inthisexample,the\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\n3 3 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\nreferredtoasthe . f e at ur e m ap\nInourexample,theideaofalasersensorthatcanprovidemeasurements\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "integervalues.Ifwenowassumethat xand waredenedonlyoninteger t,we\ncandenethediscreteconvolution:\ns t x w t () = ( )() =\na =  x a w t a ()( ) (9.3)\nInmachinelearningapplications,theinputisusuallyamultidimensional array\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "separately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\nnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\ncanimplementtheinnitesummationasasummationoveranitenumberof\narrayelements.\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\ntouseatwo-dimensionalkernel: K\nS i , j I K i , j () = ( )() =\nm\nnI m , n K i m , j n . ( )(  )(9.4)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "m\nnI m , n K i m , j n . ( )(  )(9.4)\nConvolutioniscommutative,meaningwecanequivalentlywrite:\nS i , j K I i , j () = ( )() =\nm\nnI i m , j n K m , n . (  )( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand. n\nThecommutativepropertyofconvolutionarisesbecausewehave i pp e dthe\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "inputincreases,buttheindexintothekerneldecreases.Theonlyreasontoip\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n3 3 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\nbutwithoutippingthekernel:\nS i , j I K i , j () = ( )() =\nm\nnI i m , j n K m , n . (+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "andspecifywhetherwemeantoipthekernelornotincontextswherekernel\nippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\nbasedonconvolutionwithkernelippingwilllearnakernelthatisippedrelative\ntothekernellearnedbyanalgorithmwithouttheipping.Itisalsorarefor\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "notcommuteregardlessofwhethertheconvolutionoperationipsitskernelor\nnot.\nSeegureforanexampleofconvolution(withoutkernelipping)applied 9.1\ntoa2-Dtensor.\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "m at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\nmatrixmultiplication anddoesnotdependonspecicpropertiesofthematrix\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\nfurtherspecializationsinordertodealwithlargeinputseciently,buttheseare\nnotstrictlynecessaryfromatheoreticalperspective.\n3 3 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\na b c d\ne f g h\ni j k lw x\ny z\na w + b x +\ne y + f za w + b x +\ne y + f zb w + c x +\nf y + g zb w + c x +\nf y + g zc w + d x +\ng y + h zc w + d x +\ng y + h z\ne w + f x +\ni y + j ze w + f x +\ni y + j zf w + g x +\nj y + k zf w + g x +\nj y + k zg w + h x +\nk y + l zg w + h x +\nk y + l zI nput\nK e r ne l\nO ut put\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ipping.Inthiscasewerestrict\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,calledvalid",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\nupper-leftregionoftheinputtensor.\n3 3 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.2Motivation\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\nr e pr e se n t at i o ns.Moreover,convolutionprovidesameansforworkingwith\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "unitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "pixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\nmemoryrequirementsofthemodelandimprovesitsstatisticaleciency.Italso\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\nineciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\nmatrixmultiplication requires m n parametersandthealgorithmsusedinpractice\nhave O( m n )runtime(perexample).Ifwelimitthenumberofconnections\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "k n parametersand O( k n )runtime.Formanypracticalapplications,itis\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\nkseveralordersofmagnitudesmallerthan m.Forgraphicaldemonstrationsof\nsparseconnectivity,seegureandgure.Inadeepconvolutionalnetwork, 9.2 9.3\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\nasshowningure.Thisallowsthenetworktoecientlydescribecomplicated 9.4\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "buildingblocksthateachdescribeonlysparseinteractions.\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\noftheinput(exceptperhapssomeoftheboundarypixels,dependingonthe\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n3 3 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\nandalsohighlighttheoutputunitsin sthatareaectedbythisunit. ( T o p )When sis\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaectedby 3 x.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\nalloftheoutputsareaectedby x 3.\n3 3 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : Wehighlightoneoutputunit, s 3,\nandalsohighlighttheinputunitsin xthataectthisunit.Theseunitsareknown\nasthereceptiveeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "width,onlythreeinputsaect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsaect s 3.\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\nx 4 x 4h 4 h 4\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\nFigure9.4:Thereceptiveeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\nislargerthanthereceptiveeldoftheunitsintheshallowlayers.Thiseectincreasesif\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(gure)orpooling 9.12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\ninputimage.\n3 3 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\nparameterintwodierentmodels. ( T o p )Theblackarrowsindicateusesofthecentral\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "singleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\nhasnoparametersharingsotheparameterisusedonlyonce.\nforeverylocation,welearnonlyoneset.Thisdoesnotaecttheruntimeof\nforwardpropagationit isstill O( k n )butitdoesfurtherreducethestorage\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "practicallyinsignicantcomparedto m n .Convolutionisthusdramatically more\necientthandensematrixmultiplication intermsofthememoryrequirements\nandstatisticaleciency.Foragraphicaldepictionofhowparametersharingworks,\nseegure.9.5\nAsanexampleofbothofthesersttwoprinciplesinaction,gureshows9.6\nhowsparseconnectivityandparametersharingcandramatically improvethe\neciencyofalinearfunctionfordetectingedgesinanimage.\nInthecaseofconvolution,theparticularformofparametersharingcausesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "layertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\nSpecically,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\n3 3 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nmappingoneimagefunctiontoanotherimagefunction,suchthat I= g( I)is\ntheimagefunctionwith I( x , y)= I( x 1 , y).Thisshiftseverypixelof Ione\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\ntheresultwillbethesameasifweappliedconvolutionto I,thenappliedthe\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\nthatconvolutionproducesasortoftimelinethatshowswhendierentfeatures",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "appearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "therstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\nonanindividualsface,weprobablywanttoextractdierentfeaturesatdierent\nlocationsthepartofthenetworkprocessingthetopofthefaceneedstolookfor\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\nlookforachin.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "lookforachin.\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\nforhandlingthesekindsoftransformations.\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksdenedby\nmatrixmultiplication withaxed-shapematrix.Convolutionenablesprocessing\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n9.3Pooling\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seegure).9.7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Intherststage,thelayerperformsseveralconvolutionsinparalleltoproducea\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\nanonlinearactivationfunction,suchastherectiedlinearactivationfunction.\nThisstageissometimescalledthe det e c t o rstage.Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\n3 3 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.6: E  c i e n c y o f e d g e d e t e c t i o n.Theimageontherightwasformedbytaking\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\nleft.Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "requires319 280 3=267 ,960oatingpointoperations(twomultiplicationsand\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\ntransformationwithamatrixmultiplicationwouldtake320 280 319 280,orover\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreecientfor\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\nperformsoversixteenbillionoatingpointoperations,makingconvolutionroughly60,000",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "timesmoreecientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\nandconvolutionwouldrequirethesamenumberofoatingpointoperationstocompute.\nThematrixwouldstillneedtocontain2 319 280=178 ,640entries.Convolution\nisanextremelyecientwayofdescribingtransformationsthatapplythesamelinear\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\nGoodfellow)\n3 4 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nConvolutionalLayer\nInputtolayerConvolutionstage:\nAnetransform Detectorstage:\nNonlinearity\ne.g.,rectiedlinearPoolingstageNextlayer\nInputtolayersConvolutionlayer:\nAnetransform Detectorlayer:Nonlinearity\ne.g.,rectiedlinearPoolinglayerNextlayerComplexlayerterminology Simplelayerterminology\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\neachlayerhavingmanystages.Inthisterminology,thereisaone-to-onemapping\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\nnoteverylayerhasparameters.\n3 4 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\nbasedonthedistancefromthecentralpixel.\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "outputsdonotchange.Seegureforanexampleofhowthisworks. 9.8 Invariance\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\nimportanttopreservethelocationofafeature.Forexample,ifwewanttonda",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "cornerdenedbytwoedgesmeetingataspecicorientation,weneedtopreserve\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\nTheuseofpoolingcanbeviewedasaddinganinnitelystrongpriorthat\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\nassumptioniscorrect,itcangreatlyimprovethestatisticaleciencyofthenetwork.\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "whichtransformationstobecomeinvariantto(seegure).9.9\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\ngureforanexample.Thisimprovesthecomputational eciencyofthe 9.10\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\nreductionintheinputsizecanalsoresultinimprovedstatisticaleciencyand\nreducedmemoryrequirementsforstoringtheparameters.\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize.For\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassication\nlayermusthaveaxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\nosetbetweenpoolingregionssothattheclassicationlayeralwaysreceivesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\nnalpoolinglayerofthenetworkmaybedenedtooutputfoursetsofsummary\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\n3 4 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n0. 1 1. 0. 21. 1. 1.\n0. 10. 2\n. . . . . .. . . . . .\n0. 3 0. 1 1.1. 0. 3 1.\n0. 21.\n. . . . . .. . . . . .D E T E C T O R  S T A GEP O O L I N G ST A GE\nP O O L I N G ST A GE\nD E T E C T O R  S T A GE\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "andapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\n3 4 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nL ar ge  r e s pon s e\ni n po ol i nguni tL ar ge  r e s pon s e\ni n po ol i nguni t\nL ar ge\nr e s ponse\ni n de t e c t or\nuni t  1L ar ge\nr e s ponse\ni n de t e c t or\nuni t  3\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\ntheinput.Hereweshowhowasetofthreelearnedltersandamaxpoolingunitcanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "tobecomeinvarianttorotation.Allthreeltersareintendedtodetectahand-written5.\nEachlterattemptstomatchaslightlydierentorientationofthe5.Whena5appearsin\ntheinput,thecorrespondinglterwillmatchitandcausealargeactivationinadetector\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\nwasactivated.Weshowherehowthenetworkprocessestwodierentinputs,resulting\nintwodierentdetectorunitsbeingactivated.Theeectonthepoolingunitisroughly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\ntransformations.\n0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0. 1\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\nwanttoignoresomeofthedetectorunits.\n3 4 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\ndierentsetofpoolingregionsforeachimage.Anotherapproachistolearna\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection.The20.6\ninverse-likeoperationsonpoolingunitsneededinsomedierentiablenetworkswill\nbecoveredinsection.20.10.6\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassication\nusingconvolutionandpoolingareshowningure.9.11",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "usingconvolutionandpoolingareshowningure.9.11\n9.4ConvolutionandPoolingasanInnitelyStrong\nPrior\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\nmoreactiveroleindeterminingwheretheparametersendup.\nAninnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues.\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "butwithaninnitelystrongprioroveritsweights.Thisinnitelystrongprior\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\nexceptforinthesmall,spatiallycontiguousreceptiveeldassignedtothathidden\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinnitely\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n3 4 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInputimage:\n256x256x3Outputof\nconvolution+\nReLU:256x256x64Outputofpooling\nwithstride4:\n64x64x64Outputof\nconvolution+\nReLU:64x64x64Outputofpooling\nwithstride4:\n16x16x64Outputofreshapeto\nvector:\n16,384unitsOutputofmatrix\nmultiply:1,000unitsOutputofsoftmax:\n1,000class\nprobabilities\nInputimage:\n256x256x3Outputof\nconvolution+\nReLU:256x256x64Outputofpooling\nwithstride4:\n64x64x64Outputof\nconvolution+",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "withstride4:\n64x64x64Outputof\nconvolution+\nReLU:64x64x64Outputofpoolingto\n3x3grid:3x3x64Outputofreshapeto\nvector:\n576unitsOutputofmatrix\nmultiply:1,000unitsOutputofsoftmax:\n1,000class\nprobabilities\nInputimage:\n256x256x3Outputof\nconvolution+\nReLU:256x256x64Outputofpooling\nwithstride4:\n64x64x64Outputof\nconvolution+\nReLU:64x64x64Outputof\nconvolution:\n16x16x1,000Outputofaverage\npooling:1x1x1,000Outputofsoftmax:\n1,000class\nprobabilities",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "1,000class\nprobabilities\nOutputofpooling\nwithstride4:\n16x16x64\nFigure9.11:Examplesofarchitecturesforclassicationwithconvolutionalnetworks.The\nspecicstridesanddepthsusedinthisgurearenotadvisableforrealuse;theyare\ndesignedtobeveryshallowinordertotontothepage.Realconvolutionalnetworks\nalsoofteninvolvesignicantamountsofbranching,unlikethechainstructuresused\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaxedimagesize.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\nconvolutionalfeaturemapisreshapedtoattenoutthespatialdimensions.Therest\nofthenetworkisanordinaryfeedforwardnetworkclassier,asdescribedinchapter.6\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\nbutaxednumberofpools,inordertoprovideaxed-sizevectorof576unitstothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "fullyconnectedportionofthenetwork.Aconvolutionalnetworkthatdoesnot ( R i g h t )\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\ntheargumenttothesoftmaxclassieratthetop.\n3 4 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\nequivarianttotranslation.Likewise,theuseofpoolingisaninnitelystrongprior\nthateachunitshouldbeinvarianttosmalltranslations.\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\ninnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\nofaconvolutionalnetasafullyconnectednetwithaninnitelystrongpriorcan\ngiveussomeinsightsintohowconvolutionalnetswork.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "giveussomeinsightsintohowconvolutionalnetswork.\nOnekeyinsightisthatconvolutionandpoolingcancauseundertting. Like\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "highlyinvariantfeaturesandfeaturesthatwillnotundertwhenthetranslation\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\ninappropriate.\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "areseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\nofspatialrelationshipshard-codedintothembytheirdesigner.\n9.5VariantsoftheBasicConvolutionFunction\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\nunderstoodinthemathematical literature.Thefunctionsusedinpracticedier",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "slightly.Herewedescribethesedierencesindetail,andhighlightsomeuseful\npropertiesofthefunctionsusedinneuralnetworks.\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\nnetworktoextractmanykindsoffeatures,atmanylocations.\n3 4 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations.Forexample,acolorimagehasared,green\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\ntothesecondlayeristheoutputoftherstlayer,whichusuallyhastheoutput\nofmanydierentconvolutionsateachposition.Whenworkingwithimages,we\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "oneindexintothedierentchannelsandtwoindicesintothespatialcoordinates\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdierentexamplesin\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\nkernel-ippingisused.Thesemulti-channeloperationsareonlycommutativeif",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "eachoperationhasthesamenumberofoutputchannelsasinputchannels.\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\ninput,withanosetof krowsand lcolumnsbetweentheoutputunitandthe\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "acrosswithoutipping,then V K\nZ i , j , k=\nl , m , nV l , j m , k n +  1 +  1 K i , l , m , n (9.7)\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\narraysusingafortherstentry.Thisnecessitatesthe 1 1intheaboveformula.\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler.\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "computational cost(attheexpenseofnotextractingourfeaturesasnely).We\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\ndeneadownsampledconvolutionfunctionsuchthat c\nZ i , j , k= ( ) c K V , , s i , j , k=\nl , m , n\nVl , j s m , k s n (   1 ) + (   1 ) + K i , l , m , n\n.(9.8)\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\n3 4 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ntodeneaseparatestrideforeachdirectionofmotion.Seegureforan9.12\nillustration.\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\nateachlayer.Zeropaddingtheinputallowsustocontrolthekernelwidthand\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\nkernelsbothscenariosthatsignicantlylimittheexpressivepowerofthenetwork.\nSeegureforanexample. 9.13\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\nthekernelhaswidth k,theoutputwillbeofwidth m k +1.Therateofthis\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "eventuallydropto1 1,atwhichpointadditionallayerscannotmeaningfully\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "inuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\ninanoutputimageofwidth m+ k 1.Inthiscase,theoutputpixelsnearthe\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\ncanmakeitdiculttolearnasinglekernelthatperformswellatallpositionsin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\ntermsoftestsetclassicationaccuracy)liessomewherebetweenvalidandsame\nconvolution.\n3 4 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\nx 4 x 4 x 5 x 5s 3 s 3\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\nx 4 x 4z 4 z 4\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\nc onv ol ut i on\nD ow nsampl i n g\nC onv ol ut i on\nFigure9.12:Convolutionwithastride.Inthisexample,weuseastrideoftwo.\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation. ( Bot-\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\nthatarethendiscarded.\n3 5 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n. . . . . .. . .\n. . . . . .. . . . . .. . . . . .\nFigure9.13: T h e e  e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\nshrinkbyvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\nshrinkingisinevitableinthiskindofarchitecture. Byaddingveimplicitzeroes ( Bottom )\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\nmakeanarbitrarilydeepconvolutionalnetwork.\n3 5 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,specied\nbya6-Dtensor W.Theindicesinto Warerespectively: i,theoutputchannel,\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoset\nwithintheinput,and n,thecolumnosetwithintheinput.Thelinearpartofa\nlocallyconnectedlayeristhengivenby\nZ i , j , k=",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "locallyconnectedlayeristhengivenby\nZ i , j , k=\nl , m , n[ V l , j m , k n +  1 +  1 w i , j , k, l , m , n] . (9.9)\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\nconnections.\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\nimage.\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\nwaytodothisistomaketherst moutputchannelsconnecttoonlytherst\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "inputchannels,andsoon.Seegureforanexample.Modelinginteractions 9.15\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\nreducememoryconsumptionandincreasestatisticaleciency,andalsoreduces\ntheamountofcomputationneededtoperformforwardandback-propagation. It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\nT i l e d c o n v o l ut i o n( ,;,)oersacom- GregorandLeCun2010aLeetal.2010\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\nneighboringlocationswillhavedierentlters,likeinalocallyconnectedlayer,\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\nfeaturemap.Seegureforacomparisonoflocallyconnectedlayers,tiled 9.16\nconvolution,andstandardconvolution.\n3 5 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na        b a        b a        b a        b a       a        b c       d e      f g      h  i  \nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\nFigure9.14:Comparisonoflocalconnections,convolution,andfullconnections.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\nauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\n( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\nthesameconnectivityasthelocallyconnectedlayer.Thedierenceliesnotinwhichunits\ninteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\nhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\n( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\nedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\ndiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected\nlayer.\n3 5 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nI nputT e nsorO ut putT e nsor\nS p a t i a l  c o o r d i n a t e sC h a n n e l  c o o r d i n a t e s\nFigure9.15:Aconvolutionalnetworkwiththersttwooutputchannelsconnectedto\nonlythersttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\nthesecondtwoinputchannels.\n3 5 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na        b a        b a        b a        b a      a        b c       d e      f g      h  i  \nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "x 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\na        b c         d a        b c         d a       \nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\nconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\nsizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\nThedierencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight\nbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )\ntdierentkernels.Hereweillustratethecaseof t= 2.Oneofthesekernelshasedges\nlabeledaandb,whiletheotherhasedgeslabeledcandd.Eachtimewemoveone\npixeltotherightintheoutput,wemoveontousingadierentkernel.Thismeansthat,\nlikethelocallyconnectedlayer,neighboringunitsintheoutputhavedierentparameters.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,\nwecyclebacktotherstkernel.Iftwooutputunitsareseparatedbyamultipleof t\nsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )\nconvolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated\ninthediagrambyusingthekernelwithweightslabeledaandbeverywhere.\n3 5 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTodenetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\nthedimensionscorrespondtodierentlocationsintheoutputmap.Ratherthan\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\nthroughasetof tdierentchoicesofkernelstackineachdirection.If tisequalto\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\nZ i , j , k=\nl , m , nV l , j m , k n +  1 +  1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "whereisthemodulooperation,with % t% t=0(, t+1)% t=1,etc.Itis\nstraightforwardtogeneralizethisequationtouseadierenttilingrangeforeach\ndimension.\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\ndierentlters.Iftheselterslearntodetectdierenttransformedversionsof\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\nlearnedtransformation(seegure).Convolutionallayersarehard-codedtobe 9.9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "invariantspecicallytotranslation.\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\nInsomesimplecases,thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty.\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixmultiplication (ifwerstreshapetheinputtensorintoaatvector).The\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\nnetwork.\nMultiplication bythetransposeofthematrixdenedbyconvolutionisone\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\n3 5 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagations\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\ntoldwhatthesizeoftheoriginalinputwas.\nThesethreeoperationsconvolution,backpropfromoutputtoweights,and\nbackpropfromoutputtoinputsaresucienttocomputeallofthegradients\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\nconvolution.See ()forafullderivationoftheequationsinthe Goodfellow2010",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "fullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\ndenedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "computethecostfunction J.Duringback-propagation, wewillreceiveatensor G\nsuchthat G i , j , k=\n Z i , j , kJ , . ( V K)\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\nweightsinthekernel.Todoso,wecanuseafunction\ng , , s ( G V) i , j , k, l=\n K i , j , k, lJ ,( V K) =\nm , nG i , m , n V j , m s k, n s l (   1 ) + (   1 ) + .(9.11)\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "Todoso,wecanuseafunction\nh , , s ( K G) i , j , k=\n V i , j , kJ ,( V K) (9.12)\n=\nl , m\ns . t .\n( 1 ) + = l   s m j\nn , p\ns . t .\n( 1 ) + = n   s p k\nqK q , i , m , p G q , l , n .(9.13)\nAutoencodernetworks,describedinchapter,arefeedforwardnetworks 14\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n3 5 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\nWW x.Itiscommonformoregeneral autoencoderstousemultiplication\nbythetransposeoftheweightmatrixjustasPCAdoes.Tomakesuchmodels\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedene\nareconstruction\nR K H = ( h , , s .) (9.14)\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "to Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\ndierentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\nback-propagationalgorithmonanystandardnetworkarchitectures.\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "biastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\nofhowtoshareparametersamongthebiases.Forlocallyconnectedlayersitis\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,xed\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Separatingthebiasesmayslightlyreducethestatisticaleciencyofthemodel,but\nalsoallowsthemodeltocorrectfordierencesintheimagestatisticsatdierent\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\n9.6StructuredOutputs\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\nobject,ratherthanjustpredictingaclasslabelforaclassicationtaskorareal\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "standardconvolutionallayer.Forexample,themodelmightemitatensor S,where\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\nthatfollowtheoutlinesofindividualobjects.\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\n3 5 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n Y( 1 ) Y( 1 ) Y( 2 ) Y( 2 ) Y( 3 ) Y( 3 )\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\nXXU U UV V V W W\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\nchannels(red,green,blue).Thegoalistooutputatensoroflabels Y,withaprobability\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "imagecolumns,andthedierentclasses.Ratherthanoutputting Yinasingleshot,the\nrecurrentnetworkiterativelyrenesitsestimate Ybyusingapreviousestimateof Y\nasinputforcreatinganewestimate.Thesameparametersareusedforeachupdated\nestimate,andtheestimatecanberenedasmanytimesaswewish.Thetensorof\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "hiddenvalues.Onallbuttherststep,thekernels Wareconvolvedover Ytoprovide\ninputtothehiddenlayer.Onthersttimestep,thistermisreplacedbyzero.Because\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\ndescribedinchapter.10\ninputplane,asshowningure.Inthekindsofarchitectures typicallyusedfor 9.13\nclassicationofasingleobjectinanimage,thegreatestreductioninthespatial\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\nuseapoolingoperatorwithunitstride.\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\noftheimagelabels,thenrenethisinitialguessusingtheinteractionsbetween\nneighboringpixels.Repeatingthisrenementstepseveraltimescorrespondsto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\nthearchitectureofsucharecurrentconvolutionalnetwork.\n3 5 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "canbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\n9.7DataTypes\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\neachchannelbeingtheobservationofadierentquantityatsomepointinspace\nortime.Seetableforexamplesofdatatypeswithdierentdimensionalities 9.1\nandnumberofchannels.\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\n().2010\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\nevenwhencomputational costandoverttingarenotsignicantissues.\nForexample,consideracollectionofimages,whereeachimagehasadierent\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "xedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\ndierentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\nmultiplication; thesameconvolutionkernelinducesadierentsizeofdoublyblock\ncirculantmatrixforeachsizeofinput.Sometimes theoutputofthenetworkis\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "necessary.Inothercases,thenetworkmustproducesomexed-sizeoutput,for\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\nmaintainaxednumberofpooledoutputs.Someexamplesofthiskindofstrategy\nareshowningure.9.11\n3 6 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSinglechannel Multi-channel\n1-DAudiowaveform:Theaxiswe\nconvolveovercorrespondsto\ntime.Wediscretizetimeand\nmeasuretheamplitudeofthe\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\ntionsof3-Dcomputer-rendered\ncharactersaregeneratedbyalter-\ningtheposeofaskeletonover\ntime.Ateachpointintime,the\nposeofthecharacterisdescribed\nbyaspecicationoftheanglesof\neachofthejointsinthecharac-\ntersskeleton.Eachchannelin\nthedatawefeedtotheconvolu-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "thedatawefeedtotheconvolu-\ntionalmodelrepresentstheangle\naboutoneaxisofonejoint.\n2-DAudiodatathathasbeenprepro-\ncessedwithaFouriertransform:\nWecantransformtheaudiowave-\nformintoa2Dtensorwithdif-\nferentrowscorrespondingtodif-\nferentfrequenciesanddierent\ncolumnscorrespondingtodier-\nentpointsintime.Usingconvolu-\ntioninthetimemakesthemodel\nequivarianttoshiftsintime.Us-\ningconvolutionacrossthefre-\nquencyaxismakesthemodel\nequivarianttofrequency,sothat\nthesamemelodyplayedinadif-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "thesamemelodyplayedinadif-\nferentoctaveproducesthesame\nrepresentationbutatadierent\nheightinthenetworksoutput.Colorimagedata:Onechannel\ncontainstheredpixels,onethe\ngreenpixels,andonetheblue\npixels.Theconvolutionkernel\nmovesoverboththehorizontal\nandverticalaxesoftheimage,\nconferringtranslationequivari-\nanceinbothdirections.\n3-DVolumetricdata:Acommon\nsourceofthiskindofdataismed-\nicalimagingtechnology,suchas\nCTscans.Colorvideodata:Oneaxiscorre-\nspondstotime,onetotheheight",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "spondstotime,onetotheheight\nofthevideoframe,andoneto\nthewidthofthevideoframe.\nTable9.1:Examplesofdierentformatsofdatathatcanbeusedwithconvolutional\nnetworks.\n3 6 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\nofobservationofthesamekindofthingdieren tlengthsofrecordingsover\ntime,dierentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludedierent\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\ncorrespondingtothetestscores.\n9.8EcientConvolutionAlgorithms\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\nresources,asdiscussedinsection,areessential.However,inmanycasesit 12.1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\nalgorithm.\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\nofthetwosignals,andconvertingbacktothetimedomainusinganinverse\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\nimplementationofdiscreteconvolution.\nWhena d-dimensionalkernelcanbeexpressedastheouterproductof d",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "vectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\nkernelisseparable,naiveconvolutionisinecient.Itisequivalenttocompose d\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\nissignicantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "convolutionrequires O( w d )runtimeandparameterstoragespace.Ofcourse,\nnoteveryconvolutioncanberepresentedinthisway.\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\nniquesthatimprovetheeciencyofonlyforwardpropagationareusefulbecause\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\nanetworkthantoitstraining.\n3 6 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.9RandomorUnsupervisedFeatures\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "trainingistousefeaturesthatarenottrainedinasupervisedfashion.\nTherearethreebasicstrategiesforobtainingcon volutionkernelswithout\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.PartIII",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "describesmanymoreunsupervisedlearningapproaches.Learningthefeatures\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\nclassierlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\nRandomltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "etal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\naconvolutionalnetwork:rstevaluatetheperformanceofseveralconvolutional\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "Anintermediate approachistolearnthefeatures,butusingmethodsthatdo\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintherstlayer\ninisolation,thenextractallfeaturesfromtherstlayeronlyonce,thentrainthe\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoer Leeetal.2009\n3 6 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\nWecanthenusetheparametersfromthispatch-basedmodeltodenethekernels\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "process.Usingthisapproach,wecantrainverylargemodelsandincurahigh\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\nfromroughly20072013,whenlabeleddatasetsweresmallandcomputational\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\nentirenetworkoneachtrainingiteration.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "entirenetworkoneachtrainingiteration.\nAswithotherapproachestounsupervisedpretraining,itremainsdicultto\nteaseapartthecauseofsomeofthebenetsseenwiththisapproach.Unsupervised\npretrainingmayoersomeregularizationrelativetosupervisedtraining,oritmay\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\ncostofthelearningrule.\n9.10TheNeuroscienticBasisforConvolutionalNet-\nworks\nConvolutionalnetworksareperhapsthe greatestsuccessstoryofbiologically",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "inspiredarticialintelligence.Thoughconvolutionalnetworkshavebeenguided\nbymanyotherelds,someofthekeydesignprinciplesofneuralnetworkswere\ndrawnfromneuroscience.\nThehistoryofconvolutionalnetworksbeginswithneuroscienticexperiments\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\naNobelprize.Theirndingsthathavehadthegreatestinuenceoncontemporary\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\ncats.Theyobservedhowneuronsinthecatsbrainrespondedtoimagesprojected\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspecic\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "otherpatterns.\n3 6 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\nfocusonasimplied,cartoonviewofbrainfunction.\nInthissimpliedview,wefocusonapartofthebraincalledV1,alsoknown\nasthe pr i m ar y v i sual c o r t e x.V1istherstareaofthebrainthatbeginsto\nperformsignicantlyadvancedprocessingofvisualinput.Inthiscartoonview,\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\ncalledthelateralgeniculatenucleus.Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead.\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\nmirroringthe structureoftheimageinthe retina.Forexample,light\narrivingatthelowerhalfoftheretinaaectsonlythecorrespondinghalfof\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\ndenedintermsoftwodimensionalmaps.\n2.V1containsmany si m pl e c e l l s.Asimplecellsactivitycantosomeextent\nbecharacterizedbyalinearfunction oftheimageinasmall,spatially",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "localizedreceptiveeld.Thedetectorunitsofaconvolutionalnetworkare\ndesignedtoemulatethesepropertiesofsimplecells.\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\nlayersofthebrain,weeventuallyndcellsthatrespondtosomespecicconcept",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n3 6 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nnicknamedgrandmother cellstheideaisthatapersoncouldhaveaneuronthat\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\nshadow,etc.\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\nTheyfoundwhathascometobecalledtheHalleBerryneuron:anindividual\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronreswhena\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\nthewordsHalleBerry.Ofcourse,thishasnothingtodowithHalleBerryherself;\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\nnetworkslastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\n(IT).Whenviewinganobject,informationowsfromtheretina,throughthe\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintherst\n100msofglimpsinganobject.Ifapersonisallowedtocontinuelookingatthe\nobjectformoretime,theninformationwillbegintoowbackwardsasthebrain",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "usestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\nHowever,ifweinterruptthepersonsgaze,andobserveonlytheringratesthat\nresultfromtherst100msofmostlyfeedforwardactivation,thenITprovestobe\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\nringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\nrecognitiontasks(,). DiCarlo2013\nThatbeingsaid,therearemanydierencesbetweenconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "andthemammalianvisionsystem.Someofthesedierencesarewellknown\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\nthesedierencesarenotyetknown,becausemanybasicquestionsabouthowthe\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\nThehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\n3 6 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\nsofararepurelyvisual.\nThehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\nabletounderstandentirescenesincludingmanyobjectsandrelationships\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntooeracompellingimprovement.\nWhilefeedforwardITringratescapturemuchofthesameinformationas\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\ncomputations are.Thebrainprobablyusesverydierentactivationand\npoolingfunctions.Anindividualneuronsactivationprobablyisnotwell-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "characterizedbyasinglelinearlterresponse.ArecentmodelofV1involves\nmultiplequadraticltersforeachneuron(,).Indeedour Rustetal.2005\ncartoonpictureofsimplecellsandcomplexcellsmightcreateanon-\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\nkindofcellbutwiththeirparametersenablingacontinuumofbehaviors\nrangingfromwhatwecallsimpletowhatwecallcomplex.\nItisalsoworthmentioningthatneurosciencehastoldusrelativelylittle",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\nalgorithm.\n3 6 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nLangandHinton1988()introducedtheuseofback-propagationtotrain\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscienticobserva-\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\nconvolutionappliedtoimages.\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspecic\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "Inadeep,nonlinearnetwork,itcanbediculttounderstandthefunctionof\nindividualcells.Simplecellsintherstlayerareeasiertoanalyze,becausetheir\nresponsesaredrivenbyalinearfunction.Inanarticialneuralnetwork,wecan\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimals",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "retina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\nthentalinearmodeltotheseresponsesinordertoobtainanapproximation of\ntheneuronsweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\nandShapley2004,).\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns.TheGaborfunctiondescribestheweightata2-Dpoint\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "I( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\nlocations,denedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\nofview,theresponseofasimplecelltoanimageisgivenby\ns I() =\nx  X\ny  Yw x , y I x , y . ()() (9.15)\nSpecically,takestheformofaGaborfunction: w x , y()\nw x , y  ,  (; x ,  y , f ,  , x 0 , y 0 ,  ) = exp  x x 2  y y 2\ncos( f x+)  ,(9.16)\nwhere",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "cos( f x+)  ,(9.16)\nwhere\nx= ( x x  0)cos()+(  y y  0)sin()  (9.17)\n3 6 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nand\ny= (  x x  0)sin()+(  y y  0)cos()  . (9.18)\nHere, ,  x,  y, f, , x 0, y 0,and areparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndierentsettingsoftheseparameters.\nTheparameters x 0, y 0,and deneacoordinatesystem.Wetranslateand\nrotate xand ytoform xand y.Specically,thesimplecellwillrespondtoimage\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "aswemovealongalinerotatedradiansfromthehorizontal. \nViewedasafunctionof xand y,thefunction wthenrespondstochangesin\nbrightnessaswemovealongthe xaxis.Ithastwoimportantfactors:oneisa\nGaussianfunctionandtheotherisacosinefunction.\nTheGaussianfactor exp\n  x x 2  y y 2\ncanbeseenasagatingtermthat\nensuresthesimplecellwillonlyrespondtovaluesnearwhere xand yareboth\nzero,inotherwords,nearthecenterofthecellsreceptiveeld.Thescalingfactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "adjuststhetotalmagnitudeofthesimplecellsresponse,while  xand  ycontrol\nhowquicklyitsreceptiveeldfallso.\nThecosinefactor cos( f x+ ) controlshowthesimplecellrespondstochanging\nbrightnessalongthe xaxis.Theparameter fcontrolsthefrequencyofthecosine\nandcontrolsitsphaseoset. \nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\ntoaspecicspatialfrequencyofbrightnessinaspecicdirectionataspecic\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweightswhen\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\nnegative.\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecellsresponses: c( I)=\ns 0() I2+ s 1() I2.An\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "for ,and issetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldenedinthisway\nrespondswhentheGaussianreweightedimage I( x , y)exp(   x x 2  y y 2) contains\nahighamplitudesinusoidalwavewithfrequency findirection near ( x 0 , y 0),\nregardlessofthephaseosetofthiswave.Inotherwords,thecomplexcellis\ninvarianttosmalltranslationsoftheimageindirection ,ortonegatingtheimage\n3 6 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdierentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and .EachGaborfunctioninthisgridis\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and ischosenso\nthateachGaborlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots, x 0, y 0,and arexedtozero. Gaborfunctionswith ( C e n t e r )\ndierentGaussianscaleparameters  xand  y.Gaborfunctionsarearrangedinincreasing\nwidth(decreasing  x)aswemovelefttorightthroughthegrid,andincreasingheight\n(decreasing  y)aswemovetoptobottom.Fortheothertwoplots,the valuesarexed\nto1.5 theimagewidth.Gaborfunctionswithdierentsinusoidparameters ( R i g h t ) f\nand .Aswemovetoptobottom, fincreases,andaswemovelefttoright, increases.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots,isxedto0andisxedto5theimagewidth.  f \n(replacingblackwithwhiteandviceversa).\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\nasimpleunsupervisedlearningalgorithm,sparse coding,learnsfeatureswith\nreceptiveeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\nlearningalgorithms,whichlearnthesefeaturesintheirrstlayer.Figure9.19\nshowssomeexamples.Becausesomanydierentlearningalgorithmslearnedge\ndetectors,itisdiculttoconcludethatanyspeciclearningalgorithmisthe\nrightmodelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "whenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydierent\napproachestostatisticalmodeling.SeeHyvrinen 2009etal.()forareviewofthe\neldofnaturalimagestatistics.\n3 7 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspecic\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytherstlayerofafullysupervised",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "convolutionalmaxoutnetwork.Neighboringpairsofltersdrivethesamemaxoutunit.\n9.11ConvolutionalNetworksandtheHistoryofDeep\nLearning\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\ntherstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\nconsideredviable.Convolutionalnetworkswerealsosomeoftherstneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "networkstosolveimportantcommercialapplicationsandremainattheforefront\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "andmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\n3 7 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\nlessimpactforyearsearlier.\nConvolutionalnetsweresomeoftherstworkingdeepnetworkstrainedwith\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\nsimplybethatconvolutionalnetworksweremorecomputationally ecientthan\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "andtunetheirimplementation andhyperparameters.Largernetworksalsoseem\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseorttouse",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\nnetworks.\n3 7 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1\nI n t ro d u ct i on\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\nbacktoatleastthetimeofancientGreece.ThemythicalguresPygmalion,\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\nGalatea,Talos,andPandoramayallberegardedasarticiallife( , OvidandMartin\n2004Sparkes1996Tandy1997 ;,;,).\nWhenprogrammable computerswererstconceived,peoplewonderedwhether\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "built(Lovelace1842,).Today, ar t i c i al i n t e l l i g e nc e(AI)isathrivingeldwith\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\ninmedicineandsupportbasicscienticresearch.\nIntheearlydaysofarticialintelligence,theeldrapidlytackledandsolved\nproblemsthatareintellectually dicultforhumanbeingsbutrelativelystraight-\nforwardforcomputersproblemsthatcanbedescribedbyalistofformal,math-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "ematicalrules.Thetruechallengetoarticialintelligenceprovedtobesolving\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\nformallyprobl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\nspokenwordsorfacesinimages.\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\nhierarchyofconcepts,witheachconceptdenedintermsofitsrelationtosimpler",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\n1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\ntheworld.Forexample,IBMsDeepBluechess-playingsystemdefeatedworld\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "inonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyisa\ntremendousaccomplishment,butthechallengeisnotduetothedicultyof\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\nprovidedaheadoftimebytheprogrammer.\nIronically,abstractandformaltasksthatareamongthemostdicultmental\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "havelongbeenabletodefeateventhebesthumanchessplayer,butareonly\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\norspeech.Apersonseverydayliferequiresanimmenseamountofknowledge\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\ndiculttoarticulateinaformalway.Computersneedtocapturethissame\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\narticialintelligenceishowtogetthisinformalknowledgeintoacomputer.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "Severalarticialintelligenceprojectshavesoughttohard-codeknowledgeabout\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\nk no wl e dge baseapproachtoarticialintelligence.Noneoftheseprojectshasled\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\ncalledCycL.Thesestatementsareenteredbyastaofhumansupervisors.Itisan",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\nenginedetectedaninconsistencyinthestory:itknewthatpeopledonothave\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\nentityFredWhileShavingcontainedelectricalparts.Itthereforeaskedwhether\nFredwasstillapersonwhilehewasshaving.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "Fredwasstillapersonwhilehewasshaving.\nThedicultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\n2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "onthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\nvariousoutcomes.However,itcannotinuencethewaythatthefeaturesare",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "denedinanyway.IflogisticregressionwasgivenanMRIscanofthepatient,\nratherthanthedoctorsformalizedreport,itwouldnotbeabletomakeuseful\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\ncomplications thatmightoccurduringdelivery.\nThisdependenceonrepresentationsisageneralphenomenon thatappears\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "thecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\narithmeticonArabicnumerals,butndarithmeticonRomannumeralsmuch\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\nenormouseectontheperformanceofmachinelearningalgorithms.Forasimple\nvisualexample,seegure.1.1\nManyarticialintelligencetaskscanbesolvedbydesigningtherightsetof\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithm.Forexample,ausefulfeatureforspeakeridenticationfrom\nsoundisanestimateofthesizeofspeakersvocaltract.Itthereforegivesastrong\nclueastowhetherthespeakerisaman,woman,orchild.\nHowever,formanytasks,itisdiculttoknowwhatfeaturesshouldbeextracted.\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\nofawheelasafeature.Unfortunately,itisdiculttodescribeexactlywhata",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "wheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringo\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\nobscuringpartofthewheel,andsoon.\n3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n                \n                \nFigure1.1:Exampleofdierentrepresentations:supposewewanttoseparatetwo\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Onesolutiontothisproblemistousemachinelearningtodiscovernotonly\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng.Learnedrepresentations\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\nhumantimeandeort;itcantakedecadesforanentirecommunityofresearchers.\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\nconvertstheinputdataintoadierentrepresentation,anda dec o derfunction\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "throughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\nrepresentationhavevariousniceproperties.Dierentkindsofautoencodersaimto\nachievedierentkindsofproperties.\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\ncontext,weusethewordfactorssimplytorefertoseparatesourcesofinuence;\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\n4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthataectobservablequantities.\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeakers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "age,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\nandtheangleandbrightnessofthesun.\nAmajorsourceofdicultyinmanyreal-worldarticialintelligenceapplications\nisthatmanyofthefactorsofvariationinuenceeverysinglepieceofdataweare\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\ntoblackatnight.Theshapeofthecarssilhouettedependsontheviewingangle.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\nonesthatwedonotcareabout.\nOfcourse,itcanbeverydiculttoextractsuchhigh-level,abstractfeatures\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeakersaccent,\ncanbeidentiedonlyusingsophisticated,nearlyhuman-levelunderstandingof\nthedata.Whenitisnearlyasdiculttoobtainarepresentationastosolvethe\noriginalproblem,representationlearningdoesnot,atrstglance,seemtohelpus.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\nwhichareinturndenedintermsofedges.\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\napplicationofadierentmathematical functionasprovidinganewrepresentation\noftheinput.\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "computertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\ncanbethoughtofasthestateofthecomputersmemoryafterexecutinganother\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\ninstructionsinsequence.Sequentialinstructionsoergreatpowerbecauselater\n5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nVisiblelayer\n(inputpixels)1sthiddenlayer\n(edges)2ndhiddenlayer\n(cornersand\ncontours)3rdhiddenlayer\n(objectparts)CARPERSONANIMALOutput\n(objectidentity)\nFigure1.2:Illustrationofadeeplearningmodel.Itisdicultforacomputertounderstand\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Deeplearningresolvesthisdicultybybreakingthedesiredcomplicatedmappingintoa\nseriesofnestedsimplemappings,eachdescribedbyadierentlayerofthemodel.The\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\nfeaturesfromtheimage.Theselayersarecalledhiddenbecausetheirvaluesarenotgiven\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,therstlayercaneasily\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventhersthidden\nlayersdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\nlayersdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "candetectentirepartsofspecicobjects,byndingspeciccollectionsofcontoursand\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\nfromZeilerandFergus2014().\n6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nx 1 x 1\nw 1 w 1\nx 2 x 2 w 2 w 2+El e me n t\nS e t\n+\n\n\nxx wwEl e me n t\nS e t\nL ogi s t i c\nR e gr e s s i onL ogi s t i c\nR e gr e s s i on\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\noutputbutdependsonthedenitionofwhatconstitutesapossiblecomputationalstep.\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": " ( wTx ),whereisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\nviewofdeeplearning,notalloftheinformationinalayersactivationsnecessarily\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "stateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspecically,\nbutithelpsthemodeltoorganizeitsprocessing.\nTherearetwomainwaysofmeasuringthedepthofamodel.Therstviewis\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "aowchartthatdescribeshowtocomputeeachofthemodelsoutputsgiven\nitsinputs.Justastwoequivalentcomputerprogramswillhavedierentlengths\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\nbedrawnasaowchartwithdierentdepthsdependingonwhichfunctionswe\nallowtobeusedasindividualstepsintheowchart.Figureillustrateshowthis 1.3\nchoiceoflanguagecangivetwodierentmeasurementsforthesamearchitecture.\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "modelasbeingnotthedepthofthecomputational graphbutthedepthofthe\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\noftheowchartofthecomputations neededtocomputetherepresentationof\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\nThisisbecausethesystemsunderstandingofthesimplerconceptscanberened\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\npresentaswell.Inthiscase,thegraphofconceptsonlyincludestwolayersa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "layerforeyesandalayerforfacesbutthegraphofcomputations includes 2n\nlayersifwereneourestimateofeachconceptgiventheothertimes. n\nBecauseitisnotalwaysclearwhichofthesetwoviewsthedepthofthe\ncomputational graph,orthedepthoftheprobabilisticmodelinggraphismost\nrelevant,andbecausedierentpeoplechoosedierentsetsofsmallestelements\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "acomputerprogram.Nor isthereaconsensusabouthowmuchdepthamodel\nrequirestoqualifyasdeep.However,deeplearningcansafelyberegardedasthe\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\nSpecically,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata.Accordingtotheauthorsofthis",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "book,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\nkindofmachinelearningthatachievesgreatpowerandexibilitybylearningto\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdenedin\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\noflessabstractones.Figureillustratestherelationshipbetweenthesedierent 1.4\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\narebeginningacareerindeeplearningandarticialintelligenceresearch.The\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "learningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\n8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nAIMachinelearningRepresentationlearningDeeplearning\nExample:\nKnowledge\nbasesExample:\nLogistic\nregressionExample:\nShallow\nautoencoders Example:\nMLPs\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\n9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nInputHand-\ndesigned\nprogramOutput\nInputHand-\ndesigned\nfeaturesMappingfrom\nfeaturesOutput\nInputFeaturesMappingfrom\nfeaturesOutput\nInputSimple\nfeaturesMappingfrom\nfeaturesOutput\nAdditional\nlayersofmore\nabstract\nfeatures\nRule-based\nsystemsClassic\nmachine\nlearning Representation\nlearningDeep\nlearning\nFigure1.5:FlowchartsshowinghowthedierentpartsofanAIsystemrelatetoeach\notherwithindierentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\nlearnfromdata.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "learnfromdata.\n1 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\nsearchengines,onlineadvertisingandnance.\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "widelybelievedtobeimportantforfutureresearchindeeplearning.\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\nchapterstoread,gureprovidesaowchartshowingthehigh-levelorganization 1.6\nofthebook.\nWedoassumethatallreaderscomefromacomputersciencebackground. We",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "assumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory.\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\nDeeplearninghashadalongandrichhistory,buthasgonebymanynames\nreectingdierentphilosophicalviewpoints,andhaswaxedandwanedin\npopularity.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "popularity.\nDeeplearninghasbecomemoreusefulastheamountofavailabletraining\ndatahasincreased.\nDeeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n(bothhardwareandsoftware)fordeeplearninghasimproved.\nDeeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\naccuracyovertime.\n1 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1.Introduction\nPartI:AppliedMathandMachineLearningBasics\n2.LinearAlgebra3.Probabilityand\nInformationTheory\n4.Numerical\nComputation5.MachineLearning\nBasics\nPartII:DeepNetworks:ModernPractices\n6.DeepFeedforward\nNetworks\n7.Regularization8.Optimization 9.CNNs10.RNNs\n11.Practical\nMethodology12.Applications\nPartIII:DeepLearningResearch\n13.LinearFactor\nModels14.Autoencoders15.Representation\nLearning\n16.Structured",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Learning\n16.Structured\nProbabilisticModels17.MonteCarlo\nMethods\n18.Partition\nFunction19.Inference\n20.DeepGenerative\nModels\nFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\nindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\n1 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\nw o rks\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\nexcitingnewtechnology,andaresurprisedtoseeamentionofhistoryinabook\naboutanemergingeld.Infact,deeplearningdatesbacktothe1940s.Deep\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "dierentnames,andhasonlyrecentlybecomecalleddeeplearning.Theeld\nhasbeenrebrandedmanytimes,reectingtheinuenceofdierentresearchers\nanddierentperspectives.\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\nlearningknownas c y b e r net i c sinthe1940s1960s,deeplearningknownas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "c o nnec t i o n i s minthe1980s1990s,andthecurrentresurgenceunderthename\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedingure.1.7\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\nhappensorcouldhappeninthebrain.Asaresult,oneofthenamesthatdeep\nlearninghasgonebyis ar t i c i al neur al net w o r k s(ANNs).Thecorresponding\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "computational principlesbehindthebrainandduplicateitsfunctionality.Another\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\nlightonthesebasicscienticquestionsareusefulapartfromtheirabilitytosolve\nengineeringapplications.\nThemoderntermdeeplearninggoesbeyondtheneuroscienticperspective\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "principleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\nlearningframeworksthatarenotnecessarilyneurallyinspired.\n1 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1940 1950 1960 1970 1980 1990 2000\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\nc y b e r n e t i c s\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\nFigure1.7:Thegureshowstwoofthethreehistoricalwavesofarticialneuralnets\nresearch,asmeasuredbythefrequencyofthephrasescyberneticsandconnectionismor\nneuralnetworksaccordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "rstwavestartedwithcyberneticsinthe1940s1960s, withthedevelopmentoftheories\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\ntherstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe19801995period,\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\nthecorrespondingscienticactivityoccurred.\n1 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\nmotivatedfromaneuroscienticperspective.Thesemodelsweredesignedto\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\nf ( x w, ) =x 1w 1 +    +x nw n.Thisrstwaveofneuralnetworksresearchwas\nknownascybernetics,asillustratedingure.1.7\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "ofbrainfunction.Thislinearmodelcouldrecognizetwodierentcategoriesof\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\ntocorrespondtothedesireddenitionofthecategories,theweightsneededtobe\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.Inthe1950s,\ntheperceptron(Rosenblatt19581962,,)becametherstmodelthatcouldlearn\ntheweightsdeningthecategoriesgivenexamplesofinputsfromeachcategory.\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "time,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\nandHo1960,),andcouldalsolearntopredictthesenumbersfromdata.\nThesesimplelearningalgorithmsgreatlyaectedthemodernlandscapeofma-\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\nmodiedversionsofthestochasticgradientdescentalgorithmremainthedominant\ntrainingalgorithmsfordeeplearningmodelstoday.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "trainingalgorithmsfordeeplearningmodelstoday.\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\nlearningmodels,thoughinmanycasestheyare t r a i ne dindierentwaysthanthe\noriginalmodelsweretrained.\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "andf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseawsinlinearmodelscaused\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n1969).Thiswastherstmajordipinthepopularityofneuralnetworks.\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\nlearningresearchers,butitisnolongerthepredominant guidefortheeld.\nThemainreasonforthediminishedroleofneuroscienceindeeplearning\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n1 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\ncansolvemanydierenttasks.Neuroscientistshavefoundthatferretscanlearnto\nseewiththeauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\ndierenttasksthatthebrainsolves.Beforethishypothesis,machinelearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "researchwasmorefragmented,withdierentcommunitiesofresearchersstudying\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\nthe r e c t i ed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\nfunction.Thesimpliedmodernversionwasdevelopedincorporatingideasfrom",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "manyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\nneuroscienceasaninuence,and ()citingmoreengineering- Jarrett e t a l .2009\norientedinuences.Whileneuroscienceisanimportantsourceofinspiration,it\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\ndierentfunctionsthanmodernrectiedlinearunits,butgreaterneuralrealism\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "neurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooermuch\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\ninuencethanresearchersworkinginothermachinelearningeldssuchaskernel",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyelds,\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n1 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nneuroscienceatall.\nItisworthnotingthattheeorttounderstandhowthebrainworkson\nanalgorithmiclev elisaliveandwell.Thisendeavorisprimarilyknownas\ncomputational neuroscienceandisaseparateeldofstudyfromdeeplearning.\nItiscommonforresearcherstomovebackandforthbetweenbothelds.The\neldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheeldof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "computational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\nmodelsofhowthebrainactuallyworks.\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\ni ng( ,; ,).Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\ntounderstandingthemind,combiningmultipledierentlevelsofanalysis.During",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\nDespitetheirpopularity,symbolicmodelswerediculttoexplainintermsof\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\nThecentralideainconnectionism isthatalargenumberofsimplecomputational",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\ncomputational models.\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotodaysdeeplearning.\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\nbird,greentruck,andsoon.Thisrequiresninedierentneurons,andeachneuron\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n1 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\nofonespeciccategoryofobjects.Theconceptofdistributedrepresentationis\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\nwithneuralnetworks.()and ()identiedsomeof Hochreiter1991Bengio e t a l .1994\nthefundamentalmathematical dicultiesinmodelinglongsequences,describedin\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\nmemoryorLSTMnetworktoresolvesomeofthesediculties.Today,theLSTM",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "iswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\nprocessingtasksatGoogle.\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulll\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\nothereldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "1992CortesandVapnik1995Schlkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "ThisprogramunitedmachinelearningresearchgroupsledbyGeoreyHinton\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\nandcomputervision.\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydicult\ntotrain.Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\nwiththehardwareavailableatthetime.\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\n1 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2006.GeoreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\nnetworkcouldbeecientlytrainedusingastrategycalledgreedylayer-wisepre-\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\nTheotherCIFAR-aliatedresearchgroupsquicklyshowedthatthesamestrategy\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\ntermdeeplearningtoemphasizethatresearcherswerenowabletotraindeeper\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\nmodelstoleveragelargelabeleddatasets.\n1 . 2 . 2 In creasin g D a t a s et S i zes\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "crucialtechnologythoughtherstexperimentswitharticialneuralnetworkswere\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n1 9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintoadatasetappropriateformachinelearningapplications.TheageofBig\nDatahasmademachinelearningmucheasierbecausethekeyburdenofstatistical\nestimationgeneralizingwelltonewdataafterobservingonlyasmallamount\nofdatahasbeenconsiderablylightened.Asof2016,aroughruleofthumb\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\nlearning.\n1 . 2 . 3 In creasin g Mo d el S i zes\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\nBiologicalneuronsarenotespeciallydenselyconnected.Asseeningure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\nsmalluntilquiterecently,asshowningure.Sincetheintroductionofhidden 1.11",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "units,articialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\nallowfasterscaling,articialneuralnetworkswillnothavethesamenumberof\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\nrepresentmorecomplicatedfunctionsthancurrentarticialneurons,sobiological",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksmaybeevenlargerthanthisplotportrays.\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\nneuronsthanaleechwereunabletosolvesophisticatedarticialintelligenceprob-\nlems.Eventodaysnetworks,whichweconsiderquitelargefromacomputational\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\nprimitivevertebrateanimalslikefrogs.\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n2 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1900 1950 198520002015\nYear100101102103104105106107108109Datasetsize(numberexamples)\nIrisMNISTPublicSVHN\nImageNet\nCIFAR-10ImageNet10k\nILSVRC  2014Sports-1M\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\nWMT\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "ofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\ndemonstratethatneuralnetworkswereabletolearnspecickindsoffunctions(Widrow\nandHo1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\nofthousandsofexamplessuchastheMNISTdataset(showningure)ofscans 1.9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "ofhandwrittennumbers( ,).Intherstdecadeofthe2000s,more LeCun e t a l .1998b\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\nthersthalfofthe2010s,signicantlylargerdatasets,containinghundredsofthousands\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBMsdatasetconstructed\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\n2 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nFigure1.9:ExampleinputsfromtheMNISTdataset.TheNISTstandsforNational\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\nTheMstandsformodied,sincethedatahasbeenpreprocessedforeasierusewith\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\nandassociatedlabelsdescribingwhichdigit09iscontainedineachimage.Thissimple\nclassicationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\nGeoreyHintonhasdescribeditasthe d r o s o p h i l aofmachinelearning,meaningthat\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\nconditions,muchasbiologistsoftenstudyfruities.\n2 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\nexpectedtocontinuewellintothefuture.\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "accuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\nappliedwithsuccesstobroaderandbroadersetsofapplications.\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000dierent\ncategoriesofobjects.ThelargestcontestinobjectrecognitionistheImageNet\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\nwonthischallengeforthersttimeandbyawidemargin,bringingdownthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "state-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\nforeachimageandthecorrectcategoryappearedintherstveentriesofthis\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\nasshowningure.1.12",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "asshowningure.1.12\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\nthishistoryinmoredetailinsection.12.3\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "imagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\n2013)andyieldedsuperhumanperformanceintracsignclassication(Ciresan\n2 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\n12\n34\n567\n89\n1 0\nF r ui t yMo useC a tH um a n\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinarticialneural\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\nneuronsismostlyadesignconsideration.Somearticialneuralnetworkshavenearlyas\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "tohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\nnetworksizesfrom (). Wikipedia2015\n1.Adaptivelinearelement( ,) WidrowandHo1960\n2.Neocognitron(Fukushima1980,)\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "7.Distributedautoencoder(,) Le e t al.2012\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n10.GoogLeNet( ,) Szegedy e t al.2014a\n2 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ne t a l .,).2012\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\nelementsofthesequence( ,).Recurrentneuralnetworks, GlehreandBengio2013",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustxedinputs.\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015).\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "toreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\nfuturecouldinprinciplebeappliedtonearlyanytask.\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\nalsosignicantlyimprovedtheperformanceofreinforcementlearningforrobotics\n(,). Finn e t a l .2015\nManyoftheseapplicationsofdeeplearningarehighlyprotable.Deeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "isnowusedbymanytoptechnologycompaniesincludi ngGoogle,Microsoft,\nFacebook,IBM,Baidu,Apple,Adobe,Netix,NVIDIAandNEC.\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\nDistBelief(,),Cae(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "TensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\ncommercialproducts.\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\n2 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\nscienticelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Barley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientic\neldsinthefuture.\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "bringittonewfrontiers.\n2 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1950 198520002015 2056\nYear10 210 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\n123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure1.11:Sincetheintroductionofhiddenunits,articialneuralnetworkshavedoubled\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\n1.Perceptron(,,) Rosenblatt19581962\n2.Adaptivelinearelement( ,) WidrowandHo1960\n3.Neocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "3.Neocognitron(Fukushima1980,)\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\n7.Meaneldsigmoidbeliefnetwork(,) Saul e t al.1996\n8.LeNet-5( ,) LeCun e t al.1998b\n9.Echostatenetwork( ,) JaegerandHaas2004\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n16.OMP-1network( ,) CoatesandNg2011\n17.Distributedautoencoder(,) Le e t al.2012\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n20.GoogLeNet( ,) Szegedy e t al.2014a\n2 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2010 2011 2012 2013 2014 2015\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classicationerrorrate\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\neveryyear,andyieldedlowerandlowererrorrateseachtime.DatafromRussakovsky\ne t a l . e t a l . ()and2014b He().2015\n2 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 7\nRegularization f or D e e p L e ar n i n g\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\nasregularization.As wewillseethereareagreatmanyformsofregularization\navailabletothedeeplearningpractitioner. Infact,developingmoreeective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshasbeenoneofthemajorresearcheortsintheeld.\nChapterintroducedthebasicconceptsofgeneralization, undertting,overt- 5\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\ntoformdeepmodels.\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Ifyouarealreadyfamiliarwiththeseconcepts,feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks.\nInsection,wedenedregularizationasanymodicationwemaketo 5.2.2\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\nitstrainingerror.Therearemanyregularizationstrategies.Someputextra\nconstraintsonamachinelearningmodel, suchasaddingrestrictionsonthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "parametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\n228",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\nspecickindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\nbiasforreducedvariance.Aneectiveregularizerisonethatmakesaprotable\ntrade,reducingvariancesignicantlywhilenotoverlyincreasingthebias.Whenwe\ndiscussedgeneralization andoverttinginchapter,wefocusedonthreesituations, 5\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\nprocesscorrespondingtounderttingandinducingbias,or(2)matchedthetrue",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "datageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\notherpossiblegeneratingprocessestheoverttingregimewherevariancerather\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\nmodelfromthethirdregimeintothesecondregime.\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\nextent,wearealwaystryingtotasquarepeg(thedatageneratingprocess)into",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "aroundhole(ourmodelfamily).\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\nsimplematterofndingthemodeloftherightsize,withtherightnumberof\nparameters.Instead,wemightndandindeedinpracticaldeeplearningscenarios,\nwealmostalwaysdondthatthebestttingmodel(inthesenseofminimizing\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\nmodel.\n2 2 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.1ParameterNormPenalties\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\nandeectiveregularizationstrategies.\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\nrameternormpenalty ()totheobjectivefunction J.Wedenotetheregularized\nobjectivefunctionby J:",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "objectivefunctionby J:\n J , J ,  (;Xy) = (;Xy)+() (7.1)\nwhere [0 ,)isahyperparameter thatweightstherelativecontributionofthe\nnormpenaltyterm,,relativetothestandardobjectivefunction  J.Setting to0\nresultsinnoregularization. Largervaluesof correspondtomoreregularization.\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction  Jit\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\nofthesizeoftheparameters(orsomesubsetoftheparameters).Dierent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "choicesfortheparameternormcanresultindierentsolutionsbeingpreferred. \nInthissection,wediscusstheeectsofthevariousnormswhenusedaspenalties\nonthemodelparameters.\nBeforedelvingintotheregularizationbehaviorofdierentnorms,wenotethat\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythat\npenalizes oftheanetransformationateachlayerandleaves onlytheweights\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatotaccurately",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "thantheweights.Eachweightspecieshowtwovariablesinteract.Fittingthe\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\ncanintroduceasignicantamountofundertting. Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbeaectedbyanormpenalty,whilethe\nvectordenotesalloftheparameters,includingbothwandtheunregularized\nparameters.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\npenaltywithadierent coecientforeachlayerofthenetwork.Becauseitcan\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\nsearchspace.\n2 3 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\nbyaddingaregularizationterm() =1\n2w2\n2totheobjectivefunction.Inother\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\nTikhonovregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "Tikhonovregularization.\nWecangainsomeinsightintothebehaviorofweightdecayregularization\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\npresentation,weassumenobiasparameter,soisjustw.Suchamodelhasthe\nfollowingtotalobjectivefunction:\n J , (;wXy) =\n2wwwXy +( J; ,) , (7.2)\nwiththecorrespondingparametergradient\n w J ,  (;wXy) = w+ w J , . (;wXy) (7.3)\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\nwww    ( + w J , . (;wXy)) (7.4)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "www    ( + w J , . (;wXy)) (7.4)\nWrittenanotherway,theupdateis:\nww  (1  )  w J , . (;wXy) (7.5)\nWecanseethattheadditionoftheweightdecaytermhasmodiedthelearning\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\nasinglestep.Butwhathappensovertheentirecourseoftraining?\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\nobtainsminimalunregularized trainingcost,w=argminw J(w).Iftheobjective\nfunctionistrulyquadratic,asinthecaseofttingalinearregressionmodelwith\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i  c p o i n t i n s p a c e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "a n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e  e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "m o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\n2 3 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nmeansquarederror,thentheapproximationisperfect.Theapproximation  Jis\ngivenby\n J J () =  (w)+1\n2(ww)Hww () , (7.6)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw.Thereis\nnorst-orderterminthisquadraticapproximation, becausewisdenedtobea\nminimum,wherethegradientvanishes.Likewise,becausewisthelocationofa\nminimumof,wecanconcludethatispositivesemidenite. J H\nTheminimumof Joccurswhereitsgradient\n w J() = (wHww) (7.7)\nisequalto. 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": " w J() = (wHww) (7.7)\nisequalto. 0\nTostudytheeectofweightdecay,wemodifyequationbyaddingthe 7.7\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\nversionof J.Weusethevariable wtorepresentthelocationoftheminimum.\nwH+ (ww) = 0 (7.8)\n(+ )H IwHw = (7.9)\nwHI = (+ ) 1Hw. (7.10)\nAs approaches0,theregularizedsolution wapproachesw.Butwhat\nhappensas grows?BecauseHisrealandsymmetric,wecandecomposeit\nintoadiagonalmatrix andanorthonormal basisofeigenvectors,Q,suchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "HQQ = .Applyingthedecompositiontoequation,weobtain:7.10\nwQQ = ( + ) I 1QQ w(7.11)\n=\nQIQ (+  ) 1\nQQ w(7.12)\n= (+ )Q  I 1Qw. (7.13)\nWeseethattheeectofweightdecayistorescalewalongtheaxesdenedby\ntheeigenvectorsofH.Specically,thecomponentofwthatisalignedwiththe\ni-theigenvectorofHisrescaledbyafactorof i\n i + .(Youmaywishtoreview\nhowthiskindofscalingworks,rstexplainedingure).2.3\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "where  i ,theeectofregularizationisrelativelysmall.However,components\nwith  i willbeshrunktohavenearlyzeromagnitude.Thiseectisillustrated\ningure.7.1\n2 3 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w\n w\nFigure7.1:Anillustrationoftheeectof L2(orweightdecay)regularizationonthevalue\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\nthepointw,thesecompetingobjectivesreachanequilibrium.Intherstdimension,the\neigenvalueoftheHessianof Jissmall.Theobjectivefunctiondoesnotincreasemuch",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "whenmovinghorizontallyawayfromw.Becausetheobjectivefunctiondoesnotexpress\nastrongpreferencealongthisdirection,theregularizerhasastrongeectonthisaxis.\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\nisverysensitivetomovementsawayfromw.Thecorrespondingeigenvalueislarge,\nindicatinghighcurvature.Asaresult,weightdecayaectsthepositionof w2relatively\nlittle.\nOnlydirectionsalongwhichtheparameterscontributesignicantlytoreducing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "theobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\ntellsusthatmovementinthisdirectionwillnotsignicantlyincreasethegradient.\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\nSofarwehavediscussedweightdecayintermsofitseectontheoptimization\nofanabstract,general,quadraticcostfunction.Howdotheseeectsrelateto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "machinelearninginparticular?Wecanndoutbystudyinglinearregression,a\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n2 3 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthesumofsquarederrors:\n( )Xwy( )Xwy . (7.14)\nWhenweadd L2regularization, theobjectivefunctionchangesto\n( )Xwy( )+Xwy1\n2ww . (7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (X) 1Xy (7.16)\nto\nwX= (XI+ ) 1Xy . (7.17)\nThematrixXXinequationisproportionaltothecovariancematrix 7.161\nmXX.\nUsing L2regularizationreplacesthismatrixwith\nXXI+  1inequation.7.17\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof tothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\ntoperceivetheinputXashavinghighervariance,whichmakesitshrinkthe\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\nthisaddedvariance.\n7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters.Anotheroptionistouse L1\nregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "regularization.\nFormally, L1regularizationonthemodelparameter isdenedas:w\n() =  ||||w 1=\ni| w i| , (7.18)\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\nnowdiscusstheeectof L1regularizationonthesimplelinearregressionmodel,\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\nparticular,weareinterestedindelineatingthedierencesbetween L1and L2forms",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rm() =  || w w( ) o|| 1=\ni| w i w( ) o\ni| .\n2 3 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter  .\nThus,theregularizedobjectivefunction  J , (;wXy)isgivenby\n J ,  (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\nwiththecorrespondinggradient(actually,sub-gradient):\n w J ,  (;wXy) = sign( )+w  w J ,(Xyw;) (7.20)\nwhere issimplythesignofappliedelement-wise. sign( )w w",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "Byinspectingequation,wecanseeimmediately thattheeectof 7.20 L1\nregularizationisquitedierentfromthatof L2regularization. Specically,wecan\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\nregularization.\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\ninthissettingisgivenby\n w J() = (wHww) , (7.21)\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww.\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\naccomplishedusingPCA.\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\nposesintoasumovertheparameters:\n J , J (;wXy) = (w; )+Xy ,\ni1\n2H i , i(w iw\ni)2+  w| i|\n.(7.22)\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n(foreachdimension),withthefollowingform: i\nw i= sign( w\ni)max\n| w\ni|\nH i , i,0\n. (7.23)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "i)max\n| w\ni|\nH i , i,0\n. (7.23)\n2 3 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConsiderthesituationwhere w\ni > i 0forall.Therearetwopossibleoutcomes:\n1.Thecasewhere w\ni\nH i , i.Heretheoptimalvalueof w iundertheregularized\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\ntotheregularizedobjective J(w;Xy ,)isoverwhelmedindirection iby\nthe L1regularizationwhichpushesthevalueof w itozero.\n2.Thecasewhere w\ni >\nH i , i.Inthiscase,theregularizationdoesnotmovethe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "optimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\ndistanceequalto\nH i , i.\nAsimilarprocesshappenswhen w\ni <0,butwiththe L1penaltymaking w iless\nnegativeby\nH i , i,or0.\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\ndierentbehaviorthanariseswith L2regularization. Equationgavethe7.13",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "solution  wfor L2regularization. Ifwerevisitthatequationusingtheassumption\nofadiagonalandpositivedeniteHessianHthatweintroducedforouranalysisof\nL1regularization,wendthat w i=H i , i\nH i , i + w\ni.If w\niwasnonzero,then  w iremains\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\ntobecomesparse,while L1regularizationmaydosoforlargeenough. \nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "asafeatureselectionmechanism.Featureselectionsimpliesamachinelearning\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "asMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights.For L1regu-\nlarization,thepenalty (w)= \ni| w i|usedtoregularizeacostfunctionis\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w Rn:\nlog() = pw\nilogLaplace( w i;0 ,1\n) = |||| w 1+log log2 n  n .(7.24)\n2 3 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\nignorethe termsbecausetheydonotdependon. log log2  w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\n J , J ,  . (;Xy) = (;Xy)+() (7.25)\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "functionplusasetofpenalties.Eachpenaltyisaproductbetweenacoecient,\ncalledaKarushKuhnTucker(KKT)multiplier,andafunctionrepresenting\nwhethertheconstraintissatised.Ifwewantedtoconstrain()tobelessthan\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\nL  (; ) = (; )+(()  , Xy , JXy ,  k .) (7.26)\nThesolutiontotheconstrainedproblemisgivenby\n= argmin\nmax\n ,  0L() ,  . (7.27)\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 ",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "and .Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\nconstraint.Manydierentproceduresarepossiblesomemayusegradientdescent,\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszerobutinall\nprocedures mustincreasewhenever() > kanddecreasewhenever() < k.\nAllpositive encourage ()toshrink.Theoptimalvalue willencourage ()\ntoshrink,butnotsostronglytomakebecomelessthan. () k\nTogainsomeinsightintotheeectoftheconstraint,wecanx andview\ntheproblemasjustafunctionof:",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "theproblemasjustafunctionof:\n= argmin\nL( , ) = argmin\nJ ,  (;Xy)+() .(7.28)\nThisisexactlythesameastheregularizedtrainingproblemofminimizing  J.\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe  L2norm,thentheweightsareconstrainedtolieinan L2\nball.Ifisthe  L1norm,thentheweightsareconstrainedtolieinaregionof\n2 3 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\nimposebyusingweightdecaywithcoecient becausethevalueof doesnot\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\nbetween kand dependsontheformof J.Whilewedonotknowtheexactsize\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing \ninordertogroworshrinktheconstraintregion.Larger willresultinasmaller",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "constraintregion.Smallerwillresultinalargerconstraintregion. \nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\ndescenttotakeastepdownhillon J()andthenprojectbacktothenearest\npointthatsatises() < k.Thiscanbeusefulifwehaveanideaofwhatvalue\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof that\ncorrespondstothis. k\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "constraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\nprocedurestogetstuckinlocalminimacorrespondingtosmall.Whentraining\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\ndeadunits.Theseareunitsthatdonotcontributemuchtothebehaviorofthe\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\nallverysmall.Whentrainingwithapenaltyonthenormoftheweights,these\ncongurations canbelocallyoptimal,evenifitispossibletosignicantlyreduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Jbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\nhaveaneectwhentheweightsbecomelargeandattempttoleavetheconstraint\nregion.\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "largegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\nconsistentlyincreasethesizeoftheweights,thenrapidlymovesawayfrom\ntheoriginuntilnumericaloverowoccurs.Explicitconstraintswithreprojection\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\nsomestability.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "somestability.\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n2 3 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\nanexplicitconstraintwithreprojection.\n7.3RegularizationandUnder-ConstrainedProblems\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\nerlydened.Manylinearmodelsinmachinelearning,includinglinearregression\nandPCA,dependoninvertingthematrixXX.Thisisnotpossiblewhenever\nXXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\nXXI+ instead.Thisregularizedmatrixisguaranteedtobeinvertible.\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "theclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassication,then2wwillalsoachieveperfectclassicationandhigherlikelihood.\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\nimplementationofgradientdescentwilleventuallyreachsucientlylargeweights\ntocausenumericaloverow,atwhichpointitsbehaviorwilldependonhowthe\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "Mostformsofregularizationareabletoguaranteetheconvergenceofiterative\nmethodsappliedtounderdetermined problems.Forexample,weightdecaywill\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\nslopeofthelikelihoodisequaltotheweightdecaycoecient.\nTheideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems.\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\n2 3 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMoore-Penrosepseudoinverse.Recallthatonedenitionofthepseudoinverse\nX+ofamatrixisX\nX+=lim\n 0(XXI+ ) 1X. (7.29)\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\ndecay.Specically,equationisthelimitofequationastheregularization 7.29 7.17\ncoecientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\nunderdetermined problemsusingregularization.\n7.4DatasetAugmentation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "7.4DatasetAugmentation\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata.\nThisapproachiseasiestforclassication.Aclassierneedstotakeacompli-\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Thismeansthatthemaintaskfacingaclassieristobeinvarianttoawidevariety\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\ntheinputsinourtrainingset. x\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\nisdiculttogeneratenewfakedataforadensityestimationtaskunlesswehave\nalreadysolvedthedensityestimationproblem.\nDatasetaugmentationhasbeenaparticularlyeectivetechniqueforaspecic\nclassicationproblem:objectrecognition.Imagesarehighdimensionalandinclude",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "anenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\ntheimagehavealsoprovenquiteeective.\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\ndierencebetweenbanddandthedierencebetween6and9,sohorizontal\nipsand180rotationsarenotappropriatewaysofaugmentingdatasetsforthese\ntasks.\n2 4 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTherearealsotransformationsthatwewouldlikeourclassierstobeinvariant\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\nbeimplementedasasimplegeometricoperationontheinputpixels.\nDatasetaugmentationiseectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,).\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\ncanalsobeseenasaformofdataaugmentation.Formanyclassicationand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "augmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\nthatthisapproachcanbehighlyeectiveprovidedthatthemagnitudeofthe\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\nmultiplyingbynoise.\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\ntheeectofdatasetaugmentationintoaccount.Often,hand-designeddataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "augmentationschemescandramaticallyreducethegeneralization errorofamachine\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "numeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "whileoperationsthatarespecictooneapplicationdomain(suchasrandomly\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\n2 4 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.5NoiseRobustness\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinnitesimal\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "appliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\nofthatapproach.\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,).Thiscan\nbeinterpretedasastochasticimplementation ofBayesianinferenceoverthe\nweights.TheBayesiantreatmentoflearningwouldconsiderthemodelweights",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "tobeuncertainandrepresentableviaaprobabilitydistributionthatreectsthis\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreect\nthisuncertainty.\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\nafunction  y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "costfunctionbetweenthemodelpredictions  y()xandthetruevalues: y\nJ= E p x , y ( )( y y ()x)2\n. (7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\nWenowassumethatwitheachinputpresentationwealsoincludearandom\nperturbation  WN(; 0 , I)ofthenetworkweights.Letusimaginethatwe\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelas y  W(x).Despite\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "outputofthenetwork.Theobjectivefunctionthusbecomes:\n J W= E p , y , ( x  W )\n( y  W() )x y2\n(7.31)\n= E p , y , ( x  W )\n y2\n W()2x y y  W()+x y2\n.(7.32)\nForsmall ,theminimization of Jwithaddedweightnoise(withcovariance\nI)isequivalenttominimization of Jwithanadditionalregularizationterm:\n2 4 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n E p , y ( x ) W y()x2\n.Thisformofregularizationencouragestheparametersto\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\narelativelysmallinuenceontheoutput.Inotherwords,itpushesthemodel\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\nweights,ndingpointsthatarenotmerelyminima,butminimasurroundedby\natregions(HochreiterandSchmidhuber1995,).Inthesimpliedcaseoflinear",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "regression(where,forinstance,  y(x) =wx+ b),thisregularizationtermcollapses\ninto  E p ( ) x\nx2\n,whichisnotafunctionofparametersandthereforedoesnot\ncontributetothegradientof J Wwithrespecttothemodelparameters.\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "constant ,thetrainingsetlabel yiscorrectwithprobability 1 ,andotherwise\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\nsoftmaxwith koutputvaluesbyreplacingthehardandclassicationtargets 0 1\nwithtargetsof\nk 1and1 ,respectively.Thestandardcross-entropylossmay\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "classierandhardtargetsmayactuallyneverconvergethesoftmaxcannever\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\ndiscouragingcorrectclassication.Thisstrategyhasbeenusedsincethe1980s\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "etal.,).2015\n7.6Semi-SupervisedLearning\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\nx.\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\n2 4 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\nsimilarrepresentations.Alinearclassierinthenewspacemayachievebetter\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "analysisasapre-processingstepbeforeapplyingaclassier(ontheprojected\ndata).\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\nthentrade-othesupervisedcriterion log P( y x|)withtheunsupervisedor\ngenerativeone(suchaslog P( x)orlog P( x y ,)).Thegenerativecriterionthen\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\ninthetotalcriterion,onecanndabettertrade-othanwithapurelygenerative\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\nBengio2008,).\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "functionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesignicantly. P() x P( ) y x|\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\n7.7Multi-TaskLearning\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\narisingoutofseveraltasks.Inthesamewaythatadditionaltrainingexamples",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "putmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\nconstrainedtowardsgoodvalues(assumingthesharingisjustied),oftenyielding\nbettergeneralization.\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\ndierentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\n2 4 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\nparameters:\n1.Task-specicparameters(whichonlybenetfromtheexamplesoftheirtask\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\nnetworkingure.7.2\n2.Genericparameters,sharedacrossallthetasks(whichbenetfromthe\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\ningure.7.2\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( s h a r e d )h( s h a r e d )\nxx\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\nandthisgureillustratesthecommonsituationwherethetasksshareacommoninputbut\ninvolvedierenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\ncanbesharedacrosssuchtasks,whiletask-specicparameters(associatedrespectively\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "sharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "associatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n2 4 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n0 50 100 150 200 250\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\nV a l i d a t i o n s e t l o s s\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "increaseagain,forminganasymmetricU-shapedcurve.\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\nthedierenttasksarevalid,meaningthatthereissomethingsharedacrosssome\nofthetasks.\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\nfollowing:amongthefactorsthatexplainthevariationsobservedinthedata",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "associatedwiththedierenttasks,somearesharedacrosstwoormoretasks.\n7.8EarlyStopping\nWhentraininglargemodelswithsucientrepresentationalcapacitytoovert\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\nvalidationseterrorbeginstoriseagain.Seegureforanexampleofthis 7.3\nbehavior.Thisbehavioroccursveryreliably.\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\n2 4 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\nvalidationerrorforsomepre-speciednumberofiterations.Thisprocedureis\nspeciedmoreformallyinalgorithm .7.1\nAlgorithm7.1Theearlystoppingmeta-algorithmfordeterminingthebest\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\nvalidationset.\nLetbethenumberofstepsbetweenevaluations. n",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Letbethenumberofstepsbetweenevaluations. n\nLet pbethepatience,thenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup.\nLet obetheinitialparameters.\n o\ni0\nj0\nv\n\ni i\nwhiledo j < p\nUpdatebyrunningthetrainingalgorithmforsteps.  n\ni i n +\nvValidationSetError ()\nif v< vthen\nj0\n\ni i\nv v\nelse\nj j+1\nendif\nendwhile\nBestparametersare,bestnumberoftrainingstepsis i\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "usedformofregularizationindeeplearning.Itspopularityisduebothtoits\neectivenessanditssimplicity.\nOnewaytothinkofearlystoppingisasaveryecienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\nWecanseeingurethatthishyperparameter hasaU-shapedvalidationset 7.3\n2 4 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\nU-shapedvalidationsetperformancecurve,asillustratedingure.Inthecaseof 5.3\nearlystopping,wearecontrollingtheeectivecapacityofthemodelbydetermining\nhowmanystepsitcantaketotthetrainingset.Mosthyperparametersmustbe\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\natthestartoftraining,thenruntrainingforseveralstepstoseeitseect.The",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "trainingtimehyperparam eterisuniqueinthatbydenitionasinglerunof\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysignicantcost\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "training,theseoccasionalslowwriteshavelittleeectonthetotaltrainingtime.\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\nsmallweights.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "smallweights.\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\noccuratalocalminimumofthetrainingobjective.\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\nonecanuseforthissecondtrainingprocedure.\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\n2 4 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\ntheearlystoppingproceduredeterminedwasoptimalintherstpass.Thereare\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\ntrainingsetisbigger.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "trainingsetisbigger.\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\ntotrain,thenretrainingonallthedata.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 usingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "returns i,theoptimalnumberofsteps.\nSettorandomvaluesagain. \nTrainonX( ) t r a i nandy( ) t r a i nfor isteps.\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\nfromtherstroundoftrainingandthencontinuetrainingbutnowusingallof\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\nofanumberofsteps.Instead,wecanmonitortheaveragelossfunctiononthe\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "setobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\nThisprocedureispresentedmoreformallyinalgorithm .7.3\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "oftrainingiterations,italsohasthebenetofprovidingregularizationwithout\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\nthegradientsofsuchadditionalterms.\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n2 4 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\ntivevaluewestarttoovert,thencontinuetraininguntilthatvalueisreached.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 usingX( ) s ubtr a i nand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nupdates.\n J , (X( ) s ubtr a i n,y( ) s ubtr a i n)\nwhile J ,(X( v a l i d ),y( v a l i d )) > do\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\nendwhile\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n()and ()arguedthatearlystoppinghastheeectof 1995aSjbergandLjung1995\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "spaceintheneighborhoodoftheinitialparametervalue o,asillustratedin\ngure.Morespecically,imaginetaking 7.4 optimization steps(corresponding\nto trainingiterations)andwithlearningrate .Wecanviewtheproduct  \nasameasureofeectivecapacity.Assumingthegradientisbounded,restricting\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\nspacereachablefrom o.Inthissense,  behavesasifitwerethereciprocalof\nthecoecientusedforweightdecay.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "thecoecientusedforweightdecay.\nIndeed,wecanshowhowinthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescentearlystoppingisequivalentto L2\nregularization.\nInordertocomparewithclassical L2regularization, weexamineasimple\nsettingwheretheonlyparametersarelinearweights(=w).Wecanmodel\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\nempiricallyoptimalvalueoftheweightsw:\n J J () =  (w)+1\n2(ww)Hww () , (7.33)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": " J J () =  (w)+1\n2(ww)Hww () , (7.33)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw.Giventhe\nassumptionthatwisaminimumof J(w),weknowthatHispositivesemidenite.\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\n w J() = (wHww) . (7.34)\n2 5 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2w\n w\nw 1w 2w\n w\nFigure7.4:Anillustrationoftheeectofearlystopping. ( L e f t )Thesolidcontourlines\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointwthat\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpointw.\n( R i g h t )Anillustrationoftheeectof L2regularizationforcomparison.Thedashedcircles",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "indicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\nnearertheoriginthantheminimumoftheunregularizedcost.\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\nanalyzinggradientdescenton J:\nw( ) = w( 1 )   w J(w( 1 ) ) (7.35)\n= w( 1 )  Hw(( 1 ) w) (7.36)\nw( ) w= ( )(IH w( 1 ) w) . (7.37)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "w( ) w= ( )(IH w( 1 ) w) . (7.37)\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\ntheeigendecompositionofH:H=QQ ,where isadiagonalmatrixandQ\nisanorthonormalbasisofeigenvectors.\nw( ) w= (IQQ   )(w( 1 ) w)(7.38)\nQ(w( ) w) = ( )I  Q(w( 1 ) w) (7.39)\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "a l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\ni n i t i a l v a l u e w( 0 ).\n2 5 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAssumingthatw( 0 )=0andthat ischosentobesmallenoughtoguarantee\n|1   i| <1,theparametertrajectoryduringtrainingafter parameterupdates\nisasfollows:\nQw( ) = [ ( )II  ]Qw. (7.40)\nNow,theexpressionforQwinequationfor7.13 L2regularizationcanberear-\nrangedas:\nQwI = (+  ) 1Qw(7.41)\nQwII = [(+  ) 1]Qw(7.42)\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ,\n ,andarechosensuchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": " ,andarechosensuchthat\n( )I  = (+ )  I 1 , (7.43)\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\nthatifall  iaresmall(thatis,   i1and  i /1)then\n1\n , (7.44)\n1\n . (7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations playsarole",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "inverselyproportionaltothe L2regularizationparameter,andtheinverseof  \nplaystheroleoftheweightdecaycoecient.\nParametervaluescorrespondingtodirectionsofsignicantcurvature(ofthe\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\ntodirectionsofsignicantcurvaturetendtolearnearlyrelativetoparameters\ncorrespondingtodirectionsoflesscurvature.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "correspondingtodirectionsoflesscurvature.\nThederivationsinthissectionhaveshownthatatrajectoryoflength ends\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "determinesthecorrectamountofregularizationwhileweightdecayrequiresmany\ntrainingexperimentswithdierentvaluesofitshyperparameter.\n2 5 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.9ParameterTyingandParameterSharing\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\ntotheparameters,wehavealwaysdonesowithrespecttoaxedregionorpoint.\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\ndeviatingfromthexedvalueofzero.However,sometimeswemayneedother\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "butweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\nshouldbesomedependencies betweenthemodelparameters.\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\nhavetwomodelsperformingthesameclassicationtask(withthesamesetof\nclasses)butwithsomewhatdierentinputdistributions.Formally,wehavemodel\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "maptheinputtotwodierent,butrelated outputs: y( ) A= f(w( ) A,x)and\n y( ) B= ( gw( ) B,x).\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\ntoeachother:  i, w( ) A\nishouldbecloseto w( ) B\ni.Wecanleveragethisinformation\nthroughregularization. Specically,wecanuseaparameternormpenaltyofthe\nform: (w( ) A,w( ) B)=w( ) Aw( ) B2\n2.Hereweusedan L2penalty,butother\nchoicesarealsopossible.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "choicesarealsopossible.\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\ntheparametersofonemodel,trainedasaclassierinasupervisedparadigm,to\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\nconstructedsuchthatmanyoftheparametersintheclassiermodelcouldbe\npairedtocorrespondingparametersintheunsupervisedmodel.\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "closetooneanother,themorepopularwayistouseconstraints:toforcesets\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\nassharingauniquesetofparameters.Asignicantadvantageofparametersharing\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\nmodelssuchastheconvolutionalneuralnetworkthiscanleadtosignicant",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "reductioninthememoryfootprintofthemodel.\n2 5 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\ntocomputervision.\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "iscomputedoverdierentlocationsintheinput.Thismeansthatwecannda\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\ni+1intheimage.\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\nmodelparametersandtosignicantlyincreasenetworksizeswithoutrequiringa\ncorrespondingincreaseintrainingdata.Itremainsoneofthebestexamplesof\nhowtoeectivelyincorporatedomainknowledgeintothenetworkarchitecture.\nCNNswillbediscussedinmoredetailinchapter.9\n7.10SparseRepresentations",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "7.10SparseRepresentations\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\npenaltyonthemodelparameters.\nWehavealreadydiscussed(insection)how7.1.2 L1penalizationinduces\nasparseparametrizationmeaning thatmanyoftheparametersbecomezero\n(orclosetozero).Representationalsparsity,ontheotherhand,des cribesa",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "representationwheremanyoftheelementsoftherepresentationarezero(orclose\ntozero).Asimpliedviewofthisdistinctioncanbeillustratedinthecontextof\nlinearregression:\n\n18\n5\n15\n9\n3\n=\n400 20 0 \n00 10 3 0 \n050 0 0 0\n100 10 4  \n100 0 50 \n\n2\n3\n2\n5\n1\n4\n\ny RmA Rm nx Rn(7.46)\n2 5 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n\n14\n1\n19\n2\n23\n=\n3 12 54 1  \n4 2 3 11 3  \n   15 4 2 3 2\n3 1 2 30 3  \n    54 22 5 1\n\n0\n2\n0\n0\n3\n0\n\ny RmB Rm nh Rn(7.47)\nIntherstexpression,wehaveanexampleofasparselyparametrized linear\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\ntheinformationpresentin,butdoessowithasparsevector. x",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "Representationalregularizationisaccomplishedbythesamesortsofmechanisms\nthatwehaveusedinparameterregularization.\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\n()h.Asbefore,wedenotetheregularizedlossfunctionby J:\n J , J ,  (;Xy) = (;Xy)+()h (7.48)\nwhere [0 ,)weightstherelativecontributionofthenormpenaltyterm,with\nlargervaluesofcorrespondingtomoreregularization. ",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "Justasan L1penaltyontheparametersinducesparametersparsity,an L1\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\n(h) =||||h 1=\ni| h i|.Ofcourse,the L1penaltyisonlyonechoiceofpenalty\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\nm\nih( ) i,to\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\noptimization problem\nargmin\nh h , 0 < k xWh2, (7.49)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "argmin\nh h , 0 < k xWh2, (7.49)\nwhere h 0isthenumberofnon-zeroentriesofh.Thisproblemcanbesolved\necientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\n2 5 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nOMP- kwiththevalueof kspeciedtoindicatethenumberofnon-zerofeatures\nallowed. ()demonstratedthatOMP-canbeaveryeective CoatesandNg2011 1\nfeatureextractorfordeeparchitectures.\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts.\n7.11BaggingandOtherEnsembleMethods\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "eralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\ntrainseveraldierentmodelsseparately,thenhaveallofthemodelsvoteonthe\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\nasensemblemethods.\nThereasonthatmodelaveragingworksisthatdierentmodelswillusually\nnotmakeallthesameerrorsonthetestset.\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "makesanerror  ioneachexample,withtheerrorsdrawnfromazero-mean\nmultivariatenormaldistributionwithvariances E[ 2\ni] = vandcovariances E[  i  j] =\nc.Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n1\nk\ni  i.Theexpectedsquarederroroftheensemblepredictoris\nE\n\n1\nk\ni i2\n=1\nk2E\n\ni\n 2\ni+\nj i= i  j\n\n(7.50)\n=1\nkv+k1\nkc . (7.51)\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "errorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\nensembleisonly1\nkv.Thismeansthattheexpectedsquarederroroftheensemble\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\nindependenterrors,theensemblewillperformsignicantlybetterthanitsmembers.\nDierentensemblemethodsconstructtheensembleofmodelsindierentways.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n2 5 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n8\n8F i r s t  e nse m b l e  m e m b e r\nSe c onde nse m b l e  m e m b e rO r i gi nal  data s e t\nF i r s t  r e s am pl e d  d a t a s e t\nSe c ondre s am p l e d  d a t a s e t\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodierent\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "bysamplingwithreplacement.Therstdatasetomitsthe9andrepeatsthe8.Onthis\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\nclassicationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\nachievingmaximalcondenceonlywhenbothloopsofthe8arepresent.\ndierentkindofmodelusingadierentalgorithmorobjectivefunction.Bagging",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\nfunctiontobereusedseveraltimes.\nSpecically,bagginginvolvesconstructing kdierentdatasets.Eachdataset\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\ni.Thedierencesbetweenwhichexamplesareincludedineachdatasetresultin\ndierencesbetweenthetrainedmodels.Seegureforanexample.7.5\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\noftenbenetfrommodelaveragingevenifallofthemodelsaretrainedonthesame\ndataset.Dierencesinrandominitialization, randomselectionofminibatches,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "dierencesinhyperparameters,ordierentoutcomesofnon-determinis ticimple-\nmentationsofneuralnetworksareoftenenoughtocausedierentmembersofthe\n2 5 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nensembletomakepartiallyindependenterrors.\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\nforscienticpapers,becauseanymachinelearningalgorithmcanbenetsubstan-\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ingoverdozensofmodels.ArecentprominentexampleistheNetixGrand\nPrize(Koren2009,).\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\nunitstotheneuralnetwork.\n7.12Dropout\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\npowerfulmethodofregularizingabroadfamilyofmodels.Toarstapproximation,\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\nofvetotenneuralnetworks ()usedsixtowintheILSVRC Szegedy etal.2014a\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\nneuralnetworks.\nSpecically,dropouttrainstheensembleconsistingofallsub-networksthat\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "asillustratedingure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\nanetransformationsandnonlinearities, wecaneectivelyremoveaunitfroma\nnetworkbymultiplyingitsoutputvaluebyzero.Thisprocedurerequiressome\nslightmodicationformodelssuchasradialbasisfunctionnetworks,whichtake\n2 5 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthedierencebetweentheunitsstateandsomereferencevalue.Here,wepresent\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\nbetriviallymodiedtoworkwithotheroperationsthatremoveaunitfromthe\nnetwork.\nRecallthattolearnwithbagging,wedene kdierentmodels,construct k\ndierentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "exponentiallylargenumberofneuralnetworks.Specically,totrainwithdropout,\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\nrandomlysampleadierentbinarymasktoapplytoalloftheinputandhidden\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\nincluded)isahyperparameter xedbeforetrainingbegins.Itisnotafunction",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "ofthecurrentvalueofthemodelparametersortheinputexample.Typically,\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\nwithdropout.\nMoreformally,supposethatamaskvectorspecieswhichunitstoinclude,\nand J( ,)denesthecostofthemodeldenedbyparametersandmask.\nThendropouttrainingconsistsinminimizing E  J( ,).Theexpectationcontains",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\nbysamplingvaluesof.\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\nparameters,witheachmodelinheritingadierentsubsetofparametersfromthe\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "caseofdropout,typicallymostmodelsarenotexplicitlytrainedatallusually,\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\naretheonlydierences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "theoriginaltrainingsetsampledwithreplacement.\n2 5 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1yy\nh 2 h 2\nx 1 x 1 x 2 x 2\nyy\nh 1 h 1\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2yy\nx 1 x 1 x 2 x 2yy\nh 2 h 2\nx 2 x 2\nyy\nh 1 h 1\nx 1 x 1yy\nh 1 h 1\nx 2 x 2yy\nh 2 h 2\nx 1 x 1yy\nx 1 x 1\nyy\nx 2 x 2yy\nh 2 h 2yy\nh 1 h 1yyB ase  ne t w or k\nE nse m bl e  of  s u b n e t w or k s",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "E nse m bl e  of  s u b n e t w or k s\nFigure7.6:Dropouttrainsanensembleconsistingofallsub-networksthatcanbe\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\nbydroppingoutdierentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "theinputtotheoutput.Thisproblembecomesinsignicantfornetworkswithwider\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\nsmaller.\n2 6 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n x 1 x 1\n x 1  x 1 x 1 x 1 x 2 x 2\nx 2 x 2  x 2  x 2h 1 h 1 h 2 h 2 h 1  h 1  h 2  h 2 h 1 h 1 h 2 h 2yyyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\npropagationwithdropout,werandomlysampleavectorwithoneentryforeachinput",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "orhiddenunitinthenetwork.Theentriesofarebinaryandaresampledindependently\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\ngureandrunningforwardpropagationthroughit. 7.6\n2 6 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\nitsmembers.Werefertothisprocessasinferenceinthiscontext.Sofar,our\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\nprobabilistic.Now,weassumethatthemodelsroleistooutputaprobability\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\nofthesedistributions,\n1\nkk",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ofthesedistributions,\n1\nkk\ni = 1p( ) i( ) y|x . (7.52)\nInthecaseofdropout,eachsub-modeldenedbymaskvectordenesaprob-\nabilitydistribution p( y ,|x).Thearithmeticmeanoverallmasksisgiven\nby\np p y , ()(|x) (7.53)\nwhere p()istheprobabilitydistributionthatwasusedtosampleattraining\ntime.\nBecausethissumincludesanexponentialnumberofterms,itisintractable\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\nofsimplication.Sofar,deepneuralnetsarenotknowntopermitanytractable",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "simplication.Instead,wecanapproximatetheinferencewithsampling,by\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\nsucienttoobtaingoodperformance.\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\nthearithmeticmeanoftheensemblememberspredicteddistributions.Warde-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "Farley2014etal.()presentargumentsandempiricalevidencethatthegeometric\nmeanperformscomparablytothearithmeticmeaninthiscontext.\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\ndistributiondeneddirectlybythegeometricmeanisgivenby",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": " p e nse m bl e( ) = y|x 2d\np y , (|x) (7.54)\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\ndistributionovertosimplifythepresentation,butnon-uniformdistributionsare\n2 6 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\np e nse m bl e( ) = y|x p e nse m bl e( ) y|x\ny p e nse m bl e( y|x). (7.55)\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\ni.Themotivationforthismodicationistocapturetherightexpectedvalueofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "outputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\nBecauseweusuallyuseaninclusionprobabilityof1\n2,theweightscalingrule\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\nmissingonaverage.\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\nclassierwithinputvariablesrepresentedbythevector: n v\nP y (= y | v) = softmax\nWv+b\ny. (7.56)\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\ninputwithabinaryvector: d",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "inputwithabinaryvector: d\nP y (= y | v;) = dsoftmax\nW( )+d vb\ny.(7.57)\nTheensemblepredictorisdenedbyre-normalizingthegeometricmeanoverall\nensemblememberspredictions:\nP e nse m bl e(= ) =y y| v P e nse m bl e(= )y y| v\ny P e nse m bl e(= y y| v)(7.58)\nwhere\n P e nse m bl e(= ) =y y| v2n\nd{} 0 1 ,nP y . (= y | v;)d (7.59)\n2 6 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nToseethattheweightscalingruleisexact,wecansimplify  P e nse m bl e:\n P e nse m bl e(= ) =y y| v2n\nd{} 0 1 ,nP y (= y | v;)d(7.60)\n= 2n\nd{} 0 1 ,nsoftmax (W( )+)d vby (7.61)\n= 2n\nd{} 0 1 ,nexp\nWy , :( )+d v b y\n\nyexp\nW\ny , :( )+d v b y (7.62)\n=2n\nd{} 0 1 ,nexp\nWy , :( )+d v b y\n2n\nd{} 0 1 ,n\nyexp\nW\ny , :( )+d v b y(7.63)\nBecause Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "areconstantwithrespectto: y\n P e nse m bl e(= ) y y| v2n\nd{} 0 1 ,nexp\nWy , :( )+d v b y\n(7.64)\n= exp\n1\n2n\nd{} 0 1 ,nW\ny , :( )+d v b y\n (7.65)\n= exp1\n2W\ny , : v+ b y\n. (7.66)\nSubstitutingthisbackintoequationweobtainasoftmaxclassierwithweights 7.58\n1\n2W.\nTheweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "mationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\nbetter(intermsofclassicationaccuracy)thanMonteCarloapproximations tothe\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\nthatsomemodelsobtainbetterclassicationaccuracyusingtwentysamplesand\n2 6 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\napproximationisproblem-dependent.\nSrivastava2014etal.()showedthatdropoutismoreeectivethanother\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,lter\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\nwithotherformsofregularizationtoyieldafurtherimprovement.\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "dropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\nexamples.\nAnothersignicantadvantageofdropoutisthatitdoesnotsignicantlylimit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\nthearchitectureofthemodel.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "thearchitectureofthemodel.\nThoughthecostper-stepofapplyingdropouttoaspecicmodelisnegligible,\nthecostofusingdropoutinacompletesystemcanbesignicant.Becausedropout\nisaregularizationtechnique,itreducestheeectivecapacityofamodel.Tooset\nthiseect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "datasets,regularizationconferslittlereductioningeneralization error.Inthese\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\nthebenetofregularization.\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\neective.Bayesianneuralnetworks(,)outperformdropoutonthe Neal1996\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedfeaturelearningcangainanadvantageoverdropout.\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\nisequivalentto L2weightdecay,withadierentweightdecaycoecientfor\n2 6 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\neachinputfeature.Themagnitudeofeachfeaturesweightdecaycoecientis\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\nmodels,dropoutisnotequivalenttoweightdecay.\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\napproachssuccess.Itisjustameansofapproximating thesumoverallsub-\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "convergencetimeduetothereducedstochasticityinthecomputationofthe\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\n(butalsomorecomputationally expensive)approximation totheaverageoverall\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\nproblems,buthasnotyetyieldedasignicantimprovementorbeenappliedtoa\nlargeproblem.\nJustasstochasticityisnotnecessarytoachievetheregularizingeect of",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "dropout,itisalsonotsucient.Todemonstratethis,Warde-Farley2014etal.()\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\nitsregularizingeect.Dropoutboostingtrainstheentireensembletojointly\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\ndropoutisanalogoustobagging,this approachisanalogoustoboosting.As\nintended,experimentswithdropoutboostingshowalmostnoregularizationeect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\ndropoutasrobustnesstonoise.Theregularizationeectofthebaggedensembleis\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\nperformwellindependently ofeachother.\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights.DropConnectisaspecialcaseof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodierent\nspatiallocationsofeachfeaturemap.Sofar,dropoutremainsthemostwidely\nusedimplicitensemblemethod.\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\nimplementsaformofbaggingwithparametersharing.Earlier,wedescribed\n2 6 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ndropoutasbagginganensembleofmodelsformedbyincludingorexcluding\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\ninclusionandexclusion.Inprinciple,anykindofrandommodicationisadmissible.\nInpractice,wemustchoosemodicationfamiliesthatneuralnetworksareable\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\napproximateinferencerule.Wecanthinkofanyformofmodicationparametrized",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "byavectorastraininganensembleconsistingof p( y ,|x)forallpossible\nvaluesof.Thereisnorequirementthathaveanitenumberofvalues.For\nexample,canbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\nweightsbyN( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\nE[] = 1thestandardnetworkautomatically implementsapproximate inference\nintheensemble,withoutneedinganyweightscaling.\nSofarwehavedescribeddropoutpurelyasameansofperformingecient,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "approximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\nswappinggenesbetweentwodierentorganisms,createsevolutionarypressurefor",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "genestobecomenotjustgood,buttobecomereadilyswappedbetweendierent\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropoutoersadditionalimprovementstogeneralization error",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "beyondthoseobtainedbyensemblesofindependentmodels.\nItisimportanttounderstandthatalargeportionofthepowerofdropout\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyndingthenose,\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "theimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\n2 6 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\ndistributionthatthemodelhasacquiredsofar.\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\nnoisewereadditivewithxedscale,thenarectiedlinearhiddenunit h iwith\naddednoise couldsimplylearntohave h ibecomeverylargeinordertomake\ntheaddednoise insignicantbycomparison.Multiplicativenoisedoesnotallow",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "suchapathologicalsolutiontothenoiserobustnessproblem.\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\noptimization, butthenoisecanhavearegularizingeect,andsometimesmakes\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\n7.13AdversarialTraining\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\nsearchforexamplesthatthemodelmisclassies. ()foundthat Szegedy etal.2014b\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\nproceduretosearchforaninputxnearadatapointxsuchthatthemodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "outputisverydierentatx.Inmanycases,xcanbesosimilartoxthata\nhumanobservercannottellthedierencebetweentheoriginalexampleandthe\nadversarialexample,butthenetworkcanmakehighlydierentpredictions.See\ngureforanexample.7.8\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\nthatarebeyondthescopeofthischapter.However,theyareinterestinginthe\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "testsetviaadversarialtrainingtrainingonadversariallyperturbedexamples\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarialexamplesisexcessivelinearity.Neuralnetworksarebuiltoutof\nprimarilylinearbuildingblocks.Insomeexperimentstheoverallfunctionthey\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n2 6 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n+ .007 =\nx sign( x J(x , , y))x+\nsign( x J(x , , y))\ny=panda nematodegibbon\nw/57.7%\ncondencew/8.2%\ncondencew/99.3%\ncondence\nFigure7.8:AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\nrespecttotheinput,wecanchangeGoogLeNetsclassicationoftheimage.Reproduced",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "withpermissionfrom (). Goodfellow e t a l .2014b\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\nifithasnumerousinputs.Ifwechangeeachinputby ,thenalinearfunction\nwithweightswcanchangebyasmuchas ||||w 1,whichcanbeaverylarge\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "introducingalocalconstancypriorintosupervisedneuralnets.\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\nfromnearlylineartonearlylocallyconstantandthushavetheexibilitytocapture\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\nmodelitselfassignssomelabel  y.Themodelslabel  ymaynotbethetruelabel,\nbutifthemodelishighquality,then yhasahighprobabilityofprovidingthe\ntruelabel.Wecanseekanadversarialexamplexthatcausestheclassierto\noutputalabel ywith y= y.Adversarialexamplesgeneratedusingnotthetrue\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "examples(Miyato2015etal.,).Theclassiermaythenbetrainedtoassignthe\nsamelabeltoxandx.Thisencouragestheclassiertolearnafunctionthatis\n2 6 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\nlies.Theassumptionmotivatingthisapproachisthatdierentclassesusuallylie\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\nfromoneclassmanifoldtoanotherclassmanifold.\n7.14TangentDistance,TangentProp,andManifold\nTangentClassier\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "section.5.11.3\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassier\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "onthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\nrespectivelybelong.Althoughthatmaybecomputationally dicult(itwould\nrequiresolvinganoptimization problem,tondthenearestpairofpointson M 1\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\nonetospecifythetangentvectors.\nInarelatedspirit,thetangentpropalgorithm( ,)(gure) Simardetal.19927.9\ntrainsaneuralnetclassierwithanextrapenaltytomakeeachoutput f(x)of\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\nsameclassconcentrate.Localinvarianceisachievedbyrequiring  x f(x)tobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "orthogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\nregularizationpenalty:\n() = f\ni\n( x f())xv( ) i2\n. (7.67)\n2 7 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\ntheeectoftransformationssuchastranslation,rotation,andscalinginimages.\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "butalsointhecontextofreinforcementlearning(,). Thrun1995\nTangentpropagation iscloselyrelatedtodatasetaugmentation.Inboth\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\nnetwork.Thedierenceisthatinthecaseofdatasetaugmentation, thenetworkis\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\nmorethananinnitesimalamountofthesetransformations.Tangentpropagation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\nthespeciedtransformation.Whilethisanalyticalapproac hisintellectually\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\ninnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\nlargerperturbations.Second,theinnitesimalapproachposesdicultiesformodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "basedonrectiedlinearunits.Thesemodelscanonlyshrinktheirderivatives\nbyturningunitsoorshrinkingtheirweights.Theyarenotabletoshrinktheir\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\nunitscan.Datasetaugmentation workswellwithrectiedlinearunitsbecause\ndierentsubsetsofrectiedunitscanactivatefordierenttransformedversionsof\neachoriginalinput.\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\nndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\naugmentationusingmanuallyspeciedtransformationsbothrequirethatthe\nmodelshouldbeinvarianttocertainspecieddirectionsofchangeintheinput.\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "invarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\nasdatasetaugmentationisthenon-innitesimalversionoftangentpropagation,\nadversarialtrainingisthenon-innitesimalversionofdoublebackprop.\nThemanifoldtangentclassier(,),eliminatestheneedto Rifaietal.2011c\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n2 7 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nx 1x 2N o r m a lT a ng e nt\nFigure7.9:Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\n1992 Rifai2011c )andmanifoldtangentclassier( e t a l .,),whichbothregularizethe\nclassieroutputfunction f(x).Eachcurverepresentsthemanifoldforadierentclass,\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\ntangentdirectionsandmanynormaldirections.Weexpecttheclassicationfunctionto\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\nclassierregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\nmanifold)whilethemanifoldtangentclassierestimatesthemanifoldtangentdirections\nbytraininganautoencodertotthetrainingdata.Theuseofautoencoderstoestimate\nmanifoldswillbedescribedinchapter.14\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiermakesuse\nofthistechniquetoavoidneedinguser-speciedtangentvectors.Asillustrated",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "ingure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\nandincludefactorsthatmustbelearnedbecausetheyareobject-specic(suchas\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassier\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassier\nasintangentprop(equation).7.67",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "asintangentprop(equation).7.67\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\n2 7 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\nthemeofmachinelearningisoptimization, describednext.\n2 7 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I I\nDeepLearningResearch\n486",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes\nt o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y .\nIn t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d\nlearning problems  how t o learn t o map one v e c t or t o another, given e nough\ne x amples of t he mapping.\nN ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "wis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle\nmis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples\nf r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial\napplications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d\ndata t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "t he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary\nf or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of\nt as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or\ns e mi-s up e r v is e d learning.\nMan y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d\nlearning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "deep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of\nt as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d\nlearning and s ome of t he p opular t hought ab out how w e c an make progres s in t his\n e ld.\nA c e ntral c aus e of t he diculties with uns upervis e d learning is t he high di-\nmens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "c hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l\nc h a l l e ng e r e gards generalization: t he num b e r of c ongurations we may wan t t o\ndis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and\nt his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e\nas s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or\nlearning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit\nprobabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially\nwith t he n um b e r of dimens ion s .",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "with t he n um b e r of dimens ion s .\nWith probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o\np e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on.\n I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19\nt he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "v ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r\n4 8 7",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs\nt o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization\nc ons t an t whic h s ums o v e r t he v alues of a and c .\n I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition\nf unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "bilit y f unctions c ome up in inference ( ab o v e ) as well as in learning.Man y\nprobabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately ,\nlearning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-\nr ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That\nc omputation is generally as intractable as c omputing t he partition f unction\nits e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "t e n us e d t o deal with t he partition f unction ( c omputing it or its gradient).\nU nfortun ately , MCMC metho ds s uer when t he mo des of t he mo del dis t r ibu-\nt ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces\n( s e c t ion ) . 17.5\nOne wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,\nand many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "b o ok. A nother interes t in g w ay ,als o dis c us s e d here,w ould b e t o av oid t hes e\nin t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire\ns uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n\nprop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary\napproac hes t o generativ e mo deling are dis c us s e d in c hapter . 20",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "P art is t he mos t imp ortant f or a r e s e arc hers om e one who wan t s t o un- I I I\nders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he  e ld of deep\nlearning, and pus h t he  e ld f orward t ow ards t r ue articial intelligence.\n4 8 8",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  }
]


============ ./process_document.py ============
# process_documents.py

import os
import io # For BytesIO
import logging # For logging
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any # Added Any, Tuple
from PIL import Image, ImageFilter
import pytesseract
import fitz # PyMuPDF
import pypdf # For PDF parsing
from docx import Document as DocxDocument
from pptx import Presentation
import PyPDF2 # For basic PDF metadata
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument

# --- Logger Setup ---
# Configure logging to output to console. You can customize this for file logging.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# --- NLTK Downloads (Run once if not already downloaded) ---
# Moved to a function for clarity, can be called at app startup
def download_nltk_resources():
    """Downloads necessary NLTK resources if not already present."""
    nltk_resources = {
        'punkt': 'tokenizers/punkt',
        'stopwords': 'corpora/stopwords',
        'wordnet': 'corpora/wordnet',
        'omw-1.4': 'corpora/omw-1.4' # WordNet 1.4 ontology
    }
    for res_name, res_path in nltk_resources.items():
        try:
            nltk.data.find(res_path)
            logger.info(f"NLTK resource '{res_name}' found.")
        except nltk.downloader.DownloadError:
            logger.info(f"NLTK resource '{res_name}' not found. Downloading...")
            nltk.download(res_name)
            logger.info(f"NLTK resource '{res_name}' downloaded.")

# --- SpaCy Model Download (Run once if not already downloaded) ---
def download_spacy_model(model_name: str = "en_core_web_sm"):
    """Downloads a spaCy model if not already present."""
    try:
        spacy.load(model_name)
        logger.info(f"SpaCy model '{model_name}' found.")
    except OSError:
        logger.warning(f"SpaCy model '{model_name}' not found. Attempting download...")
        try:
            os.system(f"python -m spacy download {model_name}")
            logger.info(f"SpaCy model '{model_name}' downloaded successfully. Please restart the application if it was just installed.")
            # Note: A restart might be needed for the new model to be picked up by the current Python process
            # For production, ensure models are pre-installed in the environment.
        except Exception as e:
            logger.error(f"Failed to download spaCy model '{model_name}': {e}")
            logger.error("Please install it manually: python -m spacy download en_core_web_sm")


# --- Perform Resource Downloads (Call once at application startup) ---
# In a real application, this would be part of an initialization script or a startup hook.
download_nltk_resources()
download_spacy_model()


# --- Global NLP Models (Load once) ---
logger.info("Loading global NLP models...")
try:
    nlp_core = spacy.load("en_core_web_sm")
    stop_words = set(stopwords.words('english'))
    porter_stemmer = PorterStemmer() # Not used in current clean_and_normalize_text, but kept for completeness
    wordnet_lemmatizer = WordNetLemmatizer()
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    logger.info("Global NLP models loaded successfully.")
except Exception as e:
    logger.error(f"Fatal error loading global NLP models: {e}")
    # In a real app, you might want to exit or prevent further operations if models can't load.
    raise  # Re-raise the exception to halt execution if critical models fail

# --- Import pdfplumber after other initializations ---
# pdfplumber can sometimes have import-time issues if dependencies are not perfectly met.
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
    logger.info("pdfplumber imported successfully.")
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("pdfplumber not found. Rich PDF content extraction (tables, layout) will be limited. Install with: pip install pdfplumber")


# --- File Parsing Functions ---

def parse_pdf_simple(file_path: str) -> Optional[str]:
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf:
        logger.error("pypdf library not found. PDF parsing will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        logger.info(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        logger.info(f"Extracted {len(text_content)} characters from PDF (pypdf).")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)} with pypdf: {e}")
        return None

def parse_docx(file_path: str) -> Optional[str]:
    """Extracts text content from a DOCX file."""
    if not DocxDocument:
        logger.error("python-docx library not found. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        logger.info(f"Extracted {len(text_content)} characters from DOCX: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"DOCX file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}")
        return None

def parse_txt(file_path: str) -> Optional[str]:
    """Reads text content from a TXT file (or similar plain text)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text_content = f.read()
        logger.info(f"Read {len(text_content)} characters from TXT file: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"TXT file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}")
        return None

# Conditional PPTX parsing
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path: str) -> Optional[str]:
        """Extracts text content from a PPTX file."""
        text_content = ""
        try:
            prs = Presentation(file_path)
            for slide_num, slide in enumerate(prs.slides):
                slide_text_parts = []
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            slide_text_parts.append(shape_text)
                if slide_text_parts:
                    text_content += "\n".join(slide_text_parts) + "\n\n" # Newline between shapes, double for new slide
            logger.info(f"Extracted {len(text_content)} characters from PPTX: {os.path.basename(file_path)}")
            return text_content.strip() if text_content.strip() else None
        except FileNotFoundError:
            logger.error(f"PPTX file not found: {file_path}")
            return None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}")
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped. Install with: pip install python-pptx")
    def parse_pptx(file_path: str) -> Optional[str]: # type: ignore
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path: str) -> Optional[str]:
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.info(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf_simple(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path)
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']:
        return parse_txt(file_path)
    elif ext == '.doc':
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

# --- Core Processing Functions ---

def extract_raw_content(file_path: str) -> Dict[str, Any]:
    """
    1. Extracts raw text, tables, images, and basic layout info from a document.
    Determines if the document (if PDF) is likely scanned.
    """
    logger.info(f"Starting raw content extraction for: {os.path.basename(file_path)}")
    text_content: str = ""
    tables: List[Any] = [] # Can be list of DataFrames or list of lists
    images: List[Image.Image] = []
    layout_info: List[Dict[str, Any]] = []
    is_scanned: bool = False
    file_extension: str = os.path.splitext(file_path)[1].lower()

    initial_text = parse_file(file_path) # Uses the simple parsers for a baseline
    if initial_text:
        text_content = initial_text
    else:
        logger.warning(f"Could not extract initial text from {file_path}. Proceeding with empty text.")
        # Still return structure, even if empty
        return {
            'text_content': "", 'tables': tables, 'images': images,
            'layout_info': layout_info, 'is_scanned': False, 'file_type': file_extension
        }

    # If it's a PDF, try to extract richer content
    if file_extension == '.pdf':
        if not PDFPLUMBER_AVAILABLE:
            logger.warning("pdfplumber is not available. Rich PDF extraction (tables, layout) will be skipped.")
        else:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_check_parts = []
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            pdfplumber_text_check_parts.append(page_text)
                    pdfplumber_text_check = "".join(pdfplumber_text_check_parts)

                    # Heuristic for scanned PDF: if pdfplumber extracts very little text
                    # compared to page count or absolute minimum.
                    # This assumes non-scanned PDFs have a reasonable amount of selectable text.
                    # Adjust thresholds as needed.
                    min_chars_per_page = 50 # Heuristic: expect at least this many chars per page if not scanned
                    absolute_min_chars = 200 # Heuristic: expect at least this many chars total if not scanned

                    if len(pdf.pages) > 0 and \
                       (len(pdfplumber_text_check) < min_chars_per_page * len(pdf.pages) and \
                        len(pdfplumber_text_check) < absolute_min_chars):
                        is_scanned = True
                        logger.info(f"PDF {os.path.basename(file_path)} detected as potentially scanned based on low text content from pdfplumber.")

                    full_pdfplumber_text = ""
                    for page_num, page in enumerate(pdf.pages):
                        page_text_with_layout = page.extract_text(x_tolerance=1, y_tolerance=1, layout=False) # layout=True can be slow
                        if page_text_with_layout:
                            full_pdfplumber_text += page_text_with_layout + "\n"

                        # Capture basic layout info (bounding boxes of text lines/words)
                        # This can be very verbose, enable if detailed layout is critical
                        # for char_obj in page.chars:
                        #     layout_info.append({
                        #         'char': char_obj['text'],
                        #         'bbox': (char_obj['x0'], char_obj['top'], char_obj['x1'], char_obj['bottom']),
                        #         'page': page_num
                        #     })

                        # Extract tables using pdfplumber
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data in page_tables_data:
                                if table_data and len(table_data) > 0: # Ensure there's data
                                    # Check if first row looks like a header (heuristic)
                                    if len(table_data) > 1 and all(isinstance(cell, str) for cell in table_data[0]):
                                        try:
                                            tables.append(pd.DataFrame(table_data[1:], columns=table_data[0]))
                                        except Exception as df_err:
                                            logger.warning(f"Could not convert table to DataFrame on page {page_num} (using header): {df_err}. Appending raw list.")
                                            tables.append(table_data)
                                    else: # No clear header or single row table
                                        tables.append(table_data) # Append raw list
                    
                    # Prioritize pdfplumber's text if it's richer
                    if len(full_pdfplumber_text.strip()) > len(text_content):
                        logger.info("Using text extracted by pdfplumber as it's more comprehensive.")
                        text_content = full_pdfplumber_text.strip()

            except Exception as e:
                logger.warning(f"Error during rich PDF content extraction with pdfplumber for {os.path.basename(file_path)}: {e}")
                # Fallback to initial_text if pdfplumber fails badly

        # Extract images using PyMuPDF (fitz) - generally more robust for images
        try:
            doc = fitz.open(file_path)
            for page_idx in range(len(doc)):
                for img_info in doc.get_page_images(page_idx):
                    xref = img_info[0] # XREF of the image
                    try:
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        pil_image = Image.open(io.BytesIO(image_bytes))
                        images.append(pil_image)
                    except Exception as img_err:
                        logger.warning(f"Could not open image xref {xref} from page {page_idx} of {os.path.basename(file_path)}: {img_err}")
            doc.close()
            logger.info(f"Extracted {len(images)} images using PyMuPDF from {os.path.basename(file_path)}.")
        except Exception as e:
            logger.warning(f"Error extracting images with PyMuPDF from {os.path.basename(file_path)}: {e}")

    logger.info(f"Raw content extraction complete for {os.path.basename(file_path)}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {
        'text_content': text_content,
        'tables': tables,
        'images': images,
        'layout_info': layout_info, # Often empty for non-PDFs or if detailed layout extraction is off
        'is_scanned': is_scanned,   # Primarily relevant for PDFs
        'file_type': file_extension
    }

def perform_ocr(image_data: List[Image.Image]) -> str:
    """
    2. Processes images to extract text using OCR (pytesseract).
    """
    if not image_data:
        logger.info("No images provided for OCR.")
        return ""

    logger.info(f"Performing OCR on {len(image_data)} image(s).")
    ocr_text_parts: List[str] = []
    for i, img in enumerate(image_data):
        try:
            # Basic preprocessing
            img_gray = img.convert('L')
            # img_binary = img_gray.point(lambda x: 0 if x < 150 else 255, '1') # Example threshold
            # img_filtered = img_binary.filter(ImageFilter.MedianFilter(size=3)) # Example filter

            # Tesseract works well with grayscale. Extensive preprocessing can sometimes harm.
            # Test various preprocessing steps for your specific image types.
            text = pytesseract.image_to_string(img_gray) # Using grayscale
            if text.strip():
                ocr_text_parts.append(text.strip())
            logger.debug(f"OCR successful for image {i+1}.")
        except pytesseract.TesseractNotFoundError:
            logger.error("Tesseract is not installed or not in your PATH. OCR will fail.")
            # This is a critical error, so perhaps re-raise or handle at a higher level
            raise
        except Exception as e:
            logger.error(f"Error during OCR for image {i+1}: {e}")
    
    full_ocr_text = "\n\n".join(ocr_text_parts)
    logger.info(f"OCR process completed. Extracted {len(full_ocr_text)} characters.")
    return full_ocr_text

def clean_and_normalize_text(text: str) -> str:
    """
    3. Cleans and normalizes text for NLP readiness.
    """
    if not text:
        logger.info("No text provided for cleaning and normalization.")
        return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")

    # 1. Noise Removal
    text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags more robustly
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # Remove URLs
    text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses
    # Keep essential punctuation for sentence structure, remove others
    text = re.sub(r'[^a-zA-Z0-9\s.,!?-]', '', text) # Keep alphanumeric, spaces, and basic punctuation
    text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

    # 2. Case Normalization
    text = text.lower()

    # 3. Tokenization (NLTK)
    try:
        tokens = nltk.word_tokenize(text)
    except Exception as e:
        logger.error(f"NLTK word_tokenize failed: {e}. Falling back to simple split.")
        tokens = text.split()


    # 4. Stop Word Removal
    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1] # Also remove single chars

    # 5. Lemmatization
    try:
        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in filtered_tokens]
    except Exception as e:
        logger.error(f"WordNet lemmatization failed: {e}. Using filtered tokens directly.")
        lemmatized_tokens = filtered_tokens

    cleaned_text = " ".join(lemmatized_tokens)
    logger.info(f"Text cleaning and normalization complete. Final length: {len(cleaned_text)}")
    return cleaned_text

def reconstruct_layout(original_text: str, layout_info: List[Dict[str, Any]], tables: List[Any], file_type: str) -> str:
    """
    4. Integrates tables into text. Advanced layout reconstruction is complex and
       currently simplified here. Focuses on table integration.
    """
    logger.info("Starting layout reconstruction (primarily table integration).")
    processed_text = original_text

    # De-hyphenation (simple regex-based approach)
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', processed_text)
    processed_text = re.sub(r'(\w+)-\s*(\w+)', r'\1\2', processed_text) # For hyphens not at line end

    # Integrate tables as Markdown strings
    table_text_representations: List[str] = []
    if tables:
        logger.info(f"Integrating {len(tables)} tables into the text.")
        for i, table_content in enumerate(tables):
            table_header = f"\n\n--- Table {i+1} Content ---\n"
            table_footer = "\n--- End of Table Content ---\n"
            table_md = ""
            if isinstance(table_content, pd.DataFrame):
                try:
                    table_md = table_content.to_markdown(index=False)
                except Exception as e:
                    logger.warning(f"Could not convert DataFrame table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            elif isinstance(table_content, list): # Raw list of lists from pdfplumber
                # Attempt to format as a simple markdown table
                try:
                    if table_content and isinstance(table_content[0], list):
                        # Assuming first row could be header
                        header = table_content[0]
                        data_rows = table_content[1:]
                        table_md = "| " + " | ".join(map(str, header)) + " |\n"
                        table_md += "| " + " | ".join(["---"] * len(header)) + " |\n"
                        for row in data_rows:
                            table_md += "| " + " | ".join(map(str, row)) + " |\n"
                    else: # Flat list or other structure
                        table_md = "\n".join(map(str,table_content))

                except Exception as e:
                    logger.warning(f"Could not format list-based table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            else:
                table_md = str(table_content)

            table_text_representations.append(table_header + table_md + table_footer)
        
        # Append all table representations at the end of the document text
        processed_text += "\n\n" + "\n\n".join(table_text_representations)
    
    logger.info("Layout reconstruction (table integration) complete.")
    return processed_text.strip()


def extract_metadata(file_path: str, processed_text: str, file_type_from_extraction: str) -> Dict[str, Any]:
    """
    5. Extracts standard and content-based metadata.
    """
    logger.info(f"Starting metadata extraction for: {os.path.basename(file_path)}")
    metadata: Dict[str, Any] = {}
    file_extension = os.path.splitext(file_path)[1].lower()
    if not file_extension: # If somehow extension is empty, use the one from raw content
        file_extension = file_type_from_extraction

    # Common metadata
    metadata['file_name'] = os.path.basename(file_path)
    metadata['file_path'] = file_path # Store full path for reference
    metadata['file_type'] = file_extension
    try:
        metadata['file_size_bytes'] = os.path.getsize(file_path)
        metadata['creation_time_epoch'] = os.path.getctime(file_path)
        metadata['modification_time_epoch'] = os.path.getmtime(file_path)
    except FileNotFoundError:
        logger.error(f"File not found during metadata extraction (size/time): {file_path}")
    except Exception as e:
        logger.warning(f"Could not get file system metadata for {file_path}: {e}")

    # File-type specific metadata
    page_count_approx = 0
    if file_extension == '.pdf':
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                doc_info = reader.metadata
                if doc_info:
                    metadata['pdf_title'] = doc_info.get('/Title', '').strip()
                    metadata['pdf_author'] = doc_info.get('/Author', '').strip()
                    # Add more as needed from PyPDF2
                page_count_approx = len(reader.pages)
        except Exception as e:
            logger.warning(f"Error extracting PDF-specific metadata (PyPDF2): {e}")
    elif file_extension == '.docx':
        try:
            doc = DocxDocument(file_path)
            # python-docx doesn't easily give page count. Paragraph count is a rough proxy.
            page_count_approx = sum(1 for _ in doc.paragraphs) # Number of paragraphs
            # Core properties can be accessed if set
            # metadata['docx_author'] = doc.core_properties.author
            # metadata['docx_title'] = doc.core_properties.title
        except Exception as e:
            logger.warning(f"Error extracting DOCX-specific metadata: {e}")
    elif file_extension == '.pptx' and PPTX_SUPPORTED:
        try:
            prs = Presentation(file_path)
            page_count_approx = len(prs.slides) # Number of slides
        except Exception as e:
            logger.warning(f"Error extracting PPTX-specific metadata: {e}")
    
    if not page_count_approx and processed_text: # Fallback for TXT or if above failed
        page_count_approx = processed_text.count('\n\n') + 1 # Approx by double newlines for "pages"

    metadata['approx_page_count'] = page_count_approx

    # Content-based metadata using spaCy NER
    if processed_text and nlp_core:
        logger.info("Extracting named entities using spaCy...")
        try:
            doc_nlp = nlp_core(processed_text[:1000000]) # Limit NER processing length for performance
            entities: Dict[str, List[str]] = {label: [] for label in nlp_core.pipe_labels.get("ner", [])}
            for ent in doc_nlp.ents:
                if ent.label_ not in entities: # Should not happen if initialized above
                    entities[ent.label_] = []
                entities[ent.label_].append(ent.text)
            # Filter out empty entity types
            metadata['named_entities'] = {k: v for k, v in entities.items() if v}
            logger.info(f"Extracted {sum(len(v) for v in metadata['named_entities'].values())} named entities.")
        except Exception as e:
            logger.error(f"Error during spaCy NER processing: {e}")
            metadata['named_entities'] = {}
    else:
        metadata['named_entities'] = {}
        logger.info("Skipping NER (no text or nlp_core not available).")

    logger.info(f"Metadata extraction complete for {os.path.basename(file_path)}.")
    return metadata

def chunk_text(text_to_chunk: str, base_metadata: Dict[str, Any], chunk_size: int = 512, chunk_overlap: int = 64) -> List[Dict[str, Any]]:
    """
    6. Divides text into chunks with associated metadata using RecursiveCharacterTextSplitter.
    """
    if not text_to_chunk or not isinstance(text_to_chunk, str):
        logger.warning(f"Invalid or empty text input for chunking (file: {base_metadata.get('file_name', 'unknown')}). Skipping chunking.")
        return []

    logger.info(f"Starting text chunking for {base_metadata.get('file_name', 'unknown')}. Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], # More robust separators
        keep_separator=True # Keep separators to maintain context
    )

    # Langchain's create_documents expects a list of texts, or can take metadata per document
    # Here, we process one large text derived from the whole document.
    try:
        langchain_documents: List[LangchainDocument] = text_splitter.create_documents([text_to_chunk])
    except Exception as e:
        logger.error(f"Error creating documents with RecursiveCharacterTextSplitter for {base_metadata.get('file_name', 'unknown')}: {e}")
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    file_name_prefix = os.path.splitext(base_metadata.get('file_name', 'doc'))[0]

    for i, doc_chunk in enumerate(langchain_documents):
        chunk_metadata = base_metadata.copy() # Start with base document metadata
        chunk_metadata['chunk_id'] = f"{file_name_prefix}_chunk_{i:04d}" # Padded for sorting
        chunk_metadata['chunk_index'] = i
        chunk_metadata['chunk_char_count'] = len(doc_chunk.page_content)
        
        # Add any metadata specific to this chunk if LangchainDocument provides it
        # (e.g., if splitting by pages from a PDF loader, it might have 'page_number')
        for key, value in doc_chunk.metadata.items():
            if key not in chunk_metadata:
                chunk_metadata[key] = value

        output_chunks.append({
            'id': chunk_metadata['chunk_id'],
            'text_content': doc_chunk.page_content,
            'metadata': chunk_metadata # All merged metadata
        })
    
    logger.info(f"Split '{base_metadata.get('file_name', 'unknown')}' into {len(output_chunks)} chunks.")
    return output_chunks

def generate_embeddings(chunks_to_embed: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    7. Generates vector embeddings for each text chunk.
    """
    if not chunks_to_embed:
        logger.info("No chunks provided for embedding generation.")
        return []

    logger.info(f"Starting embedding generation for {len(chunks_to_embed)} chunks.")
    texts_to_embed = [chunk['text_content'] for chunk in chunks_to_embed if chunk.get('text_content')]
    
    if not texts_to_embed:
        logger.warning("No text content found in chunks to embed.")
        return chunks_to_embed # Return original chunks, possibly without embeddings

    try:
        embeddings = embedding_model.encode(texts_to_embed, show_progress_bar=False) # Set to True for console progress
        
        # Assign embeddings back to the corresponding chunks
        # This assumes a 1:1 correspondence and order is maintained.
        # If some chunks had no text, the embedding list will be shorter.
        chunk_idx_with_text = 0
        for i, chunk in enumerate(chunks_to_embed):
            if chunk.get('text_content'):
                if chunk_idx_with_text < len(embeddings):
                    chunk['embedding_vector'] = embeddings[chunk_idx_with_text].tolist()
                    chunk_idx_with_text += 1
                else:
                    logger.warning(f"Mismatch in embedding count for chunk {chunk.get('id')}. No embedding assigned.")
                    chunk['embedding_vector'] = None # Or an empty list
            else:
                chunk['embedding_vector'] = None # Or an empty list for chunks with no text

        logger.info(f"Embeddings generated for {chunk_idx_with_text} chunks.")
        return chunks_to_embed
    except Exception as e:
        logger.error(f"Error during embedding generation: {e}")
        # Optionally, mark chunks as failed or return them without embeddings
        for chunk in chunks_to_embed:
            chunk['embedding_vector'] = None # Indicate failure
        return chunks_to_embed


# --- Main Orchestration Function ---
def process_uploaded_document(file_path: str, username: str) -> Dict[str, Any]:
    """
    Orchestrates the entire document processing pipeline for a single uploaded file.
    Takes username for logging/auditing purposes.
    Returns a dictionary with 'status' and 'data' or 'message'.
    """
    logger.info(f"User '{username}' initiated processing for file: {file_path}")

    if not os.path.exists(file_path):
        logger.error(f"File not found: {file_path}")
        return {"status": "error", "message": f"File not found: {file_path}", "file_path": file_path}

    try:
        # 1. Extract Raw Content
        logger.info("Step 1: Extracting Raw Content...")
        raw_content = extract_raw_content(file_path)
        logger.info(f"Step 1: Raw content extracted. Text length: {len(raw_content.get('text_content',''))}, Scanned: {raw_content.get('is_scanned')}")

        # 2. Perform OCR if necessary
        ocr_text_contribution = ""
        if raw_content['file_type'] == '.pdf' and raw_content['is_scanned']:
            if raw_content['images']:
                logger.info("Step 2: Detected scanned PDF with images, performing OCR...")
                ocr_text_contribution = perform_ocr(raw_content['images'])
                # Combine OCR text: prioritize if it's substantial, otherwise append.
                # This logic can be refined based on OCR quality.
                if len(ocr_text_contribution) > 0.5 * len(raw_content['text_content']): # If OCR text is significant
                     raw_content['text_content'] = ocr_text_contribution + "\n\n" + raw_content['text_content']
                else: # Append if OCR text is smaller or complementary
                     raw_content['text_content'] += "\n\n" + ocr_text_contribution
                logger.info(f"Step 2: OCR completed. Total text length after OCR: {len(raw_content['text_content'])}")
            else:
                logger.warning("Step 2: Detected scanned PDF but no images found for OCR. Proceeding with available text.")
        else:
            logger.info(f"Step 2: OCR not required or not applicable for file type: {raw_content['file_type']}")

        if not raw_content['text_content'] and not ocr_text_contribution: # Check after potential OCR
            logger.warning(f"No text content could be extracted from {os.path.basename(file_path)} after raw extraction and OCR.")
            # Depending on requirements, you might return an error or empty data
            # For now, we proceed, but chunking/embedding will yield empty results.

        # 3. Clean and Normalize Text
        logger.info("Step 3: Cleaning and Normalizing Text...")
        cleaned_text = clean_and_normalize_text(raw_content['text_content'])
        logger.info(f"Step 3: Text cleaned and normalized. Length: {len(cleaned_text)}")

        # 4. Reconstruct Layout and Integrate Tables
        logger.info("Step 4: Reconstructing Layout and Integrating Tables...")
        structurally_coherent_text = reconstruct_layout(
            cleaned_text,
            raw_content['layout_info'],
            raw_content['tables'],
            raw_content['file_type']
        )
        logger.info(f"Step 4: Layout reconstructed. Final text length for chunking: {len(structurally_coherent_text)}")

        # 5. Extract Metadata
        logger.info("Step 5: Extracting Metadata...")
        # Pass the most complete text and original file_type for robust metadata
        metadata = extract_metadata(file_path, structurally_coherent_text, raw_content['file_type'])
        metadata['processing_user'] = username # Add username to metadata
        logger.info("Step 5: Metadata extracted.")
        logger.debug(f"Extracted metadata keys: {list(metadata.keys())}")


        # 6. Chunk Text
        logger.info("Step 6: Chunking Text...")
        if not structurally_coherent_text:
             logger.warning("No text available for chunking. Skipping chunking and embedding.")
             chunks = []
        else:
            chunks = chunk_text(structurally_coherent_text, metadata, chunk_size=512, chunk_overlap=64)
        logger.info(f"Step 6: Text chunked into {len(chunks)} chunks.")

        # 7. Generate Embeddings
        logger.info("Step 7: Generating Embeddings...")
        final_chunks_with_embeddings = generate_embeddings(chunks)
        logger.info(f"Step 7: Embeddings generated for {sum(1 for c in final_chunks_with_embeddings if c.get('embedding_vector'))} chunks.")

        logger.info(f"Successfully finished processing for file: {file_path} by user '{username}'")
        return {"status": "success", "data": final_chunks_with_embeddings, "file_path": file_path}

    except pytesseract.TesseractNotFoundError:
        logger.critical("Tesseract is not installed or not found in PATH. OCR-dependent processing cannot continue.")
        return {"status": "error", "message": "Tesseract (OCR engine) not found. Please install it.", "file_path": file_path}
    except Exception as e:
        logger.error(f"Critical error during processing of {file_path} for user '{username}': {e}", exc_info=True)
        return {"status": "error", "message": f"An unexpected error occurred: {str(e)}", "file_path": file_path}


# --- Example Usage ---
# if __name__ == "__main__":
    logger.info("Starting example document processing...")

    # Create dummy files for demonstration (or use paths to your actual test files)
    # For robust testing, use real PDF, DOCX, PPTX files.
    DUMMY_FILES_DIR = "test_docs"
    os.makedirs(DUMMY_FILES_DIR, exist_ok=True)

    sample_txt_content = "This is a sample text document for testing the processing pipeline.\nIt contains multiple sentences and paragraphs.\nNatural language processing is fascinating. We will chunk this text and generate embeddings."
    sample_txt_path = os.path.join(DUMMY_FILES_DIR, "sample.txt")
    with open(sample_txt_path, "w", encoding="utf-8") as f:
        f.write(sample_txt_content)
    
    # A very basic PDF (for a real test, use a proper PDF with text, images, tables)
    sample_pdf_path = os.path.join(DUMMY_FILES_DIR, "sample.pdf")
    try:
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import letter
        c = canvas.Canvas(sample_pdf_path, pagesize=letter)
        c.drawString(72, 720, "Hello from a ReportLab PDF!")
        c.drawString(72, 700, "This is the first page with some text.")
        c.showPage()
        c.drawString(72, 720, "This is the second page.")
        # Add a dummy table-like text
        c.drawString(72, 650, "Header1,Header2")
        c.drawString(72, 635, "Data1A,Data1B")
        c.drawString(72, 620, "Data2A,Data2B")
        c.save()
        logger.info(f"Created dummy PDF: {sample_pdf_path}")
    except ImportError:
        logger.warning("ReportLab not found. Cannot create dummy PDF. Please install it (`pip install reportlab`) or provide your own PDF.")
        # Create a minimal text-based PDF if reportlab is not available
        if not os.path.exists(sample_pdf_path): # Only if reportlab failed and file doesn't exist
            with open(sample_pdf_path, "w", encoding="utf-8") as f:
                 f.write("%PDF-1.4\n1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n3 0 obj<</Type/Page/MediaBox[0 0 612 792]/Contents 4 0 R/Parent 2 0 R/Resources<<>>>>endobj\n4 0 obj<</Length 40>>stream\nBT /F1 24 Tf 100 700 Td (Minimal PDF) Tj ET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f\n0000000009 00000 n\n0000000058 00000 n\n0000000117 00000 n\n0000000200 00000 n\ntrailer<</Size 5/Root 1 0 R>>\nstartxref\n245\n%%EOF")
            logger.info(f"Created minimal fallback PDF: {sample_pdf_path}")


    # Example: Processing a TXT file
    username = "test_user"
    if os.path.exists(sample_txt_path):
        logger.info(f"\n--- Processing TXT file: {sample_txt_path} ---")
        result_txt = process_uploaded_document(sample_txt_path, username)
        if result_txt["status"] == "success":
            logger.info(f"Successfully processed {sample_txt_path}. Chunks: {len(result_txt['data'])}")
            if result_txt['data']:
                logger.info("--- Sample of TXT Output (First Chunk) ---")
                first_chunk = result_txt['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                # logger.info(f"Metadata: {first_chunk.get('metadata')}") # Can be verbose
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_txt_path}: {result_txt['message']}")
    else:
        logger.warning(f"Sample TXT file not found at {sample_txt_path}, skipping TXT test.")

    # Example: Processing a PDF file
    if os.path.exists(sample_pdf_path):
        logger.info(f"\n--- Processing PDF file: {sample_pdf_path} ---")
        result_pdf = process_uploaded_document(sample_pdf_path, username)
        if result_pdf["status"] == "success":
            logger.info(f"Successfully processed {sample_pdf_path}. Chunks: {len(result_pdf['data'])}")
            if result_pdf['data']:
                logger.info("--- Sample of PDF Output (First Chunk) ---")
                first_chunk = result_pdf['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                logger.info(f"File type from metadata: {first_chunk.get('metadata', {}).get('file_type')}")
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_pdf_path}: {result_pdf['message']}")
    else:
        logger.warning(f"Sample PDF file not found at {sample_pdf_path}, skipping PDF test.")

    # Clean up dummy files (optional)
    # logger.info(f"Cleaning up dummy files in {DUMMY_FILES_DIR}...")
    # for f_name in os.listdir(DUMMY_FILES_DIR):
    #     os.remove(os.path.join(DUMMY_FILES_DIR, f_name))
    # os.rmdir(DUMMY_FILES_DIR)
    # logger.info("Dummy files cleaned up.")


============ ./Readne.txt ============
conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app




============ ./requirements.txt ============
Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
re
typing
PIL # For image processing
pytesseract # OCR
pillow






============ ./__init__.py ============




```

`server/rag_service/config.py`

```python
# server/config.py
import os
import logging

#  Logging Configuration 
logger = logging.getLogger(__name__)
LOGGING_LEVEL_NAME = os.getenv('LOGGING_LEVEL', 'INFO').upper()
LOGGING_LEVEL      = getattr(logging, LOGGING_LEVEL_NAME, logging.INFO)
LOGGING_FORMAT     = '%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s'

# === Base Directory ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
logger.info(f"[Config] Base Directory: {BASE_DIR}")



def setup_logging():
    """Configure logging across the app."""
    root_logger = logging.getLogger()
    if not root_logger.handlers:  # prevent duplicate handlers
        handler = logging.StreamHandler()
        formatter = logging.Formatter(LOGGING_FORMAT)
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(LOGGING_LEVEL)

    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("faiss.loader").setLevel(logging.WARNING)
    logging.getLogger(__name__).info(f"Logging initialized at {LOGGING_LEVEL_NAME}")

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password") # IMPORTANT: Change this default or use ENV VAR!
NEO4J_DATABASE = os.getenv("NEO4J_DATABASE", "neo4j")
# === Embedding Model Configuration ===
DEFAULT_DOC_EMBED_MODEL = 'mixedbread-ai/mxbai-embed-large-v1'
DOCUMENT_EMBEDDING_MODEL_NAME = os.getenv('DOCUMENT_EMBEDDING_MODEL_NAME', DEFAULT_DOC_EMBED_MODEL)
MAX_TEXT_LENGTH_FOR_NER = int(os.getenv("MAX_TEXT_LENGTH_FOR_NER", 500000))
logger.info(f"[Config] Document Embedding Model: {DOCUMENT_EMBEDDING_MODEL_NAME}")

# Model dimension mapping
_MODEL_TO_DIM_MAPPING = {
    'mixedbread-ai/mxbai-embed-large-v1': 1024,
    'BAAI/bge-large-en-v1.5': 1024,
    'all-MiniLM-L6-v2': 384,
    'sentence-transformers/all-mpnet-base-v2': 768,
}
_FALLBACK_DIM = 768

DOCUMENT_VECTOR_DIMENSION = int(os.getenv(
    "DOCUMENT_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(DOCUMENT_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))
logger.info(f"[Config] Document Vector Dimension: {DOCUMENT_VECTOR_DIMENSION}")

# === AI Core Chunking Config ===
AI_CORE_CHUNK_SIZE = int(os.getenv("AI_CORE_CHUNK_SIZE", 512))
AI_CORE_CHUNK_OVERLAP = int(os.getenv("AI_CORE_CHUNK_OVERLAP", 100))
logger.info(f"[Config] Chunk Size: {AI_CORE_CHUNK_SIZE}, Overlap: {AI_CORE_CHUNK_OVERLAP}")

# === SpaCy Configuration ===
SPACY_MODEL_NAME = os.getenv('SPACY_MODEL_NAME', 'en_core_web_sm')
logger.info(f"[Config] SpaCy Model: {SPACY_MODEL_NAME}")

# === Qdrant Configuration ===
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
QDRANT_COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "my_qdrant_rag_collection")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
QDRANT_URL = os.getenv("QDRANT_URL", None)

QDRANT_COLLECTION_VECTOR_DIM = DOCUMENT_VECTOR_DIMENSION
logger.info(f"[Config] Qdrant Vector Dimension: {QDRANT_COLLECTION_VECTOR_DIM}")

# === Query Embedding Configuration ===
QUERY_EMBEDDING_MODEL_NAME = os.getenv("QUERY_EMBEDDING_MODEL_NAME", DOCUMENT_EMBEDDING_MODEL_NAME)
QUERY_VECTOR_DIMENSION = int(os.getenv(
    "QUERY_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(QUERY_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))

if QUERY_VECTOR_DIMENSION != QDRANT_COLLECTION_VECTOR_DIM:
    logger.info(f"[ Config Warning] Query vector dim ({QUERY_VECTOR_DIMENSION}) != Qdrant dim ({QDRANT_COLLECTION_VECTOR_DIM})")
    # Optionally enforce consistency
    # raise ValueError("Query and Document vector dimensions do not match!")
else:
    logger.info(f"[Config] Query Model: {QUERY_EMBEDDING_MODEL_NAME}")
    logger.info(f"[Config] Query Vector Dimension: {QUERY_VECTOR_DIMENSION}")

QDRANT_DEFAULT_SEARCH_K = int(os.getenv("QDRANT_DEFAULT_SEARCH_K", 5))
QDRANT_SEARCH_MIN_RELEVANCE_SCORE = float(os.getenv("QDRANT_SEARCH_MIN_RELEVANCE_SCORE", 0.1))

# === API Port Configuration ===
API_PORT = int(os.getenv('API_PORT', 5000))
logger.info(f"[Config] API Running Port: {API_PORT}")

# === Optional: Tesseract OCR Path (uncomment if used) ===
# TESSERACT_CMD = os.getenv('TESSERACT_CMD')
# if TESSERACT_CMD:
#     import pytesseract
#     pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD
#     logger.info(f"[Config] Tesseract Path: {TESSERACT_CMD}")


#  Library Availability Flags 
try:
    import pypdf
    PYPDF_AVAILABLE      = True
    PYPDF_PDFREADERROR   = pypdf.errors.PdfReadError
except ImportError:
    PYPDF_AVAILABLE      = False
    PYPDF_PDFREADERROR   = Exception

try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE       = True
except ImportError:
    DOCX_AVAILABLE       = False
    DocxDocument         = None

try:
    from pptx import Presentation
    PPTX_AVAILABLE       = True
except ImportError:
    PPTX_AVAILABLE       = False
    Presentation         = None

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    pdfplumber           = None

try:
    import pandas as pd
    PANDAS_AVAILABLE     = True
except ImportError:
    PANDAS_AVAILABLE     = False
    pd                   = None

try:
    from PIL import Image
    PIL_AVAILABLE        = True
except ImportError:
    PIL_AVAILABLE        = False
    Image                = None

try:
    import fitz
    FITZ_AVAILABLE       = True
except ImportError:
    FITZ_AVAILABLE       = False
    fitz                 = None

try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
    TESSERACT_ERROR       = pytesseract.TesseractNotFoundError
except ImportError:
    PYTESSERACT_AVAILABLE = False
    pytesseract           = None
    TESSERACT_ERROR       = Exception

try:
    import PyPDF2
    PYPDF2_AVAILABLE      = True
except ImportError:
    PYPDF2_AVAILABLE      = False
    PyPDF2                = None

#  Optional: Preload SpaCy & Embedding Model 
try:
    import spacy
    SPACY_LIB_AVAILABLE = True
    nlp_spacy_core      = spacy.load(SPACY_MODEL_NAME)
    SPACY_MODEL_LOADED  = True
except Exception as e:
    SPACY_LIB_AVAILABLE = False
    nlp_spacy_core      = None
    SPACY_MODEL_LOADED  = False
    logger.warning(f"Failed to load SpaCy model '{SPACY_MODEL_NAME}': {e}")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = True
    document_embedding_model = SentenceTransformer(DOCUMENT_EMBEDDING_MODEL_NAME)
    EMBEDDING_MODEL_LOADED = True
except Exception as e:
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = False
    document_embedding_model = None
    EMBEDDING_MODEL_LOADED = False
    logger.warning(f"Failed to load Sentence transformers: {e}")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_SPLITTER_AVAILABLE = True
except ImportError:
    LANGCHAIN_SPLITTER_AVAILABLE = False
    RecursiveCharacterTextSplitter = None # Placeholder
```

`server/rag_service/file_parser.py`

```python
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error

```

`server/rag_service/neo4j_handler.py`

```python
# server/rag_service/neo4j_handler.py

import logging
from neo4j import GraphDatabase, exceptions as neo4j_exceptions
import config # Assumes config.py is in the same directory or python path is set correctly

logger = logging.getLogger(__name__)

# --- Neo4j Driver Management ---
_neo4j_driver = None

def init_driver():
    """Initializes the Neo4j driver instance."""
    global _neo4j_driver
    if _neo4j_driver is not None:
        try: # Check if existing driver is still connected
            _neo4j_driver.verify_connectivity()
            logger.info("Neo4j driver already initialized and connected.")
            return
        except Exception:
            logger.warning("Existing Neo4j driver lost connection or failed verification. Re-initializing.")
            if _neo4j_driver:
                _neo4j_driver.close()
            _neo4j_driver = None # Force re-initialization

    try:
        _neo4j_driver = GraphDatabase.driver(
            config.NEO4J_URI,
            auth=(config.NEO4J_USERNAME, config.NEO4J_PASSWORD)
        )
        _neo4j_driver.verify_connectivity()
        logger.info(f"Neo4j driver initialized. Connected to: {config.NEO4J_URI} (DB: {config.NEO4J_DATABASE})")
    except neo4j_exceptions.ServiceUnavailable:
        logger.critical(f"Failed to connect to Neo4j at {config.NEO4J_URI}. Ensure Neo4j is running and accessible.")
        _neo4j_driver = None
    except neo4j_exceptions.AuthError:
        logger.critical(f"Neo4j authentication failed for user '{config.NEO4J_USERNAME}'. Check credentials.")
        _neo4j_driver = None
    except Exception as e:
        logger.critical(f"An unexpected error occurred while initializing Neo4j driver: {e}", exc_info=True)
        _neo4j_driver = None

def get_driver_instance():
    """Returns the active Neo4j driver instance, initializing if necessary."""
    if _neo4j_driver is None:
        init_driver()
    if _neo4j_driver is None: # Check again after trying to init
        raise ConnectionError("Neo4j driver is not available. Initialization failed.")
    return _neo4j_driver

def close_driver():
    """Closes the Neo4j driver instance if it exists."""
    global _neo4j_driver
    if _neo4j_driver:
        _neo4j_driver.close()
        _neo4j_driver = None
        logger.info("Neo4j driver closed.")

def check_neo4j_connectivity():
    """Checks if the Neo4j driver can connect."""
    try:
        driver = get_driver_instance() # This will try to init if not already
        driver.verify_connectivity()
        return True, "connected"
    except Exception as e:
        logger.warning(f"Neo4j connectivity check failed: {str(e)}")
        return False, f"disconnected_or_error: {str(e)}"

# --- Private Transaction Helper Functions ---
def _execute_read_tx(tx_function, *args, **kwargs):
    driver = get_driver_instance()
    with driver.session(database=config.NEO4J_DATABASE) as session:
        return session.execute_read(tx_function, *args, **kwargs)

def _execute_write_tx(tx_function, *args, **kwargs):
    driver = get_driver_instance()
    with driver.session(database=config.NEO4J_DATABASE) as session:
        return session.execute_write(tx_function, *args, **kwargs)

# --- Private Transactional Cypher Functions ---
def _delete_kg_transactional(tx, user_id, document_name):
    logger.info(f"Neo4j TX: Deleting KG for user '{user_id}', document '{document_name}'")
    query = (
        "MATCH (n:KnowledgeNode {userId: $userId, documentName: $documentName}) "
        "DETACH DELETE n"
    )
    result = tx.run(query, userId=user_id, documentName=document_name)
    summary = result.consume()
    deleted_count = summary.counters.nodes_deleted + summary.counters.relationships_deleted
    logger.info(f"Neo4j TX: Deleted {summary.counters.nodes_deleted} nodes and {summary.counters.relationships_deleted} relationships for '{document_name}'.")
    return deleted_count > 0

def _add_nodes_transactional(tx, nodes_param, user_id, document_name):
    logger.info(f"Neo4j TX: Adding/merging {len(nodes_param)} nodes for user '{user_id}', document '{document_name}'")
    # Ensure nodes have a type, default to "concept" if not provided
    # And llm_parent_id for parent from LLM's perspective
    processed_nodes = []
    for node_data in nodes_param:
        # Ensure ID is a string and not empty
        if not isinstance(node_data.get("id"), str) or not node_data.get("id").strip():
            logger.warning(f"Skipping node with invalid or missing ID: {node_data}")
            continue
        
        processed_node = {
            "id": node_data["id"].strip(), # Use the LLM's 'id' as 'nodeId'
            "type": node_data.get("type", "concept"), # Default type
            "description": node_data.get("description", ""),
            "llm_parent_id": node_data.get("parent", None) # Store the 'parent' from LLM
        }
        processed_nodes.append(processed_node)

    if not processed_nodes:
        logger.warning("No valid nodes to process after filtering.")
        return 0

    query = (
        "UNWIND $nodes_data as node_props "
        "MERGE (n:KnowledgeNode {nodeId: node_props.id, userId: $userId, documentName: $documentName}) "
        "ON CREATE SET n.type = node_props.type, "
        "              n.description = node_props.description, "
        "              n.llm_parent_id = node_props.llm_parent_id, "
        "              n.userId = $userId, " # Ensure userId is set on create
        "              n.documentName = $documentName " # Ensure documentName is set on create
        "ON MATCH SET n.type = node_props.type, " # Update existing nodes too
        "             n.description = node_props.description, "
        "             n.llm_parent_id = node_props.llm_parent_id "
        "RETURN count(n) as nodes_affected"
    )
    result = tx.run(query, nodes_data=processed_nodes, userId=user_id, documentName=document_name)
    count = result.single()[0] if result.peek() else 0
    logger.info(f"Neo4j TX: Affected (created or merged) {count} nodes for '{document_name}'.")
    return count

def _add_edges_transactional(tx, edges_param, user_id, document_name):
    logger.info(f"Neo4j TX: Adding/merging {len(edges_param)} edges for user '{user_id}', document '{document_name}'")
    if not edges_param:
        logger.info("Neo4j TX: No edges provided to add.")
        return 0
        
    # Filter out invalid edges
    valid_edges = []
    for edge_data in edges_param:
        if not (isinstance(edge_data.get("from"), str) and edge_data.get("from").strip() and
                isinstance(edge_data.get("to"), str) and edge_data.get("to").strip() and
                isinstance(edge_data.get("relationship"), str) and edge_data.get("relationship").strip()):
            logger.warning(f"Skipping invalid edge data: {edge_data}")
            continue
        valid_edges.append({
            "from": edge_data["from"].strip(),
            "to": edge_data["to"].strip(),
            "relationship": edge_data["relationship"].strip().upper().replace(" ", "_") # Sanitize relationship type
        })

    if not valid_edges:
        logger.warning("No valid edges to process after filtering.")
        return 0

    # Cypher query to create relationships. Note: relationship type is dynamic using brackets.
    # We use MERGE to avoid duplicate relationships with the same type between the same nodes.
    # Relationship properties are set using SET.
    query = (
        "UNWIND $edges_data as edge_props "
        "MATCH (startNode:KnowledgeNode {nodeId: edge_props.from, userId: $userId, documentName: $documentName}) "
        "MATCH (endNode:KnowledgeNode {nodeId: edge_props.to, userId: $userId, documentName: $documentName}) "
        "CALL apoc.merge.relationship(startNode, edge_props.relationship, {}, {type: edge_props.relationship}, endNode) YIELD rel "
        # MERGE (startNode)-[r:HAS_RELATIONSHIP]->(endNode) " # Simpler, but cannot set type dynamically easily.
        # "SET r.type = edge_props.relationship "
        "RETURN count(rel) as edges_affected"
    )
    # Note: The above MERGE using apoc.merge.relationship is more robust for dynamic relationship types.
    # If APOC is not available, a simpler MERGE (startNode)-[r:REL {type:edge_props.relationship}]->(endNode) would work.
    # Or create relationships with a generic type like :RELATED_TO and store the specific type as a property.
    # For this example, assuming APOC for dynamic relationship types. If not, adjust the query.
    # Simpler, if APOC is not available (relationship type becomes a property of a generic :RELATED_TO relationship):
    simple_query = (
        "UNWIND $edges_data as edge_props "
        "MATCH (startNode:KnowledgeNode {nodeId: edge_props.from, userId: $userId, documentName: $documentName}) "
        "MATCH (endNode:KnowledgeNode {nodeId: edge_props.to, userId: $userId, documentName: $documentName}) "
        "MERGE (startNode)-[r:RELATED_TO {type: edge_props.relationship}]->(endNode) "
        "RETURN count(r) as edges_affected"
    )
    # Let's use the simpler query for broader compatibility without APOC.
    
    result = tx.run(simple_query, edges_data=valid_edges, userId=user_id, documentName=document_name)
    count = result.single()[0] if result.peek() else 0
    logger.info(f"Neo4j TX: Affected (created or merged) {count} relationships for '{document_name}'.")
    return count

def _get_kg_transactional(tx, user_id, document_name):
    logger.info(f"Neo4j TX: Retrieving KG for user '{user_id}', document '{document_name}'")
    nodes_query = (
        "MATCH (n:KnowledgeNode {userId: $userId, documentName: $documentName}) "
        "RETURN n.nodeId AS id, n.type AS type, n.description AS description, n.llm_parent_id AS parent"
    )
    nodes_result = tx.run(nodes_query, userId=user_id, documentName=document_name)
    # Convert Neo4j records to dictionaries
    nodes_data = [dict(record) for record in nodes_result]

    edges_query = (
        "MATCH (startNode:KnowledgeNode {userId: $userId, documentName: $documentName})"
        "-[r:RELATED_TO]->" # Using the generic relationship type from the simple_query
        "(endNode:KnowledgeNode {userId: $userId, documentName: $documentName}) "
        "RETURN startNode.nodeId AS from, endNode.nodeId AS to, r.type AS relationship"
    )
    edges_result = tx.run(edges_query, userId=user_id, documentName=document_name)
    edges_data = [dict(record) for record in edges_result]

    logger.info(f"Neo4j TX: Retrieved {len(nodes_data)} nodes and {len(edges_data)} edges for '{document_name}'.")
    return {"nodes": nodes_data, "edges": edges_data}


# --- Public Service Functions ---
def ingest_knowledge_graph(user_id: str, document_name: str, nodes: list, edges: list) -> dict:
    """
    Deletes existing KG for the document and ingests new nodes and edges.
    Returns a summary of operations.
    """
    try:
        logger.info(f"Attempting to delete old KG (if any) for document '{document_name}' (User: {user_id}).")
        _execute_write_tx(_delete_kg_transactional, user_id, document_name)
        logger.info(f"Old KG (if any) deleted for '{document_name}'. Proceeding with ingestion.")

        nodes_affected = 0
        if nodes and len(nodes) > 0:
            nodes_affected = _execute_write_tx(_add_nodes_transactional, nodes, user_id, document_name)
        
        edges_affected = 0
        if edges and len(edges) > 0:
            edges_affected = _execute_write_tx(_add_edges_transactional, edges, user_id, document_name)

        message = "Knowledge Graph successfully ingested/updated."
        logger.info(f"{message} Doc: '{document_name}', User: '{user_id}'. Nodes: {nodes_affected}, Edges: {edges_affected}")
        return {
            "success": True,
            "message": message,
            "nodes_affected": nodes_affected,
            "edges_affected": edges_affected
        }
    except Exception as e:
        logger.error(f"Error during KG ingestion for document '{document_name}', user '{user_id}': {e}", exc_info=True)
        raise # Re-raise to be caught by the route handler

def get_knowledge_graph(user_id: str, document_name: str) -> dict:
    """
    Retrieves the knowledge graph for a given user and document name.
    """
    try:
        kg_data = _execute_read_tx(_get_kg_transactional, user_id, document_name)
        if not kg_data["nodes"] and not kg_data["edges"]:
            logger.info(f"No KG data found for user '{user_id}', document '{document_name}'.")
            return None # Indicate not found
        return kg_data
    except Exception as e:
        logger.error(f"Error retrieving KG for document '{document_name}', user '{user_id}': {e}", exc_info=True)
        raise

def delete_knowledge_graph(user_id: str, document_name: str) -> bool:
    """
    Deletes the knowledge graph for a given user and document name.
    Returns True if data was deleted, False otherwise.
    """
    try:
        was_deleted = _execute_write_tx(_delete_kg_transactional, user_id, document_name)
        return was_deleted
    except Exception as e:
        logger.error(f"Error deleting KG for document '{document_name}', user '{user_id}': {e}", exc_info=True)
        raise
```

`server/rag_service/Readne.txt`

```
conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app


```

`server/rag_service/requirements.txt`

```
Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
typing
pytesseract # OCR
pillow
qdrant-client
neo4j



```

`server/rag_service/vector_db_service.py`

```python
import uuid
import logging
from typing import List, Dict, Tuple, Optional, Any

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

# Assuming vector_db_service.py and config.py are in the same package directory (e.g., rag_service/)
# and you run your application as a module (e.g., python -m rag_service.main_app)
# or have otherwise correctly set up the Python path.
import config # Changed to relative import

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Document: # For search result formatting
    def __init__(self, page_content: str, metadata: dict):
        self.page_content = page_content
        self.metadata = metadata

    def to_dict(self):
        return {"page_content": self.page_content, "metadata": self.metadata}

class VectorDBService:
    def __init__(self):
        logger.info("Initializing VectorDBService...")
        logger.info(f"  Qdrant Host: {config.QDRANT_HOST}, Port: {config.QDRANT_PORT}, URL: {config.QDRANT_URL}")
        logger.info(f"  Collection: {config.QDRANT_COLLECTION_NAME}")
        logger.info(f"  Query Embedding Model: {config.QUERY_EMBEDDING_MODEL_NAME}")
        
        # The vector dimension for the Qdrant collection is defined by the DOCUMENT embedding model
        # This is set in config.QDRANT_COLLECTION_VECTOR_DIM
        self.vector_dim = config.QDRANT_COLLECTION_VECTOR_DIM
        logger.info(f"  Service expects Vector Dim for Qdrant collection: {self.vector_dim} (from document model config)")

        if config.QDRANT_URL:
            self.client = QdrantClient(
                url=config.QDRANT_URL,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )
        else:
            self.client = QdrantClient(
                host=config.QDRANT_HOST,
                port=config.QDRANT_PORT,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )

        try:
            # This model is for encoding search queries.
            # Its output dimension MUST match self.vector_dim (QDRANT_COLLECTION_VECTOR_DIM).
            logger.info(f"  Loading query embedding model: '{config.QUERY_EMBEDDING_MODEL_NAME}'")
            self.model = SentenceTransformer(config.QUERY_EMBEDDING_MODEL_NAME)
            model_embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"  Query model loaded. Output dimension: {model_embedding_dim}")

            if model_embedding_dim != self.vector_dim:
                error_msg = (
                    f"CRITICAL DIMENSION MISMATCH: Query model '{config.QUERY_EMBEDDING_MODEL_NAME}' "
                    f"outputs embeddings of dimension {model_embedding_dim}, but the Qdrant collection "
                    f"is configured for dimension {self.vector_dim} (derived from document model: "
                    f"'{config.DOCUMENT_EMBEDDING_MODEL_NAME}'). Search functionality will fail. "
                    "Ensure query and document models produce compatible embedding dimensions, "
                    "or environment variables for dimensions are correctly set."
                )
                logger.error(error_msg)
                raise ValueError(error_msg) # Critical error, stop initialization
            else:
                logger.info(f"  Query model output dimension ({model_embedding_dim}) matches "
                            f"Qdrant collection dimension ({self.vector_dim}).")

        except Exception as e:
            logger.error(f"Error initializing SentenceTransformer model '{config.QUERY_EMBEDDING_MODEL_NAME}' for query encoding: {e}", exc_info=True)
            raise # Re-raise to prevent service startup with a non-functional query encoder

        self.collection_name = config.QDRANT_COLLECTION_NAME
        # No ThreadPoolExecutor needed here if document encoding is external

    def _recreate_qdrant_collection(self):
        logger.info(f"Attempting to (re)create collection '{self.collection_name}' with vector size {self.vector_dim}.")
        try:
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=self.vector_dim,
                    distance=models.Distance.COSINE,
                ),
            )
            logger.info(f"Collection '{self.collection_name}' (re)created successfully.")
        except Exception as e_recreate:
            logger.error(f"Failed to (re)create collection '{self.collection_name}': {e_recreate}", exc_info=True)
            raise

    def setup_collection(self):
        try:
            collection_info = self.client.get_collection(collection_name=self.collection_name)
            logger.info(f"Collection '{self.collection_name}' already exists.")
            
            # Handle different Qdrant client versions for accessing vector config
            current_vectors_config = None
            if hasattr(collection_info.config.params, 'vectors'): # For simple vector config
                if isinstance(collection_info.config.params.vectors, models.VectorParams):
                     current_vectors_config = collection_info.config.params.vectors
                elif isinstance(collection_info.config.params.vectors, dict): # For named vectors
                    # Assuming default unnamed vector or first one if named
                    default_vector_name = '' # Common for single vector setup
                    if default_vector_name in collection_info.config.params.vectors:
                        current_vectors_config = collection_info.config.params.vectors[default_vector_name]
                    elif collection_info.config.params.vectors: # Get first one if default not found
                        current_vectors_config = next(iter(collection_info.config.params.vectors.values()))

            if not current_vectors_config:
                 logger.error(f"Could not determine vector configuration for existing collection '{self.collection_name}'. Recreating.")
                 self._recreate_qdrant_collection()
            elif current_vectors_config.size != self.vector_dim:
                logger.warning(f"Collection '{self.collection_name}' vector size {current_vectors_config.size} "
                               f"differs from service's expected {self.vector_dim}. Recreating.")
                self._recreate_qdrant_collection()
            elif current_vectors_config.distance != models.Distance.COSINE: # Ensure distance is also checked
                logger.warning(f"Collection '{self.collection_name}' distance {current_vectors_config.distance} "
                               f"differs from expected {models.Distance.COSINE}. Recreating.")
                self._recreate_qdrant_collection()
            else:
                logger.info(f"Collection '{self.collection_name}' configuration is compatible (Size: {current_vectors_config.size}, Distance: {current_vectors_config.distance}).")

        except Exception as e: # Broad exception for Qdrant client errors
            # More specific check for "Not found" type errors
            if "not found" in str(e).lower() or \
               (hasattr(e, 'status_code') and e.status_code == 404) or \
               " " in str(e).lower(): # "Lucky" in Bengali, seems to be part of an error message you encountered
                 logger.info(f"Collection '{self.collection_name}' not found. Attempting to create...")
            else:
                 logger.warning(f"Error checking collection '{self.collection_name}': {type(e).__name__} - {e}. Attempting to (re)create anyway...")
            self._recreate_qdrant_collection()

    def add_processed_chunks(self, processed_chunks: List[Dict[str, Any]]) -> int:
        if not processed_chunks:
            logger.warning("add_processed_chunks received an empty list. No points to upsert.")
            return 0

        points_to_upsert = []
        doc_name_for_logging = "Unknown Document"

        for chunk_data in processed_chunks:
            point_id = chunk_data.get('id', str(uuid.uuid4()))
            vector = chunk_data.get('embedding')
            
            payload = chunk_data.get('metadata', {}).copy()
            payload['chunk_text_content'] = chunk_data.get('text_content', '')

            if not doc_name_for_logging or doc_name_for_logging == "Unknown Document":
                doc_name_for_logging = payload.get('original_name', payload.get('document_name', "Unknown Document"))

            if not vector:
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' is missing 'embedding'. Skipping.")
                continue
            if not isinstance(vector, list) or not all(isinstance(x, (float, int)) for x in vector): # Allow int too, SentenceTransformer can return float32 which might be int-like in lists
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has an invalid 'embedding' format. Skipping.")
                continue
            if len(vector) != self.vector_dim:
                logger.error(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has embedding dimension {len(vector)}, "
                             f"but collection expects {self.vector_dim}. Skipping. "
                             f"Ensure ai_core's document embedding model ('{config.DOCUMENT_EMBEDDING_MODEL_NAME}') "
                             f"output dimension matches configuration.")
                continue

            points_to_upsert.append(models.PointStruct(
                id=point_id,
                vector=[float(v) for v in vector], # Ensure all are floats for Qdrant
                payload=payload
            ))

        if not points_to_upsert:
            logger.warning(f"No valid points constructed from processed_chunks for document: {doc_name_for_logging}.")
            return 0

        try:
            self.client.upsert(collection_name=self.collection_name, points=points_to_upsert, wait=True) # wait=True can be useful for debugging
            logger.info(f"Successfully upserted {len(points_to_upsert)} chunks for document: {doc_name_for_logging} into Qdrant.")
            return len(points_to_upsert)
        except Exception as e:
            logger.error(f"Error upserting processed chunks to Qdrant for document: {doc_name_for_logging}: {e}", exc_info=True)
            raise

    def search_documents(self, query: str, k: int = -1, filter_conditions: Optional[models.Filter] = None) -> Tuple[List[Document], str, Dict]:
        # Use default k from config if not provided or invalid
        if k <= 0:
            k_to_use = config.QDRANT_DEFAULT_SEARCH_K
        else:
            k_to_use = k

        context_docs = []
        formatted_context_text = "No relevant context was found in the available documents."
        context_docs_map = {}

        logger.info(f"Searching with query (first 50 chars): '{query[:50]}...', k: {k_to_use}")
        if filter_conditions:
            try: filter_dict = filter_conditions.dict()
            except AttributeError: # For older Pydantic versions
                try: filter_dict = filter_conditions.model_dump()
                except AttributeError: filter_dict = str(filter_conditions) # Fallback
            logger.info(f"Applying filter: {filter_dict}")
        else:
            logger.info("No filter applied for search.")

        try:
            query_embedding = self.model.encode(query).tolist()
            logger.debug(f"Generated query_embedding (length: {len(query_embedding)}, first 5 dims: {query_embedding[:5]})")

            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                query_filter=filter_conditions,
                limit=k_to_use,
                with_payload=True,
                score_threshold=config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE # Apply score threshold directly in search
            )
            logger.info(f"Qdrant client.search returned {len(search_results)} results (after score threshold).")

            if not search_results:
                return context_docs, formatted_context_text, context_docs_map

            for idx, point in enumerate(search_results):
                # Score threshold is already applied by Qdrant if score_threshold parameter is used.
                # If not using score_threshold in client.search, uncomment this:
                # if point.score < config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE:
                #     logger.debug(f"Skipping point ID {point.id} with score {point.score:.4f} (below threshold {config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE})")
                #     continue

                payload = point.payload
                content = payload.get("chunk_text_content", payload.get("text_content", payload.get("chunk_text", "")))

                retrieved_metadata = payload.copy()
                retrieved_metadata["qdrant_id"] = point.id
                retrieved_metadata["score"] = point.score

                doc = Document(page_content=content, metadata=retrieved_metadata)
                context_docs.append(doc)

            # Format context and citations
            formatted_context_parts = []
            for i, doc_obj in enumerate(context_docs):
                citation_index = i + 1
                doc_meta = doc_obj.metadata
                # Use more robust fetching of metadata keys
                display_subject = doc_meta.get("title", doc_meta.get("subject", "Unknown Subject")) # Prefer title for subject
                doc_name = doc_meta.get("original_name", doc_meta.get("file_name", "N/A"))
                page_num_info = f" (Page: {doc_meta.get('page_number', 'N/A')})" if doc_meta.get('page_number') else "" # Add page number if available
                
                content_preview = doc_obj.page_content[:200] + "..." if len(doc_obj.page_content) > 200 else doc_obj.page_content

                formatted = (f"[{citation_index}] Score: {doc_meta.get('score', 0.0):.4f} | "
                             f"Source: {doc_name}{page_num_info} | Subject: {display_subject}\n"
                             f"Content: {content_preview}") # Show content preview
                formatted_context_parts.append(formatted)

                context_docs_map[str(citation_index)] = {
                    "subject": display_subject,
                    "document_name": doc_name,
                    "page_number": doc_meta.get("page_number"),
                    "content_preview": content_preview, # Store preview
                    "full_content": doc_obj.page_content, # Store full content for potential later use
                    "score": doc_meta.get("score", 0.0),
                    "qdrant_id": doc_meta.get("qdrant_id"),
                    "original_metadata": doc_meta # Store all original metadata from payload
                }
            if formatted_context_parts:
                formatted_context_text = "\n\n---\n\n".join(formatted_context_parts)
            else:
                formatted_context_text = "No sufficiently relevant context was found after filtering."

        except Exception as e:
            logger.error(f"Qdrant search/RAG error: {e}", exc_info=True)
            formatted_context_text = "Error retrieving context due to an internal server error."

        return context_docs, formatted_context_text, context_docs_map
    
    # Add this method to the VectorDBService class in vector_db_service.py

    def delete_document_vectors(self, user_id: str, document_name: str) -> Dict[str, Any]:
        logger.info(f"Attempting to delete vectors for document: '{document_name}', user: '{user_id}' from Qdrant collection '{self.collection_name}'.")
        
        # These metadata keys must match what's stored during ingestion from ai_core.py
        # 'processing_user' was the user_id passed to ai_core
        # 'file_name' was the original_name passed to ai_core
        qdrant_filter = models.Filter(
            must=[
                models.FieldCondition(
                    key="processing_user", # The metadata field storing the user ID
                    match=models.MatchValue(value=user_id)
                ),
                models.FieldCondition(
                    key="file_name", # The metadata field storing the original document name
                    match=models.MatchValue(value=document_name)
                )
            ]
        )
        
        try:
            # Optional: Count points before deleting for logging/confirmation
            # count_response = self.client.count(collection_name=self.collection_name, count_filter=qdrant_filter)
            # num_to_delete = count_response.count
            # logger.info(f"Qdrant: Found {num_to_delete} points matching criteria for document '{document_name}', user '{user_id}'.")

            # if num_to_delete == 0:
            #     logger.info(f"Qdrant: No points found to delete for document '{document_name}', user '{user_id}'.")
            #     return {"success": True, "message": "No matching vectors found in Qdrant to delete.", "deleted_count": 0}

            delete_result = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(filter=qdrant_filter),
                wait=True # Make it synchronous
            )
            
            # Check the status of the delete operation
            # delete_result should be an UpdateResult object
            if delete_result.status == models.UpdateStatus.COMPLETED or delete_result.status == models.UpdateStatus.ACKNOWLEDGED:
                # The actual number of deleted points isn't directly returned by filter-based delete.
                # We can infer it was successful if no error.
                # For a precise count, you'd need to list IDs by filter, then delete by IDs.
                logger.info(f"Qdrant delete operation for document '{document_name}', user '{user_id}' acknowledged/completed. Status: {delete_result.status}")
                return {"success": True, "message": f"Qdrant vector deletion for document '{document_name}' completed. Status: {delete_result.status}."}
            else:
                logger.warning(f"Qdrant delete operation for document '{document_name}', user '{user_id}' returned status: {delete_result.status}")
                return {"success": False, "message": f"Qdrant delete operation status: {delete_result.status}"}

        except Exception as e:
            logger.error(f"Error deleting document vectors from Qdrant for document '{document_name}', user '{user_id}': {e}", exc_info=True)
            # Check for specific Qdrant client errors if possible, e.g., if the collection doesn't exist.
            return {"success": False, "message": f"Failed to delete Qdrant vectors: {str(e)}"}

    def close(self):
        logger.info("VectorDBService close called.")
        # No specific resources like ThreadPoolExecutor to release in this version.
        # QdrantClient does not have an explicit close() method in recent versions.
```

`server/rag_service/__init__.py`

```python

```

`server/routes/analysis.js`

```javascript
console.log("Hello World");

```

`server/routes/auth.js`

```javascript
// server/routes/auth.js
const express = require('express');
const { v4: uuidv4 } = require('uuid'); // For generating session IDs
const User = require('../models/User'); // Mongoose User model
require('dotenv').config();

const router = express.Router();

// --- @route   POST /api/auth/signup ---
// --- @desc    Register a new user ---
// --- @access  Public ---
router.post('/signup', async (req, res) => {
  const { username, password } = req.body;

  // Basic validation
  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }
  if (password.length < 6) {
     return res.status(400).json({ message: 'Password must be at least 6 characters long' });
  }

  try {
    // Check if user already exists
    const existingUser = await User.findOne({ username });
    if (existingUser) {
      return res.status(400).json({ message: 'Username already exists' });
    }

    // Create new user (password hashing is handled by pre-save middleware in User model)
    const newUser = new User({ username, password });
    await newUser.save();

    // Generate a new session ID for the first login
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Note: Mongoose excludes 'select: false' fields by default after save() too
    res.status(201).json({
      _id: newUser._id, // Send user ID
      username: newUser.username,
      sessionId: sessionId, // Send session ID on successful signup/login
      message: 'User registered successfully',
    });

  } catch (error) {
    console.error('Signup Error:', error);
    // Handle potential duplicate key errors more gracefully if needed
    if (error.code === 11000) {
        return res.status(400).json({ message: 'Username already exists.' });
    }
    res.status(500).json({ message: 'Server error during signup' });
  }
});

// --- @route   POST /api/auth/signin ---
// --- @desc    Authenticate user (using custom static method) ---
// --- @access  Public ---
router.post('/signin', async (req, res) => {
  const { username, password } = req.body;

  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }

  try {
    // *** CHANGE HERE: Use the static method from User model ***
    // This method finds the user AND selects the password field AND compares the password
    const user = await User.findByCredentials(username, password);

    // Check if the method returned a user (means credentials were valid)
    if (!user) {
      // findByCredentials returns null if user not found OR password doesn't match
      return res.status(401).json({ message: 'Invalid credentials' }); // Use generic message
    }

    // User authenticated successfully if we reached here

    // Generate a NEW session ID for this login session
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Even though 'user' has the password field selected from findByCredentials,
    // Mongoose's .toJSON() or spreading might still exclude it if schema default is select:false.
    // Explicitly create the response object.
    res.status(200).json({
      _id: user._id, // Send user ID
      username: user.username,
      sessionId: sessionId, // Send a *new* session ID on each successful login
      message: 'Login successful',
    });

  } catch (error) {
    // Log the specific error for debugging
    console.error('Signin Error:', error);
    // Check if the error came from the comparePassword method (e.g., bcrypt issue)
    if (error.message === "Password field not available for comparison.") {
        // This shouldn't happen if findByCredentials is used correctly, but good to check
        console.error("Developer Error: Password field was not selected before comparison attempt.");
        return res.status(500).json({ message: 'Internal server configuration error during signin.' });
    }
    res.status(500).json({ message: 'Server error during signin' });
  }
});


module.exports = router;

```

`server/routes/chat.js`

```javascript
// server/routes/chat.js
const express = require('express');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const ChatHistory = require('../models/ChatHistory');
const { v4: uuidv4 } = require('uuid');
const { generateContentWithHistory } = require('../services/geminiService');

const router = express.Router();

// --- Helper to call Python RAG Query Endpoint ---
async function queryPythonRagService(userId, query, k = 5, filter = null) { // userId is already a param
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL || process.env.DEFAULT_PYTHON_RAG_URL; // Ensure one is set
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL or DEFAULT_PYTHON_RAG_URL is not set in environment. Cannot query RAG service.");
        throw new Error("RAG service configuration error.");
    }
    const searchUrl = `${pythonServiceUrl.replace(/\/$/, '')}/query`; // Ensure no trailing slash before adding /query

    console.log(`Querying Python RAG service for User ${userId} at ${searchUrl} with query (first 50): "${query.substring(0,50)}...", k=${k}`);
    
    const payload = {
        query: query,
        user_id: userId, // <<< ADDED user_id to the payload for Python service
        k: k
    };

    if (filter && typeof filter === 'object' && Object.keys(filter).length > 0) {
        payload.filter = filter; // This filter is for Qdrant
        console.log(`  Applying Qdrant filter to Python RAG search:`, filter);
    } else {
        console.log(`  No Qdrant filter applied to Python RAG search.`);
    }

    try {
        const response = await axios.post(searchUrl, payload, { 
            headers: { 'Content-Type': 'application/json' },
            timeout: 30000 
        });

        // Python /query now returns:
        // { ..., retrieved_documents_list: [...], knowledge_graphs: { "docName1": kg1, ... } }
        if (response.data && Array.isArray(response.data.retrieved_documents_list)) {
            console.log(`Python RAG service /query returned ${response.data.qdrant_results_count} Qdrant results.`);
            
            const adaptedDocs = response.data.retrieved_documents_list.map(doc => {
                const metadata = doc.metadata || {};
                return {
                    documentName: metadata.original_name || metadata.file_name || metadata.title || 'Unknown Document',
                    content: doc.page_content || "", 
                    score: metadata.score,
                };
            });
            
            console.log(`  Adapted ${adaptedDocs.length} documents for Node.js service.`);
            return { // Return an object now
                relevantDocs: adaptedDocs,
                knowledge_graphs: response.data.knowledge_graphs || {} // Pass through the KG data
            };

        } else {
             console.warn(`Python RAG service /query returned unexpected data structure:`, response.data);
             throw new Error("Received unexpected data structure from RAG query service.");
        }
    } catch (error) {
        // ... (your existing error handling for queryPythonRagService) ...
        const errorStatus = error.response?.status;
        const errorData = error.response?.data;
        let errorMsg = "Unknown RAG search error";

        if (errorData) {
            if (typeof errorData === 'string' && errorData.toLowerCase().includes("<!doctype html>")) {
                errorMsg = `HTML error page received from Python RAG service (Status: ${errorStatus}). URL (${searchUrl}) might be incorrect or Python service has an issue.`;
            } else {
                errorMsg = errorData?.error || error.message || "Error response from RAG service had no specific message.";
            }
        } else if (error.request) {
            errorMsg = `No response received from Python RAG service at ${searchUrl}. It might be down or unreachable.`;
        } else {
            errorMsg = error.message;
        }
        console.error(`Error querying Python RAG service at ${searchUrl}. Details: ${errorMsg}`, error.response ? error.response.data : error);
        throw new Error(`RAG Query Failed: ${errorMsg}`);
    }
}

// --- @route   POST /api/chat/rag ---
router.post('/rag', tempAuth, async (req, res) => {
    const { message, filter } = req.body; // 'filter' here is the Qdrant filter if provided by client
    const userId = req.user._id.toString(); 

    if (!message || typeof message !== 'string' || message.trim() === '') {
        return res.status(400).json({ message: 'Query message text required.' });
    }

    console.log(`>>> POST /api/chat/rag: User=${userId}. Query: "${message.substring(0,50)}..."`);

    try {
        const kValue = parseInt(process.env.RAG_DEFAULT_K) || 5; 
        const clientFilterForQdrant = filter && typeof filter === 'object' ? filter : null; 
        
        // queryPythonRagService now returns an object: { relevantDocs, knowledge_graphs }
        const ragQueryResult = await queryPythonRagService(userId, message.trim(), kValue, clientFilterForQdrant); 
        
        console.log(`<<< POST /api/chat/rag successful for User ${userId}. Found ${ragQueryResult.relevantDocs.length} Qdrant docs and KGs for ${Object.keys(ragQueryResult.knowledge_graphs || {}).length} docs.`);
        
        // Send both relevantDocs (for context string) and knowledge_graphs to the client
        res.status(200).json({ 
            relevantDocs: ragQueryResult.relevantDocs, // For RAG context
            knowledge_graphs: ragQueryResult.knowledge_graphs // For client-side display/use
        });

    } catch (error) { 
        console.error(`!!! Error processing RAG query for User ${userId}:`, error.message);
        res.status(500).json({ message: error.message || "Failed to retrieve relevant documents and/or knowledge graphs." });
    }
});

function formatKnowledgeGraphForLLM(knowledgeGraphs) {
    if (!knowledgeGraphs || typeof knowledgeGraphs !== 'object' || Object.keys(knowledgeGraphs).length === 0) {
        return ""; // Return empty string if no KG data
    }

    let kgString = "\n\n--- Retrieved Knowledge Graph Information ---\n";
    let kgCitationHints = [];

    Object.entries(knowledgeGraphs).forEach(([docName, kgData]) => {
        if (!kgData || (!Array.isArray(kgData.nodes) || kgData.nodes.length === 0) && (!Array.isArray(kgData.edges) || kgData.edges.length === 0)) {
            if(kgData.message === "KG not found" || kgData.error) {
                kgString += `For document "${docName}": ${kgData.message || kgData.error}\n`;
            } else {
                kgString += `For document "${docName}": No specific KG data was extracted or found.\n`;
            }
            return;
        }
        
        kgString += `\nKnowledge Graph for document: "${docName}":\n`;
        kgCitationHints.push(docName); // Add doc name to citation hints

        // Format Nodes
        if (kgData.nodes && kgData.nodes.length > 0) {
            kgString += "Key Concepts (Nodes):\n";
            kgData.nodes.forEach(node => {
                // Example: "Concept A (Type: major): Description of A. Parent: None."
                // Example: "Sub-concept A1 (Type: subnode): Description of A1. Parent: Concept A."
                const parentInfo = node.parent ? ` (Parent: ${node.parent})` : '';
                kgString += `- ${node.id} (Type: ${node.type || 'N/A'})${parentInfo}: ${node.description || 'No description.'}\n`;
            });
        }

        // Format Edges
        if (kgData.edges && kgData.edges.length > 0) {
            kgString += "Relationships (Edges):\n";
            kgData.edges.forEach(edge => {
                // Example: "Sub-concept A1 --[subtopic_of]--> Concept A"
                kgString += `- ${edge.from} --[${edge.relationship}]--> ${edge.to}\n`;
            });
        }
        kgString += "---\n";
    });
    
    kgString += "--- End of Knowledge Graph Information ---\n";
    return { kgFormattedString: kgString, kgCitationHints: kgCitationHints };
}


router.post('/message', tempAuth, async (req, res) => {
    // Now expecting knowledge_graphs in the payload
    const { message, history, sessionId, systemPrompt, isRagEnabled, relevantDocs, knowledge_graphs } = req.body;
    const userId = req.user._id.toString();

    // ... (existing validations for message, sessionId, history) ...
    const useRAG = !!isRagEnabled;

    console.log(`>>> POST /api/chat/message: User=${userId}, Session=${sessionId}, RAG=${useRAG}. KG data received: ${!!knowledge_graphs}`);

    let textualContextString = ""; // For text docs
    let textCitationHints = [];
    let kgContextString = ""; // For KG data
    let kgCitationHints = [];

    try {
        // 1. Process text-based relevantDocs (Qdrant results)
        if (useRAG && Array.isArray(relevantDocs) && relevantDocs.length > 0) {
            console.log(`   RAG Text: Processing ${relevantDocs.length} text documents for context.`);
            textualContextString = "Answer the user's question based primarily on the following context documents and knowledge graph information.\nIf the context documents do not contain the necessary information to answer the question fully, clearly state what information is missing from the context *before* potentially providing an answer based on your general knowledge.\n\n--- Context Documents ---\n";
            
            relevantDocs.forEach((doc, index) => {
                // ... (your existing logic for formatting relevantDocs into textualContextString) ...
                if (!doc || typeof doc.documentName !== 'string' || typeof doc.content !== 'string') {
                    console.warn("   Skipping invalid/incomplete document in relevantDocs:", doc); return;
                }
                const docName = doc.documentName;
                const scoreDisplay = doc.score !== undefined ? `(Rel. Score: ${doc.score.toFixed(4)})` : '';
                textualContextString += `\n[D${index + 1}] Source: ${docName} ${scoreDisplay}\nContent:\n${doc.content}\n---\n`;
                textCitationHints.push(`[D${index + 1}] ${docName}`);
            });
            textualContextString += "\n--- End of Context Documents ---\n";
            console.log(`   Constructed text context string. ${textCitationHints.length} valid text docs used.`);
        } else if (useRAG) {
            console.log(`   RAG Text: No relevant text documents provided or RAG disabled for text.`);
        }

        // 2. Process knowledge_graphs (Neo4j results)
        if (useRAG && knowledge_graphs) {
            console.log(`   RAG KG: Processing knowledge graph data.`);
            const { kgFormattedString: formattedKgStr, kgCitationHints: retrievedKgCitations } = formatKnowledgeGraphForLLM(knowledge_graphs);
            kgContextString = formattedKgStr;
            kgCitationHints = retrievedKgCitations; // These are just document names from KG
            if (kgContextString) {
                console.log(`   Constructed KG context string. KG sources: ${kgCitationHints.join(', ')}`);
            }
        } else if (useRAG) {
            console.log(`   RAG KG: No knowledge graph data provided or RAG disabled for KG.`);
        }
        
        // Combine all context
        let combinedContext = "";
        if (textualContextString) combinedContext += textualContextString;
        if (kgContextString) combinedContext += kgContextString; // Append KG info

        const historyForGeminiAPI = history.map(msg => ({
             role: msg.role,
             parts: msg.parts.map(part => ({ text: part.text || '' }))
        })).filter(msg => msg && msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string');

        let finalUserQueryText = "";
        if (combinedContext) {
            // Update citation instruction to include both types of sources
            let allCitationExamples = [];
            if (textCitationHints.length > 0) allCitationExamples.push(...textCitationHints.slice(0, 2));
            if (kgCitationHints.length > 0) {
                // For KG, citations might be just document names if not more specific node IDs are used in formatting
                kgCitationHints.slice(0,1).forEach(name => allCitationExamples.push(`[KG - ${name}]`));
            }
            const citationExampleString = allCitationExamples.length > 0 ? `(e.g., ${allCitationExamples.join(', ')})` : "(e.g., [D1] Source Document, [KG - Another Document Name])";
            
            const citationInstruction = `When referencing information ONLY from the context documents or knowledge graphs provided above, please cite the source using the format [D-Number] Document Name for text documents, or [KG - Document Name] for knowledge graph information ${citationExampleString}.`;
            finalUserQueryText = `CONTEXT:\n${combinedContext}\nINSTRUCTIONS: ${citationInstruction}\n\nUSER QUESTION: ${message.trim()}`;
        } else {
            finalUserQueryText = message.trim();
        }

        const finalHistoryForGemini = [
            ...historyForGeminiAPI,
            { role: "user", parts: [{ text: finalUserQueryText }] }
        ];

        console.log(`   Calling Gemini API. History length for Gemini: ${finalHistoryForGemini.length}. System Prompt: ${!!systemPrompt}`);
        // console.log("   Final User Query Text (first 300 chars):", finalUserQueryText.substring(0, 300) + "...");


        const geminiResponseText = await generateContentWithHistory(finalHistoryForGemini, systemPrompt);

        const modelResponseMessage = {
            role: 'model',
            parts: [{ text: geminiResponseText }],
            timestamp: new Date()
        };

        console.log(`<<< POST /api/chat/message successful for session ${sessionId}.`);
        res.status(200).json({ reply: modelResponseMessage });

    } catch (error) {
        // ... (your existing error handling for this route) ...
        console.error(`!!! Error processing chat message for session ${sessionId}:`, error.message || error);
        let statusCode = error.status || error.response?.status || 500;
        let clientMessage = error.message || error.response?.data?.message || "Failed to get response from AI service.";
        
        if (statusCode === 500 && !error.response?.data?.message) { 
            clientMessage = "An internal server error occurred while processing the AI response.";
        }
        res.status(statusCode).json({ message: clientMessage });
    }
});

// --- @route   POST /api/chat/message ---
// router.post('/message', tempAuth, async (req, res) => {
//     // `relevantDocs` from client is an array of {documentName, content, score}
//     // `knowledge_graphs` would now also be part of the RAG step, but the Gemini prompt
//     // primarily uses `relevantDocs` for text context.
//     // The client might use `knowledge_graphs` for UI display.
//     const { message, history, sessionId, systemPrompt, isRagEnabled, relevantDocs /*, knowledge_graphs - client doesn't send KGs back here, it got them from /rag */ } = req.body;
//     const userId = req.user._id.toString();

//     // ... (rest of your existing /api/chat/message logic)
//     // The logic for constructing `contextString` from `relevantDocs` remains the same.
//     // The `knowledge_graphs` retrieved in the `/api/chat/rag` step are for the client to potentially use/display;
//     // they are not directly re-fed into the Gemini prompt in this `/message` endpoint unless you design it that way.

//     if (!message || typeof message !== 'string' || message.trim() === '') return res.status(400).json({ message: 'Message text required.' });
//     if (!sessionId || typeof sessionId !== 'string') return res.status(400).json({ message: 'Session ID required.' });
//     if (!Array.isArray(history)) return res.status(400).json({ message: 'Invalid history format.'});
//     const useRAG = !!isRagEnabled; 

//     console.log(`>>> POST /api/chat/message: User=${userId}, Session=${sessionId}, RAG=${useRAG}. Query: "${message.substring(0,50)}..."`);

//     let contextString = "";
//     let citationHints = []; 

//     try {
//         if (useRAG && Array.isArray(relevantDocs) && relevantDocs.length > 0) {
//             console.log(`   RAG Enabled: Processing ${relevantDocs.length} relevant documents provided by client for context.`);
//             contextString = "Answer the user's question based primarily on the following context documents.\nIf the context documents do not contain the necessary information to answer the question fully, clearly state what information is missing from the context *before* potentially providing an answer based on your general knowledge.\n\n--- Context Documents ---\n";
            
//             relevantDocs.forEach((doc, index) => {
//                 if (!doc || typeof doc.documentName !== 'string' || typeof doc.content !== 'string') {
//                     console.warn("   Skipping invalid/incomplete document in relevantDocs (missing 'documentName' or 'content'):", doc);
//                     return; 
//                 }
//                 const docName = doc.documentName;
//                 const scoreDisplay = doc.score !== undefined ? `(Rel. Score: ${doc.score.toFixed(4)})` : ''; 
//                 const fullContent = doc.content; 

//                 contextString += `\n[${index + 1}] Source: ${docName} ${scoreDisplay}\nContent:\n${fullContent}\n---\n`;
//                 citationHints.push(`[${index + 1}] ${docName}`);
//             });
//             contextString += "\n--- End of Context ---\n\n";
//             console.log(`   Constructed context string. ${citationHints.length} valid docs used.`);
//         } else {
//             console.log(`   RAG Disabled or no relevant documents provided by client for context.`);
//         }

//         // ... (rest of your existing /api/chat/message logic for calling Gemini and sending response) ...
//         const historyForGeminiAPI = history.map(msg => ({
//              role: msg.role,
//              parts: msg.parts.map(part => ({ text: part.text || '' }))
//         })).filter(msg => msg && msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string');

//         let finalUserQueryText = "";
//         if (contextString) { 
//             const citationInstruction = `When referencing information ONLY from the context documents provided above, please cite the source using the format [Number] Document Name (e.g., ${citationHints.slice(0, Math.min(3, citationHints.length)).join(', ')}).`;
//             finalUserQueryText = `CONTEXT:\n${contextString}\nINSTRUCTIONS: ${citationInstruction}\n\nUSER QUESTION: ${message.trim()}`;
//         } else {
//             finalUserQueryText = message.trim();
//         }

//         const finalHistoryForGemini = [
//             ...historyForGeminiAPI,
//             { role: "user", parts: [{ text: finalUserQueryText }] }
//         ];

//         console.log(`   Calling Gemini API. History length for Gemini: ${finalHistoryForGemini.length}. System Prompt: ${!!systemPrompt}`);

//         const geminiResponseText = await generateContentWithHistory(finalHistoryForGemini, systemPrompt);

//         const modelResponseMessage = {
//             role: 'model',
//             parts: [{ text: geminiResponseText }],
//             timestamp: new Date()
//         };

//         console.log(`<<< POST /api/chat/message successful for session ${sessionId}.`);
//         res.status(200).json({ reply: modelResponseMessage });

//     } catch (error) {
//         console.error(`!!! Error processing chat message for session ${sessionId}:`, error.message || error);
//         let statusCode = error.status || error.response?.status || 500;
//         let clientMessage = error.message || error.response?.data?.message || "Failed to get response from AI service.";
        
//         if (statusCode === 500 && !error.response?.data?.message) { 
//             clientMessage = "An internal server error occurred while processing the AI response.";
//         }
//         res.status(statusCode).json({ message: clientMessage });
//     }
// });

// ... (rest of your chat.js: /history, /sessions, /session/:sessionId) ...
// --- @route POST /api/chat/history --- (Keep your existing implementation)
router.post('/history', tempAuth, async (req, res) => {
    const { sessionId, messages } = req.body;
    const userId = req.user._id; 
    if (!sessionId) return res.status(400).json({ message: 'Session ID required to save history.' });
    if (!Array.isArray(messages)) return res.status(400).json({ message: 'Invalid messages format.' });

    console.log(`>>> POST /api/chat/history: User=${userId}, Session=${sessionId}, Messages=${messages.length}`);

    try {
        const validMessages = messages.filter(m =>
            m && typeof m.role === 'string' &&
            Array.isArray(m.parts) && m.parts.length > 0 &&
            typeof m.parts[0].text === 'string' &&
            m.timestamp 
        ).map(m => ({ 
            role: m.role,
            parts: [{ text: m.parts[0].text }], 
            timestamp: new Date(m.timestamp) 
        }));

        if (validMessages.length !== messages.length) {
             console.warn(`Session ${sessionId}: Filtered out ${messages.length - validMessages.length} invalid messages during save attempt.`);
        }
        if (validMessages.length === 0 && messages.length > 0) { 
            console.warn(`Session ${sessionId}: All ${messages.length} messages were invalid. No history saved.`);
            const newSessionIdForClient = uuidv4();
            return res.status(200).json({
                message: 'No valid messages to save. Chat not saved. New session ID provided.',
                savedSessionId: null,
                newSessionId: newSessionIdForClient
            });
        }
        if (validMessages.length === 0 && messages.length === 0) { 
             console.log(`Session ${sessionId}: No messages provided to save. Generating new session ID for client.`);
             const newSessionIdForClient = uuidv4();
             return res.status(200).json({
                 message: 'No history provided to save. New session ID for client.',
                 savedSessionId: null,
                 newSessionId: newSessionIdForClient
             });
        }

        const savedHistory = await ChatHistory.findOneAndUpdate(
            { sessionId: sessionId, userId: userId },
            { $set: { userId: userId, sessionId: sessionId, messages: validMessages, updatedAt: Date.now() } },
            { new: true, upsert: true, setDefaultsOnInsert: true }
        );
        const newClientSessionId = uuidv4(); 
        console.log(`<<< POST /api/chat/history: History saved for session ${savedHistory.sessionId}. New client session ID: ${newClientSessionId}`);
        res.status(200).json({
            message: 'Chat history saved successfully.',
            savedSessionId: savedHistory.sessionId,
            newSessionId: newClientSessionId 
        });
    } catch (error) {
        console.error(`!!! Error saving chat history for session ${sessionId}:`, error);
        if (error.name === 'ValidationError') return res.status(400).json({ message: "Validation Error saving history: " + error.message });
        if (error.code === 11000) return res.status(409).json({ message: "Conflict: Session ID might already exist unexpectedly." });
        res.status(500).json({ message: 'Failed to save chat history due to a server error.' });
    }
});

// --- @route GET /api/chat/sessions --- (Keep your existing implementation)
router.get('/sessions', tempAuth, async (req, res) => {
    const userId = req.user._id;
    console.log(`>>> GET /api/chat/sessions: User=${userId}`);
    try {
        const sessions = await ChatHistory.find({ userId: userId })
            .sort({ updatedAt: -1 }) 
            .select('sessionId createdAt updatedAt messages') 
            .lean(); 

        const sessionSummaries = sessions.map(session => {
             const firstUserMessage = session.messages?.find(m => m.role === 'user');
             let preview = 'Chat Session'; 
             if (firstUserMessage?.parts?.[0]?.text) {
                 preview = firstUserMessage.parts[0].text.substring(0, 75);
                 if (firstUserMessage.parts[0].text.length > 75) {
                     preview += '...';
                 }
             }
             return {
                 sessionId: session.sessionId,
                 createdAt: session.createdAt,
                 updatedAt: session.updatedAt,
                 messageCount: session.messages?.length || 0,
                 preview: preview
             };
        });
        console.log(`<<< GET /api/chat/sessions: Found ${sessionSummaries.length} sessions for User ${userId}.`);
        res.status(200).json(sessionSummaries);
    } catch (error) {
        console.error(`!!! Error fetching chat sessions for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat sessions.' });
    }
});

// --- @route GET /api/chat/session/:sessionId --- (Keep your existing implementation)
router.get('/session/:sessionId', tempAuth, async (req, res) => {
    const userId = req.user._id;
    const { sessionId } = req.params;
    console.log(`>>> GET /api/chat/session/${sessionId}: User=${userId}`);
    if (!sessionId) return res.status(400).json({ message: 'Session ID parameter is required.' });
    try {
        const session = await ChatHistory.findOne({ sessionId: sessionId, userId: userId }).lean();
        if (!session) {
            console.log(`--- GET /api/chat/session/${sessionId}: Session not found for User ${userId}.`);
            return res.status(404).json({ message: 'Chat session not found or access denied.' });
        }
        console.log(`<<< GET /api/chat/session/${sessionId}: Session found for User ${userId}.`);
        res.status(200).json(session);
    } catch (error) {
        console.error(`!!! Error fetching chat session ${sessionId} for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat session details.' });
    }
});


module.exports = router;
```

`server/routes/files.js`

```javascript
// server/routes/files.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const axios = require('axios'); // For calling Python service
const { tempAuth } = require('../middleware/authMiddleware');
const User = require('../models/User'); // Import User model

const router = express.Router();

const ASSETS_DIR = path.join(__dirname, '..', 'assets');
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets');

// --- Helper functions (sanitizeUsernameForDir, parseServerFilename, ensureDirExists are existing) ---
const sanitizeUsernameForDir = (username) => {
    if (!username) return '';
    return username.replace(/[^a-zA-Z0-9_-]/g, '_');
};

const parseServerFilename = (filename) => {
    // Matches "timestamp-originalName.ext"
    // Allows originalName to contain dots now.
    const match = filename.match(/^(\d+)-(.+?)(\.\w+)$/);
    if (match && match.length === 4) {
        return { timestamp: match[1], originalName: `${match[2]}${match[3]}`, extension: match[3] };
    }
    // Fallback for names that might not perfectly fit the new pattern, or originalName without extension before timestamp
    const ext = path.extname(filename);
    const baseWithoutExt = filename.substring(0, filename.length - ext.length);
    const tsMatch = baseWithoutExt.match(/^(\d+)-(.*)$/);
    if (tsMatch) {
        return { timestamp: tsMatch[1], originalName: `${tsMatch[2]}${ext}`, extension: ext };
    }
    // Final fallback if no timestamp prefix is reliably parsed
    return { timestamp: null, originalName: filename, extension: path.extname(filename) };
};

const ensureDirExists = async (dirPath) => {
    try { await fs.mkdir(dirPath, { recursive: true }); }
    catch (error) { if (error.code !== 'EEXIST') { console.error(`Error creating dir ${dirPath}:`, error); throw error; } }
};

// --- New Helper Function to call Python Service for Deletions ---
async function callPythonDeletionEndpoint(method, endpointPath, userId, originalName, logContext) {
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL || process.env.DEFAULT_PYTHON_RAG_URL || 'http://localhost:5000'; // Fallback if not set
    if (!pythonServiceUrl) {
        console.error(`Python Service Deletion Error for ${logContext}: PYTHON_RAG_SERVICE_URL not set.`);
        return { success: false, message: "Python service URL not configured." };
    }

    const deleteUrl = `${pythonServiceUrl.replace(/\/$/, '')}${endpointPath}`;

    try {
        console.log(`Calling Python Service (${method.toUpperCase()}) for deletion: ${deleteUrl} (Doc: ${originalName}, User: ${userId})`);
        let response;
        if (method.toUpperCase() === 'DELETE') {
            // For DELETE, data is often in query params or path, but axios allows a 'data' field for body
            response = await axios.delete(deleteUrl, {
                data: { // For Python endpoints that expect a body (like a new Qdrant delete one)
                    user_id: userId,
                    document_name: originalName
                },
                timeout: 30000 // 30s timeout
            });
        } else {
            throw new Error(`Unsupported method for Python deletion: ${method}`);
        }

        if (response.status === 200 || response.status === 204) { // 204 No Content is also success
            return { success: true, message: response.data?.message || `Successfully deleted from ${endpointPath}` };
        } else {
            return { success: false, message: response.data?.message || `Python service returned ${response.status} for ${endpointPath}` };
        }
    } catch (error) {
        const errorMsg = error.response?.data?.error || error.response?.data?.message || error.message || `Unknown error deleting from ${endpointPath}`;
        console.error(`Error calling Python Service for deletion (${deleteUrl}) for ${originalName} (User: ${userId}): ${errorMsg}`, error.response ? { status: error.response.status, data: error.response.data } : error);
        return { success: false, message: `Python service call failed for ${endpointPath}: ${errorMsg}` };
    }
}


// --- @route   GET /api/files ---
// (Keep your existing GET /api/files route as is)
router.get('/', tempAuth, async (req, res) => {
    // req.user is guaranteed to exist here because of tempAuth middleware
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username);
    if (!sanitizedUsername) {
        console.warn("GET /api/files: Invalid user identifier after sanitization.");
        return res.status(400).json({ message: 'Invalid user identifier.' });
    }

    const userAssetsDir = path.join(ASSETS_DIR, sanitizedUsername);
    const fileTypes = ['docs', 'images', 'code', 'others'];
    const userFiles = [];

    try {
        try { await fs.access(userAssetsDir); }
        catch (e) {
             if (e.code === 'ENOENT') { return res.status(200).json([]); }
             throw e;
        }

        for (const type of fileTypes) {
            const typeDir = path.join(userAssetsDir, type);
            try {
                const filesInDir = await fs.readdir(typeDir);
                for (const filename of filesInDir) {
                    const filePath = path.join(typeDir, filename);
                    try {
                        const stats = await fs.stat(filePath);
                        if (stats.isFile()) {
                            const parsed = parseServerFilename(filename);
                            userFiles.push({
                                serverFilename: filename, originalName: parsed.originalName, type: type,
                                relativePath: path.join(type, filename).replace(/\\/g, '/'),
                                size: stats.size, lastModified: stats.mtime,
                            });
                        }
                    } catch (statError) { console.warn(`GET /api/files: Stat failed for ${filePath}:`, statError.message); }
                }
            } catch (err) { if (err.code !== 'ENOENT') { console.warn(`GET /api/files: Read failed for ${typeDir}:`, err.message); } }
        }
        userFiles.sort((a, b) => a.originalName.localeCompare(b.originalName));
        res.status(200).json(userFiles);
    } catch (error) {
        console.error(`!!! Error in GET /api/files for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to retrieve file list.' });
    }
});


// --- @route   PATCH /api/files/:serverFilename ---
// (Keep your existing PATCH /api/files/:serverFilename route as is)
router.patch('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const { newOriginalName } = req.body;
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username);

    if (!sanitizedUsername) return res.status(400).json({ message: 'Invalid user identifier.' });
    if (!serverFilename) return res.status(400).json({ message: 'Server filename parameter is required.' });
    if (!newOriginalName || typeof newOriginalName !== 'string' || newOriginalName.trim() === '') return res.status(400).json({ message: 'New file name is required.' });
    if (newOriginalName.includes('/') || newOriginalName.includes('\\') || newOriginalName.includes('..')) return res.status(400).json({ message: 'New file name contains invalid characters.' });

    try {
        const parsedOld = parseServerFilename(serverFilename);
        if (!parsedOld.timestamp) return res.status(400).json({ message: 'Invalid server filename format (missing timestamp prefix).' });

        let currentPath = null; let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsername, type, serverFilename);
            try { await fs.access(potentialPath); currentPath = potentialPath; fileType = type; break; }
            catch (e) { if (e.code !== 'ENOENT') throw e; }
        }
        if (!currentPath) return res.status(404).json({ message: 'File not found or access denied.' });

        const newExt = path.extname(newOriginalName) || parsedOld.extension;
        const newBaseName = path.basename(newOriginalName, path.extname(newOriginalName));
        const sanitizedNewBase = newBaseName.replace(/[^a-zA-Z0-9._-]/g, '_');
        const finalNewOriginalName = `${sanitizedNewBase}${newExt}`;
        const newServerFilename = `${parsedOld.timestamp}-${finalNewOriginalName}`;
        const newPath = path.join(ASSETS_DIR, sanitizedUsername, fileType, newServerFilename);

        await fs.rename(currentPath, newPath);
        res.status(200).json({
            message: 'File renamed successfully!', oldFilename: serverFilename,
            newFilename: newServerFilename, newOriginalName: finalNewOriginalName,
        });
    } catch (error) {
        console.error(`!!! Error in PATCH /api/files/${serverFilename} for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to rename the file.' });
    }
});


// --- @route   DELETE /api/files/:serverFilename ---
// --- @desc    Deletes file metadata from MongoDB, associated data from Qdrant & Neo4j (via Python),
// ---           and moves physical file to backup. ---
// --- @access  Private (requires auth) ---
router.delete('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const userId = req.user._id.toString(); // Get userId from authenticated user
    const usernameForLog = req.user.username;

    if (!serverFilename) {
        return res.status(400).json({ message: 'Server filename parameter is required.' });
    }

    const parsedFileDetails = parseServerFilename(serverFilename);
    const originalName = parsedFileDetails.originalName;
    if (!originalName) {
        console.error(`DELETE /api/files: Could not parse originalName from serverFilename: ${serverFilename}`);
        return res.status(400).json({ message: 'Invalid server filename format for deletion.' });
    }
    const logContext = `File: '${originalName}' (server: ${serverFilename}), User: ${usernameForLog} (${userId})`;
    console.log(`Attempting to delete all data for ${logContext}`);

    const results = {
        mongodb: { success: false, message: "Not attempted" },
        qdrant: { success: false, message: "Not attempted" },
        neo4j: { success: false, message: "Not attempted" },
        filesystem: { success: false, message: "Not attempted" },
    };
    let overallSuccess = true; // Assume success, set to false if any critical step fails
    let httpStatus = 200;
    let fileFoundInMongo = false;
    let physicalFileFound = false;

    try {
        // 1. Delete from MongoDB
        try {
            const user = await User.findById(userId);
            if (!user) {
                results.mongodb.message = "User not found.";
                // If user not found, we can't confirm if the file was theirs.
                // Treat as if the file wasn't found for this user.
            } else {
                const docIndex = user.uploadedDocuments.findIndex(doc => doc.filename === originalName);
                if (docIndex > -1) {
                    fileFoundInMongo = true;
                    user.uploadedDocuments.splice(docIndex, 1);
                    await user.save();
                    results.mongodb.success = true;
                    results.mongodb.message = "Successfully removed from user's document list.";
                    console.log(`MongoDB: Document entry '${originalName}' removed for user ${userId}.`);
                } else {
                    results.mongodb.message = "Document not found in user's list.";
                    console.log(`MongoDB: Document entry '${originalName}' not found for user ${userId}.`);
                }
            }
        } catch (mongoError) {
            console.error(`MongoDB Deletion Error for ${logContext}:`, mongoError);
            results.mongodb.message = `MongoDB deletion failed: ${mongoError.message}`;
            overallSuccess = false; // DB error is critical
        }

        // 2. Delete from Qdrant (via Python service)
        // This endpoint will need to be created in Python: e.g., /delete_qdrant_document_data
        // It should expect { user_id: userId, document_name: originalName } in the body
        const qdrantDeleteResult = await callPythonDeletionEndpoint(
            'DELETE',
            `/delete_qdrant_document_data`,
            userId,
            originalName,
            logContext
        );
        results.qdrant = qdrantDeleteResult;
        if (!qdrantDeleteResult.success) {
            console.warn(`Qdrant deletion failed or reported no data for ${logContext}. Message: ${qdrantDeleteResult.message}`);
            // overallSuccess = false; // Non-critical for now, but log
        }

        // 3. Delete from Neo4j (via Python service)
        // This uses the existing Python endpoint: /kg/<user_id>/<document_name>
        const neo4jEndpointPath = `/kg/${userId}/${encodeURIComponent(originalName)}`;
        const neo4jDeleteResult = await callPythonDeletionEndpoint(
            'DELETE',
            neo4jEndpointPath, // userId and originalName are in the path
            userId, // still pass for logging consistency in helper
            originalName, // still pass for logging consistency in helper
            logContext
        );
        results.neo4j = neo4jDeleteResult;
        if (!neo4jDeleteResult.success) {
            console.warn(`Neo4j deletion failed or reported no data for ${logContext}. Message: ${neo4jDeleteResult.message}`);
            // overallSuccess = false; // Non-critical for now, but log
        }

        // 4. Move physical file to backup (filesystem operation)
        let currentPath = null;
        let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        const sanitizedUsernameForPath = sanitizeUsernameForDir(usernameForLog);

        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsernameForPath, type, serverFilename);
            try {
                await fs.access(potentialPath); // Check if file exists
                currentPath = potentialPath;
                fileType = type;
                physicalFileFound = true;
                break;
            } catch (e) {
                if (e.code !== 'ENOENT') {
                    console.warn(`Filesystem: Error accessing ${potentialPath} during delete scan: ${e.message}`);
                }
            }
        }

        if (currentPath) { // If physical file was found
            const backupUserDir = path.join(BACKUP_DIR, sanitizedUsernameForPath, fileType);
            await ensureDirExists(backupUserDir);
            const backupPath = path.join(backupUserDir, serverFilename);
            try {
                await fs.rename(currentPath, backupPath);
                results.filesystem = { success: true, message: "File moved to backup successfully." };
                console.log(`Filesystem: Moved '${currentPath}' to '${backupPath}'.`);
            } catch (fsError) {
                console.error(`Filesystem: Error moving file ${currentPath} to backup for ${logContext}:`, fsError);
                results.filesystem.message = `Filesystem move to backup failed: ${fsError.message}`;
                // overallSuccess = false; // Decide if this is critical enough to mark overall failure
            }
        } else {
            results.filesystem.message = "Physical file not found in assets, or already moved.";
            console.log(`Filesystem: Physical file '${serverFilename}' not found for user ${usernameForLog}.`);
        }

        // Determine final status and message
        const successfulDeletes = [results.mongodb.success, results.qdrant.success, results.neo4j.success, results.filesystem.success].filter(Boolean).length;

        if (!fileFoundInMongo && !physicalFileFound) {
            httpStatus = 404;
            finalMessage = `File '${originalName}' not found for user.`;
        } else if (results.mongodb.success) { // Primary record deleted
            if (successfulDeletes === 4) {
                finalMessage = `Successfully deleted all data associated with '${originalName}'.`;
                httpStatus = 200;
            } else {
                finalMessage = `File '${originalName}' removed from your list. Some backend data cleanup attempts had issues. Check server logs for details.`;
                httpStatus = 207; // Multi-Status
            }
        } else { // MongoDB deletion failed, but file might have existed
            finalMessage = `Failed to remove '${originalName}' from your list. Some backend data cleanup may have also failed. Check server logs.`;
            httpStatus = 500;
        }

        console.log(`Deletion outcome for ${logContext}: HTTP Status=${httpStatus}, Overall Success Flag (was pre-status logic)=${overallSuccess}`);
        return res.status(httpStatus).json({
            message: finalMessage,
            details: results
        });

    } catch (error) {
        console.error(`!!! UNEXPECTED Error in DELETE /api/files/${serverFilename} for user ${usernameForLog}:`, error);
        return res.status(500).json({
            message: 'An unexpected server error occurred during file deletion.',
            details: results // Send partial results if any
        });
    }
});


module.exports = router;
```

`server/routes/network.js`

```javascript
const express = require('express');
const router = express.Router();
const os = require('os');

function getAllIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost by default

    for (const [name, netInterface] of Object.entries(interfaces)) {
        // Skip loopback and potentially virtual interfaces if desired
        if (name.includes('lo') || name.toLowerCase().includes('virtual') || name.toLowerCase().includes('vmnet')) continue;

        for (const addr of netInterface) {
            // Focus on IPv4, non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

router.get('/ip', (req, res) => {
    res.json({
        ips: getAllIPs(),
        // req.ip might be less reliable behind proxies, but can be included
        // currentRequestIp: req.ip
    });
});

module.exports = router;

```

`server/routes/syllabus.js`

```javascript
// server/routes/syllabus.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const { tempAuth } = require('../middleware/authMiddleware'); // Protect the route

const router = express.Router();
const SYLLABI_DIR = path.join(__dirname, '..', 'syllabi');

// --- @route   GET /api/syllabus/:subjectId ---
// --- @desc    Get syllabus content for a specific subject ---
// --- @access  Private (requires auth) ---
router.get('/:subjectId', tempAuth, async (req, res) => {
    const { subjectId } = req.params;

    // Basic sanitization: Allow only alphanumeric and underscores
    // Prevents directory traversal (e.g., ../../etc/passwd)
    const sanitizedSubjectId = subjectId.replace(/[^a-zA-Z0-9_]/g, '');

    if (!sanitizedSubjectId || sanitizedSubjectId !== subjectId) {
        console.warn(`Syllabus request rejected due to invalid characters: ${subjectId}`);
        return res.status(400).json({ message: 'Invalid subject identifier format.' });
    }

    const filePath = path.join(SYLLABI_DIR, `${sanitizedSubjectId}.md`);

    try {
        // Check if file exists first (more specific error)
        await fs.access(filePath);

        // Read the file content
        const content = await fs.readFile(filePath, 'utf-8');

        res.status(200).json({ syllabus: content });

    } catch (error) {
        if (error.code === 'ENOENT') {
            console.warn(`Syllabus file not found: ${filePath}`);
            return res.status(404).json({ message: `Syllabus for '${subjectId}' not found.` });
        } else {
            console.error(`Error reading syllabus file ${filePath}:`, error);
            return res.status(500).json({ message: 'Server error retrieving syllabus.' });
        }
    }
});

module.exports = router;

```

`server/routes/upload.js`

```javascript
// server/routes/upload.js
const express = require('express');
const multer = require('multer');
const path = require('path');
const fs = require('fs');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const User = require('../models/User'); // Import the User model
const { ANALYSIS_PROMPTS } = require('../config/promptTemplates'); 
const geminiService = require('../services/geminiService');
const { threadId } = require('worker_threads');
const { log } = require('console');
const { Worker } = require('worker_threads');

const router = express.Router();

// --- Constants ---
const UPLOAD_DIR = path.join(__dirname, '..', 'assets');
const MAX_FILE_SIZE = 20 * 1024 * 1024; // 20 MB

// Define allowed types by mimetype and extension (lowercase)
// Mapping mimetype to subfolder name
const allowedMimeTypes = {
    // Documents -> 'docs'
    'application/pdf': 'docs',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docs', // .docx
    'application/msword': 'docs', // .doc (Might be less reliable mimetype)
    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'docs', // .pptx
    'application/vnd.ms-powerpoint': 'docs', // .ppt (Might be less reliable mimetype)
    'text/plain': 'docs', // .txt
    // Code -> 'code'
    'text/x-python': 'code', // .py
    'application/javascript': 'code', // .js
    'text/javascript': 'code', // .js (alternative)
    'text/markdown': 'code', // .md
    'text/html': 'code', // .html
    'application/xml': 'code', // .xml
    'text/xml': 'code', // .xml
    'application/json': 'code', // .json
    'text/csv': 'code', // .csv
    // Images -> 'images'
    'image/jpeg': 'images',
    'image/png': 'images',
    'image/bmp': 'images',
    'image/gif': 'images',
    // Add more specific types if needed, otherwise they fall into 'others'
};
// Define allowed extensions (lowercase) - This is a secondary check
const allowedExtensions = [
    '.pdf', '.docx', '.doc', '.pptx', '.ppt', '.txt',
    '.py', '.js', '.md', '.html', '.xml', '.json', '.csv', '.log', // Added .log
    '.jpg', '.jpeg', '.png', '.bmp', '.gif'
];

// --- Multer Config ---
const storage = multer.diskStorage({
    destination: (req, file, cb) => {
        // tempAuth middleware ensures req.user exists here
        if (!req.user || !req.user.username) {
            // This should ideally not happen if tempAuth works correctly
            console.error("Multer Destination Error: User context missing after auth middleware.");
            return cb(new Error("Authentication error: User context not found."));
        }
        const sanitizedUsername = req.user.username.replace(/[^a-zA-Z0-9_-]/g, '_');
        const fileMimeType = file.mimetype.toLowerCase();

        // Determine subfolder based on mimetype, default to 'others'
        const fileTypeSubfolder = allowedMimeTypes[fileMimeType] || 'others';
        const destinationPath = path.join(UPLOAD_DIR, sanitizedUsername, fileTypeSubfolder);

        // Ensure the destination directory exists (use async for safety)
        fs.mkdir(destinationPath, { recursive: true }, (err) => {
             if (err) {
                 console.error(`Error creating destination path ${destinationPath}:`, err);
                 cb(err);
             } else {
                 cb(null, destinationPath);
             }
         });
    },
    filename: (req, file, cb) => {
        const timestamp = Date.now();
        const fileExt = path.extname(file.originalname).toLowerCase();
        // Sanitize base name: remove extension, replace invalid chars, limit length
        const sanitizedBaseName = path.basename(file.originalname, fileExt)
                                      .replace(/[^a-zA-Z0-9._-]/g, '_') // Allow letters, numbers, dot, underscore, hyphen
                                      .substring(0, 100); // Limit base name length
        const uniqueFilename = `${timestamp}-${sanitizedBaseName}${fileExt}`;
        cb(null, uniqueFilename);
    }
});

const fileFilter = (req, file, cb) => {
    // tempAuth middleware should run before this, ensuring req.user exists
    if (!req.user) {
         console.warn(`Upload Rejected (File Filter): User context missing.`);
         const error = new multer.MulterError('UNAUTHENTICATED'); // Custom code?
         error.message = `User not authenticated.`;
         return cb(error, false);
    }

    const fileExt = path.extname(file.originalname).toLowerCase();
    const mimeType = file.mimetype.toLowerCase();

    // Primary check: Mimetype must be in our known list OR extension must be allowed
    // Secondary check: Extension must be in the allowed list
    const isMimeTypeKnown = !!allowedMimeTypes[mimeType];
    const isExtensionAllowed = allowedExtensions.includes(fileExt);

    // Allow if (MIME type is known OR extension is explicitly allowed) AND extension is in the allowed list
    // This allows known MIME types even if extension isn't listed, and listed extensions even if MIME isn't known (e.g. text/plain for .log)
    // But we always require the extension itself to be in the allowed list for safety.
    // if ((isMimeTypeKnown || isExtensionAllowed) && isExtensionAllowed) {

    // Stricter: Allow only if BOTH mimetype is known AND extension is allowed
    if (isMimeTypeKnown && isExtensionAllowed) {
        cb(null, true); // Accept file
    } else {
        console.warn(`Upload Rejected (File Filter): User='${req.user.username}', File='${file.originalname}', MIME='${mimeType}', Ext='${fileExt}'. MimeKnown=${isMimeTypeKnown}, ExtAllowed=${isExtensionAllowed}`);
        const error = new multer.MulterError('LIMIT_UNEXPECTED_FILE');
        error.message = `Invalid file type or extension. Allowed extensions: ${allowedExtensions.join(', ')}`;
        cb(error, false); // Reject file
    }
};

const upload = multer({
    storage: storage,
    fileFilter: fileFilter,
    limits: { fileSize: MAX_FILE_SIZE }
});
// --- End Multer Config ---


// --- Function to call Python RAG service ---
async function triggerPythonRagProcessing(userId, filePath, originalName) {
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL;
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL is not set.");
        return { success: false, message: "RAG service URL not configured.", text: null, chunksForKg: [], status: null };
    }
    const addDocumentUrl = `${pythonServiceUrl}/add_document`;
    // console.log(`RAG Trigger: Calling Python for ${originalName} (User: ${userId}) at ${addDocumentUrl}`);

    try {
        const response = await axios.post(addDocumentUrl, {
            user_id: userId,
            file_path: filePath,
            original_name: originalName
        }, { timeout: 300000 }); // 5 min timeout

        const pythonData = response.data;
        // console.log(`RAG Trigger: Raw Python response for ${originalName}:`, pythonData);

        const text = pythonData?.raw_text_for_analysis || null;
        const chunksForKg = pythonData?.chunks_with_metadata || [];
        const pythonStatus = pythonData?.status;
        const pythonMessage = pythonData?.message || "No message from Python RAG service.";

        if (pythonStatus === 'added' || pythonStatus === 'skipped') {
            return {
                success: true,
                status: pythonStatus,
                message: pythonMessage,
                text: text,
                chunksForKg: chunksForKg,
            };
        } else {
            console.warn(`RAG Trigger: Unexpected status '${pythonStatus}' from Python for ${originalName}.`);
            return {
                success: false,
                message: `Unexpected RAG status: ${pythonStatus}. ${pythonMessage}`,
                text: text, chunksForKg: chunksForKg, status: pythonStatus,
            };
        }
    } catch (error) {
        const errorMsg = error.response?.data?.error || error.message || "Unknown RAG service error";
        console.error(`RAG Trigger: Error calling Python for ${originalName}:`, errorMsg);
        return { success: false, message: `RAG service call failed: ${errorMsg}`, text: null, chunksForKg: [], status: 'error' };
    }
}
// --- End Function ---


// --- Function to call Generate Analysis
async function triggerAnalysisGeneration(userId, originalName, textForAnalysis) {
    console.log(`Starting analysis generation for document '${originalName}', User ID: ${userId}. Text length: ${textForAnalysis.length}`);

    let allAnalysesSuccessful = true; // Assume success initially
    const analysisResults = {
        faq: null,
        topics: null,
        mindmap: null
    };
    const logCtx = { userId, originalName }; // Context for logging within generateSingleAnalysis

    // Inner helper function to generate a single type of analysis
    async function generateSingleAnalysis(type, promptContent, context) {
        try {
            console.log(`Attempting to generate ${type} for '${context.originalName}' (User: ${context.userId}).`);

            // Prepare history for geminiService.generateContentWithHistory
            // The 'promptContent' (which is the system prompt) will be passed as the second argument.
            const historyForGemini = [
                { role: 'user', parts: [{ text: "Please perform the requested analysis based on the system instruction provided." }] }
            ];

            const generatedText = await geminiService.generateContentWithHistory(
                historyForGemini,
                promptContent // This is passed as systemPromptText to generateContentWithHistory
            );

            if (!generatedText || typeof generatedText !== 'string' || generatedText.trim() === "") {
                console.warn(`Gemini returned empty or invalid content for ${type} for '${context.originalName}'.`);
                allAnalysesSuccessful = false; // Update the outer scope variable
                return `Notice: No content was generated by the AI for ${type}. The input text might have been unsuitable or the AI returned an empty response.`;
            }

            console.log(`${type} generation successful for '${context.originalName}'. Length: ${generatedText.length}`);
            return generatedText.trim();

        } catch (error) {
            console.error(`Error during ${type} generation for '${context.originalName}' (User: ${context.userId}): ${error.message}`);
            allAnalysesSuccessful = false; // Update the outer scope variable
            // Return a user-friendly error message, or a snippet of the technical error
            const errorMessage = error.message || "Unknown error during AI generation.";
            return `Error generating ${type}: ${errorMessage.split('\n')[0].substring(0, 250)}`; // First line of error, truncated
        }
    }

    // 1. Generate FAQs
    console.log(`[Analysis Step 1/3] Preparing FAQ generation for '${originalName}'.`);
    const faqPrompt = ANALYSIS_PROMPTS.faq.getPrompt(textForAnalysis);
    analysisResults.faq = await generateSingleAnalysis('FAQ', faqPrompt, logCtx);
    if (!allAnalysesSuccessful) {
        console.warn(`FAQ generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
        // We continue even if one fails, allAnalysesSuccessful flag will reflect the overall status.
    }

    // 2. Generate Topics
    console.log(`[Analysis Step 2/3] Preparing Topics generation for '${originalName}'.`);
    const topicsPrompt = ANALYSIS_PROMPTS.topics.getPrompt(textForAnalysis);
    analysisResults.topics = await generateSingleAnalysis('Topics', topicsPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.topics.startsWith("Error generating Topics:")) { // Check if this specific step failed
        console.warn(`Topics generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
    }


    // 3. Generate Mindmap
    console.log(`[Analysis Step 3/3] Preparing Mindmap generation for '${originalName}'.`);
    const mindmapPrompt = ANALYSIS_PROMPTS.mindmap.getPrompt(textForAnalysis);
    analysisResults.mindmap = await generateSingleAnalysis('Mindmap', mindmapPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.mindmap.startsWith("Error generating Mindmap:")) { // Check if this specific step failed
        console.warn(`Mindmap generation failed or produced no content for '${originalName}'.`);
    }

    // Log final outcome of the analysis generation process
    if (allAnalysesSuccessful) {
        console.log(`All analyses (FAQ, Topics, Mindmap) appear to have been generated successfully for '${originalName}'.`);
    } else {
        console.warn(`One or more analyses failed or produced no content for '${originalName}'. Review individual results for details.`);
        // Log the specific results for easier debugging
        console.warn(`FAQ Result for '${originalName}': ${analysisResults.faq.substring(0,100)}...`);
        console.warn(`Topics Result for '${originalName}': ${analysisResults.topics.substring(0,100)}...`);
        console.warn(`Mindmap Result for '${originalName}': ${analysisResults.mindmap.substring(0,100)}...`);
    }

    return {
        success: allAnalysesSuccessful,
        results: analysisResults
    };
}
// --- End Analysis Generation Function ---



router.post('/', tempAuth, (req, res) => {
    const uploader = upload.single('file');

    uploader(req, res, async function (err) {
        if (!req.user) {
            console.error("Upload: User context missing.");
            return res.status(401).json({ message: "Authentication error." });
        }
        const userId = req.user._id.toString();
        const username = req.user.username;

        let absoluteFilePath = null;
        let originalName = null;
        let serverFilename = null;

        if (err) {
            console.error(`Upload: Multer error for user '${username}': ${err.message}`);
            if (err instanceof multer.MulterError) {
                return res.status(400).json({ message: err.message });
            }
            return res.status(500).json({ message: "Server error during upload prep." });
        }

        if (!req.file) {
            console.warn(`Upload: No file received for user '${username}'.`);
            return res.status(400).json({ message: "No file received." });
        }

        absoluteFilePath = path.resolve(req.file.path);
        originalName = req.file.originalname;
        serverFilename = req.file.filename;
        console.log(`Upload: Received for user '${username}', File: ${serverFilename}, Original: ${originalName}`);

        try {
            // ----- STAGE 1: MongoDB Pre-check -----
            const userForPreCheck = await User.findById(userId).select('uploadedDocuments');
            if (!userForPreCheck) {
                console.error(`Upload: User ${userId} ('${username}') not found. Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload: Cleanup error (user not found): ${e.message}`));
                return res.status(404).json({ message: "User not found." });
            }
            const existingDocument = userForPreCheck.uploadedDocuments.find(doc => doc.filename === originalName);
            if (existingDocument) {
                console.log(`Upload: '${originalName}' already exists for '${username}'. Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload: Cleanup error (duplicate): ${e.message}`));
                return res.status(409).json({
                    message: `File '${originalName}' already exists.`,
                    filename: serverFilename, originalname: originalName,
                });
            }
            console.log(`Upload: Pre-check passed for '${originalName}' (User: '${username}').`);


            // ----- STAGE 2: RAG Processing (Get data from Python) -----
            const ragResult = await triggerPythonRagProcessing(userId, absoluteFilePath, originalName);
            // ragResult now contains: { success, status, text, chunksForKg, message }

            if (!ragResult.success || ragResult.status !== 'added' || !ragResult.text || ragResult.text.trim() === '') {
                const errorMessage = (ragResult && ragResult.message) || "RAG processing failed or returned insufficient data from Python.";
                console.error(`Upload Route Error: RAG failed for '${originalName}' (User: '${username}'): ${errorMessage}. Status: ${ragResult.status}. Deleting file.`);
                if (absoluteFilePath) {
                    await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload Route: Cleanup error (RAG fail): ${e.message}`));
                }
                return res.status(500).json({ message: errorMessage, filename: serverFilename, originalname: originalName });
            }
            console.log(`RAG Stage: Python OK for '${originalName}'. Text obtained. Status: ${ragResult.status}.`);


            // ----- STAGE 2.5: Initial MongoDB Save for the Document Entry -----
            // This creates the document shell with the text from RAG.
            const newDocumentEntryData = {
                filename: originalName, // This is the key for future updates
                text: ragResult.text, // Save the text obtained from RAG
                // Initialize analysis object as per your schema
                analysis: {
                    faq: "",
                    topics: "",
                    mindmap: "",
                },
                // You might want to add uploadedAt, ragStatus, initial analysisStatus, kgStatus here
                uploadedAt: new Date(),
                ragStatus: ragResult.status,
                analysisStatus: "pending",
                kgStatus: "pending"
            };

            try {
                const updateResult = await User.updateOne(
                    { _id: userId },
                    { $push: { uploadedDocuments: newDocumentEntryData } }
                );

                if (updateResult.modifiedCount === 0 && updateResult.matchedCount === 1) {
                    // This could happen if the user doc was matched but $push didn't modify (e.g. arrayFilters condition failed, though not used here)
                    // Or if the exact same subdocument was somehow pushed again without erroring (unlikely for $push without specific checks)
                    console.warn(`DB Save: Initial document entry for '${originalName}' pushed, but modifiedCount is 0. This might indicate an issue or an idempotent push.`);
                } else if (updateResult.matchedCount === 0) {
                     throw new Error("User not found for saving initial document entry.");
                }
                console.log(`DB Save: Initial document entry for '${originalName}' added to user '${username}'.`);
            } catch (dbError) {
                console.error(`Upload Route Error: MongoDB error saving initial doc for '${originalName}': ${dbError.message}. Deleting file.`);
                if (absoluteFilePath) {
                    await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload Route: Cleanup error (initial DB save fail): ${e.message}`));
                }
                return res.status(500).json({ message: `Database error saving document: ${dbError.message}`, filename: serverFilename, originalname: originalName });
            }
            // At this point, the document shell with filename and text is in the DB.
            // We don't need to retrieve its _id for this flow.

            // Optional: Delete physical file now if content is in DB
            // await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload Route: Cleanup error (post-initial save): ${e.message}`));
            // absoluteFilePath = null;

            console.log(`Initial DB Save: Completed for '${originalName}' (User: '${username}'). Proceeding to analysis.`);



            // ----- STAGE 3: Analysis Generation -----
            const analysisOutcome = await triggerAnalysisGeneration(userId, originalName, ragResult.text);

            // ----- STAGE 4: Handle Analysis Outcome & DB Update -----
            if (analysisOutcome.success) {
                // Use originalName for logging context now
                console.log(`Analysis: Generated successfully for '${originalName}' (User: '${username}'). Storing.`);
                await User.updateOne(
                    // Query to find the user and the specific subdocument by filename
                    { _id: userId, "uploadedDocuments.filename": originalName },
                    {
                        $set: {
                            "uploadedDocuments.$.analysis.faq": analysisOutcome.results.faq,
                            "uploadedDocuments.$.analysis.topics": analysisOutcome.results.topics,
                            "uploadedDocuments.$.analysis.mindmap": analysisOutcome.results.mindmap,
                            "uploadedDocuments.$.analysisStatus": "completed"
                        }
                    }
                );
            } else {
                console.warn(`Analysis: Failed for '${originalName}' (User: '${username}'). Storing partial/error state.`);
                await User.updateOne(
                    { _id: userId, "uploadedDocuments.filename": originalName },
                    {
                        $set: {
                            "uploadedDocuments.$.analysis.faq": analysisOutcome.results.faq,
                            "uploadedDocuments.$.analysis.topics": analysisOutcome.results.topics,
                            "uploadedDocuments.$.analysis.mindmap": analysisOutcome.results.mindmap,
                            "uploadedDocuments.$.analysisStatus": "failed"
                        }
                    }
                );
            }

            // ----- STAGE 4: KG Worker Initiation -----
            let kgWorkerInitiated = false;
            // Condition for KG: RAG was successful (status 'added'), and chunks are available.
            // The initial document entry is already saved in STAGE 2.5.
            if (ragResult.status === "added" &&
                ragResult.chunksForKg && ragResult.chunksForKg.length > 0) {

                console.log(`KG Init: Conditions met for '${originalName}' (User: '${username}').`);
                const workerScriptPath = path.resolve(__dirname, '../workers/kgWorker.js');

                try {
                    const worker = new Worker(workerScriptPath, {
                        workerData: {
                            chunksForKg: ragResult.chunksForKg,
                            userId: userId,
                            originalName: originalName,
                            // documentIdInDb is no longer strictly needed if KG service uses userId/originalName to update DB
                            // but can be passed as originalName for context if kgService expects a document identifier
                        }
                    });
                    kgWorkerInitiated = true;
                    worker.on('message', (msg) => console.log(`KG Worker [Context ${msg.documentId}]: ${msg.message || JSON.stringify(msg)}`));
                    worker.on('error', (workerErr) => console.error(`KG Worker Error [Context ${originalName}]:`, workerErr));
                    worker.on('exit', (code) => console.log(`KG Worker [Context ${originalName}] exited (code ${code}).`));
                    console.log(`KG Init: Worker for '${originalName}' launched.`);
                } catch (workerLaunchError) {
                    kgWorkerInitiated = false;
                    console.error(`KG Init: Failed to launch worker for '${originalName}':`, workerLaunchError);
                }
            } else {
                console.log(`KG Init: Not triggered for '${originalName}'. RAG Status: ${ragResult.status}, Chunks: ${ragResult.chunksForKg ? ragResult.chunksForKg.length : 'N/A'}`);
            }

            // ----- Final Response to Client -----
            const analysisMessage = analysisOutcome.success ? 'completed' : 'failed';
            const kgMessage = kgWorkerInitiated ? 'initiated' : 'not triggered';
            const finalUserMessage = `File processed. Analysis: ${analysisMessage}. KG generation: ${kgMessage}.`;

            // We no longer have a specific subDocumentId to return from this main flow.
            // The client can refer to the document by its originalName.
            return res.status(analysisOutcome.success ? 200 : 422).json({
                message: finalUserMessage,
                filename: serverFilename, // The name on disk
                originalname: originalName,     // The original name uploaded by user
                analysisStatus: analysisMessage,
                kgInitiated: kgWorkerInitiated
            });

        } catch (processError) {
            console.error(`Upload Route: !!! Overall error for ${originalName || 'unknown'} (User: '${username || 'unknown'}'):`, processError.message, processError.stack);
            if (absoluteFilePath) {
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Upload Route: Cleanup error (overall fail): ${e.message}`));
            }
            return res.status(500).json({
                message: `Server error: ${processError.message || 'Unknown error.'}`,
                filename: serverFilename, originalname: originalName
            });
        }
    });
});


module.exports = router;

```

`server/server.js`

```javascript
// server/server.js
const express = require('express');
const dotenv = require('dotenv'); // Removed dotenv
const cors = require('cors');
const path = require('path');
const { getLocalIPs } = require('./utils/networkUtils');
const fs = require('fs');
const axios = require('axios');
const os = require('os');
const mongoose = require('mongoose'); // Import mongoose for closing connection
const readline = require('readline').createInterface({ // For prompting
  input: process.stdin,
  output: process.stdout,
});

// --- Custom Modules ---
const connectDB = require('./config/db');
const { performAssetCleanup } = require('./utils/assetCleanup');

// --- Configuration Loading ---
dotenv.config(); // Removed dotenv

// --- Configuration Defaults & Variables ---
const DEFAULT_PORT = 5001;
const DEFAULT_MONGO_URI = 'mongodb://localhost:27017/chatbotGeminiDB'; // Default DB URI
const DEFAULT_PYTHON_RAG_URL = 'http://localhost:5002'; // Default RAG service URL

let port = process.env.PORT || DEFAULT_PORT; // Use environment variable PORT if set, otherwise default
let mongoUri = process.env.MONGO_URI || ''; // Use environment variable if set
let pythonRagUrl = process.env.PYTHON_RAG_SERVICE_URL || ''; // Use environment variable if set
let geminiApiKey = process.env.GEMINI_API_KEY || ''; // MUST be set via environment

// --- Express Application Setup ---
const app = express();

// --- Core Middleware ---
app.use(cors()); // Allows requests from frontend (potentially on different IPs in LAN)
app.use(express.json());

// --- Basic Root Route ---
app.get('/', (req, res) => res.send('Chatbot Backend API is running...'));

// --- API Route Mounting ---
app.use('/api/network', require('./routes/network')); // For IP info
app.use('/api/auth', require('./routes/auth'));
app.use('/api/chat', require('./routes/chat'));
app.use('/api/upload', require('./routes/upload'));
app.use('/api/files', require('./routes/files'));
app.use('/api/syllabus', require('./routes/syllabus')); // <-- ADD THIS LINE
// app.use('/api/analysis', require('./routes/analysis'));
// app.use('/api/kg', require('./routes/kg')); // Knowledge Graph route



// --- Centralized Error Handling Middleware ---
app.use((err, req, res, next) => {
    console.error("Unhandled Error:", err.stack || err);
    const statusCode = err.status || 500;
    let message = err.message || 'An internal server error occurred.';
    // Sanitize potentially sensitive error details in production
    if (process.env.NODE_ENV === 'production' && statusCode === 500) {
        message = 'An internal server error occurred.';
    }
    // Ensure response is JSON for API routes
    if (req.originalUrl.startsWith('/api/')) {
         return res.status(statusCode).json({ message: message });
    }
    // Fallback for non-API routes if any
    res.status(statusCode).send(message);
});

// --- Server Instance Variable ---
let server;

// --- Graceful Shutdown Logic ---
const gracefulShutdown = async (signal) => {
    console.log(`\n${signal} received. Shutting down gracefully...`);
    readline.close(); // Close readline interface
    try {
        // Close HTTP server first to stop accepting new connections
        if (server) {
            server.close(async () => {
                console.log('HTTP server closed.');
                // Close MongoDB connection
                try {
                    await mongoose.connection.close();
                    console.log('MongoDB connection closed.');
                } catch (dbCloseError) {
                    console.error("Error closing MongoDB connection:", dbCloseError);
                }
                process.exit(0); // Exit after server and DB are closed
            });
        } else {
             // If server wasn't assigned, try closing DB and exit
             try {
                 await mongoose.connection.close();
                 console.log('MongoDB connection closed.');
             } catch (dbCloseError) {
                 console.error("Error closing MongoDB connection:", dbCloseError);
             }
            process.exit(0);
        }

        // Force exit after timeout if server.close callback doesn't finish
        setTimeout(() => {
            console.error('Graceful shutdown timed out, forcing exit.');
            process.exit(1);
        }, 10000); // 10 seconds

    } catch (shutdownError) {
        console.error("Error during graceful shutdown initiation:", shutdownError);
        process.exit(1);
    }
};

process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

// --- RAG Service Health Check ---
async function checkRagService(url) {
    console.log(`\nChecking RAG service health at ${url}...`);
    try {
        const response = await axios.get(`${url}/health`, { timeout: 7000 }); // 7 second timeout
        if (response.status === 200 && response.data?.status === 'ok') {
            console.log(' RAG service is available and healthy.');
            console.log(`  Embedding: ${response.data.embedding_model_type} (${response.data.embedding_model_name})`);
            console.log(`  Default Index Loaded: ${response.data.default_index_loaded}`);
            if (response.data.message && response.data.message.includes("Warning:")) {
                 console.warn(`  RAG Health Warning: ${response.data.message}`);
            }
            return true;
        } else {
             console.warn(`! RAG service responded but status is not OK: ${response.status} - ${JSON.stringify(response.data)}`);
             return false;
        }
    } catch (error) {
        console.warn('! RAG service is not reachable.');
        if (error.code === 'ECONNREFUSED') {
             console.warn(`  Connection refused at ${url}. Ensure the RAG service (server/rag_service/app.py) is running.`);
        } else if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
             console.warn(`  Connection timed out to ${url}. The RAG service might be slow to start or unresponsive.`);
        }
         else {
             console.warn(`  Error: ${error.message}`);
        }
        console.warn('  RAG features (document upload processing, context retrieval) will be unavailable.');
        return false;
    }
}

// --- Directory Structure Check (Simplified) ---
async function ensureServerDirectories() {
    const dirs = [
        path.join(__dirname, 'assets'),
        path.join(__dirname, 'backup_assets'),
        // Add other essential dirs if needed
    ];
    console.log("\nEnsuring server directories exist...");
    try {
        for (const dir of dirs) {
            // Check existence synchronously, create asynchronously
            if (!fs.existsSync(dir)) {
                await fs.promises.mkdir(dir, { recursive: true });
                console.log(`  Created directory: ${dir}`);
            } else {
                // console.log(`  Directory exists: ${dir}`); // Optional: be less verbose
            }
        }
        console.log(" Server directories checked/created.");
    } catch (error) {
        console.error('!!! Error creating essential server directories:', error);
        throw error; // Prevent server start if essential dirs fail
    }
}

// --- Prompt for Configuration ---
function askQuestion(query) {
    return new Promise(resolve => readline.question(query, resolve));
}

async function configureAndStart() {
    console.log("--- Starting Server Configuration ---");
    
    // 1. Gemini API Key Check
    if (!geminiApiKey) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! FATAL: GEMINI_API_KEY environment variable is not set. !!!");
        console.error("!!! Please set it before running the server:               !!!");
        console.error("!!! export GEMINI_API_KEY='YOUR_API_KEY'                   !!!");
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1);
    } else {
        console.log(" GEMINI_API_KEY found.");
    }

    // 2. MongoDB URI
    if (!
        mongoUri) {
        const answer = await askQuestion(`Enter MongoDB URI or press Enter for default (${DEFAULT_MONGO_URI}): `);
        mongoUri = answer.trim() || DEFAULT_MONGO_URI;
    }
    console.log(`Using MongoDB URI: ${mongoUri}`);

    // 3. Python RAG Service URL
    if (!pythonRagUrl) {
        const answer = await askQuestion(`Enter Python RAG Service URL or press Enter for default (${DEFAULT_PYTHON_RAG_URL}): `);
        pythonRagUrl = answer.trim() || DEFAULT_PYTHON_RAG_URL;
    }
    console.log(`Using Python RAG Service URL: ${pythonRagUrl}`);

    // 4. Port (Optional override via prompt, primarily uses ENV or default)
    // You could add a prompt here if needed, but ENV variable is common practice
    console.log(`Node.js server will listen on port: ${port}`);

    readline.close(); // Close the prompt interface

    // --- Pass configuration to other modules (if needed) ---
    // We'll make connectDB and services read directly or pass via function calls
    process.env.MONGO_URI = mongoUri; // Set for db.js
    process.env.PYTHON_RAG_SERVICE_URL = pythonRagUrl; // Set for chat.js, upload.js
    // GEMINI_API_KEY is already in process.env

    console.log("--- Configuration Complete ---");

    // --- Proceed with Server Startup ---
    await startServer();
}


// --- Asynchronous Server Startup Function ---
async function startServer() {
    console.log("\n--- Starting Server Initialization ---");
    try {
        await ensureServerDirectories(); // Check/create assets, backup_assets dirs
        await connectDB(mongoUri); // Connect to MongoDB - Pass URI explicitly
        await performAssetCleanup(); // Backup existing assets, create fresh user folders
        await checkRagService(pythonRagUrl); // Check Python RAG service status

        const PORT = port; // Use the configured port
        const availableIPs = getLocalIPs(); // Get all local IPs

        server = app.listen(PORT, '0.0.0.0', () => { // Listen on all interfaces
            console.log('\n=== Node.js Server Ready ===');
            console.log(` Server listening on port ${PORT}`);
            console.log('   Access the application via these URLs (using common frontend ports):');
            const frontendPorts = [3000, 3001, 8080, 5173]; // Common React/Vite ports
            availableIPs.forEach(ip => {
                 frontendPorts.forEach(fp => {
                    console.log(`   - http://${ip}:${fp} (Frontend) -> Connects to Backend at http://${ip}:${PORT}`);
                 });
            });
            console.log('============================\n');
            console.log(" Hint: Client automatically detects backend IP based on how you access the frontend.");
            console.log(`   Ensure firewalls allow connections on port ${PORT} (Backend) and your frontend port.`);
            console.log("--- Server Initialization Complete ---");
        });

    } catch (error) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! Failed to start Node.js server:", error.message);
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1); // Exit if initialization fails
    }
}

// --- Execute Configuration and Server Start ---
configureAndStart();

```

`server/services/geminiService.js`

```javascript
// server/services/geminiService.js
const { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } = require('@google/generative-ai');
// require('dotenv').config(); // Removed dotenv

// Read API Key directly from environment variables
const API_KEY = process.env.GEMINI_API_KEY;
const MODEL_NAME = "gemini-1.5-flash"; // Or read from env: process.env.GEMINI_MODEL_NAME || "gemini-1.5-flash";

if (!API_KEY) {
    // This check is now primarily done in server.js before starting
    // But keep a safeguard here.
    console.error("FATAL ERROR: GEMINI_API_KEY is not available in the environment. Server should have exited.");
    // Throw an error instead of exiting here, let the caller handle it
    throw new Error("GEMINI_API_KEY is missing.");
}

const genAI = new GoogleGenerativeAI(API_KEY);

const baseGenerationConfig = {
    temperature: 0.7, // Moderate temperature for creative but grounded responses
    maxOutputTokens: 4096, // Adjust as needed, Flash model limit might be higher
    // topP: 0.9, // Example: Could add nucleus sampling
    // topK: 40,  // Example: Could add top-k sampling
};

// Stricter safety settings - adjust as needed for your use case
const baseSafetySettings = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
];

const generateContentWithHistory = async (chatHistory, systemPromptText = null, relevantDocs = []) => {
    try {
        if (!Array.isArray(chatHistory) || chatHistory.length === 0) {
             throw new Error("Chat history must be a non-empty array.");
        }
        // Gemini API requires history to end with a 'user' message for sendMessage
        if (chatHistory[chatHistory.length - 1].role !== 'user') {
            console.error("History for Gemini API must end with a 'user' role message.");
            // Attempt to fix by removing trailing non-user messages if any? Risky.
            // Or just throw error.
            throw new Error("Internal error: Invalid chat history sequence for API call.");
        }

        // --- Prepare Model Options ---
        const modelOptions = {
            model: MODEL_NAME,
            generationConfig: baseGenerationConfig,
            safetySettings: baseSafetySettings,
            // Add system instruction if provided
            ...(systemPromptText && typeof systemPromptText === 'string' && systemPromptText.trim() !== '' && {
                systemInstruction: {
                    // Gemini expects system instruction parts as an array
                    parts: [{ text: systemPromptText.trim() }]
                }
             })
        };
        const model = genAI.getGenerativeModel(modelOptions);


        // --- Prepare History for startChat ---
        // History for startChat should NOT include the latest user message
        const historyForStartChat = chatHistory.slice(0, -1)
            .map(msg => ({ // Ensure correct format
                 role: msg.role,
                 parts: msg.parts.map(part => ({ text: part.text || '' }))
            }))
            .filter(msg => msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string'); // Basic validation

        // --- Start Chat Session ---
        const chat = model.startChat({
            history: historyForStartChat,
        });

        // --- Prepare the message to send ---
        // Get the text from the last user message in the original history
        let lastUserMessageText = chatHistory[chatHistory.length - 1].parts[0].text;

        // Optional: Add a subtle hint for citation if RAG was used (Gemini might pick it up)
        // if (relevantDocs.length > 0) {
        //     const citationHint = ` (Remember to cite sources like ${relevantDocs.map((doc, i) => `[${i+1}] ${doc.documentName}`).slice(0,2).join(', ')} if applicable)`;
        //     lastUserMessageText += citationHint;
        // }

        console.log(`Sending message to Gemini. History length sent to startChat: ${historyForStartChat.length}. System Prompt Used: ${!!modelOptions.systemInstruction}`);
        // console.log("Last User Message Text Sent:", lastUserMessageText.substring(0, 200) + "..."); // Log truncated message

        // --- Send Message ---
        const result = await chat.sendMessage(lastUserMessageText);

        // --- Process Response ---
        const response = result.response;
        const candidate = response?.candidates?.[0];

        // --- Validate Response ---
        if (!candidate || candidate.finishReason === 'STOP' || candidate.finishReason === 'MAX_TOKENS') {
            // Normal completion or max tokens reached
            const responseText = candidate?.content?.parts?.[0]?.text;
            if (typeof responseText === 'string') {
                return responseText; // Success
            } else {
                 console.warn("Gemini response finished normally but text content is missing or invalid.", { finishReason: candidate?.finishReason, content: candidate?.content });
                 throw new Error("Received an empty or invalid response from the AI service.");
            }
        } else {
             // Handle blocked responses or other issues
             const finishReason = candidate?.finishReason || 'Unknown';
             const safetyRatings = candidate?.safetyRatings;
             console.warn("Gemini response was potentially blocked or had issues.", { finishReason, safetyRatings });

             let blockMessage = `AI response generation failed or was blocked.`;
             if (finishReason) blockMessage += ` Reason: ${finishReason}.`;
             if (safetyRatings) {
                const blockedCategories = safetyRatings.filter(r => r.blocked).map(r => r.category).join(', ');
                if (blockedCategories) {
                    blockMessage += ` Blocked Categories: ${blockedCategories}.`;
                }
             }

             const error = new Error(blockMessage);
             error.status = 400; // Treat as a bad request or policy issue
             throw error;
        }

    } catch (error) {
        console.error("Gemini API Call Error:", error?.message || error);
        // Improve error message for client
        let clientMessage = "Failed to get response from AI service.";
        if (error.message?.includes("API key not valid")) {
            clientMessage = "AI Service Error: Invalid API Key.";
        } else if (error.message?.includes("blocked")) {
            clientMessage = error.message; // Use the specific block message
        } else if (error.status === 400) {
             clientMessage = `AI Service Error: ${error.message}`;
        }

        const enhancedError = new Error(clientMessage);
        enhancedError.status = error.status || 500; // Keep original status if available
        enhancedError.originalError = error; // Attach original error if needed for server logs
        throw enhancedError;
    }
};

module.exports = { generateContentWithHistory };

```

`server/services/kgService.js`

```javascript
// server/services/kgService.js
const geminiService = require('./geminiService'); // Assuming it's in the same folder or adjust path
const { v4: uuidv4 } = require('uuid'); // For generating unique IDs if needed, or rely on LLM
const axios = require('axios');

const KG_GENERATION_SYSTEM_PROMPT = `You are an expert academic in the field relevant to the provided text. Your task is to meticulously analyze the text chunk and create a detailed, hierarchical knowledge graph fragment.
The output MUST be a valid JSON object with "nodes" and "edges" sections.

Instructions for Node Creation:
1.  Identify CORE CONCEPTS or main topics discussed in the chunk. These should be 'major' nodes (parent: null).
2.  Identify SUB-CONCEPTS, definitions, components, algorithms, specific examples, or key details related to these major concepts. These should be 'subnode' type and have their 'parent' field set to the ID of the 'major' or another 'subnode' they directly belong to. Aim for a granular breakdown.
3.  Node 'id': Use a concise, descriptive, and specific term for the concept (e.g., "Linear Regression", "LMS Update Rule", "Feature Selection"). Capitalize appropriately.
4.  Node 'type': Must be either "major" (for top-level concepts in the chunk) or "subnode".
5.  Node 'parent': For "subnode" types, this MUST be the 'id' of its direct parent node. For "major" nodes, this MUST be null.
6.  Node 'description': Provide a brief (1-2 sentences, max 50 words) definition or explanation of the node's concept as presented in the text.

Instructions for Edge Creation:
1.  Edges represent relationships BETWEEN the nodes you've identified.
2.  The 'from' field should be the 'id' of the child/more specific node.
3.  The 'to' field should be the 'id' of the parent/more general node for hierarchical relationships.
4.  Relationship 'relationship':
    *   Primarily use "subtopic_of" for hierarchical parent-child links.
    *   Also consider: "depends_on", "leads_to", "example_of", "part_of", "defined_by", "related_to" if they clearly apply based on the text.
5.  Ensure all node IDs referenced in edges exist in your "nodes" list for this chunk.

Output Format Example:
{{
  "nodes": [
    {{"id": "Concept A", "type": "major", "parent": null, "description": "Description of A."}},
    {{"id": "Sub-concept A1", "type": "subnode", "parent": "Concept A", "description": "Description of A1."}},
    {{"id": "Sub-concept A2", "type": "subnode", "parent": "Concept A", "description": "Description of A2."}},
    {{"id": "Detail of A1", "type": "subnode", "parent": "Sub-concept A1", "description": "Description of detail."}}
  ],
  "edges": [
    {{"from": "Sub-concept A1", "to": "Concept A", "relationship": "subtopic_of"}},
    {{"from": "Sub-concept A2", "to": "Concept A", "relationship": "subtopic_of"}},
    {{"from": "Detail of A1", "to": "Sub-concept A1", "relationship": "subtopic_of"}},
    {{"from": "Sub-concept A1", "to": "Sub-concept A2", "relationship": "related_to"}} // Example of a non-hierarchical link
  ]
}}

Analyze the provided text chunk carefully and generate the JSON. Be thorough in identifying distinct concepts and their relationships to create a rich graph.
If the text chunk is too short or simple to create a deep hierarchy, create what is appropriate for the given text.
`;


function constructKgPromptForChunk(chunkText) {
    return `
Text chunk to process:
---
${chunkText}
---
Based on the instructions provided in the system prompt, generate the JSON output for nodes and edges from this text chunk.
`;
}



async function _processSingleChunkForKg(chunkTextContent, chunkMetadata, chunkIndex) {
    const userPrompt = constructKgPromptForChunk(chunkTextContent);
    // For this specific task, history is just the current request for the chunk
    const chatHistory = [
        { role: 'user', parts: [{ text: userPrompt }] }
    ];

    try {
        console.log(`[KG Service] Processing chunk ${chunkMetadata?.chunk_reference_name || `index ${chunkIndex}`} for KG generation.`);
        const responseText = await geminiService.generateContentWithHistory(
            chatHistory,
            KG_GENERATION_SYSTEM_PROMPT
        );

        if (!responseText) {
            console.warn(`[KG Service] Empty response from Gemini for chunk ${chunkMetadata?.chunk_reference_name || `index ${chunkIndex}`}.`);
            return null;
        }

        // Attempt to parse the JSON response
        // Gemini might sometimes wrap JSON in ```json ... ```
        let cleanedResponseText = responseText.trim();
        if (cleanedResponseText.startsWith("```json")) {
            cleanedResponseText = cleanedResponseText.substring(7);
            if (cleanedResponseText.endsWith("```")) {
                cleanedResponseText = cleanedResponseText.slice(0, -3);
            }
            cleanedResponseText = cleanedResponseText.trim();
        } else if (cleanedResponseText.startsWith("```")) {
            cleanedResponseText = cleanedResponseText.substring(3);
            if (cleanedResponseText.endsWith("```")) {
                cleanedResponseText = cleanedResponseText.slice(0, -3);
            }
            cleanedResponseText = cleanedResponseText.trim();
        }


        const graphData = JSON.parse(cleanedResponseText);

        // Validate structure
        if (graphData && typeof graphData === 'object' && Array.isArray(graphData.nodes) && Array.isArray(graphData.edges)) {
            // Optionally, add chunk reference to nodes/edges for easier debugging or advanced merging
            // graphData.nodes.forEach(node => node.source_chunk_ref = chunkMetadata?.chunk_reference_name);
            // graphData.edges.forEach(edge => edge.source_chunk_ref = chunkMetadata?.chunk_reference_name);
            return graphData;
        } else {
            console.warn(`[KG Service] Invalid graph structure from Gemini for chunk ${chunkMetadata?.chunk_reference_name || `index ${chunkIndex}`}. Response:`, cleanedResponseText.substring(0, 200));
            return null;
        }
    } catch (error) {
        console.error(`[KG Service] Error processing chunk ${chunkMetadata?.chunk_reference_name || `index ${chunkIndex}`}:`, error.message);
        if (error.originalError) console.error("[KG Service] Original Gemini error:", error.originalError);
        // console.error("[KG Service] Full text that caused error:", userPrompt.substring(0,500)); // Be careful with logging full text
        return null;
    }
}


function _mergeGraphFragments(graphFragments) {
    console.log(`[KG Service] Merging ${graphFragments.length} graph fragments...`);
    const finalNodesMap = new Map(); // Using Map for easier get/set
    const finalEdgesSet = new Set(); // To store unique edge strings like "from|to|relationship"

    for (const fragment of graphFragments) {
        if (!fragment || !fragment.nodes || !fragment.edges) {
            console.warn("[KG Service] Skipping invalid or null graph fragment during merge.");
            continue;
        }

        // Merge Nodes
        for (const node of fragment.nodes) {
            if (!node || typeof node.id !== 'string' || !node.id.trim()) {
                console.warn("[KG Service] Skipping invalid node during merge:", node);
                continue;
            }
            const nodeId = node.id.trim();
            if (!finalNodesMap.has(nodeId)) {
                finalNodesMap.set(nodeId, { ...node, id: nodeId }); // Store a copy
            } else {
                const existingNode = finalNodesMap.get(nodeId);
                // Update description if new one is longer/more descriptive (simple heuristic)
                if (node.description && typeof node.description === 'string' &&
                    (!existingNode.description || node.description.length > existingNode.description.length)) {
                    existingNode.description = node.description;
                }
                // Prefer a more specific type if current is generic or null
                if (node.type && (!existingNode.type || existingNode.type === "generic")) {
                    existingNode.type = node.type;
                }
                // If existing node doesn't have a parent but new one does
                if (node.parent && !existingNode.parent) {
                    existingNode.parent = node.parent;
                }
                // You might add more sophisticated merging logic here if needed
            }
        }

        // Merge Edges
        for (const edge of fragment.edges) {
            if (!edge || typeof edge.from !== 'string' || typeof edge.to !== 'string' || typeof edge.relationship !== 'string' ||
                !edge.from.trim() || !edge.to.trim() || !edge.relationship.trim()) {
                console.warn("[KG Service] Skipping invalid edge during merge:", edge);
                continue;
            }
            const edgeKey = `${edge.from.trim()}|${edge.to.trim()}|${edge.relationship.trim()}`;
            finalEdgesSet.add(edgeKey);
        }
    }

    const mergedNodes = Array.from(finalNodesMap.values());
    const mergedEdges = Array.from(finalEdgesSet).map(edgeKey => {
        const [from, to, relationship] = edgeKey.split('|');
        return { from, to, relationship };
    });

    console.log(`[KG Service] Merged into ${mergedNodes.length} nodes and ${mergedEdges.length} edges.`);
    return { nodes: mergedNodes, edges: mergedEdges };
}


/**
 * Generates a knowledge graph from chunks of text using Gemini and merges the results.
 * @param {Array<Object>} chunksForKg - Array of chunks, e.g., [{id, text_content, metadata}, ...]
 * @param {string} userId - The ID of the user.
 * @param {string} originalName - The original name of the uploaded file.
 * @returns {Promise<Object|null>} The merged knowledge graph {nodes, edges} or null on failure.
 */




async function generateAndStoreKg(chunksForKg, userId, originalName) {
    console.log(`[KG Service] Starting KG generation for document: ${originalName} (User: ${userId}) with ${chunksForKg.length} chunks.`);

    if (!chunksForKg || chunksForKg.length === 0) {
        console.warn("[KG Service] No chunks provided for KG generation.");
        return null;
    }

    const graphFragments = [];
    // Process chunks sequentially to avoid overwhelming Gemini or hitting rate limits quickly.
    // For parallel processing with controlled concurrency, libraries like 'p-limit' can be used.
    // For now, let's do it sequentially for simplicity and rate-limit safety. If you have many chunks, consider p-limit.
    let chunkIndex = 0;
    for (const chunk of chunksForKg) {
        if (!chunk || !chunk.text_content) {
            console.warn(`[KG Service] Skipping chunk index ${chunkIndex} due to missing text_content.`);
            chunkIndex++;
            continue;
        }
        const fragment = await _processSingleChunkForKg(chunk.text_content, chunk.metadata, chunkIndex);
        if (fragment) {
            graphFragments.push(fragment);
        }
        chunkIndex++;
        // Optional: Add a small delay if you're concerned about hitting API rate limits rapidly
        // await new Promise(resolve => setTimeout(resolve, 200)); // 200ms delay
    }


    // Alternative: Parallel processing with Promise.all (can be faster, but watch for rate limits)
    /*
    const processingPromises = chunksForKg.map((chunk, index) => {
        if (!chunk || !chunk.text_content) {
            console.warn(`[KG Service] Skipping chunk index ${index} due to missing text_content.`);
            return Promise.resolve(null); // Resolve with null for invalid chunks
        }
        return _processSingleChunkForKg(chunk.text_content, chunk.metadata, index);
    });

    const settledFragments = await Promise.allSettled(processingPromises);
    const graphFragments = settledFragments
        .filter(result => result.status === 'fulfilled' && result.value)
        .map(result => result.value);

    settledFragments.forEach(result => {
        if (result.status === 'rejected') {
            console.error("[KG Service] A chunk processing promise was rejected:", result.reason);
        }
    });
    */


    if (graphFragments.length === 0) {
        console.warn(`[KG Service] No valid graph fragments were generated for ${originalName}.`);
        return null;
    }

    const finalKg = _mergeGraphFragments(graphFragments);

    // --- THIS IS WHERE YOU'D INTEGRATE NEO4J LATER ---
    // For now, we just log it.
    // console.log(`[KG Service] Successfully generated KG for document: ${originalName}`);
    // console.log("[KG Service] Final Merged KG (Nodes):", JSON.stringify(finalKg.nodes.slice(0, 5), null, 2) + (finalKg.nodes.length > 5 ? "\n..." : "")); // Log first 5 nodes
    // console.log("[KG Service] Final Merged KG (Edges):", JSON.stringify(finalKg.edges.slice(0, 5), null, 2) + (finalKg.edges.length > 5 ? "\n..." : "")); // Log first 5 edges
    // To see the full graph: console.log("[KG Service] Full Final Merged KG:", JSON.stringify(finalKg, null, 2));
    console.log("[KG Service] Full Final Merged KG : ", finalKg);
    

        // --- Determine and Call the KG Ingestion API ---
    const baseRagUrl = process.env.DEFAULT_PYTHON_RAG_URL || 'http://localhost:5000'; // Use default if not set
    const kgIngestionApiUrl = `${baseRagUrl.replace(/\/$/, '')}/kg`; // Ensure no double slash and append /kg

    if (!kgIngestionApiUrl.startsWith('http')) { // Basic check if a valid URL was formed
        console.error("[KG Service] KG Ingestion API URL could not be determined properly. Check DEFAULT_PYTHON_RAG_URL. Current URL:", kgIngestionApiUrl);
        return {
            success: false,
            message: "KG generated, but KG Ingestion API URL is invalid. KG not stored.",
        };
    }

    console.log(`[KG Service] Sending KG for '${originalName}' to KG Ingestion API: ${kgIngestionApiUrl}`);
    try {
        const payload = {
            userId: userId,
            originalName: originalName,
            nodes: finalKg.nodes,
            edges: finalKg.edges
        };

        const serviceResponse = await axios.post(kgIngestionApiUrl, payload, {
            timeout: 180000 // 3 minute timeout
        });

        const responseData = serviceResponse.data;
        console.log(`[KG Service] KG Ingestion API response for '${originalName}':`, responseData);

        // --- Define how to determine success from your API's response ---
        // Example: expecting { "document-name": "...", "status": "completed", "userId": "..." }
        const API_SUCCESS_STATUS_VALUE = "completed"; // <<<< IMPORTANT: CHANGE THIS to your API's actual success status string
        // ---

        if (serviceResponse.status >= 200 && serviceResponse.status < 300 && responseData && responseData.status === API_SUCCESS_STATUS_VALUE) {
            if (responseData['document-name'] !== originalName || responseData.userId !== userId) {
                console.warn(`[KG Service] Mismatch in KG API response for '${originalName}'. Expected doc/user: ${originalName}/${userId}, Got: ${responseData['document-name']}/${responseData.userId}`);
            }
            const successMessage = `KG for '${originalName}' successfully processed by Ingestion API. Status: ${responseData.status}.`;
            console.log(`[KG Service] ${successMessage}`);
            return {
                success: true,
                message: successMessage,
                serviceResponseData: responseData // Pass back the API's response data
            };
        } else {
            const failureMessage = `KG Ingestion API for '${originalName}' indicated failure or unexpected status. HTTP: ${serviceResponse.status}, API Status: '${responseData?.status || "N/A"}'. Response Msg: ${responseData?.message || responseData?.error || 'No specific error from API.'}`;
            console.warn(`[KG Service] ${failureMessage}`);
            return {
                success: false,
                message: failureMessage,
                serviceResponseData: responseData
            };
        }
    } catch (error) {
        const errorMsg = error.response?.data?.message || error.response?.data?.error || error.message || "Unknown error calling KG Ingestion API";
        console.error(`[KG Service] Error calling KG Ingestion API for '${originalName}':`, errorMsg);
        if (error.response?.data) console.error("[KG Service] KG API Error Response Data:", error.response.data);
        return {
            success: false,
            message: `KG generated, but error calling KG Ingestion API: ${errorMsg}`,
        };
    }
}

module.exports = { generateAndStoreKg };
```

`server/utils/assetCleanup.js`

```javascript
const fs = require('fs').promises; // Use fs.promises for async operations
const path = require('path');

// Define constants relative to this file's location (server/utils)
const ASSETS_DIR = path.join(__dirname, '..', 'assets'); // Go up one level to server/assets
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets'); // Go up one level to server/backup_assets
const FOLDER_TYPES = ['docs', 'images', 'code', 'others']; // Folders within each user's asset dir

/**
 * Moves existing user asset folders (docs, images, code, others) to a timestamped
 * backup location and recreates empty asset folders for each user on server startup.
 */
async function performAssetCleanup() {
    console.log("\n--- Starting Asset Cleanup ---");
    try {
        // Ensure backup base directory exists
        await fs.mkdir(BACKUP_DIR, { recursive: true });

        // List potential user directories in assets
        let userDirs = [];
        try {
            userDirs = await fs.readdir(ASSETS_DIR);
        } catch (err) {
            if (err.code === 'ENOENT') {
                console.log("Assets directory doesn't exist yet, creating it and skipping cleanup.");
                await fs.mkdir(ASSETS_DIR, { recursive: true }); // Ensure assets dir exists
                console.log("--- Finished Asset Cleanup (No existing assets found) ---");
                return; // Nothing to clean up
            }
            throw err; // Re-throw other errors accessing assets dir
        }

        if (userDirs.length === 0) {
             console.log("Assets directory is empty. Skipping backup/move operations.");
             console.log("--- Finished Asset Cleanup (No user assets found) ---");
             return;
        }

        const timestamp = new Date().toISOString().replace(/[:.]/g, '-'); // Create a safe timestamp string

        for (const userName of userDirs) {
            const userAssetPath = path.join(ASSETS_DIR, userName);
            const userBackupPathBase = path.join(BACKUP_DIR, userName);
            const userTimestampBackupPath = path.join(userBackupPathBase, `backup_${timestamp}`);

            try {
                // Check if the item in assets is actually a directory
                const stats = await fs.stat(userAssetPath);
                if (!stats.isDirectory()) {
                    console.log(`  Skipping non-directory item in assets: ${userName}`);
                    continue;
                }

                console.log(`  Processing assets for user: [${userName}]`);
                let backupDirCreated = false; // Track if backup dir was created for this user/run
                let movedSomething = false; // Track if anything was actually moved

                // Process each defined folder type (docs, images, etc.)
                for (const type of FOLDER_TYPES) {
                    const sourceTypePath = path.join(userAssetPath, type);
                    try {
                        // Check if the source type directory exists before trying to move
                        await fs.access(sourceTypePath);

                        // If source exists, ensure the timestamped backup directory is ready
                        if (!backupDirCreated) {
                            await fs.mkdir(userTimestampBackupPath, { recursive: true });
                            backupDirCreated = true;
                            // console.log(`    Created backup directory: ${userTimestampBackupPath}`);
                        }

                        // Define the destination path in the backup folder
                        const backupTypePath = path.join(userTimestampBackupPath, type);
                        // console.log(`    Moving ${sourceTypePath} to ${backupTypePath}`);
                        // Move the existing type folder to the backup location
                        await fs.rename(sourceTypePath, backupTypePath);
                        movedSomething = true;

                    } catch (accessErr) {
                        // Ignore error if the source directory doesn't exist (ENOENT)
                        if (accessErr.code !== 'ENOENT') {
                            console.error(`    Error accessing source folder ${sourceTypePath}:`, accessErr.message);
                        }
                        // If ENOENT, the folder doesn't exist, nothing to move.
                    }

                    // Always ensure the empty type directory exists in the main assets folder
                    try {
                        // console.log(`    Ensuring empty directory: ${sourceTypePath}`);
                        await fs.mkdir(sourceTypePath, { recursive: true });
                    } catch (mkdirErr) {
                         console.error(`    Failed to recreate directory ${sourceTypePath}:`, mkdirErr.message);
                    }
                } // End loop through FOLDER_TYPES

                 if (movedSomething) {
                     console.log(`  Finished backup for user [${userName}] to backup_${timestamp}`);
                 } else {
                     console.log(`  No existing asset types found to backup for user [${userName}]`);
                 }


            } catch (userDirStatErr) {
                 // Error checking if the item in assets is a directory
                 console.error(`Error processing potential user asset directory ${userAssetPath}:`, userDirStatErr.message);
            }
        } // End loop through userDirs

        console.log("--- Finished Asset Cleanup ---");

    } catch (error) {
        // Catch errors related to backup dir creation or reading the main assets dir
        console.error("!!! Critical Error during Asset Cleanup process:", error);
    }
}

// Export the function to be used elsewhere
module.exports = { performAssetCleanup };

```

`server/utils/networkUtils.js`

```javascript
const os = require('os');

function getLocalIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost

    for (const iface of Object.values(interfaces)) {
        for (const addr of iface) {
            // Include IPv4 non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

function getPreferredLocalIP() {
    const ips = getLocalIPs();
    // Prioritize non-localhost, non-link-local (169.254) IPs
    // Often 192.168.* or 10.* or 172.16-31.* are common private ranges
    return ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost' && (ip.startsWith('192.168.') || ip.startsWith('10.') || ip.match(/^172\.(1[6-9]|2[0-9]|3[0-1])\./))) ||
           ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost') || // Any other non-link-local
           'localhost'; // Fallback
}

module.exports = { getLocalIPs, getPreferredLocalIP };

```

`server/workers/kgWorker.js`

```javascript
// File: server/workers/kgWorker.js
const { workerData, parentPort } = require('worker_threads');

// Adjust the path to YOUR kgService.js file
// If kgWorker.js is in 'server/workers/' and kgService.js is in 'server/services/'
// then the path would be '../services/kgService'.
const kgService = require('../services/kgService'); // <<<< IMPORTANT: CHECK THIS PATH

async function runKgGeneration() {
    const { chunksForKg, userId, originalName } = workerData;

    console.log(`[KG Worker ${process.pid}] Received task. Starting KG generation via kgService for:`);
    console.log(`  User ID: ${userId}`);
    console.log(`  Original Filename: ${originalName}`);
    console.log(`  Number of Chunks: ${chunksForKg ? chunksForKg.length : 0}`);

    try {
        if (!chunksForKg || chunksForKg.length === 0) {
            throw new Error("No chunksForKg provided to worker.");
        }

        // Call the main function from your kgService
        const result = await kgService.generateAndStoreKg(chunksForKg, userId, originalName);

        if (result && result.success) {
            console.log(`[KG Worker ${process.pid}] KG generation successful for document: ${originalName}`);
            if (parentPort) {
                parentPort.postMessage({
                    success: true,
                    message: "KG generation completed successfully by worker.",
                    // Optionally, send back a summary if needed by the main thread
                    // knowledgeGraphSummary: {
                    //     nodes: result.knowledgeGraph.nodes.length,
                    //     edges: result.knowledgeGraph.edges.length
                    // }
                });
            }
        } else {
            const errorMessage = (result && result.message) ? result.message : "KG generation process failed in service or returned no result.";
            console.error(`[KG Worker ${process.pid}] KG generation failed for document: ${originalName}. Error: ${errorMessage}`);
            if (parentPort) {
                parentPort.postMessage({
                    success: false,
                    error: errorMessage,
                    document: originalName
                });
            }
        }
    } catch (error) {
        console.error(`[KG Worker ${process.pid}] Critical error during KG generation task for document: ${originalName}:`, error);
        if (parentPort) {
            parentPort.postMessage({
                success: false,
                error: error.message || "Unknown critical error in KG worker.",
                document: originalName
            });
        }
    } finally {
        // If not using parentPort.postMessage , the worker might exit.
        // If parentPort is used, the main thread controls when the worker instance might be terminated
        // or if the worker should explicitly close itself (e.g., if it's designed for one-off tasks).
        // For simple fire-and-forget, this is okay.
        if (!parentPort) { // If run directly, not as a worker
            process.exit(error ? 1 : 0);
        }
    }
}

// Start the generation process when the worker is invoked
runKgGeneration();
```

