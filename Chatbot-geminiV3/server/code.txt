.env


PORT=5001 # Port for the backend (make sure it's free)
MONGO_URI = "mongodb://localhost:27017/chatbot_gemini" # Your MongoDB connection string
JWT_SECRET = "your_super_strong_and_secret_jwt_key_12345" # A strong, random secret key for JWT
GEMINI_API_KEY = "AIzaSyBiCYeCICXAuEEqVvwwaiOpO9gTg6mJLzw" # Your actual Gemini API Key



code.txt





config/db.js

javascript
const mongoose = require('mongoose');
// const dotenv = require('dotenv'); // Removed dotenv

// dotenv.config(); // Removed dotenv

// Modified connectDB to accept the URI as an argument
const connectDB = async (mongoUri) => {
  if (!mongoUri) {
      console.error('MongoDB Connection Error: URI is missing.');
      process.exit(1);
  }
  try {
    // console.log(`Attempting MongoDB connection to: ${mongoUri}`); // Debug: Careful logging URI
    const conn = await mongoose.connect(mongoUri, {
      // Mongoose 6+ uses these defaults, so they are not needed
      // useNewUrlParser: true,
      // useUnifiedTopology: true,
      // serverSelectionTimeoutMS: 5000 // Example: Optional: Timeout faster
    });

    console.log(`âœ“ MongoDB Connected Successfully`); // Simpler success message
    return conn; // Return connection object if needed elsewhere
  } catch (error) {
    console.error('MongoDB Connection Error:', error.message);
    // Exit process with failure
    process.exit(1);
  }
};

module.exports = connectDB;



config/promptTemplates.js

javascript
// server/config/promptTemplates.js

const ANALYSIS_THINKING_PREFIX_TEMPLATE = `**STEP 1: THINKING PROCESS (Recommended):**
*   Before generating the analysis, briefly outline your plan in \`<thinking>\` tags. Example: \`<thinking>Analyzing for FAQs. Will scan for key questions and answers presented in the text.</thinking>\`
*   If you include thinking, place the final analysis *after* the \`</thinking>\` tag.

**STEP 2: ANALYSIS OUTPUT:**
*   Generate the requested analysis based **strictly** on the text provided below.
*   Follow the specific OUTPUT FORMAT instructions carefully.

--- START DOCUMENT TEXT ---
{doc_text_for_llm}
--- END DOCUMENT TEXT ---
`; // Note: Escaped backticks if your template string itself uses them inside.

const ANALYSIS_PROMPTS = {
    faq: {
        // No need for PromptTemplate class here, just the string parts
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate 5-7 Frequently Asked Questions (FAQs) with concise answers based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first FAQ (after thinking, if used). Do **NOT** include preamble.
*   Format each FAQ as:
    Q: [Question derived ONLY from the text]
    A: [Answer derived ONLY from the text, concise]
*   If the text doesn't support an answer, don't invent one. Use Markdown for formatting if appropriate (e.g., lists within an answer).

**BEGIN OUTPUT (Start with 'Q:' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    topics: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Identify the 5-8 most important topics discussed. Provide a 1-2 sentence explanation per topic based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first topic (after thinking, if used). Do **NOT** include preamble.
*   Format as a Markdown bulleted list:
    *   **Topic Name:** Brief explanation derived ONLY from the text content (1-2 sentences max).

**BEGIN OUTPUT (Start with '*   **' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    mindmap: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate a mind map outline in Markdown list format representing key concepts and hierarchy ONLY from the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the main topic as the top-level item (using '-') (after thinking, if used). Do **NOT** include preamble.
*   Use nested Markdown lists ('-' or '*') with indentation (2 or 4 spaces) for hierarchy.
*   Focus **strictly** on concepts and relationships mentioned in the text. Be concise.

**BEGIN OUTPUT (Start with e.g., '- Main Topic' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    }
};

module.exports = { ANALYSIS_PROMPTS };


faiss_indices/sample.txt





middleware/authMiddleware.js

javascript
// server/middleware/authMiddleware.js
const User = require('../models/User');

// TEMPORARY Authentication Middleware (INSECURE - for debugging only)
// Checks for 'X-User-ID' header and attaches user to req.user
const tempAuth = async (req, res, next) => {
    const userId = req.headers['x-user-id']; // Read custom header (lowercase)

    // console.log("TempAuth Middleware: Checking for X-User-ID:", userId); // Debug log

    if (!userId) {
        console.warn("TempAuth Middleware: Missing X-User-ID header.");
        // Send 401 immediately if header is missing
        return res.status(401).json({ message: 'Unauthorized: Missing User ID header' });
    }

    try {
        // Find user by the ID provided in the header
        // Ensure Mongoose is connected before this runs (handled by server.js)
        const user = await User.findById(userId).select('-password'); // Exclude password

        if (!user) {
            console.warn(`TempAuth Middleware: User not found for ID: ${userId}`);
            // Send 401 if user ID is provided but not found in DB
            return res.status(401).json({ message: 'Unauthorized: User not found' });
        }

        // Attach user object to the request
        req.user = user;
        // console.log("TempAuth Middleware: User attached:", req.user.username); // Debug log
        next(); // Proceed to the next middleware or route handler

    } catch (error) {
        console.error('TempAuth Middleware: Error fetching user:', error);
        // Handle potential invalid ObjectId format errors
        if (error.name === 'CastError' && error.kind === 'ObjectId') {
             return res.status(400).json({ message: 'Bad Request: Invalid User ID format' });
        }
        // Send 500 for other unexpected errors during auth check
        res.status(500).json({ message: 'Server error during temporary authentication' });
    }
};

// Export the temporary middleware
module.exports = { tempAuth };



models/ChatHistory.js

javascript
const mongoose = require('mongoose');

const MessageSchema = new mongoose.Schema({
    role: {
        type: String,
        enum: ['user', 'model'], // Gemini roles
        required: true
    },
    parts: [{
        text: {
            type: String,
            required: true
        }
        // _id: false // Mongoose adds _id by default, can disable if truly not needed per part
    }],
    timestamp: {
        type: Date,
        default: Date.now
    }
}, { _id: false }); // Don't create separate _id for each message object in the array

const ChatHistorySchema = new mongoose.Schema({
    userId: {
        type: mongoose.Schema.Types.ObjectId,
        ref: 'User',
        required: true,
        index: true,
    },
    sessionId: {
        type: String,
        required: true,
        unique: true,
        index: true,
    },
    messages: [MessageSchema], // Array of message objects
    createdAt: {
        type: Date,
        default: Date.now,
    },
    updatedAt: {
        type: Date,
        default: Date.now,
    }
});

// Update `updatedAt` timestamp before saving any changes
ChatHistorySchema.pre('save', function (next) {
    if (this.isModified()) { // Only update if document changed
      this.updatedAt = Date.now();
    }
    next();
});

// Also update `updatedAt` on findOneAndUpdate operations if messages are modified
ChatHistorySchema.pre('findOneAndUpdate', function(next) {
  this.set({ updatedAt: new Date() });
  next();
});


const ChatHistory = mongoose.model('ChatHistory', ChatHistorySchema);

module.exports = ChatHistory;



models/User.js

javascript
const mongoose = require('mongoose');
const bcrypt = require('bcryptjs');

const UserSchema = new mongoose.Schema({
  username: {
    type: String,
    required: [true, 'Please provide a username'],
    unique: true,
    trim: true,
  },
  password: {
    type: String,
    required: [true, 'Please provide a password'],
    minlength: 6,
    select: false, // Explicitly prevent password from being returned by default
  },
  uploadedDocuments: [
    {
      filename: {
        type: String,
      },
      text: {
        type: String,
        default: "",
      },
      analysis: {
        faq: {
          type: String,
          default: "",
        },
        topics: {
          type: String,
          default: "",
        },
        mindmap: {
          type: String,
          default: "",
        },
      },
    },
  ],
  createdAt: {
    type: Date,
    default: Date.now,
  },
});

// Password hashing middleware before saving
UserSchema.pre('save', async function (next) {
  // Only hash the password if it has been modified (or is new)
  if (!this.isModified('password')) {
    return next();
  }
  try {
    const salt = await bcrypt.genSalt(10);
    this.password = await bcrypt.hash(this.password, salt);
    next();
  } catch (err) {
    next(err);
  }
});

// Method to compare entered password with hashed password
// Ensure we fetch the password field when needed for comparison
UserSchema.methods.comparePassword = async function (candidatePassword) {
  // 'this.password' might be undefined due to 'select: false'
  // Fetch the user again including the password if needed, or ensure the calling context selects it
  // However, bcrypt.compare handles the comparison securely.
  // We assume 'this.password' is available in the context where comparePassword is called.
  if (!this.password) {
      // This scenario should be handled by the calling code (e.g., findOne().select('+password'))
      // Or by using a static method like findByCredentials
      console.error("Attempted to compare password, but password field was not loaded on the User object."); // Added more specific log
      throw new Error("Password field not available for comparison.");
  }
  // Use bcryptjs's compare function
  return await bcrypt.compare(candidatePassword, this.password);
};

// Ensure password is selected when finding user for login comparison
UserSchema.statics.findByCredentials = async function(username, password) {
    // Find user by username AND explicitly select the password field
    const user = await this.findOne({ username }).select('+password');
    if (!user) {
        console.log(`findByCredentials: User not found for username: ${username}`); // Debug log
        return null; // User not found
    }
    // Now 'user' object has the password field, safe to call comparePassword
    const isMatch = await user.comparePassword(password);
    if (!isMatch) {
        console.log(`findByCredentials: Password mismatch for username: ${username}`); // Debug log
        return null; // Password doesn't match
    }
    console.log(`findByCredentials: Credentials match for username: ${username}`); // Debug log
    // Return user object (password will still be selected here, but won't be sent in JSON response usually)
    return user;
};


const User = mongoose.model('User', UserSchema);

module.exports = User;



rag_service/ai_core.py

python
# ./ai_core.py

# Standard Library Imports
import logging
import os
import io
import re
import copy
import uuid
from typing import Any, Callable, Dict, List, Optional


# --- Global Initializations ---
logger = logging.getLogger(__name__)

# --- Configuration Import ---
# Assumes 'server/config.py' is the actual config file.
# `import config` will work if 'server/' directory is in sys.path.
try:
    import config # This should import server/config.py
except ImportError as e:
    logger.info(f"CRITICAL: Failed to import 'config' (expected server/config.py): {e}. ")



# Local aliases for config flags, models, constants, and classes

# Availability Flags
PYPDF_AVAILABLE = config.PYPDF_AVAILABLE
PDFPLUMBER_AVAILABLE = config.PDFPLUMBER_AVAILABLE
PANDAS_AVAILABLE = config.PANDAS_AVAILABLE
DOCX_AVAILABLE = config.DOCX_AVAILABLE
PIL_AVAILABLE = config.PIL_AVAILABLE
FITZ_AVAILABLE = config.FITZ_AVAILABLE
PYTESSERACT_AVAILABLE = config.PYTESSERACT_AVAILABLE
SPACY_MODEL_LOADED = config.SPACY_MODEL_LOADED
PYPDF2_AVAILABLE = config.PYPDF2_AVAILABLE
EMBEDDING_MODEL_LOADED = config.EMBEDDING_MODEL_LOADED
MAX_TEXT_LENGTH_FOR_NER  = config.MAX_TEXT_LENGTH_FOR_NER
LANGCHAIN_SPLITTER_AVAILABLE = config.LANGCHAIN_SPLITTER_AVAILABLE

# Error Strings
PYPDF_PDFREADERROR = config.PYPDF_PDFREADERROR
TESSERACT_ERROR = config.TESSERACT_ERROR

# Libraries and Models
pypdf = config.pypdf
PyPDF2 = config.PyPDF2
pdfplumber = config.pdfplumber
pd = config.pd
DocxDocument = config.DocxDocument
Image = config.Image
fitz = config.fitz
pytesseract = config.pytesseract
nlp_spacy_core = config.nlp_spacy_core
document_embedding_model = config.document_embedding_model
RecursiveCharacterTextSplitter = config.RecursiveCharacterTextSplitter

# Constants
AI_CORE_CHUNK_SIZE = config.AI_CORE_CHUNK_SIZE
AI_CORE_CHUNK_OVERLAP = config.AI_CORE_CHUNK_OVERLAP
DOCUMENT_EMBEDDING_MODEL_NAME = config.DOCUMENT_EMBEDDING_MODEL_NAME


# --- Stage 1: File Parsing and Raw Content Extraction --- (Functions as previously corrected)
def _parse_pdf_content(file_path: str) -> Optional[str]:
    if not PYPDF_AVAILABLE or not pypdf:
        logger.error("pypdf library not available. PDF parsing with pypdf will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text: text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"pypdf: Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pypdf: File not found: {file_path}"); return None
    except PYPDF_PDFREADERROR as pdf_err: 
        logger.error(f"pypdf: Error reading PDF {os.path.basename(file_path)}: {pdf_err}"); return None
    except Exception as e:
        logger.error(f"pypdf: Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_docx_content(file_path: str) -> Optional[str]:
    if not DOCX_AVAILABLE or not DocxDocument:
        logger.error("python-docx library not available. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"docx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"docx: Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_txt_content(file_path: str) -> Optional[str]:
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: text_content = f.read()
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"txt: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"txt: Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_pptx_content(file_path: str) -> Optional[str]:
    if not PPTX_AVAILABLE or not Presentation:
        logger.warning(f"python-pptx not available. PPTX parsing for {os.path.basename(file_path)} skipped.")
        return None
    text_content = ""
    try:
        prs = Presentation(file_path)
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip(): text_content += shape.text.strip() + "\n\n"
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pptx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"pptx: Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _get_parser_for_file(file_path: str) -> Optional[Callable]:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf': return _parse_pdf_content
    if ext == '.docx': return _parse_docx_content
    if ext == '.pptx': return _parse_pptx_content
    if ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: return _parse_txt_content
    logger.warning(f"Unsupported file extension for basic parsing: {ext} ({os.path.basename(file_path)})")
    return None

def extract_raw_content_from_file(file_path: str) -> Dict[str, Any]:
    file_base_name = os.path.basename(file_path)
    logger.info(f"Starting raw content extraction for: {file_base_name}")
    text_content, tables, images, is_scanned = "", [], [], False
    file_extension = os.path.splitext(file_path)[1].lower()

    parser_func = _get_parser_for_file(file_path)
    if parser_func:
        initial_text = parser_func(file_path)
        if initial_text: text_content = initial_text

    if file_extension == '.pdf':
        if PDFPLUMBER_AVAILABLE and pdfplumber:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_parts = [p.extract_text(x_tolerance=1, y_tolerance=1) or "" for p in pdf.pages]
                    pdfplumber_text = "\n".join(pdfplumber_text_parts)
                    clean_pdfplumber_text_len = len(pdfplumber_text.replace("\n", ""))

                    if len(pdf.pages) > 0 and (clean_pdfplumber_text_len < 50 * len(pdf.pages) and clean_pdfplumber_text_len < 200):
                        is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (low text from pdfplumber).")
                    
                    if len(pdfplumber_text.strip()) > len(text_content.strip()): text_content = pdfplumber_text.strip()
                    elif not text_content.strip() and pdfplumber_text.strip(): text_content = pdfplumber_text.strip()
                    
                    for page_num, page in enumerate(pdf.pages): 
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data_list in page_tables_data:
                                if table_data_list and PANDAS_AVAILABLE and pd:
                                    try:
                                        if len(table_data_list) > 1 and all(isinstance(c, str) or c is None for c in table_data_list[0]):
                                            tables.append(pd.DataFrame(table_data_list[1:], columns=table_data_list[0]))
                                        else: tables.append(table_data_list)
                                    except Exception as df_err: logger.warning(f"pdfplumber: DataFrame conversion error for table on page {page_num} of {file_base_name}: {df_err}"); tables.append(table_data_list)
                                elif table_data_list: 
                                    tables.append(table_data_list)
                    if tables: logger.info(f"pdfplumber: Extracted {len(tables)} tables from {file_base_name}.")
            except Exception as e: logger.warning(f"pdfplumber: Error during rich PDF processing for {file_base_name}: {e}", exc_info=True)
        
        elif not text_content.strip() and PYPDF_AVAILABLE and pypdf: 
            try:
                if len(pypdf.PdfReader(file_path).pages) > 0: is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (no text from pypdf and pdfplumber not used/failed).")
            except: pass

        if FITZ_AVAILABLE and fitz and PIL_AVAILABLE and Image: 
            try:
                doc = fitz.open(file_path)
                for page_idx in range(len(doc)):
                    for img_info in doc.get_page_images(page_idx):
                        try: images.append(Image.open(io.BytesIO(doc.extract_image(img_info[0])["image"])))
                        except Exception as img_err: logger.warning(f"fitz: Could not open image xref {img_info[0]} from page {page_idx} of {file_base_name}: {img_err}")
                if images: logger.info(f"fitz: Extracted {len(images)} images from {file_base_name}.")
                doc.close()
            except Exception as e: logger.warning(f"fitz: Error extracting images from PDF {file_base_name}: {e}", exc_info=True)

    if not text_content.strip() and file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif']:
        is_scanned = True
        if PIL_AVAILABLE and Image:
            try: images.append(Image.open(file_path))
            except Exception as e: logger.error(f"Could not open image file {file_base_name}: {e}", exc_info=True)

    logger.info(f"Raw content extraction complete for {file_base_name}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {'text_content': text_content.strip(), 'tables': tables, 'images': images, 'is_scanned': is_scanned, 'file_type': file_extension}

# --- Stage 2: OCR --- (Function as previously corrected)
def perform_ocr_on_images(image_objects: List[Any]) -> str:
    if not image_objects: return ""
    if not PYTESSERACT_AVAILABLE or not pytesseract:
        logger.error("Pytesseract is not available. OCR cannot be performed.")
        return ""

    logger.info(f"Performing OCR on {len(image_objects)} image(s).")
    ocr_text_parts = []
    images_ocrd = 0
    for i, img in enumerate(image_objects):
        try:
            if not (PIL_AVAILABLE and Image and isinstance(img, Image.Image)):
                logger.warning(f"Skipping non-PIL Image object at index {i} for OCR.")
                continue
            text = pytesseract.image_to_string(img.convert('L'))
            if text and text.strip(): 
                ocr_text_parts.append(text.strip())
                images_ocrd += 1
        except Exception as e:
            if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
                logger.critical("Tesseract executable not found in PATH. OCR will fail for subsequent images too.")
                raise 
            logger.error(f"Error during OCR for image {i+1}/{len(image_objects)}: {e}", exc_info=True)
    
    full_ocr_text = "\n\n--- OCR Text from Image ---\n\n".join(ocr_text_parts)
    logger.info(f"OCR: Extracted {len(full_ocr_text)} chars from {images_ocrd} image(s) (out of {len(image_objects)} provided).")
    return full_ocr_text

# --- Stage 3: Text Cleaning and Normalization --- (Function as previously corrected)
def clean_and_normalize_text_content(text: str) -> str:
    if not text or not text.strip(): return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")
    text = re.sub(r'<script[^>]*>.*?</script>|<style[^>]*>.*?</style>|<[^>]+>', ' ', text, flags=re.I | re.S)
    text = re.sub(r'http\S+|www\S+|https\S+|\S*@\S*\s?', '', text, flags=re.MULTILINE)
    text = re.sub(r'[\n\t\r]+', ' ', text) 
    text = re.sub(r'\s+', ' ', text).strip() 
    text = re.sub(r'[^\w\s.,!?-]', '', text) 
    text_lower = text.lower()

    if not SPACY_MODEL_LOADED or not nlp_spacy_core:
        logger.warning("SpaCy model not loaded. Skipping tokenization/lemmatization. Returning regex-cleaned text.")
        return text_lower
    try:
        doc = nlp_spacy_core(text_lower, disable=['parser', 'ner']) 
        lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 1]
        final_cleaned_text = " ".join(lemmatized_tokens)
        logger.info(f"SpaCy-based text cleaning and normalization complete. Final length: {len(final_cleaned_text)}")
        if final_cleaned_text: logger.info(f"Final cleaned text (first 200 chars): {final_cleaned_text[:200]}...")
        return final_cleaned_text
    except Exception as e:
        logger.error(f"SpaCy processing failed: {e}. Returning pre-SpaCy cleaned text.", exc_info=True)
        return text_lower

# --- Stage 4: Layout Reconstruction & Table Integration --- (Function as previously corrected)
def reconstruct_document_layout(text_content: str, tables_data: List[Any], file_type: str) -> str:
    if not text_content and not tables_data: return ""
    logger.info(f"Starting layout reconstruction for file type '{file_type}'. Initial text length: {len(text_content)}, Tables: {len(tables_data)}.")
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text_content) 
    processed_text = re.sub(r'(\w+)-(\w+)', r'\1\2', processed_text)       

    if tables_data:
        table_md_parts = []
        for i, table_obj in enumerate(tables_data):
            header_md = f"\n\n[START OF TABLE {i+1}]\n"
            footer_md = f"\n[END OF TABLE {i+1}]\n"
            md_content = ""
            try:
                if PANDAS_AVAILABLE and pd and isinstance(table_obj, pd.DataFrame): 
                    md_content = table_obj.to_markdown(index=False)
                elif isinstance(table_obj, list) and table_obj and all(isinstance(r, list) for r in table_obj):
                    if table_obj and table_obj[0]: 
                        md_content = "| " + " | ".join(map(str, table_obj[0])) + " |\n"
                        md_content += "| " + " | ".join(["---"] * len(table_obj[0])) + " |\n"
                        for row_idx, row_data in enumerate(table_obj[1:]):
                            if len(row_data) == len(table_obj[0]): 
                                md_content += "| " + " | ".join(map(str, row_data)) + " |\n"
                            else:
                                logger.warning(f"Table {i+1}, row {row_idx+1} length mismatch with header. Skipping row.")
                    else: md_content = "[Empty Table Data]"
                else: md_content = str(table_obj)
            except Exception as e: 
                logger.warning(f"Table {i+1} to markdown conversion error: {e}. Using string representation."); 
                md_content = str(table_obj)
            if md_content: table_md_parts.append(header_md + md_content + footer_md)
        if table_md_parts: processed_text += "\n\n" + "\n\n".join(table_md_parts)
    
    processed_text = re.sub(r'\s+', ' ', processed_text).strip()
    logger.info(f"Layout reconstruction complete. Final text length: {len(processed_text)}")
    return processed_text

# --- Stage 5: Metadata Extraction --- (Function as previously corrected, uses original_file_name for logging)
def extract_document_metadata_info(file_path: str, processed_text: str, file_type_from_extraction: str, original_file_name: str, user_id: str) -> Dict[str, Any]:
    logger.info(f"Starting metadata extraction for: {original_file_name} (User: {user_id})")
    doc_meta = {'file_name': original_file_name, 'file_path_on_server': file_path, 'original_file_type': file_type_from_extraction,
                'processing_user': user_id, 'title': original_file_name, 'author': "Unknown", 'creation_date': None,
                'modification_date': None, 'page_count': 0, 'char_count_processed_text': len(processed_text),
                'named_entities': {}, 'structural_elements': "Paragraphs, Tables (inferred)", 'is_scanned_pdf': False}
    try:
        doc_meta['file_size_bytes'] = os.path.getsize(file_path)
        if PANDAS_AVAILABLE and pd:
            doc_meta['creation_date_os'] = pd.Timestamp(os.path.getctime(file_path), unit='s').isoformat()
            doc_meta['modification_date_os'] = pd.Timestamp(os.path.getmtime(file_path), unit='s').isoformat()
    except Exception as e: logger.warning(f"Metadata: OS metadata error for {original_file_name}: {e}")

    if file_type_from_extraction == '.pdf' and PYPDF2_AVAILABLE and PyPDF2:
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                info = reader.metadata
                if info:
                    if hasattr(info, 'title') and info.title: doc_meta['title'] = str(info.title).strip()
                    if hasattr(info, 'author') and info.author: doc_meta['author'] = str(info.author).strip()
                    if hasattr(info, 'creation_date') and info.creation_date and PANDAS_AVAILABLE and pd: 
                        doc_meta['creation_date'] = pd.Timestamp(info.creation_date).isoformat()
                doc_meta['page_count'] = len(reader.pages)
        except Exception as e: logger.warning(f"Metadata: PyPDF2 error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.docx' and DOCX_AVAILABLE and DocxDocument:
        try:
            doc = DocxDocument(file_path)
            props = doc.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = sum(1 for p in doc.paragraphs if p.text.strip()) or 1
        except Exception as e: logger.warning(f"Metadata: DOCX error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.pptx' and PPTX_AVAILABLE and Presentation:
        try:
            prs = Presentation(file_path)
            props = prs.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = len(prs.slides)
        except Exception as e: logger.warning(f"Metadata: PPTX error for {original_file_name}: {e}", exc_info=True)

    if doc_meta['page_count'] == 0 and processed_text: doc_meta['page_count'] = processed_text.count('\n\n') + 1

    if processed_text and SPACY_MODEL_LOADED and nlp_spacy_core:
        logger.info(f"Extracting named entities using SpaCy for {original_file_name}...")
        try:
            max_len = getattr(config, 'MAX_TEXT_LENGTH_FOR_NER', 500000) 
            text_for_ner = processed_text[:max_len]
            spacy_doc = nlp_spacy_core(text_for_ner) 
            ner_labels = nlp_spacy_core.pipe_labels.get("ner", [])
            entities = {label: [] for label in ner_labels}
            for ent in spacy_doc.ents:
                if ent.label_ in entities: entities[ent.label_].append(ent.text)
            doc_meta['named_entities'] = {k: list(set(v)) for k, v in entities.items() if v} 
            num_entities = sum(len(v_list) for v_list in doc_meta['named_entities'].values())
            logger.info(f"Extracted {num_entities} named entities for {original_file_name}.")
        except Exception as e: logger.error(f"Metadata: NER error for {original_file_name}: {e}", exc_info=True); doc_meta['named_entities'] = {}
    else: logger.info(f"Skipping NER for {original_file_name} (no text or SpaCy model not available/loaded)."); doc_meta['named_entities'] = {}
    
    logger.info(f"Metadata extraction complete for {original_file_name}.")
    return doc_meta

# --- Stage 6: Text Chunking ---
def chunk_document_into_segments(
    text_to_chunk: str,
    document_level_metadata: Dict[str, Any]
) -> List[Dict[str, Any]]:
    if not text_to_chunk or not text_to_chunk.strip():
        logger.warning(f"Chunking: No text for {document_level_metadata.get('file_name', 'unknown')}.")
        return []

    if not LANGCHAIN_SPLITTER_AVAILABLE or not RecursiveCharacterTextSplitter:
        logger.error("RecursiveCharacterTextSplitter not available. Cannot chunk text.")
        return []
        
    # CORRECTED: Use AI_CORE_CHUNK_SIZE and AI_CORE_CHUNK_OVERLAP from server/config.py
    chunk_s = AI_CORE_CHUNK_SIZE
    chunk_o = AI_CORE_CHUNK_OVERLAP

    original_doc_name_for_log = document_level_metadata.get('file_name', 'unknown')
    logger.info(f"Starting text chunking for {original_doc_name_for_log}. "
                f"Using config: CHUNK_SIZE={chunk_s}, CHUNK_OVERLAP={chunk_o}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_s,
        chunk_overlap=chunk_o,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], 
        keep_separator=True 
    )

    try: raw_text_segments: List[str] = text_splitter.split_text(text_to_chunk)
    except Exception as e: 
        logger.error(f"Chunking: Error splitting text for {original_doc_name_for_log}: {e}", exc_info=True)
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    base_file_name_for_ref = os.path.splitext(original_doc_name_for_log)[0] 

    for i, segment_content in enumerate(raw_text_segments):
        if not segment_content.strip(): 
            logger.debug(f"Skipping empty chunk at index {i} for {original_doc_name_for_log}.")
            continue

        chunk_specific_metadata = copy.deepcopy(document_level_metadata)
        
        qdrant_point_id = str(uuid.uuid4()) # Generate UUID for Qdrant

        chunk_specific_metadata['chunk_id'] = qdrant_point_id 
        chunk_specific_metadata['chunk_reference_name'] = f"{base_file_name_for_ref}_chunk_{i:04d}"
        chunk_specific_metadata['chunk_index'] = i
        chunk_specific_metadata['chunk_char_count'] = len(segment_content)
        
        output_chunks.append({
            'id': qdrant_point_id, 
            'text_content': segment_content,
            'metadata': chunk_specific_metadata 
        })
    
    logger.info(f"Chunking: Split '{original_doc_name_for_log}' into {len(output_chunks)} non-empty chunks with UUID IDs.")
    return output_chunks

# --- Stage 7: Embedding Generation --- (Function as previously corrected)
def generate_segment_embeddings(document_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not document_chunks: return []
    if not EMBEDDING_MODEL_LOADED or not document_embedding_model: # Check if model is loaded
        logger.error("Embedding model not loaded. Cannot generate embeddings.")
        for chunk_dict in document_chunks: chunk_dict['embedding'] = None # Ensure key exists
        return document_chunks

    # CORRECTED: Get model name from DOCUMENT_EMBEDDING_MODEL_NAME
    model_name_for_logging = getattr(config, 'DOCUMENT_EMBEDDING_MODEL_NAME', "Unknown Model")
    logger.info(f"Embedding: Starting embedding generation for {len(document_chunks)} chunks "
                f"using model: {model_name_for_logging}.")
    
    texts_to_embed: List[str] = []
    valid_chunk_indices: List[int] = []

    for i, chunk_dict in enumerate(document_chunks):
        text_content = chunk_dict.get('text_content')
        if text_content and text_content.strip():
            texts_to_embed.append(text_content)
            valid_chunk_indices.append(i)
        else:
            chunk_dict['embedding'] = None
            logger.debug(f"Embedding: Chunk {chunk_dict.get('id', i)} has no text content, skipping embedding.")

    if not texts_to_embed:
        logger.warning("Embedding: No actual text content found in any provided chunks to generate embeddings.")
        return document_chunks

    try:
        logger.info(f"Embedding: Generating embeddings for {len(texts_to_embed)} non-empty text segments...")
        embeddings_np_array = document_embedding_model.encode(texts_to_embed, show_progress_bar=False)
        
        for i, original_chunk_index in enumerate(valid_chunk_indices):
            if i < len(embeddings_np_array):
                document_chunks[original_chunk_index]['embedding'] = embeddings_np_array[i].tolist()
            else:
                logger.error(f"Embedding: Mismatch in embedding count for chunk at original index {original_chunk_index}. Assigning None.")
                document_chunks[original_chunk_index]['embedding'] = None
        
        logger.info(f"Embedding: Embeddings generated and assigned to {len(valid_chunk_indices)} chunks.")
        
    except Exception as e:
        logger.error(f"Embedding: Error during embedding generation with model {model_name_for_logging}: {e}", exc_info=True)
        for original_chunk_index in valid_chunk_indices:
            document_chunks[original_chunk_index]['embedding'] = None
        
    return document_chunks


# --- Main Orchestration Function (to be called by app.py) ---
def process_document_for_qdrant(file_path: str, original_name: str, user_id: str) -> List[Dict[str, Any]]:
    logger.info(f"ai_core: Orchestrating document processing for {original_name}, user {user_id}")
    if not os.path.exists(file_path): 
        logger.error(f"File not found at ai_core entry point: {file_path}")
        raise FileNotFoundError(f"File not found: {file_path}")

    try:

        # Step 1: Extract Raw content
        raw_content = extract_raw_content_from_file(file_path)
        # Example of how to access: raw_content['text_content'], raw_content['images'], etc.
        # file_type will be important: raw_content['file_type']
        # is_scanned will be important: raw_content['is_scanned']
        initial_extracted_text = raw_content.get('text_content', "") # THIS IS WHAT YOU WANT TO RETURN for Node.js analysis


        # Step 2: Perform OCR if needed
        ocr_text_output = ""
        if raw_content.get('is_scanned') and raw_content.get('images'):
            if PYTESSERACT_AVAILABLE and pytesseract:
                 ocr_text_output = perform_ocr_on_images(raw_content['images'])
            else:
                logger.warning(f"OCR requested for {original_name} but Pytesseract is not available/configured. Skipping OCR.")

        combined_text = raw_content.get('text_content', "")
        if ocr_text_output:
            if raw_content.get('is_scanned') and len(ocr_text_output) > len(combined_text) / 2:
                 combined_text = ocr_text_output + "\n\n" + combined_text
            else:
                 combined_text += "\n\n" + ocr_text_output
        
        if not combined_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No text content for {original_name} after raw extraction/OCR, and no tables. Returning empty.")
            return []

        cleaned_text = clean_and_normalize_text_content(combined_text)
        if not cleaned_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No meaningful text for {original_name} after cleaning, and no tables. Returning empty.")
            return []

        text_for_metadata_and_chunking = reconstruct_document_layout(
            cleaned_text,
            raw_content.get('tables', []),
            raw_content.get('file_type', '')
        )

        doc_metadata = extract_document_metadata_info(
            file_path,
            text_for_metadata_and_chunking, 
            raw_content.get('file_type', ''),
            original_name,
            user_id
        )
        doc_metadata['is_scanned_pdf'] = raw_content.get('is_scanned', False)

        # This now uses corrected config variable names and UUIDs for IDs
        chunks_with_metadata = chunk_document_into_segments( 
            text_for_metadata_and_chunking,
            doc_metadata
        )
        if not chunks_with_metadata:
            logger.warning(f"No chunks produced for {original_name}. Returning empty list.")
            return []

        # This now uses corrected config variable name for logging
        final_chunks_with_embeddings = generate_segment_embeddings(chunks_with_metadata)
        
        logger.info(f"ai_core: Successfully processed {original_name}. Generated {len(final_chunks_with_embeddings)} chunks.")
        return final_chunks_with_embeddings, initial_extracted_text

    except Exception as e: 
        if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
            logger.critical(f"ai_core: Tesseract (OCR engine) was not found during processing of {original_name}. OCR could not be performed.", exc_info=False)
            raise 
        
        logger.error(f"ai_core: Critical error during processing of {original_name}: {e}", exc_info=True)
        raise



rag_service/app.py

python
# server/main_qdrant_app.py

import os
import sys
import traceback # For detailed error logging
from flask import Flask, request, jsonify, current_app
import logging

# --- Add server directory to sys.path ---
# This allows us to import modules from the 'server' directory and sub-packages like 'rag_service'
# Assumes this file (main_qdrant_app.py) is in the 'server/' directory.
SERVER_DIR = os.path.dirname(os.path.abspath(__file__))
if SERVER_DIR not in sys.path:
    sys.path.insert(0, SERVER_DIR)

import config # Should pick up server/config.py
config.setup_logging()


# --- Import configurations and services ---
try:
    from vector_db_service import VectorDBService # From server/vector_db_service.py
    import ai_core # From server/rag_service/ai_core.py
except ImportError as e:
    print(f"CRITICAL IMPORT ERROR: {e}. Ensure all modules are correctly placed and server directory is in PYTHONPATH.")
    print("PYTHONPATH:", sys.path)
    sys.exit(1)

logger = logging.getLogger(__name__)
app = Flask(__name__)

# --- Initialize VectorDBService ---
vector_service = None
try:
    logger.info("Initializing VectorDBService for Qdrant...")
    vector_service = VectorDBService()
    vector_service.setup_collection() # Create/validate Qdrant collection on startup
    app.vector_service = vector_service
    logger.info("VectorDBService initialized and collection setup successfully.")
except Exception as e:
    logger.critical(f"Failed to initialize VectorDBService or setup Qdrant collection: {e}", exc_info=True)
    app.vector_service = None
    # Application might be non-functional, consider exiting or running in a degraded state
    # For now, vector_service will be None, and endpoints will fail gracefully.

# --- Helper for Error Responses ---
def create_error_response(message, status_code=500):
    current_app.logger.error(f"API Error ({status_code}): {message}")
    return jsonify({"error": message}), status_code

# === API Endpoints ===

@app.route('/health', methods=['GET'])
def health_check():
    current_app.logger.info("--- Health Check Request ---")
    status_details = {
        "status": "error",
        "qdrant_service": "not_initialized",
        "qdrant_collection_name": config.QDRANT_COLLECTION_NAME,
        "qdrant_collection_status": "unknown",
        "document_embedding_model": config.DOCUMENT_EMBEDDING_MODEL_NAME,
        "query_embedding_model": config.QUERY_EMBEDDING_MODEL_NAME,
        "expected_vector_dimension": config.QDRANT_COLLECTION_VECTOR_DIM,
    }
    http_status_code = 503

    if not vector_service:
        status_details["qdrant_service"] = "failed_to_initialize"
        return jsonify(status_details), http_status_code

    status_details["qdrant_service"] = "initialized"
    try:
        collection_info = vector_service.client.get_collection(collection_name=vector_service.collection_name)
        status_details["qdrant_collection_status"] = "exists"
        if hasattr(collection_info.config.params.vectors, 'size'): # Simple vector config
             actual_dim = collection_info.config.params.vectors.size
        elif isinstance(collection_info.config.params.vectors, dict): # Named vectors
            default_vector_conf = collection_info.config.params.vectors.get('')
            actual_dim = default_vector_conf.size if default_vector_conf else "multiple_named_not_checked"
        else:
            actual_dim = "unknown_format"

        status_details["actual_vector_dimension"] = actual_dim
        if actual_dim == config.QDRANT_COLLECTION_VECTOR_DIM:
            status_details["status"] = "ok"
            http_status_code = 200
            current_app.logger.info("Health check successful.")
        else:
            status_details["qdrant_collection_status"] = f"exists_with_dimension_mismatch (Expected {config.QDRANT_COLLECTION_VECTOR_DIM}, Got {actual_dim})"
            current_app.logger.warning(f"Health check: Qdrant dimension mismatch.")

    except Exception as e:
        status_details["qdrant_collection_status"] = f"error_accessing_collection: {str(e)}"
        current_app.logger.error(f"Health check: Error accessing Qdrant collection: {e}", exc_info=True)

    return jsonify(status_details), http_status_code


@app.route('/add_document', methods=['POST'])
def add_document_qdrant():
    current_app.logger.info("--- /add_document Request (Qdrant) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    user_id = data.get('user_id')
    # file_path is path ON THE SERVER where Flask can access it.
    # In a real system, this often comes from a file upload process that saves the file first.
    file_path = data.get('file_path')
    original_name = data.get('original_name') # Original filename from client

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    current_app.logger.info(f"Processing file: '{original_name}' (Path: '{file_path}') for user: '{user_id}'")

    if not os.path.exists(file_path):
        current_app.logger.error(f"File not found at server path: {file_path}")
        return create_error_response(f"File not found at server path: {file_path}", 404)

    try:
        # Assuming ai_core.process_document_for_embeddings is updated to return two values,
        # or you have a new function like ai_core.process_document_for_qdrant
        current_app.logger.info(f"Calling ai_core to process document: '{original_name}'")
        processed_chunks_with_embeddings, raw_text_for_node_analysis = ai_core.process_document_for_qdrant(
            file_path=file_path,
            original_name=original_name,
            user_id=user_id
        )
        # If you kept the old function name and just changed its return:
        # processed_chunks_with_embeddings, raw_text_for_node_analysis = ai_core.process_document_for_embeddings(...)

        num_chunks_added_to_qdrant = 0
        processing_status = "processed_no_content"

        # Check if chunks were generated before trying to add them
        if processed_chunks_with_embeddings:
            current_app.logger.info(f"ai_core generated {len(processed_chunks_with_embeddings)} chunks for '{original_name}'. Adding to Qdrant.")
            # Ensure vector_service is used correctly here
            num_chunks_added_to_qdrant = current_app.vector_service.add_processed_chunks(processed_chunks_with_embeddings)
            if num_chunks_added_to_qdrant > 0:
                processing_status = "added"
                current_app.logger.info(f"Successfully added {num_chunks_added_to_qdrant} chunks from '{original_name}' to Qdrant.")
            else:
                processing_status = "processed_chunks_not_added"
        elif raw_text_for_node_analysis: # If no chunks, but raw text was extracted
            current_app.logger.info(f"ai_core produced no processable RAG chunks for '{original_name}', but raw text was extracted.")
            processing_status = "processed_for_analysis_only" # No RAG chunks, but text for analysis
        else: # No chunks and no raw text (e.g., empty or unparseable file)
            current_app.logger.warning(f"ai_core produced no RAG chunks and no raw text for '{original_name}'.")
            # Return a specific response indicating nothing useful was extracted
            return jsonify({
                "message": f"Processed '{original_name}' but no content was extracted for RAG or analysis.",
                "status": "no_content_extracted",
                "filename": original_name,
                "user_id": user_id,
                "num_chunks_added_to_qdrant": 0,
                "raw_text_for_analysis": "" # Empty string
            }), 200 # 200 OK as the processing attempt itself didn't fail

        # Construct the success response for Node.js
        response_payload = {
            "message": f"Successfully processed '{original_name}'. Embeddings operation completed.",
            "status": processing_status,
            "filename": original_name,
            "user_id": user_id,
            "num_chunks_added_to_qdrvecorant": num_chunks_added_to_qdrant,
            "raw_text_for_analysis": raw_text_for_node_analysis if raw_text_for_node_analysis is not None else "" # Ensure it's always a string
        }
        current_app.logger.info(f"Successfully processed '{original_name}'. Returning raw text and Qdrant status.")
        return jsonify(response_payload), 201 # 201 Created if resources (chunks) were made

    except FileNotFoundError as e:
        current_app.logger.error(f"Add Document Error for '{original_name}' - FileNotFoundError: {e}", exc_info=True)
        return create_error_response(f"File not found during processing: {str(e)}", 404)
    except config.TESSERACT_ERROR: # Make sure config is imported and TESSERACT_ERROR is defined
        current_app.logger.critical(f"Add Document Error for '{original_name}' - Tesseract (OCR) not found.")
        return create_error_response("OCR engine (Tesseract) not found or not configured correctly on the server.", 500)
    except ValueError as e: # e.g., vector dimension mismatch from add_processed_chunks
        current_app.logger.error(f"Add Document Error for '{original_name}' - ValueError: {e}", exc_info=True)
        return create_error_response(f"Configuration or data error: {str(e)}", 400)
    except Exception as e:
        current_app.logger.error(f"Add Document Error for '{original_name}' - Unexpected Exception: {e}\n{traceback.format_exc()}", exc_info=True) # Ensure traceback is imported
        return create_error_response(f"Failed to process document '{original_name}' due to an internal error.", 500)


@app.route('/query', methods=['POST']) # Changed from /query to /search for Qdrant context
def search_qdrant_documents():
    current_app.logger.info("--- /search Request (Qdrant) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    query_text = data.get('query')
    k = data.get('k', config.QDRANT_DEFAULT_SEARCH_K) # Use default from config
    
    # Optional: Allow passing filter conditions in the request body
    # Example filter: {"user_id": "some_user", "original_name": "some_doc.pdf"}
    filter_payload = data.get('filter') 
    qdrant_filters = None

    if filter_payload and isinstance(filter_payload, dict):
        from qdrant_client import models as qdrant_models # Import here to keep it local
        conditions = []
        for key, value in filter_payload.items():
            # Ensure key is a valid payload key you store (e.g., user_id, original_name, page_number)
            # This simple example assumes exact match for string values.
            # For numerical ranges or other conditions, qdrant_models.Range, etc. would be used.
            conditions.append(qdrant_models.FieldCondition(key=key, match=qdrant_models.MatchValue(value=value)))
        
        if conditions:
            qdrant_filters = qdrant_models.Filter(must=conditions)
            current_app.logger.info(f"Applying Qdrant filter: {filter_payload}")


    if not query_text:
        return create_error_response("Missing 'query' field in request body", 400)

    try:
        k = int(k)
    except ValueError:
        return create_error_response("'k' must be an integer", 400)

    current_app.logger.info(f"Performing Qdrant search for query (first 100 chars): '{query_text[:100]}...' with k={k}")

    try:
        # `search_documents` returns: docs_list, formatted_context_string, context_map
        docs, formatted_context, docs_map = vector_service.search_documents(
            query=query_text,
            k=k,
            filter_conditions=qdrant_filters
        )

        # The response from your original /query endpoint was `{"relevantDocs": [...]}`
        # Let's adapt to return something similar, but with more Qdrant-style info.
        # `docs` is a list of `Document` objects from `vector_db_service`.
        
        response_payload = {
            "query": query_text,
            "k_requested": k,
            "filter_applied": filter_payload, # Show what filter was used
            "results_count": len(docs),
            "formatted_context_snippet": formatted_context, # The RAG-style formatted string
            "retrieved_documents_map": docs_map, # The map with citation index as key
            "retrieved_documents_list": [doc.to_dict() for doc in docs] # List of Document objects as dicts
        }
        current_app.logger.info(f"Qdrant search successful. Returning {len(docs)} results.")
        return jsonify(response_payload), 200

    except Exception as e:
        current_app.logger.error(f"Qdrant search failed: {e}\n{traceback.format_exc()}")
        return create_error_response(f"Error during Qdrant search: {str(e)}", 500)


if __name__ == '__main__':
    logger.info(f"--- Starting Qdrant RAG API Service on port {config.API_PORT} ---")
    logger.info(f"Document Embedding Model (ai_core): {config.DOCUMENT_EMBEDDING_MODEL_NAME} (Dim: {config.DOCUMENT_VECTOR_DIMENSION})")
    logger.info(f"Query Embedding Model (vector_db_service): {config.QUERY_EMBEDDING_MODEL_NAME} (Dim: {config.QUERY_VECTOR_DIMENSION})")
    logger.info(f"Qdrant Collection: {config.QDRANT_COLLECTION_NAME} (Expected Dim: {config.QDRANT_COLLECTION_VECTOR_DIM})")
    
    # For production, use a proper WSGI server like gunicorn or waitress
    # Example: gunicorn --workers 4 --bind 0.0.0.0:5000 main_qdrant_app:app
    app.run(host='0.0.0.0', port=config.API_PORT, debug=True) # debug=True for development


rag_service/code.txt


============ ./ai_core.py ============
import os
process_pdf_for_embeddings
from PIL import Image # For image processing
import pytesseract # OCR
import fitz # PyMuPDF for PDF parsing
import pdfplumber # For tables and layout
import PyPDF2 # For basic metadata
import re # Regular expressions for cleaning



============ ./app.py ============
# server/rag_service/app.py

import os
import sys
from flask import Flask, request, jsonify
from process_document import process_uploaded_document
import ai_core

# Add server directory to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
sys.path.insert(0, server_dir) # Ensure rag_service can be imported

# Now import local modules AFTER adjusting sys.path
from rag_service import config
import rag_service.file_parser as file_parser
import rag_service.faiss_handler as faiss_handler
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

def create_error_response(message, status_code=500):
    logger.error(f"API Error Response ({status_code}): {message}")
    return jsonify({"error": message}), status_code

@app.route('/health', methods=['GET'])
def health_check():
    logger.info("\n--- Received request at /health ---")
    status_details = {
        "status": "error",
        "embedding_model_type": config.EMBEDDING_TYPE,
        "embedding_model_name": config.EMBEDDING_MODEL_NAME,
        "embedding_dimension": None,
        "sentence_transformer_load": None,
        "default_index_loaded": False,
        "default_index_vectors": 0,
        "default_index_dim": None,
        "message": ""
    }
    http_status_code = 503

    try:
        # Check Embedding Model
        model = faiss_handler.embedding_model
        if model is None:
            status_details["message"] = "Embedding model could not be initialized during startup."
            status_details["sentence_transformer_load"] = "Failed"
            raise RuntimeError(status_details["message"])
        else:
            status_details["sentence_transformer_load"] = "OK"
            try:
                 status_details["embedding_dimension"] = faiss_handler.get_embedding_dimension(model)
            except Exception as dim_err:
                 status_details["embedding_dimension"] = f"Error: {dim_err}"


        # Check Default Index
        if config.DEFAULT_INDEX_USER_ID in faiss_handler.loaded_indices:
            status_details["default_index_loaded"] = True
            default_index = faiss_handler.loaded_indices[config.DEFAULT_INDEX_USER_ID]
            if hasattr(default_index, 'index') and default_index.index:
                status_details["default_index_vectors"] = default_index.index.ntotal
                status_details["default_index_dim"] = default_index.index.d
            logger.info("Default index found in cache.")
        else:
            logger.info("Attempting to load default index for health check...")
            try:
                default_index = faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID)
                status_details["default_index_loaded"] = True
                if hasattr(default_index, 'index') and default_index.index:
                    status_details["default_index_vectors"] = default_index.index.ntotal
                    status_details["default_index_dim"] = default_index.index.d
                logger.info("Default index loaded successfully during health check.")
            except Exception as index_load_err:
                logger.error(f"Health check failed to load default index: {index_load_err}", exc_info=True)
                status_details["message"] = f"Failed to load default index: {index_load_err}"
                status_details["default_index_loaded"] = False
                raise # Re-raise to indicate failure

        # Final Status
        status_details["status"] = "ok"
        status_details["message"] = "RAG service is running, embedding model accessible, default index loaded."
        http_status_code = 200
        logger.info("Health check successful.")

    except Exception as e:
        logger.error(f"--- Health Check Error ---", exc_info=True)
        if not status_details["message"]: # Avoid overwriting specific error messages
            status_details["message"] = f"Health check failed: {str(e)}"
        # Ensure status is error if exception occurred
        status_details["status"] = "error"
        http_status_code = 503 # Service unavailable if health check fails critically

    return jsonify(status_details), http_status_code


@app.route('/add_document', methods=['POST'])
def add_document():
    logger.info("\n--- Received request at /add_document ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    file_path = data.get('file_path')
    original_name = data.get('original_name')

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    logger.info(f"Processing file: {original_name} for user: {user_id}")
    logger.info(f"File path: {file_path}")

    if not os.path.exists(file_path):
        return create_error_response(f"File not found at path: {file_path}", 404)

    try:
        logger.info(f"Initialising [ process_file function ] for : {original_name} for user: {user_id}")

        final_chunks_with_embeddings = ai_core.process_pdf_for_embeddings(file_path, original_name, user_id)

        if not final_chunks_with_embeddings:
            logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
            return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200
        
        logger.info(f"Final Chunks with Embeddings: {final_chunks_with_embeddings}")

    except Exception as e:
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)

        
    # try:
    #     # 1. Parse File
    #     username = "abcd"
    #     text_content = process_uploaded_document(file_path, username=username)
    #     if text_content is None:
    #         logger.warning(f"Skipping embedding for {original_name}: File type not supported or parsing failed.")
    #         return jsonify({"message": f"File type of '{original_name}' not supported for RAG or parsing failed.", "filename": original_name, "status": "skipped"}), 200

    #     logger.info("Text_Content : {text_content}")
    #     # if not text_content.strip():
    #     #     logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
    #     #     return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 2. Chunk Text
    #     # documents = file_parser.chunk_text(text_content, original_name, user_id)
    #     # if not documents:
    #     #     logger.warning(f"No chunks created for {original_name}. Skipping add.")
    #     #     return jsonify({"message": f"No text chunks generated for '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 3. Add to Index (faiss_handler now handles dimension checks/recreation)
    #     # faiss_handler.add_documents_to_index(user_id, documents)

    #     # logger.info(f"Successfully processed and added document: {original_name} for user: {user_id}")
    #     # return jsonify({
    #     #     "message": f"Document '{original_name}' processed and added to index.",
    #     #     "filename": original_name,
    #     #     "chunks_added": len(documents),
    #     #     "status": "added"
    #     # }), 200
    # except Exception as e:
        # Log the specific error from faiss_handler if it raised one
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)


@app.route('/query', methods=['POST'])
def query_index_route():
    print("Called")
    logger.info("\n--- Received request at /query ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    query = data.get('query')
    k = data.get('k', 5) # Default to k=5 now

    if not user_id or not query:
        return create_error_response("Missing required fields: user_id, query", 400)

    logger.info(f"Querying for user: {user_id} with k={k}")
    # Avoid logging potentially sensitive query text in production
    logger.debug(f"Query text: '{query[:100]}...'")

    try:
        results = faiss_handler.query_index(user_id, query, k=k)

        formatted_results = []
        for doc, score in results:
            # --- SEND FULL CONTENT ---
            content = doc.page_content
            # --- (No snippet generation needed) ---

            formatted_results.append({
                "documentName": doc.metadata.get("documentName", "Unknown"),
                "score": float(score),
                "content": content, # Send the full content
                # Removed "content_snippet"
            })

        logger.info(f"Query successful for user {user_id}. Returning {len(formatted_results)} results.")
        return jsonify({"relevantDocs": formatted_results}), 200
    except Exception as e:
        logger.error(f"--- Query Error ---", exc_info=True)
        return create_error_response(f"Failed to query index: {str(e)}", 500)

if __name__ == '__main__':
    # Ensure base FAISS directory exists on startup
    try:
        faiss_handler.ensure_faiss_dir()
    except Exception as e:
        logger.critical(f"CRITICAL: Could not create FAISS base directory '{config.FAISS_INDEX_DIR}'. Exiting. Error: {e}", exc_info=True)
        sys.exit(1) # Exit if base dir cannot be created

    # Attempt to initialize embedding model on startup
    try:
        faiss_handler.get_embedding_model() # This also determines the dimension
        logger.info("Embedding model initialized successfully on startup.")
    except Exception as e:
        logger.error(f"CRITICAL: Embedding model failed to initialize on startup: {e}", exc_info=True)
        logger.error("Endpoints requiring embeddings (/add_document, /query) will fail.")
        # Decide if you want to exit or run in a degraded state
        sys.exit(1) # Exit if embedding model fails - essential service

    # Attempt to load/check the default index on startup
    try:
        faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID) # This checks/creates/validates dimension
        logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' loaded/checked/created on startup.")
    except Exception as e:
        logger.warning(f"Warning: Could not load/create default index '{config.DEFAULT_INDEX_USER_ID}' on startup: {e}", exc_info=True)
        # Don't necessarily exit, but log clearly. Queries might only use user indices.

    # Start Flask App
    port = config.RAG_SERVICE_PORT
    logger.info(f"--- Starting RAG service ---")
    logger.info(f"Listening on: http://0.0.0.0:{port}")
    logger.info(f"Using Embedding: {config.EMBEDDING_TYPE} ({config.EMBEDDING_MODEL_NAME})")
    try:
        logger.info(f"Embedding Dimension: {faiss_handler.get_embedding_dimension(faiss_handler.embedding_model)}")
    except: pass # Dimension already logged or failed earlier
    logger.info(f"FAISS Index Path: {config.FAISS_INDEX_DIR}")
    logger.info("-----------------------------")
    # Use waitress or gunicorn for production instead of Flask's development server
    app.run(host='0.0.0.0', port=port, debug=os.getenv('FLASK_DEBUG') == '1')



============ ./code.txt ============



============ ./config.py ============
# server/rag_service/config.py
import os
import sys

# --- Determine Server Directory ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SERVER_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '..')) # Path to the 'server' directory
print(f"[rag_service/config.py] Determined SERVER_DIR: {SERVER_DIR}")

# --- Embedding Model Configuration ---
# Switch to Sentence Transformer
EMBEDDING_TYPE = 'sentence-transformer'
# Specify the model name (make sure you have internet access for download on first run)
# Or choose another model compatible with sentence-transformers library
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'BAAI/bge-large-en-v1.5')
EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'mixedbread-ai/mxbai-embed-large-v1')
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'e5-large-v2')
EMBEDDING_MODEL_NAME = EMBEDDING_MODEL_NAME_ST
print(f"Using Sentence Transformer model: {EMBEDDING_MODEL_NAME}")

# --- FAISS Configuration ---
FAISS_INDEX_DIR = os.path.join(SERVER_DIR, 'faiss_indices')
DEFAULT_ASSETS_DIR = os.path.join(SERVER_DIR, 'default_assets', 'engineering')
DEFAULT_INDEX_USER_ID = '__DEFAULT__'

# --- Text Splitting Configuration ---
CHUNK_SIZE = 512#1000
CHUNK_OVERLAP = 100#150

# --- API Configuration ---
RAG_SERVICE_PORT = int(os.getenv('RAG_SERVICE_PORT', 5002))

# --- Print effective configuration ---
print(f"FAISS Index Directory: {FAISS_INDEX_DIR}")
print(f"Default Assets Directory (for default.py): {DEFAULT_ASSETS_DIR}")
print(f"RAG Service Port: {RAG_SERVICE_PORT}")
print(f"Default Index User ID: {DEFAULT_INDEX_USER_ID}")
print(f"Chunk Size: {CHUNK_SIZE}, Chunk Overlap: {CHUNK_OVERLAP}")





============ ./default.py ============
import os
import logging
import sys
import traceback

# --- Path Setup ---
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
project_root_dir = os.path.dirname(server_dir)
sys.path.insert(0, server_dir)
# --- End Path Setup ---

try:
    from rag_service import config
    from rag_service import faiss_handler
    from rag_service import file_parser
except ImportError as e:
     print("ImportError:", e)
     print("Failed to import modules. Ensure the script is run correctly relative to the project structure.")
     print("Current sys.path:", sys.path)
     exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


class DefaultVectorDBBuilder:
    def __init__(self):
        logger.info("Initializing embedding model...")
        try:
            # Use SentenceTransformer as configured in config.py
            self.embed_model = faiss_handler.get_embedding_model()
            if self.embed_model is None:
                 raise RuntimeError("Failed to initialize Sentence Transformer embedding model.")
        except Exception as e:
            logger.error(f"Fatal error initializing embedding model: {e}", exc_info=True)
            raise

        self.chunk_size = config.CHUNK_SIZE
        self.chunk_overlap = config.CHUNK_OVERLAP

        self.default_docs_dir = config.DEFAULT_ASSETS_DIR
        self.index_dir = config.FAISS_INDEX_DIR
        self.default_user_id = config.DEFAULT_INDEX_USER_ID

        self.default_index_user_path = faiss_handler.get_user_index_path(self.default_user_id)
        self.index_file_path = os.path.join(self.default_index_user_path, "index.faiss")
        self.pkl_file_path = os.path.join(self.default_index_user_path, "index.pkl")

        try:
            faiss_handler.ensure_faiss_dir()
            os.makedirs(self.default_index_user_path, exist_ok=True)
        except Exception as e:
             logger.error(f"Failed to create necessary directories: {e}")
             raise

        logger.info(f"Default assets directory: {self.default_docs_dir}")
        logger.info(f"Default index directory: {self.default_index_user_path}")


    def create_default_index(self, force_rebuild=True): # Keep force_rebuild flag
        """Scans default assets, parses files, creates embeddings, and saves the FAISS index."""
        logger.info("--- Starting Default Index Creation ---")

        # --- Force Rebuild Logic ---
        if force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
            logger.warning(f"force_rebuild=True. Deleting existing default index files in {self.default_index_user_path}.")
            try:
                if os.path.exists(self.index_file_path): os.remove(self.index_file_path)
                if os.path.exists(self.pkl_file_path): os.remove(self.pkl_file_path)
                # Clear from cache if loaded
                if self.default_user_id in faiss_handler.loaded_indices:
                    del faiss_handler.loaded_indices[self.default_user_id]
                logger.info("Removed existing default index files and cleared cache.")
            except OSError as e:
                logger.error(f"Error removing existing index files: {e}")
                return False # Stop if we can't remove old files
        elif not force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
             logger.info("Default index already exists and force_rebuild=False. Skipping creation.")
             # Try loading it to confirm validity
             try:
                 faiss_handler.load_or_create_index(self.default_user_id)
                 logger.info("Existing default index loaded successfully.")
                 return True
             except Exception as load_err:
                 logger.error(f"Failed to load existing default index: {load_err}. Consider running with force_rebuild=True.")
                 return False


        # --- Process Documents ---
        all_documents = []
        files_processed = 0
        files_skipped = 0

        logger.info(f"Scanning for processable files in: {self.default_docs_dir}")
        if not os.path.isdir(self.default_docs_dir):
            logger.error(f"Default assets directory not found: {self.default_docs_dir}")
            return False

        for root, _, files in os.walk(self.default_docs_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                # logger.debug(f"Found file: {filename}")
                try:
                    text_content = file_parser.parse_file(file_path)
                    if text_content and text_content.strip():
                        langchain_docs = file_parser.chunk_text(
                            text_content, filename, self.default_user_id
                        )
                        if langchain_docs:
                            all_documents.extend(langchain_docs)
                            files_processed += 1
                            logger.info(f"Parsed and chunked: {filename} ({len(langchain_docs)} chunks)")
                        else:
                            logger.warning(f"Skipped {filename}: No chunks generated.")
                            files_skipped += 1
                    else:
                        logger.warning(f"Skipped {filename}: No text content or unsupported type.")
                        files_skipped += 1
                except Exception as e:
                    logger.error(f"Error processing file {filename}: {e}")
                    traceback.print_exc()
                    files_skipped += 1

        if not all_documents:
            logger.error(f"No processable documents found or generated in {self.default_docs_dir}. Cannot create index.")
            # Still create an empty index structure if the directory was valid
            try:
                 logger.info("Creating an empty index structure as no documents were found.")
                 faiss_handler.load_or_create_index(self.default_user_id) # Creates empty index
                 logger.info("Empty default index created successfully.")
                 return True # Success, but empty
            except Exception as empty_create_err:
                 logger.error(f"Failed to create empty index structure: {empty_create_err}", exc_info=True)
                 return False


        logger.info(f"Total files processed: {files_processed}, skipped: {files_skipped}")
        logger.info(f"Creating embeddings and adding {len(all_documents)} total chunks to index...")

        try:
            # The load_or_create_index function will handle creating the empty structure
            # if it doesn't exist (or after deletion if force_rebuild=True)
            logger.info("Ensuring FAISS index structure exists...")
            index_instance = faiss_handler.load_or_create_index(self.default_user_id)

            logger.info(f"Adding {len(all_documents)} documents to the default index '{self.default_user_id}'...")
            # Use the updated handler function which now manages IDs correctly
            faiss_handler.add_documents_to_index(self.default_user_id, all_documents)

            # Verify save occurred
            if not os.path.exists(self.index_file_path) or not os.path.exists(self.pkl_file_path):
                 logger.error("Index files were not found after adding documents. Check permissions or disk space.")
                 return False

            logger.info(f"Successfully created/updated and saved default index ({self.default_user_id}) with {len(all_documents)} document chunks.")
            logger.info("--- Default Index Creation Finished ---")
            return True

        except Exception as e:
            logger.error(f"Failed during embedding or index creation: {e}", exc_info=True)
            logger.error("--- Default Index Creation Failed ---")
            return False

def main():
    print("--- Running Default Index Builder ---")
    try:
        builder = DefaultVectorDBBuilder()
    except Exception as init_err:
        print(f"FATAL: Failed to initialize builder: {init_err}")
        sys.exit(1)

    if not os.path.isdir(builder.default_docs_dir):
         logger.error(f"Default assets directory '{builder.default_docs_dir}' is missing.")
         sys.exit(1)

    # --- Always force rebuild as requested ---
    force = True
    logger.info(f"Starting index creation (force_rebuild={force})...")
    if not builder.create_default_index(force_rebuild=force):
        logger.error("Index creation process failed.")
        sys.exit(1)
    else:
        logger.info("Default index creation process completed successfully.")
        sys.exit(0)

if __name__ == "__main__":
    main()



============ ./faiss_handler.py ============
# server/rag_service/faiss_handler.py

import os
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.embeddings import Embeddings as LangchainEmbeddings
from langchain_core.documents import Document as LangchainDocument
from langchain_community.docstore import InMemoryDocstore
from rag_service import config
import numpy as np
import time
import logging
import pickle
import uuid
import shutil # Import shutil for removing directories

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)

embedding_model: LangchainEmbeddings | None = None
loaded_indices = {}
_embedding_dimension = None # Cache the dimension

def get_embedding_dimension(embedder: LangchainEmbeddings) -> int:
    """Gets and caches the embedding dimension."""
    global _embedding_dimension
    if _embedding_dimension is None:
        try:
            logger.info("Determining embedding dimension...")
            dummy_embedding = embedder.embed_query("dimension_check")
            dimension = len(dummy_embedding)
            if not isinstance(dimension, int) or dimension <= 0:
                raise ValueError(f"Invalid embedding dimension obtained: {dimension}")
            _embedding_dimension = dimension
            logger.info(f"Detected embedding dimension: {_embedding_dimension}")
        except Exception as e:
            logger.error(f"CRITICAL ERROR determining embedding dimension: {e}", exc_info=True)
            raise RuntimeError(f"Failed to determine embedding dimension: {e}")
    return _embedding_dimension

def get_embedding_model():
    global embedding_model
    if embedding_model is None:
        if config.EMBEDDING_TYPE == 'sentence-transformer':
            logger.info(f"Initializing HuggingFace Embeddings for Sentence Transformer (Model: {config.EMBEDDING_MODEL_NAME})")
            try:
                # Try CUDA first, fallback to CPU
                try:
                    if faiss.get_num_gpus() > 0:
                        device = 'cuda'
                        logger.info("CUDA detected. Using GPU for embeddings.")
                    else:
                        raise RuntimeError("No GPU found") # Force fallback
                except Exception:
                    device = 'cpu'
                    logger.warning("CUDA not available or GPU check failed. Using CPU for embeddings. This might be slow.")

                embedding_model = HuggingFaceEmbeddings(
                    model_name=config.EMBEDDING_MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True} # Often recommended for cosine similarity / MIPS with FAISS
                )
                # Determine and cache dimension on successful load
                get_embedding_dimension(embedding_model)

                logger.info("Testing embedding function...")
                test_embedding_doc = embedding_model.embed_documents(["test document"])
                test_embedding_query = embedding_model.embed_query("test query")
                if not test_embedding_doc or not test_embedding_query:
                    raise ValueError("Embedding test failed, returned empty results.")
                logger.info(f"Embedding test successful.")

            except Exception as e:
                logger.error(f"Error loading HuggingFace Embeddings for '{config.EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
                embedding_model = None
                raise RuntimeError(f"Failed to load embedding model: {e}")
        else:
            raise ValueError(f"Unsupported embedding type in config: {config.EMBEDDING_TYPE}. Expected 'sentence-transformer'.")
    return embedding_model

def get_user_index_path(user_id):
    safe_user_id = str(user_id).replace('.', '_').replace('/', '_').replace('\\', '_')
    user_dir = os.path.join(config.FAISS_INDEX_DIR, f"user_{safe_user_id}")
    return user_dir

def _delete_index_files(index_path, user_id):
    """Safely deletes index files for a user."""
    logger.warning(f"Deleting potentially incompatible index files for user '{user_id}' at {index_path}")
    try:
        if os.path.isdir(index_path):
            shutil.rmtree(index_path)
            logger.info(f"Successfully deleted directory: {index_path}")
        # If only loose files exist (less likely with save_local)
        index_file = os.path.join(index_path, "index.faiss")
        pkl_file = os.path.join(index_path, "index.pkl")
        if os.path.exists(index_file): os.remove(index_file)
        if os.path.exists(pkl_file): os.remove(pkl_file)
    except OSError as e:
        logger.error(f"Error deleting index files/directory for user '{user_id}' at {index_path}: {e}", exc_info=True)
        # Don't raise here, allow fallback to creating new index if possible

def load_or_create_index(user_id):
    global loaded_indices
    if user_id in loaded_indices:
        # **Even if cached, re-verify dimension on subsequent loads in case model changed**
        index = loaded_indices[user_id]
        embedder = get_embedding_model() # Ensure model is loaded
        current_dim = get_embedding_dimension(embedder)
        if hasattr(index, 'index') and index.index is not None and index.index.d != current_dim:
            logger.warning(f"Cached index for user '{user_id}' has dimension {index.index.d}, but current model has dimension {current_dim}. Discarding cache and forcing reload/recreate.")
            del loaded_indices[user_id] # Remove from cache
            # Fall through to load/create logic below
        else:
            logger.debug(f"Returning cached index for user '{user_id}'.")
            return index # Return cached and verified index

    index_path = get_user_index_path(user_id)
    index_file = os.path.join(index_path, "index.faiss")
    pkl_file = os.path.join(index_path, "index.pkl")

    embedder = get_embedding_model()
    if embedder is None:
        raise RuntimeError("Embedding model is not available.")
    current_embedding_dim = get_embedding_dimension(embedder)

    force_recreate = False
    if os.path.exists(index_file) and os.path.exists(pkl_file):
        logger.info(f"Attempting to load existing FAISS index for user '{user_id}' from {index_path}")
        try:
            start_time = time.time()
            # Temporarily load to check dimension
            index = FAISS.load_local(
                folder_path=index_path,
                embeddings=embedder,
                allow_dangerous_deserialization=True # Use with caution if index source isn't trusted
            )
            end_time = time.time()

            # --- CRITICAL DIMENSION CHECK ---
            if not hasattr(index, 'index') or index.index is None:
                 logger.warning(f"Loaded index for user '{user_id}' has no 'index' attribute or it's None. Forcing recreation.")
                 force_recreate = True
            elif index.index.d != current_embedding_dim:
                logger.warning(f"DIMENSION MISMATCH! Index for user '{user_id}' has dimension {index.index.d}, but current embedding model has dimension {current_embedding_dim}. Index is incompatible and will be recreated.")
                force_recreate = True
            elif index.index.ntotal == 0:
                logger.info(f"Loaded index for user '{user_id}' is empty (0 vectors). Will use it but note it's empty.")
                 # Not forcing recreate, just noting it's empty
            # --- END DIMENSION CHECK ---

            if force_recreate:
                _delete_index_files(index_path, user_id)
                # Don't return the incompatible index, fall through to create new one
            else:
                # If dimensions match and index is valid
                logger.info(f"Index for user '{user_id}' loaded successfully in {end_time - start_time:.2f} seconds. Dimension ({index.index.d}) matches. Contains {index.index.ntotal} vectors.")
                loaded_indices[user_id] = index
                return index

        except (pickle.UnpicklingError, EOFError, ModuleNotFoundError, AttributeError, ValueError) as load_err:
            logger.error(f"Error loading index for user '{user_id}' from {index_path}: {load_err}")
            logger.warning("Index files might be corrupted or incompatible. Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs
        except Exception as e:
            logger.error(f"Unexpected error loading index for user '{user_id}': {e}", exc_info=True)
            logger.warning("Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs

    # --- Create New Index Logic ---
    # This block runs if files didn't exist OR force_recreate is True
    logger.info(f"Creating new FAISS index structure for user '{user_id}' at {index_path} with dimension {current_embedding_dim}")
    try:
        # Ensure directory exists (it might have been deleted)
        os.makedirs(index_path, exist_ok=True)

        # Use the already determined dimension
        # Use IndexFlatIP if embeddings are normalized (recommended)
        faiss_index = faiss.IndexIDMap(faiss.IndexFlatIP(current_embedding_dim))
        # faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(current_embedding_dim)) # Use L2 if not normalized

        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}

        index = FAISS(
            embedding_function=embedder,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            normalize_L2=False # Set True if using IndexFlatIP and normalized embeddings (which we are with encode_kwargs)
        )

        logger.info(f"Initialized empty index structure for user '{user_id}'.")
        loaded_indices[user_id] = index # Add to cache immediately
        save_index(user_id) # Save the empty structure
        logger.info(f"New empty index for user '{user_id}' created and saved.")
        return index
    except Exception as e:
        logger.error(f"CRITICAL ERROR creating new index for user '{user_id}': {e}", exc_info=True)
        if user_id in loaded_indices:
            del loaded_indices[user_id] # Clean up cache on failure
        # Attempt to clean up directory if creation failed badly
        _delete_index_files(index_path, user_id)
        raise RuntimeError(f"Failed to initialize FAISS index for user '{user_id}'")


def add_documents_to_index(user_id, documents: list[LangchainDocument]):
    if not documents:
        logger.warning(f"No documents provided to add for user '{user_id}'.")
        return

    try:
        index = load_or_create_index(user_id) # This now handles dimension checks/recreation
        embedder = get_embedding_model() # Ensure model is loaded

        # --- VERIFY DIMENSIONS AGAIN before adding (paranoid check) ---
        current_dim = get_embedding_dimension(embedder)
        if not hasattr(index, 'index') or index.index is None:
             logger.error(f"Index object for user '{user_id}' is invalid after load/create. Cannot add documents.")
             raise RuntimeError("Failed to get valid index structure.")
        if index.index.d != current_dim:
             logger.error(f"FATAL: Dimension mismatch just before adding documents for user '{user_id}'. Index: {index.index.d}, Model: {current_dim}. This shouldn't happen if load_or_create_index worked.")
             # Attempt recovery by deleting and trying again? Risky loop potential.
             _delete_index_files(get_user_index_path(user_id), user_id)
             if user_id in loaded_indices: del loaded_indices[user_id]
             raise RuntimeError(f"Inconsistent index dimension detected for user '{user_id}'. Please retry.")
        # --- END VERIFY ---

        logger.info(f"Adding {len(documents)} documents to index for user '{user_id}' (Index dim: {index.index.d})...")
        start_time = time.time()

        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        # Generate embeddings using the current model
        embeddings = embedder.embed_documents(texts)
        if not embeddings or len(embeddings) != len(texts):
             logger.error(f"Embedding generation failed or returned unexpected number of vectors for user '{user_id}'.")
             raise ValueError("Embedding generation failed.")
        if len(embeddings[0]) != current_dim:
             logger.error(f"Generated embeddings have incorrect dimension ({len(embeddings[0])}) for user '{user_id}', expected {current_dim}.")
             raise ValueError("Generated embedding dimension mismatch.")

        embeddings_np = np.array(embeddings, dtype=np.float32)

        # Generate unique IDs for FAISS
        ids = [str(uuid.uuid4()) for _ in texts]
        ids_np = np.array([uuid.UUID(id_).int & (2**63 - 1) for id_ in ids], dtype=np.int64)


        # Add embeddings and their corresponding IDs to the FAISS index
        index.index.add_with_ids(embeddings_np, ids_np)

        # Add the original documents and their metadata to the Langchain Docstore,
        # using the generated string UUIDs as keys.
        # Map the FAISS integer ID back to the string UUID used in the docstore.
        docstore_additions = {doc_id: doc for doc_id, doc in zip(ids, documents)}
        index.docstore.add(docstore_additions)
        for i, faiss_id in enumerate(ids_np):
            index.index_to_docstore_id[int(faiss_id)] = ids[i] # Map FAISS int ID -> string UUID

        end_time = time.time()
        logger.info(f"Successfully added {len(documents)} vectors/documents for user '{user_id}' in {end_time - start_time:.2f} seconds. Total vectors: {index.index.ntotal}")
        save_index(user_id)
    except Exception as e:
        logger.error(f"Error adding documents for user '{user_id}': {e}", exc_info=True)
        # Don't re-raise here if app.py handles it, but ensure logging is clear
        raise # Re-raise the exception so app.py can catch it and return 500

def query_index(user_id, query_text, k=3):
    all_results_with_scores = []
    embedder = get_embedding_model()

    if embedder is None:
        logger.error("Embedding model is not available for query.")
        raise ConnectionError("Embedding model is not available for query.")

    try:
        start_time = time.time()
        user_index = None # Initialize to None
        default_index = None # Initialize to None

        # Query User Index
        try:
            user_index = load_or_create_index(user_id) # Assign to user_index
            if hasattr(user_index, 'index') and user_index.index is not None and user_index.index.ntotal > 0:
                logger.info(f"Querying index for user: '{user_id}' (Dim: {user_index.index.d}, Vectors: {user_index.index.ntotal}) with k={k}")
                user_results = user_index.similarity_search_with_score(query_text, k=k)
                logger.info(f"User index '{user_id}' query returned {len(user_results)} results.")
                all_results_with_scores.extend(user_results)
            else:
                logger.info(f"Skipping query for user '{user_id}': Index is empty or invalid.")
        except FileNotFoundError:
            logger.warning(f"User index files for '{user_id}' not found on disk (might be first time). Skipping query for this index.")
        except RuntimeError as e:
            logger.error(f"Could not load or create user index for '{user_id}': {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error querying user index for '{user_id}': {e}", exc_info=True)


        # Query Default Index (if different from user_id)
        if user_id != config.DEFAULT_INDEX_USER_ID:
            try:
                default_index = load_or_create_index(config.DEFAULT_INDEX_USER_ID) # Assign to default_index
                if hasattr(default_index, 'index') and default_index.index is not None and default_index.index.ntotal > 0:
                    logger.info(f"Querying default index '{config.DEFAULT_INDEX_USER_ID}' (Dim: {default_index.index.d}, Vectors: {default_index.index.ntotal}) with k={k}")
                    default_results = default_index.similarity_search_with_score(query_text, k=k)
                    logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' query returned {len(default_results)} results.")
                    all_results_with_scores.extend(default_results)
                else:
                    logger.info(f"Skipping query for default index '{config.DEFAULT_INDEX_USER_ID}': Index is empty or invalid.")
            except FileNotFoundError:
                 logger.warning(f"Default index '{config.DEFAULT_INDEX_USER_ID}' not found on disk (run default.py?). Skipping query.")
            except RuntimeError as e:
                logger.error(f"Could not load or create default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)
            except Exception as e:
                logger.error(f"Unexpected error querying default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)

        query_time = time.time()
        logger.info(f"Completed all index queries in {query_time - start_time:.2f} seconds. Found {len(all_results_with_scores)} raw results.")

        # --- Deduplication and Sorting ---
        unique_results = {}
        for doc, score in all_results_with_scores:
            if not doc or not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                logger.warning(f"Skipping invalid document object in results: {doc}")
                continue

            # Use the fallback content-based key
            content_key = f"{doc.metadata.get('documentName', 'Unknown')}_{doc.page_content[:200]}"
            unique_key = content_key # Use the content key directly

            # Add or update if the new score is better (lower for L2 distance / IP distance if normalized)
            if unique_key not in unique_results or score < unique_results[unique_key][1]:
                unique_results[unique_key] = (doc, score)

        # Sort by score (ascending for L2 distance / IP distance)
        sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
        final_results = sorted_results[:k] # Get top k unique results

        logger.info(f"Returning {len(final_results)} unique results after filtering and sorting.")
        return final_results
    except Exception as e:
        logger.error(f"Error during query processing for user '{user_id}': {e}", exc_info=True)
        return [] # Return empty list on error


def save_index(user_id):
    global loaded_indices
    if user_id not in loaded_indices:
        logger.warning(f"Index for user '{user_id}' not found in cache, cannot save.")
        return

    index = loaded_indices[user_id]
    index_path = get_user_index_path(user_id)

    if not isinstance(index, FAISS) or not hasattr(index, 'index') or not hasattr(index, 'docstore') or not hasattr(index, 'index_to_docstore_id'):
        logger.error(f"Cannot save index for user '{user_id}': Invalid index object in cache.")
        return

    # Ensure the target directory exists before saving
    try:
        os.makedirs(index_path, exist_ok=True)
        logger.info(f"Saving FAISS index for user '{user_id}' to {index_path} (Vectors: {index.index.ntotal if hasattr(index.index, 'ntotal') else 'N/A'})...")
        start_time = time.time()
        # This saves index.faiss and index.pkl
        index.save_local(folder_path=index_path)
        end_time = time.time()
        logger.info(f"Index for user '{user_id}' saved successfully in {end_time - start_time:.2f} seconds.")
    except Exception as e:
        logger.error(f"Error saving FAISS index for user '{user_id}' to {index_path}: {e}", exc_info=True)

# --- ADD THIS FUNCTION DEFINITION BACK ---
def ensure_faiss_dir():
    """Ensures the base FAISS index directory exists."""
    try:
        os.makedirs(config.FAISS_INDEX_DIR, exist_ok=True)
        logger.info(f"Ensured FAISS base directory exists: {config.FAISS_INDEX_DIR}")
    except OSError as e:
        logger.error(f"Could not create FAISS base directory {config.FAISS_INDEX_DIR}: {e}")
        raise # Raise the error to prevent startup if dir creation fails
# --- END OF ADDED FUNCTION ---



============ ./file_parser.py ============
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error



============ ./generate_code.sh ============
#!/bin/bash

# Output file
OUTPUT_FILE="code.txt"

# Clear previous output
> "$OUTPUT_FILE"

# Traverse files and directories, excluding __pycache__ and default.faiss
find . -type f \
    ! -path "*/__pycache__/*" \
    ! -name "*.pyc" \
    ! -name "default.faiss" \
    | while read file; do
        echo "============ $file ============" >> "$OUTPUT_FILE"
        cat "$file" >> "$OUTPUT_FILE"
        echo -e "\n\n" >> "$OUTPUT_FILE"
    done

echo "âœ… Code has been saved to $OUTPUT_FILE"



============ ./indexes/default_metadata.json ============
[
  {
    "content": "C h a p t e r 6\nD e e p F e e d f orw ard N e t w orks\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\nThegoalofafeedforwardnetworkistoapproximatesomefunction fâˆ—.Forexample,\nforaclassiï¬er, y= fâˆ—(x)mapsaninputxtoacategory y.Afeedforwardnetwork\ndeï¬nesamappingy= f(x;Î¸)andlearnsthevalueoftheparametersÎ¸thatresult\ninthebestfunctionapproximation.\nThesemodelsarecalledfeedforwardbecauseinformationï¬‚owsthroughthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "functionbeingevaluatedfromx,throughtheintermediate computations usedto\ndeï¬ne f,andï¬nallytotheoutputy.Therearenofeedbackconnectionsinwhich\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\nnetworks,presentedinchapter.10\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\nlanguageapplications.\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\nrepresentedbycomposingtogethermanydiï¬€erentfunctions.Themodelisasso-\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "together.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheï¬rst\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\n168",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\nthenameâ€œdeeplearningâ€arises.Theï¬nallayerofafeedforwardnetworkiscalled\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch fâˆ—(x).\nThetrainingdataprovidesuswithnoisy,approximateexamplesof fâˆ—(x) evaluated\natdiï¬€erenttrainingpoints.Eachexamplexisaccompanied byalabel y fâ‰ˆâˆ—(x).\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "x;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\nnotdirectlyspeciï¬edbythetrainingdata.Â Thelearningalgorithmmustdecide\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\ndecidehowtousetheselayerstobestimplementanapproximation of fâˆ—.Because\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\nlayersarecalledhiddenlayers.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "layersarecalledhiddenlayers.\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\nactivationvalue.Â Theideaofusingmanylayersofvector-valuedrepresentation\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\ntheserepresentationsisalsolooselyguidedbyneuroscientiï¬cobservationsabout\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\nandconsiderhowtoovercometheirlimitations.Â Linearmodels,suchaslogistic\nregressionandlinearregression,areappealingbecausetheymaybeï¬teï¬ƒciently",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "andreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\nthelinearmodelnottoxitselfbuttoatransformedinput Ï†(x),where Ï†isa\n1 6 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\nthe Ï†mapping.Wecanthinkof Ï†asprovidingasetoffeaturesdescribingx,or\nasprovidinganewrepresentationfor.x\nThequestionisthenhowtochoosethemapping. Ï†\n1.Oneoptionistouseaverygeneric Ï†,suchastheinï¬nite-dimens ional Ï†that\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.Â If Ï†(x)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "ofhighenoughdimension,wecanalwayshaveenoughcapacitytoï¬tthe\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\nproblems.\n2.Anotheroptionistomanuallyengineer Ï†.Untiltheadventofdeeplearning,\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\neï¬€ortforÂ eachseparateÂ task,Â withpractitionersÂ specializingÂ in diï¬€erent",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "domainssuchasspeechÂ recognition orÂ computer vision,Â andÂ with little\ntransferbetweendomains.\n3.Thestrategyofdeeplearningistolearn Ï†.Inthisapproach,wehaveamodel\ny= f(x;Î¸w ,) = Ï†(x;Î¸)î€¾w.WenowhaveparametersÎ¸thatweusetolearn\nÏ†fromabroadclassoffunctions,andparameterswthatmapfrom Ï†(x)to\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\nÏ†deï¬ningahiddenlayer.Â Thisapproachistheonlyoneofthethreethat\ngivesupontheconvexityofthetrainingproblem,butthebeneï¬tsoutweigh",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "theharms.Inthisapproach,weparametrizetherepresentationas Ï†(x;Î¸)\nandusetheoptimization algorithmtoï¬ndtheÎ¸thatcorrespondstoagood\nrepresentation.Ifwewish,thisapproachcancapturethebeneï¬toftheï¬rst\napproachbybeinghighlygenericâ€”wedosobyusingaverybroadfamily\nÏ†(x;Î¸).Thisapproachcanalsocapturethebeneï¬tofthesecondapproach.\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\ndesigningfamilies Ï†(x;Î¸)thattheyexpectwillperformwell.Theadvantage",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "isthatthehumandesigneronlyneedstoï¬ndtherightgeneralfunction\nfamilyratherthanï¬ndingpreciselytherightfunction.\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\n1 7 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmappingsfromxtoythatlackfeedbackconnections.Â Othermodelspresented\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "layersshouldÂ beconnectedtoÂ eachÂ other,Â and howmanyunitsshouldÂ bein\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\nmoderngeneralizations ,whichcanbeusedtoeï¬ƒcientlycomputethesegradients.\nFinally,weclosewithsomehistoricalperspective.\n6. 1 E x am p l e: L earni n g X O R\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "theXORfunction.\nTheXORfunction(â€œexclusiveorâ€)isanoperationontwobinaryvalues, x 1\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\ny= fâˆ—(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;Î¸)and\nourlearningalgorithmwilladapttheparametersÎ¸tomake fassimilaraspossible\nto fâˆ—.\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0]î€¾,[0 ,1]î€¾,\n[1 ,0]î€¾,and[1 ,1]î€¾}.Â Wewilltrainthenetworkonallfourofthesepoints.Â The\nonlychallengeistoï¬tthetrainingset.\nWecantreatthisproblemasaregressionproblemanduseameansquared\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\n1 7 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\naredescribedinsection.6.2.2.2\nEvaluatedonourwholetrainingset,theMSElossfunctionis\nJ() =Î¸1\n4î˜\nxâˆˆ X( fâˆ—() (;))xâˆ’ fxÎ¸2. (6.1)\nNowwemustchoosetheformofourmodel, f(x;Î¸).Supposethatwechoose\nalinearmodel,withconsistingofand.Ourmodelisdeï¬nedtobe Î¸w b\nf , b (;xw) = xî€¾w+ b . (6.2)\nWecanminimize J(Î¸)inclosedformwithrespecttowand busingthenormal\nequations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "equations.\nAftersolvingthenormalequations,weobtainw= 0and b=1\n2.Â Thelinear\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\nthisproblemistouseamodelthatlearnsadiï¬€erentfeaturespaceinwhicha\nlinearmodelisabletorepresentthesolution.\nSpeciï¬cally,wewillintroduceaverysimplefeedforwardnetworkwithone\nhiddenlayercontainingtwohiddenunits.Seeï¬gureforanillustrationof 6.2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "f , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =Wî€¾x\nand f( 2 )(h) =hî€¾w.Then f(x) =wî€¾Wî€¾x.Wecouldrepresentthisfunctionas\nf() = xxî€¾wî€°wherewî€°= Ww.\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networksdosousinganaï¬ƒnetransformationcontrolledbylearnedparameters,\nfollowedbyaï¬xed,nonlinearfunctioncalledanactivationfunction.Weusethat\nstrategyhere,bydeï¬ningh= g(Wî€¾x+c) ,whereWprovidestheweightsofa\nlineartransformationandcthebiases.Previously,todescribealinearregression\n1 7 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 1\nx 101x 2O r i g i n a l s p a c e x\n0 1 2\nh 101h 2L e a r n e d s p a c e h\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\nfunction.When x1= 0,themodelâ€™soutputmustincreaseas x2increases.When x1= 1,\nthemodelâ€™soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaï¬xed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "coeï¬ƒcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\nthecoeï¬ƒcienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\nmappedbothx= [1 ,0]î€¾andx= [0 ,1]î€¾toasinglepointinfeaturespace,h= [1 ,0]î€¾.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\ncapacitygreatersothatitcanï¬tthetrainingset.Inmorerealisticapplications,learned\nrepresentationscanalsohelpthemodeltogeneralize.\n1 7 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nyy\nhh\nx xWwyy\nh 1 h 1\nx 1 x 1h 2 h 2\nx 2 x 2\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodiï¬€erentstyles.Speciï¬cally,\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "eachentirevectorrepresentingalayerâ€™sactivations.Â Thisstyleismuchmorecompact.\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\nofdrawing.\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "aï¬ƒnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\nanaï¬ƒnetransformationfromavectorxtoavectorh,soanentirevectorofbias\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\nthatisappliedelement-wise,with h i= g(xî€¾W : , i+ c i).Inmodernneuralnetworks,\nthedefaultrecommendation istousetherectiï¬edlinearunitorReLU(Jarrett\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deï¬nedbytheactivation 2011a\nfunction depictedinï¬gure. g z , z () = max0{} 6.3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "function depictedinï¬gure. g z , z () = max0{} 6.3\nWecannowspecifyourcompletenetworkas\nf , , , b (;xWcw) = wî€¾max0{ ,Wî€¾xc+}+ b . (6.3)\nWecannowspecifyasolutiontotheXORproblem.Let\nW=î€”11\n11î€•\n, (6.4)\nc=î€”\n0\nâˆ’1î€•\n, (6.5)\n1 7 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0\nz0g z ( ) = m a x 0{ , z}\nFigure6.3:Therectiï¬edlinearactivationfunction.Thisactivationfunctionisthedefault\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\nfunctionwithtwolinearpieces.Becauserectiï¬edlinearunitsarenearlylinear,they",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "preservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\ncomplicatedsystemsfromminimalcomponents.Â MuchasaTuringmachineâ€™smemory\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\nfromrectiï¬edlinearfunctions.\n1 7 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nw=î€”1\nâˆ’2î€•\n, (6.6)\nand. b= 0\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\nwithoneexampleperrow:\nX=ï£®\nï£¯ï£¯ï£°00\n01\n10\n11ï£¹\nï£ºï£ºï£». (6.7)\nTheï¬rststepintheneuralnetworkistomultiplytheinputmatrixbytheï¬rst\nlayerâ€™sweightmatrix:\nXW=ï£®\nï£¯ï£¯ï£°00\n11\n11\n22ï£¹\nï£ºï£ºï£». (6.8)\nNext,weaddthebiasvector,toobtainc\nï£®\nï£¯ï£¯ï£°0 1âˆ’\n10\n10\n21ï£¹\nï£ºï£ºï£». (6.9)\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\nAlinearmodelcannotimplementsuchafunction.Toï¬nishcomputingthevalue\nofforeachexample,weapplytherectiï¬edlineartransformation: h\nï£®\nï£¯ï£¯ï£°00\n10\n10\n21ï£¹\nï£ºï£ºï£». (6.10)\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\nlongerlieonasingleline.Asshowninï¬gure,theynowlieinaspacewherea 6.1\nlinearmodelcansolvetheproblem.\nWeï¬nishbymultiplyingbytheweightvector:w\nï£®\nï£¯ï£¯ï£°0\n1\n1\n0ï£¹\nï£ºï£ºï£». (6.11)\n1 7 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\nInthisexample,wesimplyspeciï¬edthesolution,thenshowedthatitobtained\nzeroerror.Â Inarealsituation,theremightbebillionsofmodelparametersand\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\nhere.Instead,agradient-basedoptimization algorithmcanï¬ndparametersthat\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "globalminimumofthelossfunction,sogradientdescentcouldconvergetothis\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\ndescentcouldalsoï¬nd.Theconvergencepointofgradientdescentdependsonthe\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\nï¬ndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\nhere.\n6. 2 Gradi en t - Bas e d L earni n g\nDesigningandtraininganeuralnetworkisnotmuchdiï¬€erentfromtrainingany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\nacostfunction,andamodelfamily.\nThelargestdiï¬€erencebetweenthelinearmodelswehaveseensofarandneural\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\nconvergesstartingfromanyinitialparameters(intheoryâ€”inpracticeitisvery\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "importanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\nparticulardiscussedinsection.Forthemoment,itsuï¬ƒcestounderstandthat 8.4\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\ncostfunctioninonewayoranother.Â The speciï¬calgorithmsareimprovements",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "andreï¬nementsontheideasofgradientdescent,introducedinsection,and,4.3\n1 7 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmorespeciï¬cally,aremostoftenimprovementsofthestochasticgradientdescent\nalgorithm,introducedinsection.5.9\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\nmuchdiï¬€erentfromtraininganyothermodel.Computingthegradientisslightly\nmorecomplicatedforaneuralnetwork,butcanstillbedoneeï¬ƒcientlyandexactly.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\ntheneuralnetworksscenario.\n6.2.1CostFunctions\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\nInmostcases,ourparametricmodeldeï¬nesadistribution p(yx|;Î¸)and\nwesimplyuseÂ theprincipleÂ ofmaximumlikelihood.Thismeansweusethe\ncross-entropybetweenthetrainingdataandthemodelâ€™spredictionsasthecost\nfunction.\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "on.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "describedinchapter.7\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\n1 7 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\ncostfunctionisgivenby\nJ() = Î¸ âˆ’ E x y ,âˆ¼ Ë† pdatalog p m o de l( )yx| . (6.12)\nThespeciï¬cformofthecostfunctionchangesfrommodeltomodel,depending\nonthespeciï¬cformoflog p m o de l.Theexpansionoftheaboveequationtypically\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;Î¸) ,I),\nthenwerecoverthemeansquarederrorcost,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "thenwerecoverthemeansquarederrorcost,\nJ Î¸() =1\n2E x y ,âˆ¼ Ë† pdata||âˆ’ ||y f(;)xÎ¸2+const , (6.13)\nuptoascalingfactorof1\n2andatermthatdoesnotdependon.ThediscardedÎ¸\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)xÎ¸",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Anadvantageofthisapproachofderivingthecostfunctionfrommaximum\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\nforthelearningalgorithm.Functionsthatsaturate(becomeveryï¬‚at)undermine\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\nhiddenunitsortheoutputunitssaturate.Â Thenegativelog-likelihoodhelpsto\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\nsection.6.2.2\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\n1 7 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\ncross-entropyapproachingnegativeinï¬nity.Regularizationtechniquesdescribed\ninchapterprovideseveraldiï¬€erentwaysofmodifyingthelearningproblemso 7\nthatthemodelcannotreapunlimitedrewardinthisway.\n6.2.1.2LearningConditionalStatistics",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "6.2.1.2LearningConditionalStatistics\nInsteadoflearningafullprobabilitydistribution p(yx|;Î¸)weoftenwanttolearn\njustoneconditionalstatisticofgiven.yx\nForexample,wemayhaveapredictor f(x;Î¸) thatwewishtopredictthemean\nof.y\nIfweuseasuï¬ƒcientlypowerfulneuralnetwork,wecanthinkoftheneural\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\nratherthanbyhavingaspeciï¬cparametricform.Fromthispointofview,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "canviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciï¬c\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "toolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\nusedtoderivethefollowingtworesults.\nOurï¬rstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\ntionproblem\nfâˆ—= argmin\nfE x y ,âˆ¼ pdata||âˆ’ ||y f()x2(6.14)\nyields\nfâˆ—() = x E yâˆ¼ pdata ( ) y x|[]y , (6.15)\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "couldtrainoninï¬nitelymanysamplesfromthetruedatageneratingdistribution,\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\nmeanofforeachvalueof. y x\n1 8 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nDiï¬€erentcostfunctionsgivediï¬€erentstatistics.Asecondresultderivedusing\ncalculusofvariationsisthat\nfâˆ—= argmin\nfE x y ,âˆ¼ pdata||âˆ’ ||y f()x 1 (6.16)\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\nfunctioniscommonlycalled . meanabsoluteerror\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "saturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\nentiredistribution. p( )yx|\n6.2.2OutputUnits\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\nmodeldistribution.Â Thechoiceofhowtorepresenttheoutputthendetermines\ntheformofthecross-entropyfunction.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "theformofthecross-entropyfunction.\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\nsetofhiddenfeaturesdeï¬nedbyh= f(x;Î¸).Theroleoftheoutputlayeristhen\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thatthenetworkmustperform.\n6.2.2.1LinearUnitsforGaussianOutputDistributions\nOnesimplekindofoutputunitisanoutputunitbasedonanaï¬ƒnetransformation\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\nGivenfeaturesh,alayeroflinearoutputunitsproducesavectorË†y=Wî€¾h+b.\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\nGaussiandistribution:\np( ) = (;yx| NyË†yI ,) . (6.17)\n1 8 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\nerror.\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\ndeï¬nitematrixforallinputs.Itisdiï¬ƒculttosatisfysuchconstraintswithalinear\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\nBecauselinearunitsdonotsaturate,theyposelittlediï¬ƒcultyforgradient-\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\nalgorithms.\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\nManytasksrequirepredictingthevalueofabinaryvariable y.Classiï¬cation\nproblemswithtwoclassescanbecastinthisform.\nThemaximum-likelihoodapproachistodeï¬neaBernoullidistributionover y\nconditionedon.x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "conditionedon.x\nABernoullidistributionisdeï¬nedbyjustasinglenumber.Theneuralnet\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\nmustlieintheinterval[0,1].\nSatisfyingthisconstraintrequiressomecarefuldesigneï¬€ort.Supposewewere\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\nP y(= 1 ) = max |xî®\n0min ,î®\n1 ,wî€¾h+ bî¯î¯\n.(6.18)\nThiswouldindeeddeï¬neavalidconditionaldistribution,butwewouldnotbeable\ntotrainitveryeï¬€ectivelywithgradientdescent.Anytimethatwî€¾h+ bstrayed",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\nparameters.\nInstead,itisbettertouseadiï¬€erentapproachthatensuresthereisalwaysa\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\nAsigmoidoutputunitisdeï¬nedby\nË† y Ïƒ= î€\nwî€¾h+ bî€‘\n(6.19)\n1 8 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwhereisthelogisticsigmoidfunctiondescribedinsection. Ïƒ 3.10\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\nusesalinearlayertocompute z=wî€¾h+ b.Next,itusesthesigmoidactivation\nfunctiontoconvertintoaprobability. z\nWeomitthedependenceonxforthemomenttodiscusshowtodeï¬nea\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\nbyconstructinganunnormalized probabilitydistributionËœ P( y),whichdoesnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\ncontrolledbyasigmoidaltransformationof: z\nlogËœ P y y z () = (6.20)\nËœ P y y z () = exp() (6.21)\nP y() =exp() y zî1\nyî€°= 0exp( yî€°z)(6.22)\nP y Ïƒ y z . () = ((2âˆ’1)) (6.23)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "P y Ïƒ y z . () = ((2âˆ’1)) (6.23)\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\nthroughoutthestatisticalmodelingliterature.The zvariabledeï¬ningsucha\ndistributionoverbinaryvariablesiscalleda.logit\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\nlikelihoodisâˆ’log P( y|x),theloginthecostfunctionundoestheexpofthe\nsigmoid.Withoutthiseï¬€ect,thesaturationofthesigmoidcouldpreventgradient-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "basedÂ learningfromÂ makinggoodprogress.ThelossÂ functionforÂ maximum\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\nJ P y () = logÎ¸ âˆ’ (|x) (6.24)\n= log((2 1)) âˆ’ Ïƒ yâˆ’ z (6.25)\n= ((12)) Î¶ âˆ’ y z . (6.26)\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\n(1âˆ’2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\nhastherightanswerâ€”when y= 1and zisverypositive,or y= 0and zisvery",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "negative.When zhasthewrongsign,theargumenttothesoftplusfunction,\n1 8 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n(1âˆ’2 y) z,maybesimpliï¬edto|| z.As|| zbecomeslargewhile zhasthewrongsign,\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\ncorrectamistaken. z\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "saturateanytime Ïƒ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\noutputunits.\nAnalytically,thelogarithmofthesigmoidisalwaysdeï¬nedandï¬nite,because",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "thesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\nfunctionof z,ratherthanasafunctionofË† y= Ïƒ( z).Ifthesigmoidfunction\nunderï¬‚owstozero,thentakingthelogarithmofË† yyieldsnegativeinï¬nity.\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "with npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\ndistributionoverabinaryvariable.\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassiï¬er,torepresent\ntheprobabilitydistributionover ndiï¬€erentclasses.Morerarely,softmaxfunctions\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\nndiï¬€erentoptionsforsomeinternalvariable.\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "Ë† y P y . = (= 1 )|x (6.27)\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\nthelog-likelihood,wechosetoinsteadpredictanumber z=logËœ P( y=1|x).\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\nsigmoidfunction.\n1 8 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\ntoproduceavectorË†y,with Ë† y i= P( y= i|x).Werequirenotonlythateach\nelementofË† y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\nlayerpredictsunnormalized logprobabilities:\nzW= î€¾hb+ , (6.28)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "zW= î€¾hb+ , (6.28)\nwhere z i=logËœ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\nnormalizetoobtainthedesired z Ë†y.Formally,thesoftmaxfunctionisgivenby\nsoftmax()z i=exp( z i)î\njexp( z j). (6.29)\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deï¬ningthe\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\ntheofthesoftmax: exp",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theofthesoftmax: exp\nlogsoftmax()z i= z iâˆ’logî˜\njexp( z j) . (6.30)\nTheï¬rsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,theï¬rst 6.30\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\npusheddown.Togainsomeintuitionforthesecondterm,logî\njexp( z j),observe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j),observe\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\nbasedontheideathatexp( z k) isinsigniï¬cantforany z kthatisnoticeablylessthan\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\ntheâˆ’ z itermandthelogî\njexp( z j)â‰ˆmax j z j= z itermswillroughlycancel.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "jexp( z j)â‰ˆmax j z j= z itermswillroughlycancel.\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiï¬ed.\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\n1 8 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\nsoftmax((;))zxÎ¸ iâ‰ˆîm\nj = 1 1y() j= i , x() j= xîm\nj = 1 1x() j = x. (6.31)\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\nmodelisonlyabletoapproximatethesefractions.\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "withthesoftmaxfunction.Speciï¬cally,objectivefunctionsthatdonotusealogto\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\noutput,evenwhenthemodelmakeshighlyconï¬dentincorrectpredictions(,Bridle\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\nthesoftmaxfunctionitself.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesoftmaxfunctionitself.\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\noutputvaluescansaturatewhenthediï¬€erencesbetweeninputvaluesbecome\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\nToseethatthesoftmaxfunctionrespondstothediï¬€erencebetweenitsinputs,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\ninputs:\nsoftmax() = softmax(+) zz c . (6.32)\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\nsoftmax() = softmax( max zzâˆ’\niz i) . (6.33)\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\nbytheamountthatitsargumentsdeviatefrommax i z i.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "bytheamountthatitsargumentsdeviatefrommax i z i.\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\n1 8 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ncancausesimilardiï¬ƒcultiesforlearningifthelossfunctionisnotdesignedto\ncompensateforit.\nTheargumentztothesoftmaxfunctioncanbeproducedintwodiï¬€erentways.\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\neveryelementofz,asdescribedaboveusingthelinearlayerz=Wî€¾h+b.While\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\nconstraintthatthe noutputsmustsumtomeansthatonly 1 nâˆ’1parametersare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "necessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\nï¬rst nâˆ’1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\nofzbeï¬xed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\nwhatthesigmoidunitdoes.Deï¬ning P( y= 1|x) = Ïƒ( z)isequivalenttodeï¬ning\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe nâˆ’1\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\nsetofprobabilitydistributions,buthavediï¬€erentlearningdynamics.Inpractice,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "thereisrarelymuchdiï¬€erencebetweenusingtheoverparametrized versionorthe\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\nFromaneuroscientiï¬cpointofview,itisinterestingtothinkofthesoftmaxas\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "extreme(whenthediï¬€erencebetweenthemaximal a iandtheothersislargein\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\nandtheothersarenearly0).\nThenameâ€œsoftmaxâ€canbesomewhatconfusing.Thefunctionismoreclosely\nrelatedtotheargmaxfunctionthanthemaxfunction.Â Thetermâ€œsoftâ€derives\nfromthefactthatthesoftmaxfunctioniscontinuousanddiï¬€erentiable. The\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\nordiï¬€erentiable. Thesoftmaxfunctionthusprovidesaâ€œsoftenedâ€versionofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "argmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)î€¾z.\nItwouldperhapsbebettertocallthesoftmaxfunctionâ€œsoftargmax,â€Â butthe\ncurrentnameisanentrenchedconvention.\n6.2.2.4OtherOutputTypes\nThelinear,Â sigmoid,Â andsoftmaxoutputunitsdescribedabovearethemost\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\n1 8 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nagoodcostfunctionfornearlyanykindofoutputlayer.\nIngeneral,ifwedeï¬neaconditionaldistribution p(yx|;Î¸),theprincipleof\nmaximumlikelihoodsuggestsweuse asourcostfunction. âˆ’ | log( pyxÎ¸;)\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;Î¸).\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\nf(x;Î¸) =Ï‰providestheparametersforadistributionover y.Ourlossfunction\ncanthenbeinterpretedas . âˆ’log(;()) p yÏ‰x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "canthenbeinterpretedas . âˆ’log(;()) p yÏ‰x\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\ngiven x.Inthesimplecase,wherethevariance Ïƒ2isaconstant,thereisaclosed\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\nempiricalmeanofthesquareddiï¬€erencebetweenobservations yandtheirexpected\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "distribution p( y|x)thatiscontrolledbyÏ‰= f(x;Î¸).Thenegativelog-likelihood\nâˆ’log p(y;Ï‰(x))willthenprovideacostfunctionwiththeappropriateterms\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\ncanmakeanewparameterinthenetworkthatiscopieddirectlyintoÏ‰.Thisnew\nparametermightbe Ïƒitselforcouldbeaparameter vrepresenting Ïƒ2oritcould\nbeaparameter Î²representing1\nÏƒ2,dependingonhowwechoosetoparametrize",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "Ïƒ2,dependingonhowwechoosetoparametrize\nthedistribution.Wemaywishourmodeltopredictadiï¬€erentamountofvariance\nin yfordiï¬€erentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\nheteroscedasticcase,wesimplymakethespeciï¬cationofthevariancebeoneof\nthevaluesoutputby f( x;Î¸).AtypicalwaytodothisistoformulatetheGaussian\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\ndiag (6.34) ()Î² .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "diag (6.34) ()Î² .\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\nlog-likelihoodoftheGaussiandistributionparametrized byÎ²involvesonlymul-\ntiplicationby Î² iandadditionoflogÎ² i.Thegradientofmultiplication, addition,\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "arbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\ncanvanishnearzero,makingitdiï¬ƒculttolearnparametersthataresquared.\n1 8 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\nensurethatthecovariancematrixoftheGaussianispositivedeï¬nite.Â Because\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\npositivedeï¬nite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Ifwesupposethataistherawactivationofthemodelusedtodeterminethe\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\nvector:Î²= Î¶(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\ndiagonalmatrix.\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\ndiagonal.Â Ifthecovarianceisfullandconditional,thenaparametrization must",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "bechosenthatguaranteespositive-deï¬nitenessofthepredictedcovariancematrix.\nThiscanbeachievedbywriting Î£() = ()xBxBî€¾()x,whereBisanunconstrained\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\nlikelihoodisexpensive,witha d dÃ—matrixrequiring O( d3)computationforthe\ndeterminantandinverseof Î£(x)(orequivalently,andmorecommonlydone,its\neigendecompositionorthatof).Bx()\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiï¬€erent\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdeï¬nedby\ntheconditionalprobabilitydistribution\np( ) =yx|nî˜\ni = 1p i (= c |Nx)(;yÂµ( ) i()x , Î£( ) i())x .(6.35)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Theneuralnetworkmusthavethreeoutputs:avectordeï¬ning p(c= i|x),a\nmatrixprovidingÂµ( ) i(x)forall i,andatensorproviding Î£( ) i(x)forall i.These\noutputsmustsatisfydiï¬€erentconstraints:\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\noverthe ndiï¬€erentcomponentsassociatedwithlatentvariable1c,andcan\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "y , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\nra n d o m v a ria b l e .\n1 8 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\nthattheseoutputsarepositiveandsumto1.\n2.MeansÂµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\nan n dÃ—matrixcontainingall nofthese d-dimensionalvectors.Â Learning\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "learningthemeansofadistributionwithonlyoneoutputmode.Weonly\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\neachexampleâ€™scontributiontothelossforeachcomponentbytheprobability\nthatthecomponentproducedtheexample.\n3.Covariances Î£( ) i(x):thesespecifythecovariancematrixforeachcomponent\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\nspeciï¬cationofthenegativelog-likelihoodunderthemixturemodel.\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\nthegradientsheuristically( ,). MurrayandLarochelle2014\nGaussianmixtureoutputsareparticularlyeï¬€ectiveingenerativemodelsof\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\ndensitynetworkisshowninï¬gure.6.4\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\nthatformsasentence.IntheseÂ cases,wemaycontinuetousetheprinciple\nofmaximumlikelihoodappliedtoourmodel p(y;Ï‰(x)),butthemodelweuse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "1 9 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nxy\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "predictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\nChapterdescribeshowtouserecurrentneuralnetworkstodeï¬nesuchmodels 10\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\nprobabilitydistributions.\n6. 3 Hi d d en Un i t s\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "arecommontomostparametricmachinelearningmodelstrainedwithgradient-\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\nmodel.\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\nyethavemanydeï¬nitiveguidingtheoreticalprinciples.\nRectiï¬edlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\ntypesofhiddenunitsareavailable.Itcanbediï¬ƒculttodeterminewhentouse",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "whichkind(thoughrectiï¬edlinearunitsareusuallyanacceptablechoice).Â We\n1 9 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\nonavalidationset.\nSomeofthehiddenunitsincludedinthislistarenotactuallydiï¬€erentiableat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "allinputpoints.Forexample,therectiï¬edlinearfunction g( z) =max{0 , z}isnot\ndiï¬€erentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\nforthesemodelstobeusedformachinelearningtasks.Â Thisisinpartbecause\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\nthecostfunction,butinsteadmerelyreduceitsvaluesigniï¬cantly,asshownin\nï¬gure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "expecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\nfortheminimaofthecostfunctiontocorrespondtopointswithundeï¬nedgradient.\nHiddenunitsthatarenotdiï¬€erentiableareusuallynon-diï¬€erentiable atonlya\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeï¬ned\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\ndeï¬nedbytheslopeofthefunctionimmediately totherightof z.Afunction\nisdiï¬€erentiableat zonlyifboththeleftderivativeandtherightderivativeare",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "deï¬nedandequaltoeachother.Thefunctionsusedinthecontextofneural\nnetworksusuallyhavedeï¬nedleftderivativesanddeï¬nedrightderivatives.Inthe\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\ntheone-sidedderivativesratherthanreportingthatthederivativeisundeï¬nedor\nraisinganerror.Â Thismaybeheuristicallyjustiï¬edbyobservingthatgradient-\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 î€thatwasrounded\nto.Insomecontexts,moretheoreticallypleasingjustiï¬cationsareavailable,but 0\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\ninpracticeonecansafelydisregardthenon-diï¬€erentiabilityofthehiddenunit\nactivationfunctionsdescribedbelow.\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "avectorofinputsx,computinganaï¬ƒnetransformationz=Wî€¾x+b,and\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\nfunction. g()z\n1 9 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.1Rectiï¬edLinearUnitsandTheirGeneralizations\nRectiï¬edlinearunitsusetheactivationfunction . g z , z () = max0{}\nRectiï¬edlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\nunits.Theonlydiï¬€erencebetweenalinearunitandarectiï¬edlinearunitis\nthatarectiï¬edlinearunitoutputszeroacrosshalfitsdomain.Â This makesthe\nderivativesthrougharectiï¬edlinearunitremainlargewhenevertheunitisactive.\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "rectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\nthatintroducesecond-ordereï¬€ects.\nRectiï¬edlinearunitsaretypicallyusedontopofanaï¬ƒnetransformation:\nhW= ( gî€¾xb+) . (6.36)\nWheninitializingtheparametersoftheaï¬ƒnetransformation,itcanbeagood\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "itverylikelythattherectiï¬edlinearunitswillbeinitiallyactiveformostinputs\ninthetrainingsetandallowthederivativestopassthrough.\nSeveralgeneralizations ofrectiï¬edlinearunitsexist.Mostofthesegeneral-\nizationsperformcomparablytorectiï¬edlinearunitsandoccasionallyperform\nbetter.\nOnedrawbacktorectiï¬edlinearunitsisthattheycannotlearnviagradient-\nbasedÂ methodsÂ onexamplesÂ forÂ whichÂ theirÂ activ ationÂ iszero.AvarietyÂ of\ngeneralizations ofrectiï¬edlinearunitsguaranteethattheyreceivegradientevery-\nwhere.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "where.\nThreegeneralizations ofrectiï¬edlinearunitsarebasedonusinganon-zero\nslope Î± iwhen z i <0: h i= g(zÎ± ,) i=max(0 , z i)+ Î± imin(0 , z i).Absolutevalue\nrectiï¬cationï¬xes Î± i=âˆ’1toobtain g( z) =|| z.Itisusedforobjectrecognition\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\nofrectiï¬edlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "2013)ï¬xes Î± itoasmallvaluelike0.01whileaparametricReLUorPReLU\ntreats Î± iasalearnableparameter(,). He e t a l .2015\nMaxoutunits( ,)generalizerectiï¬edlinearunits Goodfellow e t a l .2013a\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\n1 9 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\noneofthesegroups:\ng()z i=max\njâˆˆ G() iz j (6.37)\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( iâˆ’1) k+1 , . . . , i k}.\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\ndirectionsintheinputspace.x\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "learntoapproximateanyconvexfunctionwitharbitraryï¬delity.Inparticular,\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\ninputxasatraditionallayerusingtherectiï¬edlinearactivationfunction,absolute\nvaluerectiï¬cationfunction,ortheleakyorparametricReLU,orcanlearnto\nimplementatotallydiï¬€erentfunctionaltogether.Themaxoutlayerwillofcourse\nbeparametrized diï¬€erentlyfromanyoftheseotherlayertypes,sothelearning\ndynamicswillbediï¬€erenteveninthecaseswheremaxoutlearnstoimplementthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "samefunctionofasoneoftheotherlayertypes. x\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\nsomaxoutunitstypicallyneedmoreregularizationthanrectiï¬edlinearunits.They\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\npiecesperunitiskeptlow(,). Cai e t a l .2013\nMaxoutunitshaveafewotherbeneï¬ts.Insomecases,onecangainsomesta-\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speciï¬cally,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "ifthefeaturescapturedby ndiï¬€erentlinearï¬lterscanbesummarizedwithout\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\nlayercangetbywithtimesfewerweights. k\nBecauseeachunitisdrivenbymultipleï¬lters,maxoutunitshavesomeredun-\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\nthepast( ,). Goodfellow e t a l .2014a\nRectiï¬edlinearunitsandallofthesegeneralizations ofthemarebasedonthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "1 9 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\narchitectures,theLSTM,propagatesinformationthroughtimeviasummationâ€”a\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\ninsection.10.10\n6.3.2LogisticSigmoidandHyperbolicTangent\nPriortotheintroduction ofrectiï¬edlinearunits,mostneuralnetworksusedthe\nlogisticsigmoidactivationfunction\ng z Ïƒ z () = () (6.38)\northehyperbolictangentactivationfunction\ng z z . () = tanh( ) (6.39)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "g z z . () = tanh( ) (6.39)\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z Ïƒ zâˆ’\nWeÂ havealreadyÂ seen sigmoidÂ unitsasoutputÂ units,Â usedtoÂ predictthe\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\nunitssaturateacrossmostoftheirdomainâ€”they saturatetoahighvaluewhen\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "sigmoidalunitscanmakegradient-basedlearningverydiï¬ƒcult.Forthisreason,\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\nlayer.\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while Ïƒ(0) =1\n2.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "2.\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\nnetworkË† y=wî€¾tanh(Uî€¾tanh(Vî€¾x))resemblestrainingalinearmodelË† y=\nwî€¾Uî€¾Vî€¾xsolongastheactivationsofthenetworkcanbekeptsmall.This\nmakestrainingthenetworkeasier. tanh\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\ndrawbacksofsaturation.\n1 9 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.3.3OtherHiddenUnits\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\nIngeneral,awidevarietyofdiï¬€erentiable functionsperformperfectlywell.\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "functions.Duringresearchanddevelopmentofnewtechniques,itiscommon\ntotestmanydiï¬€erentactivationfunctionsandï¬ndthatseveralvariationson\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniï¬cant\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\ntypesaresocommonastobeuninteresting.\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "intheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "aneuralnetworklayerwith ninputsand poutputs,h= g(Wî€¾x+b).Wemay\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\nusingweightmatrixV.Iftheï¬rstlayerhasnoactivationfunction,thenwehave\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\nfactoredapproachistocomputeh= g(Vî€¾Uî€¾x+b).IfUproduces qoutputs,\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "comesatthecostofconstrainingthelineartransformationtobelow-rank,but\ntheselow-rankrelationshipsareoftensuï¬ƒcient.Linearhiddenunitsthusoï¬€eran\neï¬€ectivewayofreducingthenumberofparametersinanetwork.\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "unitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\nmanipulatememory,describedinsection.10.12\n1 9 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAfewotherreasonablycommonhiddenunittypesinclude:\nâ€¢RadialbasisfunctionorRBFunit: h i=expî€\nâˆ’1\nÏƒ2\ni||W : , iâˆ’||x2î€‘\n.This\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\nsaturatestoformost,itcanbediï¬ƒculttooptimize. 0x\nâ€¢Softplus: g( a) = Î¶( a) =log(1+ ea).Thisisasmoothversionoftherectiï¬er,\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "models. ()comparedthesoftplusandrectiï¬erandfound Glorot e t a l .2011a\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\nbeverycounterintuitiveâ€”onemightexpectittohaveanadvantageover\ntherectiï¬erduetobeingdiï¬€erentiableeverywhereorduetosaturatingless\ncompletely,butempiricallyitdoesnot.\nâ€¢Hardtanh:thisisshapedsimilarlytothetanhandtherectiï¬erbutunlike\nthelatter,itisbounded, g( a)=max(âˆ’1 ,min(1 , a)).Itwasintroduced",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "by(). Collobert2004\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\nunittypesremaintobediscovered.\n6. 4 A rc h i t ec t u re D es i gn\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers.Â Most\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "layerbeingafunctionofthelayerthatprecededit.Inthisstructure,theï¬rstlayer\nisgivenby\nh( 1 )= g( 1 )î€\nW( 1 )î€¾xb+( 1 )î€‘\n, (6.40)\nthesecondlayerisgivenby\nh( 2 )= g( 2 )î€\nW( 2 )î€¾h( 1 )+b( 2 )î€‘\n, (6.41)\nandsoon.\n1 9 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\nanetworkwithevenonehiddenlayerissuï¬ƒcienttoï¬tthetrainingset.Deeper\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize.Â The\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\nmonitoringthevalidationseterror.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "monitoringthevalidationseterror.\n6.4.1UniversalApproximationPropertiesandDepth\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\nbydeï¬nitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\nAtï¬rstglance,wemightpresumethatlearninganonlinearfunctionrequires\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\nmationframework.Speciï¬cally,theuniversalapproximationtheorem(Hornik\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\nlayerandatleastonehiddenlayerwithanyâ€œsquashingâ€activationfunction(such\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\nfunctionfromoneï¬nite-dimensional spacetoanotherwithanydesirednon-zero\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "derivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\nisbeyondthescopeofthisbook;Â forourpurposesitsuï¬ƒcestosaythatany\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\nalsoapproximateanyfunctionmappingfromanyï¬nitedimensionaldiscretespace\ntoanother.Whiletheoriginaltheoremswereï¬rststatedintermsofunitswith",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "activationfunctionsthatsaturatebothforverynegativeandforverypositive\narguments,universalapproximation theoremshavealsobeenprovedforawider\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiï¬edlinear\nunit( ,). Leshno e t a l .1993\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "to l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\ncanfailfortwodiï¬€erentreasons.First,theoptimizationalgorithmusedfortraining\n1 9 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmaynotbeabletoï¬ndthevalueoftheparametersthatcorrespondstothedesired\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\noverï¬tting.Recallfromsectionthattheâ€œnofreelunchâ€theoremshowsthat 5.2.1\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "isnouniversalprocedureforexaminingatrainingsetofspeciï¬cexamplesand\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "withonehiddenunitcorrespondingtoeachinputconï¬gurationthatneedstobe\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\nnumberofpossiblebinaryfunctionsonvectorsvâˆˆ{0 ,1}nis22nandselecting\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\nfreedom.\nInsummary,afeedforwardnetworkwithasinglelayerissuï¬ƒcienttorepresent\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\namountofgeneralization error.\nThereexistfamiliesoffunctionswhichcanbeapproximated eï¬ƒcientlybyan\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n.Â Suchresults\nwereï¬rstprovedformodelsthatdonotresemblethecontinuous,diï¬€erentiable",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\nmodels.Theï¬rstresultswereforcircuitsoflogicgates(,).Later HÃ¥stad1986\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\n( ,; ,),andthentonetworkswith HÃ¥stadandGoldmann1991Hajnal e t a l .1993\ncontinuous-valuedactivations(,; ,).Â Manymodern Maass1992Maass e t a l .1994\nneuralnetworksuserectiï¬edlinearunits. ()demonstrated Leshno e t a l .1993",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\nincludingrectiï¬edlinearunits,haveuniversalapproximation properties,butthese\nresultsdonotaddressthequestionsofdepthoreï¬ƒciencyâ€”theyspecifyonlythat\nasuï¬ƒcientlywiderectiï¬ernetworkcouldrepresentanyfunction.Montufar e t a l .\n1 9 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n()showedthatfunctionsrepresentablewithadeeprectiï¬ernetcanrequire 2014\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\nfromrectiï¬ernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\nanetworkwithabsolutevaluerectiï¬cationcreatesmirrorimagesofthefunction",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "computedontopofsomehiddenunit,withrespecttotheinputofthathidden\nunit.Eachhiddenunitspeciï¬eswheretofoldtheinputspaceinordertocreate\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "rectiï¬ernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiï¬cation\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\nofsymmetryisgivenbythehyperplanedeï¬nedbytheweightsandbiasoftheunit.A\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "befoldedontopoftheï¬rst(byanotherdownstreamunit)toobtainanothersymmetry\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\npermissionfrom (). Montufar e t a l .2014\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\nnumberoflinearregionscarvedoutbyadeeprectiï¬ernetworkwith dinputs,\ndepth,andunitsperhiddenlayer,is l n\nOî€ î€’n\ndî€“d l (âˆ’ 1 )\nndî€¡\n, (6.42)\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithï¬ltersper l k\nunit,thenumberoflinearregionsis\nOî€",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "unit,thenumberoflinearregionsis\nOî€\nk( 1 ) + lâˆ’ dî€‘\n. (6.43)\n2 0 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\nWemayalsowanttochooseadeepmodelforstatisticalreasons.Â Anytime\nwechooseaspeciï¬cmachinelearningalgorithm,weareimplicitlystatingsome\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\nmultiplesteps,whereeachstepmakesuseofthepreviousstepâ€™soutput.Â These",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "intermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,;2014dSzegedy ,).Seeï¬gureandï¬gureforexamplesof 2014a 6.6 6.7\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\n6.4.2OtherArchitecturalConsiderations\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\nInpractice,neuralnetworksshowconsiderablymorediversity.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "Manyneuralnetworkarchitectures havebeendevelopedforspeciï¬ctasks.\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\nhavetheirownarchitecturalconsiderations.\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoï¬‚owfrom\noutputlayerstolayersnearertheinput.\n2 0 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n3 4 5 6 7 8 9 1 0 1 1\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\ne t a l .().Â Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth.Â See 2014d",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "ï¬gureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\ndonotyieldthesameeï¬€ect.\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "theoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\nthatareveryeï¬€ectiveforcomputervisionproblems.Inthischapter,itisdiï¬ƒcult\ntogivemuchmorespeciï¬cadviceconcerningthearchitectureofagenericneural\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "havebeenfoundtoworkwellfordiï¬€erentapplicationdomains.\n2 0 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\nN u m b e r o f p a r a m e t e r s Ã— 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\n3,fullyconnected\n11,convolutional\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "nearlyaseï¬€ectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\ncontextoverï¬tataround20millionparameterswhiledeeponescanbeneï¬tfromhaving\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\nthespaceoffunctionsthemodelcanlearn.Speciï¬cally,itexpressesabeliefthatthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "functionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\ncornersdeï¬nedintermsofedges)orinlearningaprogramwithsequentiallydependent\nsteps(e.g.,ï¬rstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\nthem).\n2 0 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i ï¬€ eren t i at i on A l go-\nri t h m s\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\noutput Ë†y,informationï¬‚owsforwardthroughthenetwork.Theinputsxprovide\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\nandï¬nallyproduces Ë†y.Thisiscalledforwardpropagation.Duringtraining,\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(Î¸).",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\nbackprop,allowstheinformationfromthecosttothenï¬‚owbackwardsthrough\nthenetwork,inordertocomputethegradient.\nComputingananalyticalexpressionforthegradientisstraightforward,but\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\nThetermback-propagation isoftenÂ misunders toodasmeaningthewhole",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\nFurthermore,back-propagation isoftenmisunderstoodasbeingspeciï¬ctomulti-\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\nfunctionisundeï¬ned).Speciï¬cally,wewilldescribehowtocomputethegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "âˆ‡ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\nâˆ‡ Î¸ J(Î¸).Manymachinelearningtasksinvolvecomputingotherderivatives,either\naspartofÂ thelearningÂ process,Â orÂ to analyzethelearnedÂ model. TheÂ back-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\nusedcasewherehasasingleoutput. f\n2 0 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.1ComputationalGraphs\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\nmoreprecise language. computationalgraph\nManywaysofformalizingcomputationasgraphsarepossible.\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\ntogether.\nWithoutlossofgenerality,Â wedeï¬neanoperationtoreturnonlyasingle\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\nconceptualunderstanding.\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\nwedrawadirectededgefrom xto y.Â Wesometimesannotatetheoutputnode\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\noperationisclearfromcontext.\nExamplesofcomputational graphsareshowninï¬gure.6.8\n6.5.2ChainRuleofCalculus\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\nchainrule,withaspeciï¬corderofoperationsthatishighlyeï¬ƒcient.\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\nthechainrulestatesthatd z\nd x=d z\nd yd y\nd x. (6.44)\nWecangeneralizethisbeyondthescalarcase.Supposethatxâˆˆ Rm,yâˆˆ Rn,\n2 0 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxx yy\n( a)Ã—\nx x ww\n( b)u( 1 )u( 1 )\nd o t\nbbu( 2 )u( 2 )\n+Ë† y Ë† y\nÏƒ\n( c )XX WWU( 1 )U( 1 )\nm a t m u l\nb bU( 2 )U( 2 )\n+HH\nr e l u\nx x ww\n( d)Ë† yË† y\nd o t\nÎ» Î»u( 1 )u( 1 )\ns q ru( 2 )u( 2 )\ns u mu( 3 )u( 3 )\nÃ—\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) Ã—operationto\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) Ë† y= Ïƒî€€\nxî€¾w+ bî€\n.\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "butneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\nmatrixofrectiï¬edlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\nofinputsX.Examplesaâ€“cappliedatmostoneoperationtoeachvariable,butit ( d )\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\nweightsareusedtomakeboththepredictionË† yandtheweightdecaypenalty Î»î",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "iw2\ni.\n2 0 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\nâˆ‚ z\nâˆ‚ x i=î˜\njâˆ‚ z\nâˆ‚ y jâˆ‚ y j\nâˆ‚ x i. (6.45)\nInvectornotation,thismaybeequivalentlywrittenas\nâˆ‡ x z=î€’âˆ‚y\nâˆ‚xî€“î€¾\nâˆ‡ y z , (6.46)\nwhereâˆ‚ y\nâˆ‚ xistheJacobianmatrixof. n mÃ— g\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\naJacobianmatrixâˆ‚ y\nâˆ‚ xbyagradientâˆ‡ y z.Theback-propagation algorithmconsists\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "Usuallywedonotapplytheback-propagationalgorithmmerelytovectors,\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\nsameasback-propagation withvectors.Theonlydiï¬€erenceishowthenumbers\narearrangedinagridtoformatensor.Wecouldimagineï¬‚atteningeachtensor\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\nback-propagationisstilljustmultiplyingJacobiansbygradients.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "Todenotethegradientofavalue zwithrespecttoatensor X,wewrite âˆ‡ X z,\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinatesâ€”for\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\npossibleindextuples i,(âˆ‡ X z) igivesâˆ‚ z\nâˆ‚ X i.Thisisexactlythesameashowforall\npossibleintegerindices iintoavector,(âˆ‡ x z) igivesâˆ‚ z\nâˆ‚ x i.Usingthisnotation,we",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "âˆ‚ x i.Usingthisnotation,we\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\nâˆ‡ X z=î˜\nj(âˆ‡ X Y j)âˆ‚ z\nâˆ‚ Y j. (6.47)\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\nintroducessomeextraconsiderations.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "introducessomeextraconsiderations.\nSpeciï¬cally,manysubexpressionsmayberepeatedseveraltimeswithinthe\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\n2 0 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\nï¬gure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\nbewasteful.Â Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\nreducememoryconsumptionatthecostofhigherruntime.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "reducememoryconsumptionatthecostofhigherruntime.\nWeï¬rstbeginbyaversionoftheback-propagationalgorithmthatspeciï¬esthe\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeciï¬cation\nofthecomputational graphforcomputingtheback-propagation. However,this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "formulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\nsymbolicgraphthatperformsthegradientcomputation.Â Such aformulationis\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\nnodesthatcontainarbitrarytensors.\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).Â In\notherwordswewishtocomputeâˆ‚ u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "otherwordswewishtocomputeâˆ‚ u() n\nâˆ‚ u() iforall iâˆˆ{1 ,2 , . . . , n i}.Intheapplication\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\ncorrespondtotheparametersofthemodel.\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\ngoingupto u( ) n.Asdeï¬nedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "operation f( ) iandiscomputedbyevaluatingthefunction\nu( ) i= ( f A( ) i) (6.48)\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\nThatalgorithmspeciï¬estheforwardpropagationcomputation,whichwecould\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\nthederivativeâˆ‚ u() n",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "thederivativeâˆ‚ u() n\nâˆ‚ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\n2 0 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\nu( 1 )to u( n i )toanoutput u( ) n.Thisdeï¬nesacomputational graphwhereeachnode\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P aâˆˆ ( u( ) i).The\ninputtothecomputational graphisthevectorx,andissetintotheï¬rst n inodes\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadoï¬€thelast(output)\nnode u( ) n.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "node u( ) n.\nfor i , . . . , n = 1 ido\nu( ) iâ† x i\nendfor\nfor i n= i+1 , . . . , ndo\nA( ) iâ†{ u( ) j|âˆˆ j P a u(( ) i)}\nu( ) iâ† f( ) i( A( ) i)\nendfor\nreturn u( ) n\nusingthechainrulewithrespecttoscalaroutput u( ) n:\nâˆ‚ u( ) n\nâˆ‚ u( ) j=î˜\ni j P a u :âˆˆ (() i )âˆ‚ u( ) n\nâˆ‚ u( ) iâˆ‚ u( ) i\nâˆ‚ u( ) j(6.49)\nasspeciï¬edbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\nthecomputationofâˆ‚ u() i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "thecomputationofâˆ‚ u() i\nâˆ‚ u() j.Inaddition,adotproductisperformedforeachnode,\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\nof u( ) jandthevectorcontainingthepartialderivativesâˆ‚ u() i\nâˆ‚ u() jforthesamechildren\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nodewithrespecttooneofitsparents)aswellasperformingonemultiplication\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\neï¬ƒcientimplementations.\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\nsubexpressionswithoutregardtomemory.Speciï¬cally,itperformsontheorder\nofoneJacobianproductpernodeinthegraph.Â Thiscanbeseenfromthefact\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "thegraphexactlyonceinordertoobtaintheassociatedpartialderivativeâˆ‚ u() i\nâˆ‚ u() j.\n2 0 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.2Simpliï¬edversionoftheback-propagation algorithmforcomputing\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\nintendedtofurtherunderstandingbyshowingasimpliï¬edcasewhereallvariables\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\nThissimpliï¬edversioncomputesthederivativesofallnodesinthegraph.Â The\ncomputational costofthisalgorithmisproportional tothenumberofedgesin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\ntheforwardpropagation. Eachâˆ‚ u() i\nâˆ‚ u() jisafunctionoftheparents u( ) jof u( ) i,thus\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\ngraph.\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\ntionsofthenetwork\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "beencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\nâˆ‚ u() n\nâˆ‚ u() i.\ng r a d t a b l e_ [ u( ) n] 1â†\nfor do j n= âˆ’1downto1\nThenextlinecomputesâˆ‚ u() n\nâˆ‚ u() j=î\ni j P a u :âˆˆ (() i )âˆ‚ u() n\nâˆ‚ u() iâˆ‚ u() i\nâˆ‚ u() jusingstoredvalues:\ng r a d t a b l e_ [ u( ) j] â†î\ni j P a u :âˆˆ (() i ) g r a d t a b l e_ [ u( ) i]âˆ‚ u() i\nâˆ‚ u() j\nendfor\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\nsimpliï¬cationsonthecomputational graph,ormaybeabletoconservememoryby\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\nafterdescribingtheback-propagation algorithmitself.\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\nToclarifytheabovedeï¬nitionoftheback-propagation computation,letusconsider\nthespeciï¬cgraphassociatedwithafully-connected multi-layerMLP.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "Algorithmï¬rstshowstheforwardpropagation, whichmapsparametersto 6.3\nthesupervisedloss L(Ë†yy ,)associatedwithasingle(input,target) trainingexample\n( )xy ,,with Ë†ytheoutputoftheneuralnetworkwhenisprovidedininput. x\nAlgorithmÂ  thenÂ showsÂ thecorrespondingÂ computationÂ to beÂ donefor 6.4\n2 1 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfff\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\nthegradient.Let wâˆˆ Rbetheinputtothegraph.Weusethesamefunction f: R Râ†’\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\nTocomputeâˆ‚ z\nâˆ‚ w,weapplyequationandobtain: 6.44\nâˆ‚ z\nâˆ‚ w(6.50)\n=âˆ‚ z\nâˆ‚ yâˆ‚ y\nâˆ‚ xâˆ‚ x\nâˆ‚ w(6.51)\n= fî€°() y fî€°() x fî€°() w (6.52)\n= fî€°((())) f f w fî€°(()) f w fî€°() w (6.53)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "= fî€°((())) f f w fî€°(()) f w fî€°() w (6.53)\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "runtime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\nusefulwhenmemoryislimited.\n2 1 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napplyingtheback-propagation algorithmtothisgraph.\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\nstraightforwardtounderstand.However,Â theyarespecializedtoonespeciï¬c\nproblem.\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\ncomputation.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "computation.\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\nthecomputationofthecostfunction.Theloss L(Ë†yy ,)dependsontheoutput\nË†yandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer â„¦( Î¸),where Î¸\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "useaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\nRequire:Networkdepth, l\nRequire:W( ) i, i , . . . , l , âˆˆ{1 }theweightmatricesofthemodel\nRequire:b( ) i, i , . . . , l , âˆˆ{1 }thebiasparametersofthemodel\nRequire:x,theinputtoprocess\nRequire:y,thetargetoutput\nh( 0 )= x\nfordo k , . . . , l = 1\na( ) k= b( ) k+W( ) kh( 1 ) kâˆ’\nh( ) k= ( fa( ) k)\nendfor\nË†yh= ( ) l\nJ L= (Ë†yy ,)+â„¦() Î» Î¸\n6.5.5Symbol-to-SymbolDerivatives\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "variablesÂ thatdoÂ notÂ havespeciï¬cÂ values.ThesealgebraicÂ and graph-based\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\ntrainaneuralnetwork,wemustassignspeciï¬cvaluestothesesymbols.We\nreplaceasymbolicinputtothenetworkxwithaspeciï¬cnumericvalue,suchas\n[123765 18] . , . ,âˆ’ .î€¾.\n2 1 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\noutputlayerandgoingbackwardstotheï¬rsthiddenlayer.Fromthesegradients,\nwhichcanbeinterpretedasanindicationofhoweachlayerâ€™soutputshouldchange\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "gradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\ncomputed)orusedwithothergradient-basedoptimization methods.\nAftertheforwardcomputation,computethegradientontheoutputlayer:\ngâ†âˆ‡ Ë† y J= âˆ‡ Ë† y L(Ë†yy ,)\nfor do k l , l , . . . , = âˆ’1 1\nConvertÂ thegradientonÂ thelayerâ€™sÂ outputÂ intoÂ aÂ gradientÂ intoÂ thepre-\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\ngâ†âˆ‡a() k J f = gî€Œî€°(a( ) k)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "gâ†âˆ‡a() k J f = gî€Œî€°(a( ) k)\nComputegradientsonweightsandbiases(includingtheregularizationterm,\nwhereneeded):\nâˆ‡b() k J Î» = +g âˆ‡b() kâ„¦() Î¸\nâˆ‡W() k J= gh( 1 ) kâˆ’î€¾+ Î»âˆ‡W() kâ„¦() Î¸\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayerâ€™sactivations:\ngâ†âˆ‡h(1) k âˆ’ J= W( ) kî€¾g\nendfor\n2 1 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nz z\nxxyy\nw wfffz z\nxxyy\nw wfff\nd z\nd yd z\nd yfî€¡\nd y\nd xd y\nd xfî€¡\nd z\nd xd z\nd xÃ—\nd x\nd wd x\nd wfî€¡\nd z\nd wd z\nd wÃ—\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\nspeciï¬cnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "derivativesforanyspeciï¬cnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\nittoconstructthegraphfortheexpressioncorrespondingtod z\nd w.Inthisexample,wedo\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\nderivative.\nSomeapproachestoback-propagationtakeacomputational graphandaset",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "ofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproachâ€œsymbol-\nto-numberâ€diï¬€erentiation. ThisistheapproachusedbylibrariessuchasTorch\n( ,)andCaï¬€e(,). Collobert e t a l .2011b Jia2013\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "andTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\nisillustratedinï¬gure.Theprimaryadvantageofthisapproachisthat 6.10\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\nback-propagationagain,diï¬€erentiating thederivativesinordertoobtainhigher\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "2 1 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\ngraphmaythenbeevaluatedusingspeciï¬cnumericalvaluesatalatertime.This\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\nparentsâ€™valuesareavailable.\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "performingexactlythesamecomputations asaredoneinthegraphbuiltbythe\nsymbol-to-symbolapproach.Thekeydiï¬€erenceisthatthesymbol-to-number\napproachdoesnotexposethegraph.\n6.5.6GeneralBack-Propagation\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\nthatthegradientwithrespectto zisgivenbyd z\nd z=1.Wecanthencompute\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "currentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\ntwoormorepaths,wesimplysumthegradientsarrivingfromdiï¬€erentpathsat\nthatnode.\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\nmaximumgenerality,wedescribethisvariableasbeingatensor V.Â Tensorcan\ningeneralhaveanynumberofdimensions.Â Theysubsumescalars,vectors,and\nmatrices.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "matrices.\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\nâ€¢ g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\noperation,andtheget_operationfunction.Supposewehaveavariablethat\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\nreturnsapointertoaninstanceofthecorrespondingC++class.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "â€¢ g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\nVinthecomputational graph.G\nâ€¢ G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\ninthecomputational graph.G\n2 1 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "ofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\nisresponsiblefordeï¬ningtwoback-propagation rules,oneforeachofitsinput\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\nisgivenbyGBî€¾.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "b p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAî€¾G.The\nback-propagationalgorithmitselfdoesnotneedtoknowanydiï¬€erentiation rules.It\nonlyneedstocalleachoperationâ€™sbpropruleswiththerightarguments.Formally,\no p b p r o p i n p u t s . ( , , X G)mustreturn\nî˜\ni(âˆ‡ X o p f i n p u t s .( ) i) G i , (6.54)\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "mathematical functionthattheoperationimplements, Xistheinputwhosegradient\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\nderivativeon. x",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "derivativeon. x\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\nanynewoperationsmanually.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "anynewoperationsmanually.\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\n2 1 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\ninthe subroutineofalgorithm build_grad 6.6.\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\nRequire:G,thecomputational graph\nRequire: z,thevariabletobediï¬€erentiated\nLetGî€°beGprunedtocontainonlynodesthatareancestorsof zanddescendents\nofnodesin. T",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "ofnodesin. T\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\ng r a d t a b l e_ [] 1 zâ†\nfordo Vin T\nb u i l d g r a d_ ( V , ,GGî€°, g r a d t a b l e_ )\nendfor\nReturn restrictedto grad_table T\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\nNowthatwehavespeciï¬edtheback-propagationalgorithm,wecanunderstandits",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "computational cost.Ifweassumethateachoperationevaluationhasroughlythe\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "O( n2) operations.Herewearecountingoperationsinthecomputational graph,not\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\n2 1 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GGî€°, g r a d t a b l e_ )of\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdeï¬ned\ninalgorithm .6.5\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\nRequire:G,thegraphtomodify.\nRequire:Gî€°,therestrictionoftonodesthatparticipateinthegradient. G\nRequire:grad_table,adatastructuremappingnodestotheirgradients\nif then Visingrad_table\nReturn_ g r a d t a b l e[] V\nendif\niâ†1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "Return_ g r a d t a b l e[] V\nendif\niâ†1\nfor C V in_ g e t c o n s u m e r s( ,Gî€°)do\no p g e t o p e r a t i o n â†_ () C\nD C â† b u i l d g r a d_ ( , ,GGî€°, g r a d t a b l e_ )\nG( ) iâ† G o p b p r o p g e t i n p u t s . (_ ( C ,î€°) ) , , V D\ni iâ†+1\nendfor\nGâ†î\ni G( ) i\ng r a d t a b l e_ [] = V G\nInsertandtheoperationscreatingitinto G G\nReturn G\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\ntherecursivechainrule(equation)non-recursively: 6.49\nâˆ‚ u( ) n\nâˆ‚ u( ) j=î˜\npa t h ( u( Ï€1), u( Ï€2), . . . , u( Ï€ t)) ,\nf r o m Ï€1 = t o j Ï€ t = ntî™\nk = 2âˆ‚ u( Ï€ k )\nâˆ‚ u( Ï€ k âˆ’1 ). (6.55)\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "graph.Thislargecostwouldbeincurredbecausethesamecomputationfor\nâˆ‚ u() i\nâˆ‚ u() jwouldberedonemanytimes.Â Toavoidsuchrecomputation, wecanthink\nofback-propagation asatable-ï¬llingalgorithmthattakesadvantageofstoring\nintermediateresultsâˆ‚ u() n\nâˆ‚ u() i.Eachnodeinthegraphhasacorrespondingslotina\ntabletostorethegradientforthatnode.Byï¬llinginthesetableentriesinorder,\n2 1 8",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ï¬lling\nstrategyissometimescalled . dynamicprogramming\n6.5.7Example:Back-PropagationforMLPTraining\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\ntrainamultilayerperceptron.\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "singleminibatch.Speciï¬cally,weuseaminibatchofexamplesfromthetraining\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "operationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\ndistributiondeï¬nedbytheseunnormalized logprobabilities. Theresultingcross-\nentropydeï¬nesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\nlikelihoodestimationoftheclassiï¬er.However,tomakethisexamplemorerealistic,\nwealsoincludearegularizationterm.Thetotalcost\nJ J= M LE+ Î»ï£«\nï£­î˜\ni , jî€\nW( 1 )\ni , jî€‘2\n+î˜\ni , jî€\nW( 2 )\ni , jî€‘2ï£¶\nï£¸ (6.56)\nconsistsofthecross-entropyandaweightdecaytermwithcoeï¬ƒcient Î».The",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "computational graphisillustratedinï¬gure.6.11\nThecomputational graphforthegradientofthisexampleislargeenoughthat\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneï¬ts\noftheback-propagation algorithm,whichisthatitcanautomatically generate\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\nderivemanually.\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\nbylookingattheforwardpropagationgraphinï¬gure.Totrain,wewish 6.11",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "tocomputebothâˆ‡W(1) Jand âˆ‡W(2) J.Therearetwodiï¬€erentpathsleading\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\nalwayscontribute 2 Î»W( ) itothegradientonW( ) i.\n2 1 9",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nXXW( 1 )W( 1 )U( 1 )U( 1 )\nm a t m u lHH\nr e l u\nU( 3 )U( 3 )\ns q ru( 4 )u( 4 )\ns u mÎ» Î» u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\nm a t m u ly yJ M L E J M L E\nc r o s s _ e n t r o p y\nU( 5 )U( 5 )\ns q ru( 6 )u( 6 )\ns u mu( 8 )u( 8 )J J\n+\nÃ—\n+\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\nexploretwodiï¬€erentbranches.Ontheshorterbranch,itaddsHî€¾Gtothe\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\ncomputes âˆ‡ H J=GW( 2 )î€¾usingtheback-propagationrulefortheï¬rstargument",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "tothematrixmultiplication operation.Next,thereluoperationusesitsback-\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 Gî€°.Thelaststepofthe\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\nargumentoftheoperationtoadd matmul Xî€¾Gî€°tothegradientonW( 1 ).\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "updatetheparameters.\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\n2 2 0",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "numberofexamplesintheminibatchand n histhenumberofhiddenunits.\n6.5.8Complications\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\nmentationsactuallyusedinpractice.\nAsnotedabove,wehaverestrictedthedeï¬nitionofanoperationtobea\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "besttocomputebothinasinglepassthroughmemory,soitismosteï¬ƒcientto\nimplementthisprocedureasasingleoperationwithtwooutputs.\nWeÂ havenotÂ describedÂ howÂ tocontrolthememoryconsumptionÂ ofback-\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuï¬€erand\naddingeachvaluetothatbuï¬€erasitiscomputed.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "addingeachvaluetothatbuï¬€erasitiscomputed.\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\ndatatypes,suchas32-bitï¬‚oatingpoint,64-bitï¬‚oatingpoint,andintegervalues.\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\nSomeoperationshaveundeï¬nedgradients,anditisimportanttotrackthese\ncasesanddeterminewhetherthegradientrequestedbytheuserisundeï¬ned.\nVariousothertechnicalitiesmakereal-worlddiï¬€erentiation morecomplicated.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\nthatmanymoresubtletiesexist.\n6.5.9Diï¬€erentiationoutsidetheDeepLearningCommunity\nTheÂ deeplearningÂ comm unityhasÂ beensomewhatÂ isolatedfromÂ theÂ broader\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\n2 2 1",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nconcerninghowtoperformdiï¬€erentiation. Moregenerally,theï¬eldofautomatic\ndiï¬€erentiationisconcernedwithhowtocomputederivativesalgorithmically .\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\ndiï¬€erentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\nruleindiï¬€erentorders.Ingeneral,Â determining theorderofevaluationthat",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "resultsinthelowestcomputational costisadiï¬ƒcultproblem.Findingtheoptimal\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\nexpensiveform.\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\nwedeï¬ne\nq i=exp( z i)î\niexp( z i), (6.57)",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "wedeï¬ne\nq i=exp( z i)î\niexp( z i), (6.57)\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\noperations,Â andÂ constructÂ a cross-entropyloss J=âˆ’î\ni p ilog q i.Ahuman\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\nsimpleform: q iâˆ’ p i.Theback-propagation algorithmisnotcapableofsimplifying\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "librariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\nbythepureback-propagation algorithm.\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\nâˆ‚ u() i\nâˆ‚ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\nthesameorderasthenumberofcomputations fortheforwardcomputation: this",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "canbeseeninalgorithm becauseeachlocalpartialderivative 6.2âˆ‚ u() i\nâˆ‚ u() jneedsto\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\ntask.Â ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "matchingknownsimpliï¬cationpatternsinordertoiterativelyattempttosimplify\nthegraph.Wedeï¬nedback-propagation onlyforthecomputationofagradientofa\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\nof kdiï¬€erentscalarnodesinthegraph,orofatensor-valuednodecontaining k\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\n2 2 2",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\nanotherformofautomaticdiï¬€erentiationcalledforwardmodeaccumulation.\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\noï¬€computational eï¬ƒciencyformemory.Therelationshipbetweenforwardmode\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\nright-multiplyingasequenceofmatrices,suchas\nABCD , (6.58)\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "andgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\nthemultiplications left-to-right,correspondingtotheforwardmode.\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\nplementdiï¬€erentiationsoftwarethatactsdirectlyontraditionalprogramming",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "languagecode,suchasPythonorCcode,andautomatically generatesprograms\nthatdiï¬€erentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\nrequiringthelibrarydevelopertodeï¬nethebpropmethodsforeveryoperation\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeï¬ned.\nHowever,thespecializedapproachalsohasthebeneï¬tofallowingcustomized",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "back-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\nwouldpresumablybeunabletoreplicate.\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\nlearningcommunityverywell.Inthefuture,diï¬€erentiation technologyfordeep\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\ninthebroaderï¬eldofautomaticdiï¬€erentiation.\n2 2 3",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\n6.5.10Higher-OrderDerivatives\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\nderivativesastheyusetodescribetheoriginalfunctionbeingdiï¬€erentiated.This\nmeansthatthesymbolicdiï¬€erentiation machinerycanbeappliedtoderivatives.\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\nmatrix.Ifwehaveafunction f: Rnâ†’ R,thentheHessianmatrixisofsize n nÃ—.\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\nthusinfeasibletoevenrepresent.\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "performingvariousoperationslikeapproximately invertingamatrixorï¬nding\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\notherthanmatrix-vector products.\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\nHv= âˆ‡ xî¨\n(âˆ‡ x f x())î€¾vî©\n. (6.59)\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\ntakesthegradientofafunctionoftheinnergradientexpression.\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\nspecifythattheautomaticdiï¬€erentiationsoftwareshouldnotdiï¬€erentiatethrough\nthegraphthatproduced.v\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\ne( ) iistheone-hotvectorwith e( ) i",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "e( ) iistheone-hotvectorwith e( ) i\ni= 1andallotherentriesequalto0.\n6. 6 Hi s t or i c a l Not es\nFeedforwardnetworkscanbeseenaseï¬ƒcientnonlinearfunctionapproximators\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\n2 2 4",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\ncenturiesofprogressonthegeneralfunctionapproximationtask.\nThechainrulethatunderliestheback-propagation algorithmwasinvented\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676Lâ€™HÃ´pital1696\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\noptimization problemsuntilthe19thcentury(Cauchy1847,).",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\nseveraloftheï¬‚awsofthelinearmodelfamily,suchasitsinabilitytolearnthe\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\nceptronandameansofcomputingthegradientthroughsuchamodel.Eï¬ƒcient",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "applicationsofthechainrulebasedondynamicprogramming begantoappear\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\ntechniquestotrainingartiï¬cialneuralnetworks.Theideawasï¬nallydeveloped\ninpracticeafterbeingindependentlyrediscoveredindiï¬€erentways(,;LeCun1985\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "cessingpresentedtheresultsofsomeoftheï¬rstsuccessfulexperimentswith\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\nresearchinmulti-layerneuralnetworks.Â However,theideasputforwardbythe\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\nback-propagation.Â Theyincludecrucialideasaboutthepossiblecomputational\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "underthenameofâ€œconnectionismâ€ becauseoftheimportancethisschoolofthought\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\ne t a l .,).1986\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\nbeganin2006.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "beganin2006.\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\nstantiallysincethe1980s.Â Thesameback-propagationalgorithmandthesame\n2 2 5",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\nnetworksnoticeably.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "networksnoticeably.\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\nhadpreviouslysuï¬€eredfromsaturationandslowlearningwhenusingthemean",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "squarederrorloss.\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\nlinearhiddenunits,suchasrectiï¬edlinearunits.Rectiï¬cationusingthemax{0 , z}\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\nmodelsdidÂ notuserectiï¬edÂ linearunits,Â butÂ insteadappliedrectiï¬cationÂ to",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "nonlinearfunctions.Despitetheearlypopularityofrectiï¬cation,rectiï¬cationwas\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiï¬edlinearunits\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\nnon-diï¬€erentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\nJarrett2009 e t a l .()observedthatâ€œusingarectifyingnonlinearityisthesinglemost",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "importantfactorinimprovingtheperformanceofarecognitionsystemâ€among\nseveraldiï¬€erentfactorsofneuralnetworkarchitecturedesign.\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\nRandomweightsaresuï¬ƒcienttopropagateusefulinformationthrougharectiï¬ed\nlinearnetwork,allowingtheclassiï¬erlayeratthetoptolearnhowtomapdiï¬€erent\nfeaturevectorstoclassidentities.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "featurevectorstoclassidentities.\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\nshowedthatlearningisfareasierindeeprectiï¬edlinearnetworksthanindeep\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\n2 2 6",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER6.DEEPFEEDFORWARDNETWORKS\nRectiï¬edlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\nneurosciencehascontinuedtohaveÂ aninï¬‚uenceontheÂ developmentofdeep\nlearningalgorithms. ()motivaterectiï¬edlinearunitsfrom Glorot e t a l .2011a\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\ncompletelyinactive.2)Forsomeinputs,abiologicalneuronâ€™soutputisproportional",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "toitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "ismorecommontousesupervisedlearningtosupportunsupervisedlearning.\nFeedforwardnetworkscontinuetohaveunfulï¬lledpotential.Inthefuture,we\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\nsubsequentchapters,weturntohowtousethesemodelsâ€”howtoregularizeand\ntrainthem.\n2 2 7",
    "metadata": {
      "source": "[11]part-2-chapter-6.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 0\nS e qu e n ce Mo d e l i n g: Recurren t\nan d Recursiv e N e t s\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) Ï„.Justasconvolutionalnetworks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\nlength.\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\nthe1980s:sharingparametersacrossdiï¬€erentpartsofamodel.Parametersharing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "makesitpossibletoextendandapplythemodeltoexamplesofdiï¬€erentforms\n(diï¬€erentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\nseenduringtraining,norsharestatisticalstrengthacrossdiï¬€erentsequencelengths\nandacrossdiï¬€erentpositionsintime.Suchsharingisparticularlyimportantwhen\naspeciï¬cpieceofinformationcanoccuratmultiplepositionswithinthesequence.\nForexample,considerthetwosentencesâ€œIwenttoNepalin2009â€andâ€œIn2009,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "IwenttoNepal.â€Ifweaskamachinelearningmodeltoreadeachsentenceand\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\n373",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\nnetworkthatprocessessentencesofï¬xedlength.Atraditionalfullyconnected\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\nacrossseveraltimesteps.\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\nstep.Recurrentnetworksshareparametersinadiï¬€erentway.Eachmemberofthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\ndeepcomputational graph.\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 Ï„.In\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\nwithadiï¬€erentsequencelength Ï„foreachmemberoftheminibatch.Wehave",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\nThischapterextendstheideaofacomputational graphtoincludecycles.These",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "cyclesrepresenttheinï¬‚uenceofthepresentvalueofavariableonitsownvalue\natafuturetimestep.Suchcomputational graphsallowustodeï¬nerecurrent\nneuralnetworks.Wethendescribemanydiï¬€erentwaystoconstruct,train,and\nuserecurrentneuralnetworks.\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\nchapter,wereferthereadertothetextbookofGraves2012().\n3 7 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.1UnfoldingComputationalGraphs\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\nstructure.\nForexample,considertheclassicalformofadynamicalsystem:\ns( ) t= ( fs( 1 ) t âˆ’;)Î¸ , (10.1)\nwheres( ) tiscalledthestateofthesystem.\nEquationisrecurrentbecausethedeï¬nitionof 10.1 sattime trefersbackto\nthesamedeï¬nitionattime. tâˆ’1\nForaï¬nitenumberoftimesteps Ï„,thegraphcanbeunfoldedbyapplying\nthedeï¬nition Ï„âˆ’1times.Forexample,ifweunfoldequationfor10.1 Ï„= 3time\nsteps,weobtain\ns( 3 )=( fs( 2 );)Î¸ (10.2)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "steps,weobtain\ns( 3 )=( fs( 2 );)Î¸ (10.2)\n=(( f fs( 1 ););)Î¸Î¸ (10.3)\nUnfoldingtheequationbyrepeatedlyapplyingthedeï¬nitioninthiswayhas\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\nnowberepresentedbyatraditionaldirectedacycliccomputational graph.Â The\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\nï¬gure.10.1\ns( t âˆ’ 1 )s( t âˆ’ 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\nf f f f f f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "f f f f f f\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\nunfoldedcomputationalgraph.Â Eachnoderepresentsthestateatsometime tandthe\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\nofusedtoparametrize)areusedforalltimesteps. Î¸ f\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\nsignalx( ) t,\ns( ) t= ( fs( 1 ) t âˆ’,x( ) t;)Î¸ , (10.4)\n3 7 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\nRecurrentneuralnetworkscanbebuiltinmanydiï¬€erentways.Muchas\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\ndeï¬nethevaluesoftheirhiddenunits.Â Toindicatethatthestateisthehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\nthestate:\nh( ) t= ( fh( 1 ) t âˆ’,x( ) t;)Î¸ , (10.5)\nillustratedinï¬gure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\n(x( ) t,x( 1 ) t âˆ’,x( 2 ) t âˆ’, . . . ,x( 2 ),x( 1 ))toaï¬xedlengthvectorh( ) t.Dependingonthe\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "uptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\n(chapter).14\nf fhh\nx xh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\nf f\nU nf ol df f f f f\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "informationfromtheinputxbyincorporatingitintothestatehthatispassedforward\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\nnodeisnowassociatedwithoneparticulartimeinstance.\nEquationcanbedrawnintwodiï¬€erentways.OnewaytodrawtheRNN 10.5\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\n3 7 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\nview,thenetworkdeï¬nesacircuitthatoperatesinrealtime,withphysicalparts\nwhosecurrentstatecaninï¬‚uencetheirfuturestate,asintheleftofï¬gure.10.2\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "unfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\ndiï¬€erentvariables,withonevariablepertimestep,representingthestateofthe\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\nseparatenodeofthecomputational graph,asintherightofï¬gure.Whatwe10.2\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideoftheï¬gure\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\ngraphnowhasasizethatdependsonthesequencelength.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "graphnowhasasizethatdependsonthesequencelength.\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\nh( ) t= g( ) t(x( ) t,x( 1 ) t âˆ’,x( 2 ) t âˆ’, . . . ,x( 2 ),x( 1 )) (10.6)\n=( fh( 1 ) t âˆ’,x( ) t;)Î¸ (10.7)\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t âˆ’,x( 2 ) t âˆ’, . . . ,x( 2 ),x( 1 ))\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "processthusintroducestwomajoradvantages:\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\ninputsize,becauseitisspeciï¬edintermsoftransitionfromonestateto\nanotherstate,ratherthanspeciï¬edintermsofavariable-length historyof\nstates.\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\nateverytimestep.\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\nrequiredwithoutparametersharing.\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\n3 7 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninformationï¬‚owforwardintime(computingoutputsandlosses)andbackward\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\ninformationï¬‚ows.\n10.2RecurrentNeuralNetworks\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\ncandesignawidevarietyofrecurrentneuralnetworks.\nUUV V\nWWo( t âˆ’ 1 )o( t âˆ’ 1 )\nhhooy y\nLL",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "UUV V\nWWo( t âˆ’ 1 )o( t âˆ’ 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\nUU UU UUU nf ol d\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "thatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\ncomputesË†y=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aweightmatrixV.Equationdeï¬nesforwardpropagationinthismodel. 10.8 ( L e f t )The\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\ntimeinstance.\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\nincludethefollowing:\n3 7 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nâ€¢Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsbetweenhiddenunits,illustratedinï¬gure.10.3\nâ€¢Recurrentnetworksthatproduceanoutputateachtimestepandhave\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\nunitsatthenexttimestep,illustratedinï¬gure10.4\nâ€¢Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\nï¬gure.10.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "ï¬gure.10.5\nï¬gureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\nmostofthechapter.\nTherecurrentneuralnetworkofï¬gureandequationisuniversalinthe 10.3 10.8\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\narecurrentnetworkofaï¬nitesize.TheoutputcanbereadfromtheRNNafter\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\nfunctionsinthissettingusingasinglespeciï¬cRNNofï¬nitesize(Siegelmannand\nSontag1995()use886units).Theâ€œinputâ€oftheTuringmachineisaspeciï¬cation",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "ofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\nmachineissuï¬ƒcientforallproblems.ThetheoreticalRNNusedfortheproof\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\nrationalnumbersofunboundedprecision.\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\nï¬gure.Theï¬guredoesnotspecifythechoiceofactivationfunctionforthe 10.3\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "theï¬guredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\nobtainavectorË†yofnormalizedprobabilitiesovertheoutput.Forwardpropagation\nbeginswithaspeciï¬cationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\n3 7 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nUV\nWo( t âˆ’ 1 )o( t âˆ’ 1 )\nhhooy y\nLL\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\nh( ) . . .h( ) . . .V V V\nU U UU nf ol d\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "tothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbyï¬gure.TheRNN 10.3\ninï¬gurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\nrepresentationhandtransmithtothefuture.TheRNNinthisï¬gureistrainedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "putaspeciï¬coutputvalueintoo,andoistheonlyinformationitisallowedtosend\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\nfromthepast.ThismakestheRNNinthisï¬gurelesspowerful,butitmaybeeasierto\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\nparallelizationduringtraining,asdescribedinsection.10.2.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "3 8 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nt t Ï„ = 1to= ,weapplythefollowingupdateequations:\na( ) t= +bWh( 1 ) t âˆ’+Ux( ) t(10.8)\nh( ) t=tanh(a( ) t) (10.9)\no( ) t= +cVh( ) t(10.10)\nË†y( ) t=softmax(o( ) t) (10.11)\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\nLî€\n{x( 1 ), . . . ,x( ) Ï„}{ ,y( 1 ), . . . ,y( ) Ï„}î€‘\n(10.12)\n=î˜\ntL( ) t(10.13)\n=âˆ’î˜\ntlog p m o de lî€\ny( ) t|{x( 1 ), . . . ,x( ) t}î€‘\n, (10.14)\nwhere p m o de lî€€\ny( ) t|{x( 1 ), . . . ,x( ) t}î€\nisgivenbyreadingtheentryfor y( ) tfromthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "isgivenbyreadingtheentryfor y( ) tfromthe\nmodelâ€™soutputvectorË†y( ) t.Computingthegradientofthislossfunctionwithrespect\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\noftheunrolledgraphinï¬gure,followedbyabackwardpropagationpass 10.3\nmovingrighttoleftthroughthegraph.Theruntimeis O( Ï„) andcannotbereduced\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "eachtimestepmayonlybecomputedafterthepreviousone.Â Statescomputed\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\npass,sothememorycostisalso O( Ï„).Theback-propagation algorithmapplied\ntotheunrolledgraphwith O( Ï„)costiscalledback-propagationthroughtime\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\nalternative?\n10.2.1TeacherForcingandNetworkswithOutputRecurrence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\nthehiddenunitsatthenexttimestep(showninï¬gure)isstrictlylesspowerful 10.4\n3 8 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\nneedtocomputetheoutputfortheprevioustimestepï¬rst,becausethetraining\nsetprovidestheidealvalueofthatoutput.\nh( t âˆ’ 1 )h( t âˆ’ 1 )\nWh( ) th( ) t . . . . . .",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "h( t âˆ’ 1 )h( t âˆ’ 1 )\nWh( ) th( ) t . . . . . .\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\nU U Uh( ) Ï„h( ) Ï„\nx( ) Ï„x( ) Ï„W\nUo( ) Ï„o( ) Ï„y( ) Ï„y( ) Ï„L( ) Ï„L( ) Ï„\nV\n. . . . . .\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\nï¬xed-sizerepresentationusedasinputforfurtherprocessing.Â Theremightbeatarget\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "back-propagatingfromfurtherdownstreammodules.\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.Â Wecansee\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\n3 8 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t âˆ’ 1 )o( t âˆ’ 1 )o( ) to( ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tW\nV V\nU Uo( t âˆ’ 1 )o( t âˆ’ 1 )o( ) to( ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tW\nV V\nU U\nT r ai nÂ  t i m e T e s t Â  t i m e\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodelâ€™soutput\no( ) t,andfeedtheoutputbackintothemodel.\n3 8 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nlikelihoodcriterionis\nlog pî€\ny( 1 ),y( 2 )|x( 1 ),x( 2 )î€‘\n(10.15)\n=log pî€\ny( 2 )|y( 1 ),x( 1 ),x( 2 )î€‘\n+log pî€\ny( 1 )|x( 1 ),x( 2 )î€‘\n(10.16)\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\nvaluefromthetrainingset.Maximumlikelihoodthusspeciï¬esthatduringtraining,\nratherthanfeedingthemodelâ€™sownoutputbackintoitself,theseconnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\nThisisillustratedinï¬gure.10.6\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "timesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\nwithbothteacherforcingandBPTT.\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\ntheoutputdistribution)fedbackasinput.Â Inthiscase,thekindofinputsthat\nthenetworkseesduringtrainingcouldbequitediï¬€erentfromthekindofinputs\nthatitwillseeattesttime.Â Onewaytomitigatethisproblemistotrainwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "bothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "inputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\nusemoreofthegeneratedvaluesasinput.\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\n3 8 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\ngradient-basedtechniquestotrainanRNN.\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "tforx( ) t,h( ) t,o( ) tand L( ) t.Â Foreachnode Nweneedtocomputethegradient\nâˆ‡ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheï¬nalloss\nâˆ‚ L\nâˆ‚ L( ) t= 1 . (10.17)\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\nsoftmaxfunctiontoobtainthevectorË†yofprobabilitiesovertheoutput.Wealso\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "inputsofar.Thegradientâˆ‡o( ) t Lontheoutputsattimestep t,forall i , t,isas\nfollows:\n(âˆ‡o( ) t L)i=âˆ‚ L\nâˆ‚ o( ) t\ni=âˆ‚ L\nâˆ‚ L( ) tâˆ‚ L( ) t\nâˆ‚ o( ) t\ni=Ë† y( ) t\niâˆ’ 1i , y( ) t .(10.18)\nWeworkourwaybackwards,startingfromtheendofthesequence.Attheï¬nal\ntimestep, Ï„h( ) Ï„onlyhaso( ) Ï„asadescendent,soitsgradientissimple:\nâˆ‡h( ) Ï„ L= Vî€¾âˆ‡o( ) Ï„ L. (10.19)\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\nfrom t= Ï„âˆ’1downto t= 1,notingthath( ) t(for t < Ï„)hasasdescendentsboth",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "o( ) tandh( + 1 ) t.Itsgradientisthusgivenby\nâˆ‡h( ) t L=î€ \nâˆ‚h( + 1 ) t\nâˆ‚h( ) tî€¡î€¾\n(âˆ‡h( +1) t L)+î€ \nâˆ‚o( ) t\nâˆ‚h( ) tî€¡î€¾\n(âˆ‡o( ) t L) (10.20)\n= Wî€¾(âˆ‡h( +1) t L)diagî€’\n1âˆ’î€\nh( + 1 ) tî€‘2î€“\n+Vî€¾(âˆ‡o( ) t L)(10.21)\nwhere diagî€\n1âˆ’î€€\nh( + 1 ) tî€2î€‘\nindicatesthediagonalmatrixcontainingtheelements\n1âˆ’( h( + 1 ) t\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\nhiddenunitattime. i t+1\n3 8 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nOncethegradientsontheÂ internalnodesoftheÂ computational graphare\nobtained,Â wecanobtainthegradientsontheparameternodes.Becausethe\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\nofasingleedgeinthecomputational graphtothegradient.However,theâˆ‡ W f",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "operatorusedincalculustakesintoaccountthecontributionofWtothevalue\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\nintroducedummyvariablesW( ) tthataredeï¬nedtobecopiesofWbutwitheach\nW( ) tusedonlyattimestep t.Wemaythenuseâˆ‡W( ) ttodenotethecontribution\noftheweightsattimesteptothegradient. t\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\nâˆ‡ c L=î˜\ntî€ \nâˆ‚o( ) t\nâˆ‚cî€¡î€¾\nâˆ‡o( ) t L=î˜\ntâˆ‡o( ) t L (10.22)\nâˆ‡ b L=î˜\ntî€ \nâˆ‚h( ) t\nâˆ‚b( ) tî€¡î€¾\nâˆ‡h( ) t L=î˜\ntdiagî€’\n1âˆ’î€\nh( ) tî€‘2î€“",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "âˆ‚b( ) tî€¡î€¾\nâˆ‡h( ) t L=î˜\ntdiagî€’\n1âˆ’î€\nh( ) tî€‘2î€“\nâˆ‡h( ) t L(10.23)\nâˆ‡ V L=î˜\ntî˜\niî€ \nâˆ‚ L\nâˆ‚ o( ) t\niî€¡\nâˆ‡ V o( ) t\ni=î˜\nt(âˆ‡o( ) t L)h( ) tî€¾(10.24)\nâˆ‡ W L=î˜\ntî˜\niî€ \nâˆ‚ L\nâˆ‚ h( ) t\niî€¡\nâˆ‡W( ) t h( ) t\ni (10.25)\n=î˜\ntdiagî€’\n1âˆ’î€\nh( ) tî€‘2î€“\n(âˆ‡h( ) t L)h( 1 ) t âˆ’î€¾(10.26)\nâˆ‡ U L=î˜\ntî˜\niî€ \nâˆ‚ L\nâˆ‚ h( ) t\niî€¡\nâˆ‡U( ) t h( ) t\ni (10.27)\n=î˜\ntdiagî€’\n1âˆ’î€\nh( ) tî€‘2î€“\n(âˆ‡h( ) t L)x( ) tî€¾(10.28)\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\nitdoesnothaveanyparametersasancestorsinthecomputational graphdeï¬ning\ntheloss.\n3 8 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.2.3RecurrentNetworksasDirectedGraphicalModels\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "weusuallyusethecross-entropyassociatedwiththatdistributiontodeï¬netheloss.\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\nWhenÂ weÂ useÂ apredictivelog-likelihoodÂ trainingobjective,suchÂ asequa-\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\nthelog-likelihood\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "log( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\ntimestep,\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t âˆ’) . (10.30)\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "fromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\nvaluesinthepasttothecurrent y( ) tvalue.\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) Ï„},withnoadditionalinputs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "x.Theinputattimestep tissimplytheoutputattimestep tâˆ’1.TheRNNthen\ndeï¬nesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\nprobabilities:\nP P () = Y ( y( 1 ), . . . , y( ) Ï„) =Ï„î™\nt = 1P( y( ) t| y( 1 ) t âˆ’, y( 2 ) t âˆ’, . . . , y( 1 ))(10.31)\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) Ï„}accordingtosuchamodel\n3 8 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\npastobservation y( ) imayinï¬‚uencetheconditionaldistributionofsome y( ) t(for t > i),\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\ngraph(asinequation)mightbeveryineï¬ƒcient,withanevergrowingnumberof 10.6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\nconnectivitybuteï¬ƒcientparametrization,asillustratedinï¬gure.10.8\nis\nL=î˜\ntL( ) t(10.32)\nwhere\nL( ) t= log( âˆ’ Py( ) t= y( ) t| y( 1 ) t âˆ’, y( 2 ) t âˆ’, . . . , y( 1 )) .(10.33)\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\neï¬ƒcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\nsharethesameparameterswiththeotherstages.\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\neï¬ƒciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\n3 8 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\nshouldonlycontainedgesfrom{y( ) t k âˆ’, . . . ,y( 1 ) t âˆ’}toy( ) t,ratherthancontaining\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\ninputsshouldhaveaninï¬‚uenceonthenextelementofthesequence.RNNsare\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\nfromthedistantpastinawaythatisnotcapturedbytheeï¬€ectofy( ) iony( 1 ) t âˆ’.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\ndeï¬ningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\nvalueswiththecompletegraphstructureisshowninï¬gure.Thecomplete10.7\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\nmarginalizing themoutofthemodel.\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeï¬ƒcient\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\nrepresentationâ€”anarraycontainingaseparateentryforeachpossibleassignment\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\noccurring.Â If ycantakeon kdiï¬€erentvalues,thetabularrepresentationwould\nhave O( kÏ„)parameters.Bycomparison,duetoparametersharing,thenumberof",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "parametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\nlong-termrelationshipsbetweenvariableseï¬ƒciently,usingrecurrentapplications\nofthesamefunction fandsameparametersÎ¸ateachtimestep.Figure10.8\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "quantitybetweenthem.Avariable y( ) iinthedistantpastmayinï¬‚uenceavariable\ny( ) tviaitseï¬€ectonh.Thestructureofthisgraphshowsthatthemodelcanbe\neï¬ƒcientlyparametrized byusingthesameconditionalprobabilitydistributionsat\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\njointassignmentofallvariablescanbeevaluatedeï¬ƒciently.\nEvenwiththeeï¬ƒcientparametrization ofthegraphicalmodel,someoperations\nremaincomputationally challenging.Forexample,itisdiï¬ƒculttopredictmissing",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\nh i d d e n u n i t s .\n3 8 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nvaluesinthemiddleofthesequence.\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\nthat theparametersmaybediï¬ƒcult. o p t i m i z i ng\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\nthatthesameparameterscanbeusedfordiï¬€erenttimesteps.Equivalently,the\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "betweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\ndiï¬€erenttimesteps.Thiswouldalreadybemuchbetterthanusingadiï¬€erent\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\nextrapolatewhenfacedwithnewvaluesof. t\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\nsimplytosamplefromtheconditionaldistributionateachtimestep.Â However,\nthereisoneadditionalcomplication.Â The RNNmusthavesomemechanismfor\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "weinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) Ï„\nineachtrainingexample.\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\nsequenceendsorcontinuesateachtimestep.\nAnotherwaytodeterminethesequencelength Ï„istoaddanextraoutputto\nthemodelthatpredictstheinteger Ï„itself.Themodelcansampleavalueof Ï„\nandthensample Ï„stepsworthofdata.Thisapproachrequiresaddinganextra\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\ncaneitherconsistofthevalueof Ï„orcanconsistof Ï„ tâˆ’,thenumberofremaining\n3 9 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ntimesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat\nendabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis\nbasedonthedecomposition\nP(x( 1 ), . . . ,x( ) Ï„) = ()( P Ï„ Px( 1 ), . . . ,x( ) Ï„| Ï„ .)(10.34)\nThestrategyofpredicting Ï„directlyisusedforexamplebyGoodfellow e t a l .\n().2014d\n10.2.4ModelingSequencesConditionedonContextwithRNNs\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of\ncourse,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8\ninputsx( 1 ),x( 2 ), . . . ,x( ) Ï„.Ingeneral,RNNsallowtheextensionofthegraphical\nmodelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut\nalsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof\nfeedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;Î¸)\ncanbereinterpretedasamodelrepresentingaconditionaldistribution P(yÏ‰|)",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "withÏ‰=Î¸.Wecanextendsuchamodeltorepresentadistribution P(yx|)by\nusingthesame P(yÏ‰|)asbefore,butmakingÏ‰afunctionofx.Inthecaseof\nanRNN,thiscanbeachievedindiï¬€erentways.Wereviewherethemostcommon\nandobviouschoices.\nPreviously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor\nt=1 , . . . , Ï„asinput.Â Anotheroptionistotakeonlyasinglevectorxasinput.\nWhenxisaï¬xed-sizevector,wecansimplymakeitanextrainputoftheRNN\nthatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto\nanRNNare:",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "anRNNare:\n1.Â asanextrainputateachtimestep,or\n2.Â astheinitialstateh( 0 ),or\n3.Â both.\nTheï¬rstandmostcommonapproachisillustratedinï¬gure.Theinteraction10.9\nbetweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly\nintroducedweightmatrixRthatwasabsentfromthemodelofonlythesequence\nof yvalues.Â Thesameproductxî€¾Risaddedasadditionalinputtothehidden\nunitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue\n3 9 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nofxî€¾Rthatiseï¬€ectivelyanewbiasparameterusedforeachofthehiddenunits.\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\ntheparametersÎ¸ofthenon-conditional modelandturningthemintoÏ‰,where\nthebiasparameterswithinarenowafunctionoftheinput. Ï‰\no( t âˆ’ 1 )o( t âˆ’ 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\nx xy( ) . . .y( ) . . .\nR R R R R\nFigure10.9:AnRNNthatmapsaï¬xed-lengthvectorxintoadistributionoversequences\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) Ï„|x( 1 ), . . . ,x( ) Ï„)thatmakesa\nconditionalindependence assumptionthatthisdistributionfactorizesas\nî™\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\ntheoutputattime ttothehiddenunitattime t+1,asshowninï¬gure.The10.10",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\n3 9 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\no( t âˆ’ 1 )o( t âˆ’ 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\nU U U\nx( t âˆ’ 1 )x( t âˆ’ 1 )R\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\nFigure10.10:Â Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "ofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\nï¬gure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\ngivensequencesofxofthesamelength.TheRNNofï¬gureisonlyabletorepresent 10.3\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\nthevalues.x\n3 9 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\no( t âˆ’ 1 )o( t âˆ’ 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t âˆ’ 1 )L( t âˆ’ 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t âˆ’ 1 )y( t âˆ’ 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\nx( t âˆ’ 1 )x( t âˆ’ 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t âˆ’ 1 )g( t âˆ’ 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Figure10.11:Â Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\npoint t,theoutputunitso( ) tcanbeneï¬tfromarelevantsummaryofthepastinitsh( ) t\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\n10.3BidirectionalRNNs",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "10.3BidirectionalRNNs\nAlloftherecurrentnetworkswehaveconsidereduptonowhaveaâ€œcausalâ€struc-\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\nx( 1 ), . . . ,x( 1 ) t âˆ’,andthepresentinputx( ) t.Someofthemodelswehavediscussed\nalsoallowinformationfrompastyvaluestoaï¬€ectthecurrentstatewhenthey\nvaluesareavailable.\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\n3 9 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\ntasks,describedinthenextsection.\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\ne t a l .,).1999",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).1999\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\nsub-RNNthatmovesbackwardthroughtime.Â Thisallowstheoutputunitso( ) t",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "tocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\nï¬xed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\naconvolutionalnetwork,oraregularRNNwithaï¬xed-sizelook-aheadbuï¬€er).\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\nhavingRNNs,eachonegoinginoneofthefourdirections:Â up, down,left, f o u r\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "representationthatwouldcapturemostlylocalinformationbutcouldalsodepend\nonÂ long-range inputs,iftheÂ RNNÂ isableÂ tolearnÂ tocarryÂ thatÂ information.\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\ninteractions.\n3 9 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\ntures\nWehaveseeninï¬gurehowanRNNcanmapaninputsequencetoaï¬xed-size 10.5\nvector.Wehaveseeninï¬gurehowanRNNcanmapaï¬xed-sizevectortoa 10.9\nsequence.Â Wehaveseeninï¬gures,,andhowanRNNcan 10.310.410.1010.11\nmapaninputsequencetoanoutputsequenceofthesamelength.\nE nc ode r\nâ€¦\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\nD e c ode r\nâ€¦",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "D e c ode r\nâ€¦\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\nFigure10.12:Â Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\ngivenoutputsequence).Theï¬nalhiddenstateoftheencoderRNNisusedtocomputea",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "generallyï¬xed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\nsequenceandisgivenasinputtothedecoderRNN.\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\noutputsequencewhichisnotnecessarilyofthesamelength.Â This comesupin\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\n3 9 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\nnotofthesamelength(althoughtheirlengthsmightberelated).\nWeoftencalltheinputtotheRNNtheâ€œcontext.â€Wewanttoproducea\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\nThesimplestRNNarchitectureformappingavariable-length sequenceto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "anothervariable-length sequencewasï¬rstproposedby ()and Cho e t a l .2014a\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\ntectureandweretheï¬rsttoobtainstate-of-the-art translationusingthisapproach.\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\nthetranslations.Â Theseauthorsrespectivelycalledthisarchitecture, illustrated",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "inï¬gure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\nï¬nalhiddenstate.Â (2)adecoderorwriteroroutputRNNisconditionedon\nthatï¬xed-lengthvector(justlikeinï¬gure)togeneratetheoutputsequence 10.9\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "varyfromeachother,whilepreviousarchitectures constrained n x= n y= Ï„.Ina\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\ndecoderRNN.\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\nateachtimestep.Thesetwowayscanalsobecombined.\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\nasthedecoder.\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "sequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\nthanaï¬xed-sizevector.Additionally,theyintroducedanattentionmechanism\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\n3 9 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsequence.Seesectionformoredetails. 12.4.5.1\n10.5DeepRecurrentNetworks\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\nandassociatedtransformations:\n1.Â fromtheinputtothehiddenstate,\n2.Â fromtheprevioushiddenstatetothenexthiddenstate,and\n3.Â fromthehiddenstatetotheoutput.\nWiththeRNNarchitectureofï¬gure,eachofthesethreeblocksisassociated 10.3\nwithasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "ofthesecorrespondstoashallowtransformation.Â Byashallowtransformation,\nwemeanatransformationthatwouldberepresentedbyasinglelayerwithin\nadeepMLP.Typicallythisisatransformationrepresentedbyalearnedaï¬ƒne\ntransformationfollowedbyaï¬xednonlinearity.\nWoulditbeadvantageoustointroducedepthineachoftheseoperations?\nExperimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests\nso.Theexperimentalevidenceisinagreementwiththeideathatweneedenough",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "depthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(),\nElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs.\nGraves2013 e t a l .()weretheï¬rsttoshowasigniï¬cantbeneï¬tofdecomposing\nthestateofanRNNintomultiplelayersasinï¬gure(left).Wecanthink 10.13\nofthelowerlayersinthehierarchydepictedinï¬gureaasplayingarole 10.13\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\nthehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\nenumeratedabove,asillustratedinï¬gureb.Considerationsofrepresentational 10.13\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\nsobyaddingdepthmayhurtlearningbymakingoptimization diï¬ƒcult.Ingeneral,\nitiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof\nï¬gurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable\nintimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\nshortestpathbetweenvariablesinanytwodiï¬€erenttimesteps,comparedwiththe\nordinaryRNNofï¬gure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this\n3 9 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhy\nxz\n( a) ( b) ( c )xhy\nxhy\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\nhidden,hidden-to-hiddenandhidden-to-outputparts.Â Thismaylengthentheshortest\npathlinkingdiï¬€erenttimesteps.Thepath-lengtheningeï¬€ectcanbemitigatedby ( c )\nintroducingskipconnections.\n3 9 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\nillustratedinï¬gurec.10.13\n10.6RecursiveNeuralNetworks\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\nx( 4 )x( 4 )Voo\nU W U WUW\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\nbemappedtoaï¬xed-sizerepresentation(theoutputo),withaï¬xedsetofparameters",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "(theweightmatricesU,V,W).Theï¬gureillustratesasupervisedlearningcaseinwhich\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\nnetworks,withadiï¬€erentkindofcomputational graph,whichisstructuredasa\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\ngraphforarecursivenetworkisillustratedinï¬gure.Recursiveneural 10.14",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "2W e s u g g e s t t o n o t a b b re v i a t e â€œ re c u rs i v e n e u ra l n e t w o rk â€ a s â€œ R NNâ€ t o a v o i d c o n f u s i o n with\nâ€œ re c u rre n t n e u ra l n e t w o rk . â€\n4 0 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\ncomputervision( ,). Socher e t a l .2011b\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "ofthesamelength Ï„,thedepth(measuredasthenumberofcompositionsof\nnonlinearoperations)canbedrasticallyreducedfrom Ï„to O(log Ï„),whichmight\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\nlanguagesentences,thetreestructurefortherecursivenetworkcanbeï¬xedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\nparser( ,,).Â Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\nsuggestedby(). Bottou2011\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\nandassociatetheÂ inputsandtargetswithÂ individualnodesoftheÂ tree.The",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "computationperformedbyeachnodedoesnothavetobethetraditionalartiï¬cial\nneuroncomputation(aï¬ƒnetransformationofallinputsfollowedbyamonotone\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\nrepresentedbycontinuousvectors(embeddings).\n10.7TheChallengeofLong-TermDependencies",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "10.7TheChallengeofLong-TermDependencies\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\nwithgradientsnotexploding),thediï¬ƒcultywithlong-termdependenciesarises",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "fromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\n4 0 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nâˆ’ âˆ’ âˆ’ 6 0 4 0 2 0 0 2 0 4 0 6 0\nI nput c o o r di na t eâˆ’ 4âˆ’ 3âˆ’ 2âˆ’ 101234P r o j e c t i o n o f o utput0\n1\n2\n3\n4\n5\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "downtoasingledimension,plottedonthe y-axis.Â The x-axisisthecoordinateofthe\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\nhasbeencomposed.\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\nbehavior,asillustratedinï¬gure.10.15\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\nh( ) t= Wî€¾h( 1 ) t âˆ’(10.36)\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\nandlackinginputsx.AsÂ describedÂ insectionÂ ,Â thisrecurrencerelation 8.2.5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "essentiallydescribesthepowermethod.Itmaybesimpliï¬edto\nh( ) t=î€€\nWtî€î€¾h( 0 ), (10.37)\nandifadmitsaneigendecompositionoftheform W\nWQQ = Î›î€¾, (10.38)\n4 0 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwithorthogonal ,therecurrencemaybesimpliï¬edfurtherto Q\nh( ) t= Qî€¾Î›tQh( 0 ). (10.39)\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\nwilleventuallybediscarded.\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "multiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\nnetworkthathasadiï¬€erentweight w( ) tateachtimestep,thesituationisdiï¬€erent.\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenbyî‘\nt w( ) t.Suppose\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\ndesiredvariance vâˆ—wemaychoosetheindividualweightswithvariance v=nâˆš",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "vâˆ—.\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\nThevanishingandexplodinggradientproblemforRNNswasindependently\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "mustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\n1994).Speciï¬cally,wheneverthemodelisabletorepresentlongtermdependencies,\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\nthegradientofashortterminteraction.Â It doesnotmeanthatitisimpossible\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\nï¬‚uctuationsarisingfromshort-termdependencies.Inpractice,theexperiments",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "in ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydiï¬ƒcult,\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\nreaching0forsequencesofonlylength10or20.\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "approachesthathavebeenproposedtoreducethediï¬ƒcultyoflearninglong-\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\n4 0 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\noneofthemainchallengesindeeplearning.\n10.8EchoStateNetworks\nTherecurrentweightsmappingfromh( 1 ) t âˆ’toh( ) tandtheinputweightsmapping\nfromx( ) ttoh( ) taresomeofthemostdiï¬ƒcultparameterstolearninarecurrent\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\nJaeger2007b,)approachtoavoidingthisdiï¬ƒcultyistosettherecurrentweights",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "reservoircomputing(LukoÅ¡eviÄiusandJaeger2009,)todenotethefactthat\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediï¬€erent\naspectsofthehistoryofinputs.\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\nhistoryofinputsuptotime t)intoaï¬xed-lengthvector(therecurrentstateh( ) t),\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\nsimplelearningalgorithms(,). Jaeger2003\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "networkstate?Â Theanswerproposedinthereservoircomputingliteratureisto\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\nJ( ) t=âˆ‚ s( ) t\nâˆ‚ s( 1 ) t âˆ’.OfparticularimportanceisthespectralradiusofJ( ) t,deï¬nedto",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "bethemaximumoftheabsolutevaluesofitseigenvalues.\n4 0 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTounderstandtheeï¬€ectofthespectralradius,considerthesimplecaseof\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\naneigenvectorvwithcorrespondingeigenvalue Î».Considerwhathappensaswe\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "stepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\naperturbedversionofg.Ifwebeginwithg+ Î´v,thenafteronestep,wewill\nhaveJ(g+ Î´v).After nsteps,wewillhaveJn(g+ Î´v).Fromthiswecansee\nthatback-propagationstartingfromgandback-propagationstartingfromg+ Î´v\ndivergeby Î´Jnvafter nstepsofback-propagation.Ifvischosentobeaunit\neigenvectorofJwitheigenvalue Î»,thenmultiplicationbytheJacobiansimply\nscalesthediï¬€erenceateachstep.Thetwoexecutionsofback-propagationare",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "separatedbyadistanceof Î´ Î»||n.Whenvcorrespondstothelargestvalueof|| Î»,\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\nofsize. Î´\nWhen || Î» >1,thedeviationsize Î´ Î»||ngrowsexponentiallylarge.When || Î» <1,\nthedeviationsizebecomesexponentiallysmall.\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "manytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\nradius.Â Indeed,themostrecentworkonechostatenetworksadvocatesusinga\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\nwherethestateh( + 1 ) t= h( ) t î€¾W.\nWhenalinearmapWî€¾alwaysshrinkshasmeasuredbythe L2norm,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "wesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\nthepastwhenweuseaï¬nitelevelofprecision(suchas32bitintegers)tostore\nthestatevector.\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "duringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\n4 0 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\ncoeï¬ƒcients,Â whenwemultiplythematrixbythevector.Aneigenvaluewith\nmagnitudegreaterthanonecorrespondstomagniï¬cation (exponentialgrowth,if",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "appliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\nWithanonlinearmap,Â theJacobianisfreetochangeateachstep.The\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\ndiï¬€erencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\nbounded.NotethatÂ itispossibleÂ forback-propagation toÂ retainunbounded",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\nThestrategyofechostatenetworksissimplytoï¬xtheweightstohavesome\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\ndoesnotexplodeduetothestabilizingeï¬€ectofsaturatingnonlinearities liketanh.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\nwiththesparseinitialization schemedescribedinsection.8.4\n10.9LeakyUnitsandOtherStrategiesforMultiple\nTimeScales",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "TimeScales\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\natmultipletimescales,sothatsomepartsofthemodeloperateatï¬ne-grained\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\nscalesandtransferinformationfromthedistantpasttothepresentmoreeï¬ƒciently.\nVariousstrategiesforbuildingbothï¬neandcoarsetimescalesarepossible.These\nincludetheadditionofskipconnectionsacrosstime,â€œleakyunitsâ€thatintegrate",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "signalswithdiï¬€erenttimeconstants,andtheremovalofsomeoftheconnections\n4 0 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nusedtomodelï¬ne-grainedtimescales.\n10.9.1AddingSkipConnectionsthroughTime\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\ndiminishexponentiallyasafunctionofÏ„\ndratherthan Ï„.Sincethereareboth\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin Ï„.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\nlong-termdependencies mayberepresentedwellinthisway.\n10.9.2LeakyUnitsandaSpectrumofDiï¬€erentTimeScales\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\nWhenweaccumulatearunningaverage Âµ( ) tofsomevalue v( ) tbyapplyingthe\nupdate Âµ( ) tâ† Î± Âµ( 1 ) t âˆ’+(1âˆ’ Î±) v( ) tthe Î±parameterisanexampleofalinearself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "connectionfrom Âµ( 1 ) t âˆ’to Âµ( ) t.When Î±isnearone,therunningaverageremembers\ninformationaboutthepastforalongtime,andwhen Î±isnearzero,information\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\nunits.\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\nalwayslearntobeinï¬‚uencedbyavaluefrom dtimestepsearlier.Theuseofa\nlinearself-connectionwithaweightnearoneisadiï¬€erentwayofensuringthatthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\nthiseï¬€ecttobeadaptedmoresmoothlyandï¬‚exiblybyadjustingthereal-valued\nÎ±ratherthanbyadjustingtheinteger-valuedskiplength.\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\n(,). Jaeger e t a l .2007\n4 0 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\nunits.Â Onestrategyistomanuallyï¬xthemtovaluesthatremainconstant,for\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\nHavingsuchleakyunitsatdiï¬€erenttimescalesappearstohelpwithlong-term\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\n10.9.3RemovingConnections",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "10.9.3RemovingConnections\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\ninformationï¬‚owingmoreeasilythroughlongdistancesattheslowertimescales.\nThisideadiï¬€ersfromtheskipconnectionsthroughtimediscussedearlier\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\nwithlongerconnections.Unitsmodiï¬edinsuchawayareforcedtooperateona",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "longtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\nfocusontheirothershort-termconnections.\nTherearediï¬€erentwaysinwhichagroupofrecurrentunitscanbeforcedto\noperateatdiï¬€erenttimescales.Oneoptionistomaketherecurrentunitsleaky,\nbuttohavediï¬€erentgroupsofunitsassociatedwithdiï¬€erentï¬xedtimescales.\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "e t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\natdiï¬€erenttimes,withadiï¬€erentfrequencyfordiï¬€erentgroupsofunits.Thisis\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\nwellonanumberofbenchmarkdatasets.\n10.10TheLongShort-TermMemoryandOtherGated\nRNNs\nAsofthiswriting,themosteï¬€ectivesequencemodelsusedinpracticalapplications\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\nnetworksbasedonthe . gatedrecurrentunit",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "networksbasedonthe . gatedrecurrentunit\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\ntimethathavederivativesthatneithervanishnorexplode.LeakyunitsÂ did\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\n4 0 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nateachtimestep.\nÃ—\ni nput i nputÂ gate f or ge t Â  gate outputÂ gateoutput\ns t at es e l f - l oopÃ—\n+ Ã—\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetworkâ€œcell.â€Cellsareconnected\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\nAninputfeatureiscomputedwitharegularartiï¬cialneuronunit.Itsvaluecanbe\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\nbeshutoï¬€bytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\ninputunitcanhaveanysquashingnonlinearity.Â Thestateunitcanalsobeusedasan\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\nforaparticularfeatureorcategory)overalongduration.However,oncethat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "informationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\n4 0 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\niswhatgatedRNNsdo.\n10.10.1LSTM\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\ncanï¬‚owforlongdurationsisacorecontributionoftheinitiallongshort-term\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\nï¬xed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\nInthiscase,wemeanthatevenforanLSTMwithï¬xedparameters,thetimescale\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\ninmanyapplications,Â suchasunconstrainedhandwriting recognition(Graves\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\nparsing(Vinyals2014a e t a l .,).\nTheLSTMblockdiagramisillustratedinï¬gure.Thecorresponding 10.16\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wisenonlinearitytotheaï¬ƒnetransformationofinputsandrecurrentunits,LSTM\nrecurrentnetworkshaveâ€œLSTMcellsâ€thathaveaninternalrecurrence(aself-loop),\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\nsystemofgatingunitsthatcontrolstheï¬‚owofinformation. Themostimportant\ncomponentisthestateunit s( ) t\nithathasalinearself-loopsimilartotheleaky\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "associatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\ni(fortimestep t\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\nf( ) t\ni= Ïƒï£«\nï£­ bf\ni+î˜\njUf\ni , j x( ) t\nj+î˜\njWf\ni , j h( 1 ) t âˆ’\njï£¶\nï£¸ ,(10.40)\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\n4 1 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\nf( ) t\ni:\ns( ) t\ni= f( ) t\ni s( 1 ) t âˆ’\ni + g( ) t\ni Ïƒï£«\nï£­ b i+î˜\njU i , j x( ) t\nj+î˜\njW i , j h( 1 ) t âˆ’\njï£¶\nï£¸ ,(10.41)\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\niiscomputed\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\n0and1),butwithitsownparameters:\ng( ) t\ni= Ïƒï£«\nï£­ bg\ni+î˜",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "g( ) t\ni= Ïƒï£«\nï£­ bg\ni+î˜\njUg\ni , j x( ) t\nj+î˜\njWg\ni , j h( 1 ) t âˆ’\njï£¶\nï£¸ .(10.42)\nTheoutput h( ) t\nioftheLSTMcellcanalsobeshutoï¬€,viatheoutputgate q( ) t\ni,\nwhichalsousesasigmoidunitforgating:\nh( ) t\ni= tanhî€\ns( ) t\niî€‘\nq( ) t\ni (10.43)\nq( ) t\ni= Ïƒï£«\nï£­ bo\ni+î˜\njUo\ni , j x( ) t\nj+î˜\njWo\ni , j h( 1 ) t âˆ’\njï£¶\nï£¸ (10.44)\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\ni",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "i\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\ninï¬gure.Thiswouldrequirethreeadditionalparameters. 10.16\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\nthanthesimplerecurrentarchitectures,ï¬rstonartiï¬cialdatasetsdesignedfor\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "Graves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\nhavebeenstudiedandusedandarediscussednext.\n10.10.2OtherGatedRNNs\nWhichpiecesÂ oftheÂ LSTMarchitecture areÂ actually necessary?Whatother\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\ncontrolthetimescaleandforgettingbehaviorofdiï¬€erentunits?\n4 1 1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\ndiï¬€erencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\narethefollowing:\nh( ) t\ni= u( 1 ) t âˆ’\ni h( 1 ) t âˆ’\ni+(1âˆ’ u( 1 ) t âˆ’\ni) Ïƒï£«\nï£­ b i+î˜",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "i h( 1 ) t âˆ’\ni+(1âˆ’ u( 1 ) t âˆ’\ni) Ïƒï£«\nï£­ b i+î˜\njU i , j x( 1 ) t âˆ’\nj +î˜\njW i , j r( 1 ) t âˆ’\nj h( 1 ) t âˆ’\njï£¶\nï£¸ ,\n(10.45)\nwhereustandsforâ€œupdateâ€gateandrforâ€œresetâ€gate.Theirvalueisdeï¬nedas\nusual:\nu( ) t\ni= Ïƒï£«\nï£­ bu\ni+î˜\njUu\ni , j x( ) t\nj+î˜\njWu\ni , j h( ) t\njï£¶\nï£¸ (10.46)\nand\nr( ) t\ni= Ïƒï£«\nï£­ br\ni+î˜\njUr\ni , j x( ) t\nj+î˜\njWr\ni , j h( ) t\njï£¶\nï£¸ .(10.47)\nTheresetandupdatesgatescanindividuallyâ€œignoreâ€partsofthestatevector.\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\nignoreit(attheotherextreme)byreplacingitbythenewâ€œtargetstateâ€value\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\nadditionalnonlineareï¬€ectintherelationshipbetweenpaststateandfuturestate.\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\nacrossawiderangeoftasks(,; Greï¬€ e t a l .2015Jozefowicz2015Greï¬€ e t a l .,).\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "advocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\nexploredarchitecturalvariants.\n4 1 2",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n10.11OptimizationforLong-TermDependencies\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\nderivativesmayvanishatthesametimethatï¬rstderivativesvanish.Second-order\noptimization algorithmsmayroughlybeunderstoodasdividingtheï¬rstderivative",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "bythesecondderivative(inhigherdimension,multiplyingthegradientbythe\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheï¬rst\nderivative,thentheratioofï¬rstandsecondderivativesmayremainrelatively\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "methodssuchasNesterovmomentumwithcarefulinitialization couldachieve\nsimilarresults.SeeSutskever2012()formoredetail.Â Bothoftheseapproaches\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\npowerfuloptimization algorithm.\n10.11.1ClippingGradients\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "byarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\neitherverylargeorverysmallinmagnitude.Thisisillustratedinï¬gureand8.3\nï¬gure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\nparameters)hasaâ€œlandscapeâ€Â inwhichoneï¬ndsâ€œcliï¬€sâ€:wideandratherï¬‚at\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\nformingakindofcliï¬€.\nThediï¬ƒcultythatarisesisthatwhentheparametergradientisverylarge,a\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "regionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\ncorrespondstothesteepestdescentwithinaninï¬nitesimalregionsurroundingthe\ncurrentparameters.Outsideofthisinï¬nitesimalregion,thecostfunctionmay\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\n4 1 3",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\nlandscapeonthenextstep.\nî·\nî¢îŠî·î€» î¢\nî€¨î€©î— î© î´ î¨ î¯ îµ î´ î€  î£ î¬ î© î° î° î© î® î§\nî·\nî¢îŠî·î€» î¢\nî€¨î€©î— î© î´ î¨ î€  î£ î¬ î© î° î° î© î® î§\nFigure10.17:Exampleoftheeï¬€ectofgradientclippinginarecurrentnetworkwith",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "twoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\nreasonablyinthevicinityofextremelysteepcliï¬€s.Thesesteepcliï¬€scommonlyoccur\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\nThecliï¬€isexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "fromthecliï¬€face.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\nreactiontothecliï¬€.Whileitdoesascendthecliï¬€face,thestepsizeisrestrictedsothat\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\npermissionfromPascanu2013 e t a l .().\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\nclippingthegradient.Therearediï¬€erentinstancesofthisidea(Mikolov2012,;",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "Pascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\nupdate:\nif||||g > v (10.48)\ngâ†g v\n||||g(10.49)\n4 1 4",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\ngradientofalltheparameters(includingdiï¬€erentgroupsofparameters,suchas\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "normclipping,theparameterupdatevectornormisnowbounded.Thisbounded\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\ngradientisnumerically InforNan(consideredinï¬niteornot-a-number),then\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\nnumericallyunstableconï¬guration. Clippingthegradientnormper-minibatchwill",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "notchangethedirectionofthegradientforanindividualminibatch.However,\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\ntotheï¬naldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "conjecturethatallthesemethodsbehavesimilarly.\n10.11.2RegularizingtoEncourageInformationFlow\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "loopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\ntoregularizeorconstraintheparameterssoastoencourageâ€œinformationï¬‚ow.â€\nInparticular,wewouldlikethegradientvectorâˆ‡h( ) t Lbeingback-propagatedto\n4 1 5",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\nendofthesequence.Formally,wewant\n(âˆ‡h( ) t L)âˆ‚h( ) t\nâˆ‚h( 1 ) t âˆ’(10.50)\ntobeaslargeas\nâˆ‡h( ) t L. (10.51)\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\nâ„¦ =î˜\ntï£«\nï£­î€Œî€Œî€Œ|âˆ‡(h( ) t L)âˆ‚ h( ) t\nâˆ‚ h( 1 ) t âˆ’î€Œî€Œî€Œ|\n||âˆ‡h( ) t L||âˆ’1ï£¶\nï£¸2\n. (10.52)\nComputingthegradientofthisregularizermayappeardiï¬ƒcult,butPascanu",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\nvectorsâˆ‡h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\nthedependenciesthatanRNNcanlearn.Â BecauseitkeepstheRNNdynamics\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\nAkeyweaknessofthisapproachisthatitisnotaseï¬€ectiveastheLSTMfor\ntaskswheredataisabundant,suchaslanguagemodeling.\n10.12ExplicitMemory\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\ntherearediï¬€erentkindsofknowledge.Someknowledgecanbeimplicit,sub-\nconscious,anddiï¬ƒculttoverbalizeâ€”suchashowtowalk,orhowadoglooks",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\nstraightforwardtoputintowordsâ€”everydaycommonsense knowledge,likeâ€œacat\nisakindofanimal,â€orveryspeciï¬cfactsthatyouneedtoknowtoaccomplish\nyourcurrentgoals,likeâ€œthemeetingwiththesalesteamisat3:00PMinroom\n141.â€\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\nmemorizefacts.Â Stochasticgradientdescentrequiresmanypresentationsofthe\n4 1 6",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nT ask Â  ne t w or k ,\nc ontrol l i ngÂ th e Â  m e m o r yMe m or y Â  c e l l s\nW r i t i ng\nm e c hani s mR e adi ng\nm e c hani s m\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\ndistinguishtheâ€œrepresentationâ€partofthemodel(theâ€œtasknetwork,â€herearecurrent\nnetinthebottom)fromtheâ€œmemoryâ€partofthemodel(thesetofcells),whichcan",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "storefacts.Thetasknetworklearnstoâ€œcontrolâ€thememory,decidingwheretoreadfrom\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\n4 1 7",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\ninformationthatÂ arerelevanttoÂ achievingÂ some goal.SuchexplicitÂ memory\ncomponentswouldallowoursystemsnotonlytorapidlyandâ€œintentionallyâ€store",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "andretrievespeciï¬cfactsbutalsotosequentiallyreasonwiththem.Theneed\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\nresponsestotheinput(,). Hinton1990\nToresolvethisdiï¬ƒculty,Weston2014 e t a l .()introducedmemorynetworks\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\nLSTMsandGRUs.Thediï¬€erenceisthatthenetworkoutputsaninternalstate\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\ndigitalcomputerreadfromorwritetoaspeciï¬caddress.",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "digitalcomputerreadfromorwritetoaspeciï¬caddress.\nItisdiï¬ƒculttooptimizefunctionsthatproduceexact,integeraddresses.To\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\nmodifymultiplecellsbydiï¬€erentamounts.Thecoeï¬ƒcientsfortheseoperations\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "thefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\ndescent.Thegradientonthesecoeï¬ƒcientsindicateswhethereachofthemshould\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\nmemoryaddressesreceivingalargecoeï¬ƒcient.\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\n4 1 8",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\ncostofaccessingamemorycell.Â Wepaythecomputational costofproducinga\ncoeï¬ƒcientformanycells,butweexpectthesecoeï¬ƒcientstoclusteraroundasmall\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\noï¬€setsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "theyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\nreadinstructionassaying,â€œRetrievethelyricsofthesongthathasthechorusâ€˜We\nallliveinayellowsubmarine.â€™â€Content-basedaddressingismoreusefulwhenwe",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "maketheobjectstoberetrievedlargeâ€”ifeveryletterofthesongwasstoredina\nseparatememorycell,wewouldnotbeabletoï¬ndthemthisway.Bycomparison,\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\nWecanthinkofalocation-basedreadinstructionassayingâ€œRetrievethelyricsof\nthesonginslot347.â€Location-basedaddressingcanoftenbeaperfectlysensible\nmechanismevenwhenthememorycellsaresmall.\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "theinformationitcontainscanbepropagatedforwardintimeandthegradients\npropagatedbackwardintimewithouteithervanishingorexploding.\nTheexplicitmemoryapproachisillustratedinï¬gure,whereweseethat 10.18\naâ€œtaskneuralnetworkâ€Â iscoupledwithamemory.Althoughthattaskneural\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\nThetasknetworkcanchoosetoreadfromorwritetospeciï¬cmemoryaddresses.\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "RNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\nforverylongdurations.\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\ncells,wecaninterpretthememoryaddressingcoeï¬ƒcientsasprobabilities and\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "insection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\ndecisions.\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\n4 1 9",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\nmechanismÂ forchoosingÂ anaddressÂ isinÂ itsformÂ identicalÂ totheattention\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\ntranslation( ,)anddiscussedinsection.Â Theidea Bahdanau e t a l .2015 12.4.5.1\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "thecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\nattentioncanmovetoacompletelydiï¬€erentplace,comparedtothepreviousstep.\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\ntasks.\n4 2 0",
    "metadata": {
      "source": "[15]part-2-chapter-10.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 5\nMac h i n e L e ar n i n g B asics\nDeeplearningisaspeciï¬ckindofmachinelearning.Inordertounderstand\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\nonÂ traditional machinelearningÂ techniquesÂ thathavestronglyÂ inï¬‚uenced the\ndevelopmentofdeeplearningalgorithms.\nWebeginwithadeï¬nitionofwhatalearningalgorithmis,andpresentan\nexample:thelinearregressionalgorithm.Â W ethenproceedtodescribehowthe\nchallengeofï¬ttingthetrainingdatadiï¬€ersfromthechallengeofï¬ndingpatterns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\nonprovingconï¬denceintervalsaroundthesefunctions;wethereforepresentthe\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\nexamplesofsimplelearningalgorithmsfromeachcategory.Â Mostdeeplearning\nalgorithmsareÂ basedonanÂ optimization algorithmcalledÂ stochasticgradient\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\n98",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\novercometheseobstacles.\n5.1LearningAlgorithms\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\nwhatdowemeanbylearning?Mitchell1997()providesthedeï¬nitionâ€œAcomputer",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\nimproveswithexperienceE.â€Onecanimagineaverywidevarietyofexperiences\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\nbooktoprovideaformaldeï¬nitionofwhatmaybeusedforeachoftheseentities.\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\ndiï¬€erentkindsoftasks,performance measuresandexperiencesthatcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "toconstructmachinelearningalgorithms.\n5.1.1TheTask, T\nMachinelearningallowsustotackletasksthataretoodiï¬ƒculttosolvewith\nï¬xedprogramswrittenanddesignedbyhumanbeings.Fromascientiï¬cand\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\nprinciplesthatunderlieintelligence.\nInthisrelativelyformaldeï¬nitionofthewordâ€œtask,â€theprocessoflearning\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\naprogramthatspeciï¬eshowtowalkmanually.\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vectorxâˆˆ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\n9 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\ncommonmachinelearningtasksincludethefollowing:\nâ€¢Classiï¬cation:Inthistypeoftask,thecomputerprogramisaskedtospecify\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\nalgorithmisusuallyaskedtoproduceafunctionf: Rnâ†’{1,...,k}.When\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\nidentiï¬edbynumericcodey.Thereareothervariantsoftheclassiï¬cation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "task,forexample,wherefoutputsaprobabilitydistributionoverclasses.\nAnexampleofaclassiï¬cationtaskisobjectrecognition,wheretheinput\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\ndiï¬€erentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "deeplearning( ,; ,).Object Krizhevskyetal.2012Ioï¬€eandSzegedy2015\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\ntheirusers.\nâ€¢Classiï¬cationwithmissinginputs:Classiï¬cationbecomesmorechal-\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiï¬cation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "task,thelearningalgorithmonlyhastodeï¬neafunctionmapping single\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\nbemissing,ratherthanprovidingasingleclassiï¬cationfunction,thelearning\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\nfyingxwithadiï¬€erentsubsetofitsinputsmissing.Thiskindofsituation\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\nareexpensiveorinvasive.Onewaytoeï¬ƒcientlydeï¬nesuchalargeset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "offunctionsistolearnaprobabilitydistributionoveralloftherelevant\nvariables,thensolvetheclassiï¬cationtaskbymarginalizing outthemissing\nvariables.Withninputvariables,wecannowobtainall2ndiï¬€erentclassiï¬-\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "sectioncanalsobegeneralizedtoworkwithmissinginputs;classiï¬cation\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\n1 0 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nâ€¢Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\nisaskedtooutputafunctionf: Rnâ†’ R.Thistypeoftaskissimilarto\nclassiï¬cation,exceptthattheformatofoutputisdiï¬€erent.Anexampleof\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\nalgorithmictrading.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "algorithmictrading.\nâ€¢Transcription:Inthistypeoftask,themachinelearningsystemisasked\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\nrecognition,thecomputerprogramisshownaphotographcontainingan\nimageoftextandisaskedtoreturnthistextintheformofasequence\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\n2012b).\nâ€¢Machinetranslation:Inamachinetranslationtask,theinputalready\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "mustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\nâ€¢Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\nimportantrelationshipsbetweenthediï¬€erentelements.Thisisabroad",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "category,andsubsumesthetranscriptionandtranslationtasksdescribed\nabove,butalsomanyothertasks.Oneexampleisparsingâ€”mappinga\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\ntask.Anotherexampleispixel-wisesegmentationofimages,Â wherethe\ncomputerprogramassignseverypixelinanimagetoaspeciï¬ccategory.For\n1 0 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "calledstructuredoutputtasksbecausetheprogrammustoutputseveral\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\nanimagecaptioningprogrammustformavalidsentence.\nâ€¢Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\nthroughasetofeventsorobjects,andï¬‚agssomeofthemasbeingunusual\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "information,thethiefâ€™spurchaseswilloftencomefromadiï¬€erentprobability\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\nsurveyofanomalydetectionmethods.\nâ€¢Synthesisandsampling:Inthistypeoftask,themachinelearningal-\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\ntrainingdata.Â Synthesisandsamplingviamachinelearningcanbeuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "formediaapplicationswhereitcanbeexpensiveorboringforanartistto\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeciï¬c\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "containingaspokenversionofthatsentence.Â Thisisakindofstructured\noutputtask,butwiththeaddedqualiï¬cationthatthereisnosinglecorrect\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\nâ€¢Imputationofmissingvalues:Inthistypeoftask,themachinelearning\nalgorithmisgivenanewexamplexâˆˆ Rn,butwithsomeentriesx iofx\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\nentries.\n1 0 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nâ€¢Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\ninputacorruptedexampleËœxâˆˆ Rnobtainedbyanunknowncorruptionprocess\nfromacleanexamplexâˆˆ Rn.Thelearnermustpredictthecleanexample\nxfromitscorruptedversionËœx,ormoregenerallypredicttheconditional\nprobabilitydistributionp(x|Ëœx).\nâ€¢Densityestimationorprobabilitymassfunctionestimation:In\nthedensityestimationproblem,themachinelearningalgorithmisasked\ntolearnafunctionpmodel: Rnâ†’ R,wherepmodel(x)canbeinterpreted",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "asaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistribution.Densityestimationallowsustoexplicitlycapture\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\navaluex iismissingandalloftheothervalues,denotedx âˆ’ i,aregiven,\nthenweknowthedistributionoveritisgivenbyp(x i|x âˆ’ i).Inpractice,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\nintractable.\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\ndo,nottodeï¬nearigidtaxonomyoftasks.\n5.1.2ThePerformanceMeasure, P\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "speciï¬ctothetaskbeingcarriedoutbythesystem. T\nFortaskssuchasclassiï¬cation,classiï¬cationwithmissinginputs,andtran-\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\n1 0 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\nifitiscorrectlyclassiï¬edand1ifitisnot.Fortaskssuchasdensityestimation,\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\nloss.Instead,wemustuseadiï¬€erentperformancemetricthatgivesthemodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "acontinuous-valuedscoreforeachexample.Themostcommonapproachisto\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\nlearningsystem.\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "butitisoftendiï¬ƒculttochooseaperformancemeasurethatcorrespondswellto\nthedesiredbehaviorofthesystem.\nInsomecases,thisisbecauseitisdiï¬ƒculttodecidewhatshouldbemeasured.\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\nofthesystemattranscribingentiresequences,orshouldweuseamoreï¬ne-grained\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\naspeciï¬cpointinspaceinmanysuchmodelsisintractable.Inthesecases,one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\nordesignagoodapproximationtothedesiredcriterion.\n5.1.3TheExperience, E\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\nlearningprocess.\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\n1 0 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndeï¬nedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\ndiï¬€erentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "alsorecordswhichspecieseachplantbelongedto.Threediï¬€erentspeciesare\nrepresentedinthedataset.\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "performotherroles,likeclustering,whichconsistsofdividingthedatasetinto\nclustersofsimilarexamples.\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\ndiï¬€erentspeciesbasedontheirmeasurements.\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\nUnsupervisedlearningandsupervisedlearningarenotformallydeï¬nedterms.\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\nthatforavector xâˆˆ Rn,thejointdistributioncanbedecomposedas\np() = xnî™\ni=1p(x i|x1,...,x i âˆ’1). (5.1)\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "modelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\n1 0 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\nunsupervisedÂ learningtechnologiestoÂ learnÂ thejointdistributionp( x,y)and\ninferring\npy(| x) =p,y( x)î\nyî€°p,y( xî€°). (5.2)\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiï¬cation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "andstructuredoutputproblemsassupervisedlearning.Densityestimationin\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "withdeepmodels,seeKotzias 2015etal.().\nSomemachinelearningalgorithmsdonotjustexperienceaï¬xeddataset.For\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.Â Such\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\nwhichareinturncollectionsoffeatures.\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\nmatrixisamatrixcontainingadiï¬€erentexampleineachrow.Eachcolumnofthe\nmatrixcorrespondstoadiï¬€erentfeature.Forinstance,theIrisdatasetcontains\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thedatasetwithadesignmatrixXâˆˆ R1504 Ã—,whereX i ,1isthesepallengthof\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\nwithdiï¬€erentwidthsandheights,thendiï¬€erentphotographswillcontaindiï¬€erent",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "numbersofpixels,sonotallofthephotographs maybedescribedwiththesame\nlengthofvector.Sectionandchapterdescribehowtohandlediï¬€erent 9.7 10\n1 0 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\nx() iandx() jhavethesamesize.\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "appearsineachofthephotos.Wemightdothiswithanumericcode,with0\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Justasthereisnoformaldeï¬nitionofsupervisedandunsupervisedlearning,\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\n5.1.4Example:LinearRegression\nOurdeï¬nitionofamachinelearningalgorithmasanalgorithmthatiscapable\nofimprovingacomputerprogramâ€™sperformanceatsometaskviaexperienceis\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "simplemachinelearningalgorithm:linearregression.Wewillreturntothis\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\nunderstanditsbehavior.\nAsthenameimplies,linearregressionsolvesaregressionproblem.Â Inother\nwords,thegoalistobuildasystemthatcantakeavectorxâˆˆ Rnasinputand\npredictthevalueofascalaryâˆˆ Rasitsoutput.Inthecaseoflinearregression,\ntheoutputisalinearfunctionoftheinput.LetË†ybethevaluethatourmodel\npredictsshouldtakeon.Wedeï¬netheoutputtobe y\nË†y= wî€¾x (5.3)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "Ë†y= wî€¾x (5.3)\nwherewâˆˆ Rnisavectorof .parameters\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\nthecoeï¬ƒcientthatwemultiplybyfeaturex ibeforesummingupthecontributions\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\neachfeatureaï¬€ectstheprediction.Â If afeaturex ireceivesapositiveweightw i,\n1 0 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction Ë†y.\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\ndecreasesthevalueofourprediction.Ifafeatureâ€™sweightislargeinmagnitude,\nthenithasalargeeï¬€ectontheprediction.Ifafeatureâ€™sweightiszero,ithasno\neï¬€ectontheprediction.\nWethushaveadeï¬nitionofourtaskT:Â topredictyfromxbyoutputting\nË†y= wî€¾x.Nextweneedadeï¬nitionofourperformancemeasure,.P",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "Supposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\ntargetsasy()test.\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\nsquarederrorofthemodelonthetestset.IfË†y()testgivesthepredictionsofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "modelonthetestset,thenthemeansquarederrorisgivenby\nMSEtest=1\nmî˜\ni(Ë†y()testâˆ’y()test)2\ni. (5.4)\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when Ë†y()test=y()test.\nWecanalsoseethat\nMSEtest=1\nm||Ë†y()testâˆ’y()test||2\n2, (5.5)\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\nandthetargetsincreases.\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "isallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\nminimizethemeansquarederroronthetrainingset,MSEtrain.\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\nâˆ‡ wMSEtrain= 0 (5.6)\nâ‡’âˆ‡ w1\nm||Ë†y()trainâˆ’y()train||2\n2= 0 (5.7)\nâ‡’1\nmâˆ‡ w||X()trainwyâˆ’()train||2\n2= 0 (5.8)\n1 0 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nâˆ’ âˆ’ 1 0 . 0 5 0 0 0 5 1 0 . . . .\nx1âˆ’ 3âˆ’ 2âˆ’ 10123yL i n ea r r eg r es s i o n ex a m p l e\n0 5 1 0 1 5 . . .\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "tosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\nâ‡’âˆ‡ wî€\nX()trainwyâˆ’()trainî€‘î€¾î€\nX()trainwyâˆ’()trainî€‘\n= 0(5.9)\nâ‡’âˆ‡ wî€\nwî€¾X()train î€¾X()trainwwâˆ’2î€¾X()train î€¾y()train+y()train î€¾y()trainî€‘\n= 0\n(5.10)\nâ‡’2X()train î€¾X()trainwXâˆ’2()train î€¾y()train= 0(5.11)\nâ‡’w=î€\nX()train î€¾X()trainî€‘âˆ’1\nX()train î€¾y()train(5.12)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "X()train î€¾X()trainî€‘âˆ’1\nX()train î€¾y()train(5.12)\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\nseeï¬gure.5.1\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\naslightlymoresophisticatedmodelwithoneadditionalparameterâ€”an intercept\nterm.Inthismodelb\nË†y= wî€¾x+b (5.13)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "term.Inthismodelb\nË†y= wî€¾x+b (5.13)\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\nmappingfromfeaturestopredictionsisnowanaï¬ƒnefunction.Thisextensionto\naï¬ƒnefunctionsmeansthattheplotofthemodelâ€™spredictionsstilllookslikea\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\n1 0 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\nplaystheroleofthebiasparameter.Wewillfrequentlyusethetermâ€œlinearâ€when\nreferringtoaï¬ƒnefunctionsthroughoutthisbook.\nTheintercepttermbisoftencalledthebiasparameteroftheaï¬ƒnetransfor-\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "isdiï¬€erentfromtheideaofastatisticalbias,inwhichastatisticalestimation\nalgorithmâ€™sexpectedestimateofaquantityisnotequaltothetruequantity.\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\nlearningalgorithms.\n5.2Capacity,Overï¬ttingandUnderï¬tting",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,Overï¬ttingandUnderï¬tting\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\npreviouslyunseeninputsâ€”notjustthoseonwhichourmodelwastrained.Â The\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "anoptimization problem.Whatseparatesmachinelearningfromoptimization is\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\nwell.Â Thegeneralization errorisdeï¬nedastheexpectedvalueoftheerrorona\nnewinput.Heretheexpectationistakenacrossdiï¬€erentpossibleinputs,drawn\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "fromthetrainingset.\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\ntrainingerror,\n1\nm()train||X()trainwyâˆ’()train||2\n2, (5.14)\nbutweactuallycareaboutthetesterror,1\nm()test||X()testwyâˆ’()test||2\n2.\nHowcanweaï¬€ectperformanceonthetestsetwhenwegettoobserveonlythe\n1 1 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntrainingset?Theï¬eldofstatisticallearningtheoryprovidessomeanswers.If\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\ndo.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\nsetarecollected,thenwecanmakesomeprogress.\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\ncalledthedatageneratingprocess.Wetypicallymakeasetofassumptions\nknowncollectivelyasthei.i.d.Â assumptions.Â Theseassumptionsarethatthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "examplesineachdatasetareindependentfromeachother,andthatthetrain\nsetandtestsetareidenticallydistributed,drawnfromthesameprobability\ndistributionaseachother.Â Thisassumptionallowsustodescribethedatagen-\neratingprocesswithaprobabilitydistributionoverasingleexample.Thesame\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\nWecallthatsharedunderlyingdistributionthedatageneratingdistribution,\ndenotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "mathematically studytherelationshipbetweentrainingerrorandtesterror.\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\nexpectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution\np(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\nset.Forsomeï¬xedvaluew,theexpectedtrainingseterrorisexactlythesameas\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "datasetsamplingprocess.Theonlydiï¬€erencebetweenthetwoconditionsisthe\nnameweassigntothedatasetwesample.\nOfcourse,Â whenÂ weuseamachinelearningÂ algorithm,Â w edonotï¬xthe\nparametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset,\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\ntestset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\ntheexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine\nlearningalgorithmwillperformareitsabilityto:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmwillperformareitsabilityto:\n1.Â Makethetrainingerrorsmall.\n2.Â Makethegapbetweentrainingandtesterrorsmall.\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\nunderï¬ttingandoverï¬tting.Underï¬ttingoccurswhenthemodelisnotableto\nobtainasuï¬ƒcientlylowerrorvalueonthetrainingset.Overï¬ttingoccurswhen\nthegapbetweenthetrainingerrorandtesterroristoolarge.\nWecancontrolwhetheramodelismorelikelytooverï¬torunderï¬tbyaltering",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "itscapacity.Informally,amodelâ€™scapacityisitsabilitytoï¬tawidevarietyof\n1 1 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nfunctions.Modelswithlowcapacitymaystruggletoï¬tthetrainingset.Models\nwithhighcapacitycanoverï¬tbymemorizingpropertiesofthetrainingsetthatdo\nnotservethemwellonthetestset.\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\nhypothesisspace.Doingsoincreasesthemodelâ€™scapacity.\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\narealreadyfamiliar,withprediction\nË†ybwx. = + (5.15)\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\ncanlearnamodelthatisquadraticasafunctionof:x\nË†ybw = +1xw+2x2. (5.16)\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "stillalinearfunctionoftheparameters,sowecanstillusethenormalequations\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\nË†yb= +9î˜\ni=1w ixi. (5.17)\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\namountoftrainingdatatheyareprovidedwith.Modelswithinsuï¬ƒcientcapacity\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "tasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\nmayoverï¬t.\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\nanddegree-9predictorattemptingtoï¬taproblemwherethetrueunderlying\nfunctionisquadratic.Â Thelinearfunctionisunabletocapturethecurvaturein\nthetrueunderlyingproblem,soitunderï¬ts.Thedegree-9predictoriscapableof\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginï¬nitely\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\n1 1 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\nasolutionthatgeneralizeswellwhensomanywildlydiï¬€erentsolutionsexist.In\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\nthetasksoitgeneralizeswelltonewdata.\nî¸î€°î¹î•î® î¤ î¥ î² î¦ î© î´ î´ î© î® î§\nî¸î€°î¹îî° î° î² î¯ î° î² î© î¡ î´ î¥ î€  î£ î¡ î° î¡ î£ î© î´ î¹\nî¸î€°î¹î î¶ î¥ î² î¦ î© î´ î´ î© î® î§\nFigure5.2:Weï¬tthreemodelstothisexampletrainingset.Thetrainingdatawas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "generatedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\nbyevaluatingaquadraticfunction.Â  ( L e f t )Alinearfunctionï¬ttothedatasuï¬€ersfrom\nunderï¬ttingâ€”itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\nquadraticfunctionï¬ttothedatageneralizeswelltounseenpoints.Itdoesnotsuï¬€erfrom\nasigniï¬cantamountofoverï¬ttingorunderï¬tting.Apolynomialofdegree9ï¬tto ( R i g h t )\nthedatasuï¬€ersfromoverï¬tting.HereweusedtheMoore-Penrosepseudoinversetosolve",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "theunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\nfunctiondecreasesinthisarea.\nSofarwehavedescribedonlyonewayofchangingamodelâ€™scapacity:by\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "parametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\namodelâ€™scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\nmodelspeciï¬eswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\ntherepresentationalcapacityofthemodel.Inmanycases,ï¬ndingthebest\nfunctionwithinthisfamilyisaverydiï¬ƒcultoptimization problem.Inpractice,\nthelearningalgorithmdoesnotactuallyï¬ndthebestfunction,butmerelyone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "thatsigniï¬cantlyreducesthetrainingerror.Theseadditionallimitations,suchas\n1 1 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithmâ€™s\neï¬€ectivecapacitymaybelessthantherepresentationalcapacityofthemodel\nfamily.\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\nmodelsarereï¬nementsofthoughtdatingbacktophilosophersatleastasearly\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\nmostwidelyknownasOccamâ€™srazor(c.1287-1347).Thisprinciplestatesthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "amongcompetinghypothesesthatexplainknownobservationsequallywell,one\nshouldchoosetheâ€œsimplestâ€one.Thisideawasformalizedandmademoreprecise\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiï¬er.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "VCdimensionisdeï¬nedasbeingthelargestpossiblevalueofmforwhichthere\nexistsatrainingsetofmdiï¬€erentxpointsthattheclassiï¬ercanlabelarbitrarily.\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\nintellectualjustiï¬cationthatmachinelearningalgorithmscanwork,buttheyare\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\ndiï¬ƒculttodeterminethecapacityofdeeplearningalgorithms.Â Theproblemof\ndeterminingthecapacityofadeeplearningmodelisespeciallydiï¬ƒcultbecausethe\neï¬€ectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "wehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\nproblemsinvolvedindeeplearning.\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\nsuï¬ƒcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "generalization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\nillustratedinï¬gure.5.3\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\n1 1 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0 O pti m a l C a pa c i t y\nC a pa c i t yE r r o rU nde r ï¬tti ng z o ne O v e r ï¬tti ng z o ne\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\nG e n e r a l i z a t i o n e r r o r\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\nbehavediï¬€erently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\narebothhigh.Thisistheunderï¬ttingregime.Asweincreasecapacity,trainingerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverï¬tting\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\nbyaparametervectorwhosesizeisï¬niteandï¬xedbeforeanydataisobserved.\nNon-parametric modelshavenosuchlimitation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "Non-parametric modelshavenosuchlimitation.\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\nwhichhasaï¬xed-lengthvectorofweights,thenearestneighborregressionmodel",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "simplystorestheXandyfromthetrainingset.Â Whenaskedtoclassifyatest\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\nassociatedregressiontarget.Inotherwords,Ë†y=y iwherei=argmin||X i ,:âˆ’||x2\n2.\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiï¬€erent\noutputs)onanyregressiondataset.\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\n1 1 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\npolynomialexpansionoftheinput.\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\nthatgeneratesthedata.Â Evensuchamodelwillstillincursomeerroronmany\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "ofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\ndistributioniscalledthe p,y(x)Bayeserror.\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thebestpossibleerrorisachieved.Anyï¬xedparametricmodelwithlessthan\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\nï¬gureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\nexamples.\n5.2.1TheNoFreeLunchTheorem\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "aï¬nitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\nisnotlogicallyvalid.Â Tologicallyinferaruledescribingeverymemberofaset,\nonemusthaveinformationabouteverymemberofthatset.\nInpart,machinelearningavoidsthisproblembyoï¬€eringonlyprobabilisticrules,\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning.Â Machine\nlearningpromisestoï¬ndrulesthatareprobably most correctaboutmembersof\nthesettheyconcern.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "thesettheyconcern.\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\nallpossibledatageneratingdistributions,everyclassiï¬cationalgorithmhasthe\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\n1 1 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nî€± î€°î€°î€± î€°î€±î€± î€°î€²î€± î€°î€³î€± î€°î€´î€± î€°î€µ\nîŽîµî­ î¢ î¥î²î€  î¯ î¦ î€  î´ î²î¡ î© î® î© î® î§ î€  î¥î¸ î¡ î­ î° î¬ î¥ î³î€° î€® î€°î€° î€® î€µî€± î€® î€°î€± î€® î€µî€² î€® î€°î€² î€® î€µî€³ î€® î€°î€³ î€® î€µî… î²î²î¯ î²î€  î€¨ î î“ î… î€©î‚ î¡ î¹ î¥ î³î€  î¥ î² î² î¯ î²\nî” î² î¡ î© î® î€  î€¨ î± îµ î¡ î¤ î² î¡ î´ î© î£ î€©\nî” î¥ î³î´ î€  î€¨ î± îµ î¡ î¤ î² î¡ î´ î© î£ î€©\nî” î¥ î³î´ î€  î€¨ î¯ î° î´ î© î­ î¡ î¬ î€  î£ î¡ î° î¡ î£ î© î´ î¹ î€©\nî” î² î¡ î© î® î€  î€¨ î¯ î° î´ î© î­ î¡ î¬ î€  î£ î¡ î° î¡ î£ î© î´ î¹ î€©\nî€± î€°î€°î€± î€°î€±î€± î€°î€²î€± î€°î€³î€± î€°î€´î€± î€°î€µ\nîŽîµî­ î¢ î¥î²î€  î¯ î¦ î€  î´ î²î¡ î© î® î© î® î§ î€  î¥î¸ î¡ î­ î° î¬ î¥ î³î€°î€µî€± î€°î€± î€µî€² î€°î î° î´ î© î­ î¡ î¬ î€  î£î¡ î° î¡ î£î©î´î¹ î€  î€¨ î° î¯ î¬ î¹ î® î¯ î­ î© î¡ î¬ î€  î¤ î¥ î§ î²î¥ î¥ î€©",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "Figure5.4:Theeï¬€ectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\nandthengeneratedseveraldiï¬€erentsizesoftrainingset.Foreachsize,wegenerated40\ndiï¬€erenttrainingsetsinordertoploterrorbarsshowing95percentconï¬denceintervals.\n( T o p )TheMSEonthetrainingandtestsetfortwodiï¬€erentmodels:aquadraticmodel,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andamodelwithdegreechosentominimizethetesterror.Bothareï¬tinclosedform.For\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\nThisisbecauselargerdatasetsarehardertoï¬t.Simultaneously,thetesterrordecreases,\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\ntomemorizespeciï¬cinstancesofthetrainingset.Asthetrainingsizeincreasestoinï¬nity,\nthetrainingerrorofanyï¬xed-capacitymodel(here,thequadraticmodel)mustrisetoat\nleasttheBayeserror.Â Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases.Â Theoptimal\ncapacityplateausafterreachingsuï¬ƒcientcomplexitytosolvethetask.\n1 1 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\ntothesameclass.\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\nalgorithmsthatperformwellonthesedistributions.\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\nunderstandwhatkindsofdistributionsarerelevanttotheâ€œrealworldâ€thatanAI\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\n5.2.2Regularization\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\nalgorithmstoperformwellonaspeciï¬ctask.Wedosobybuildingasetof\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "thelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\nconcretelyistoincreaseordecreasethemodelâ€™srepresentationalcapacitybyadding\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\nisabletochoose.Wegavethespeciï¬cexampleofincreasingordecreasingthe\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\nfarisoversimpliï¬ed.\nThebehaviorofouralgorithmisstronglyaï¬€ectednotjustbyhowlargewe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "makethesetoffunctionsallowedinitshypothesisspace,butbythespeciï¬cidentity\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "controltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\nfunctions.\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\nispreferred.Theunpreferredsolutionwillbechosenonlyifitï¬tsthetraining\n1 1 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndatasigniï¬cantlybetterthanthepreferredsolution.\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciï¬cally,\nJ() = wMSEtrain+Î»wî€¾w, (5.18)\nwhereÎ»isavaluechosenaheadoftimethatcontrolsthestrengthofourpreference",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "forsmallerweights.WhenÎ»= 0,weimposenopreference,andlargerÎ»forcesthe\nweightstobecomesmaller.Â MinimizingJ(w)resultsinachoiceofweightsthat\nmakeatradeoï¬€betweenï¬ttingthetrainingdataandbeingsmall.Thisgivesus\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\nexampleofhowwecancontrolamodelâ€™stendencytooverï¬torunderï¬tviaweight\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdiï¬€erentvalues\nof.Seeï¬gurefortheresults. Î» 5.5\nî¸î€°î¹î• î® î¤ î¥ î² î¦ î© î´ î´ î© î® î§\nî€¨ î… î¸ î£ î¥ î³î³î©î¶ î¥ î€  î‚¸ î€©",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "î¸î€°î¹î• î® î¤ î¥ î² î¦ î© î´ î´ î© î® î§\nî€¨ î… î¸ î£ î¥ î³î³î©î¶ î¥ î€  î‚¸ î€©\nî¸î€°î¹î î° î° î² î¯ î° î² î© î¡ î´ î¥ î€  î· î¥ î© î§ î¨ î´ î€  î¤ î¥ î£ î¡ î¹\nî€¨ î î¥ î¤ î© îµ î­ î€  î‚¸ î€©\nî¸î€°î¹î î¶ î¥ î² î¦ î© î´ î´ î© î® î§\nî€¨ î€° î€© î‚¸ î€¡\nFigure5.5:Weï¬tahigh-degreepolynomialregressionmodeltoourexampletrainingset\nfromï¬gure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverï¬tting.\n( L e f t )WithverylargeÎ»,wecanforcethemodeltolearnafunctionwithnoslopeat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "all.Thisunderï¬tsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. Î»\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\ncoeï¬ƒcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "degree-9polynomialoverï¬tssigniï¬cantly,aswesawinï¬gure.5.2\n1 1 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;Î¸)by\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\ndecay,theregularizerisâ„¦(w) =wî€¾w.Inchapter,wewillseethatmanyother 7\nregularizersarepossible.\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\nofcontrollingamodelâ€™scapacitythanincludingorexcludingmembersfromthe\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "expressinganinï¬nitelystrongpreferenceagainstthatfunction.\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\ndeï¬nedwithsmallerweightsexplicitly,Â viaanextraterminthecriterionwe\nminimize.ThereareÂ manyÂ otherwaysofÂ expressing preferencesforÂ diï¬€erent\nsolutions,bothimplicitlyandexplicitly.Together,thesediï¬€erentapproaches\nareknownasregularization.Â Regularizationisanymodiï¬cationwemaketoa\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "trainingerror.Regularizationisoneofthecentralconcernsoftheï¬eldofmachine\nlearning,rivaledinitsimportanceonlybyoptimization.\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "thatpeoplecando)mayallbesolvedeï¬€ectivelyusingverygeneral-purposeforms\nofregularization.\n5.3HyperparametersandValidationSets\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\nitself(thoughwecanÂ designaÂ nestedlearningÂ procedureÂ where oneÂ learning\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Inthepolynomialregressionexamplewesawinï¬gure,thereisasingle 5.2\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\nparameter.TheÎ»valueusedtocontrolthestrengthofweightdecayisanother\nexampleofahyperparameter.\nSometimesasettingischosentobeahyperparameter thatthelearningal-\ngorithmdoesnotlearnbecauseitisdiï¬ƒculttooptimize.Morefrequently,the\n1 2 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\nalwayschoosethemaximumpossiblemodelcapacity,resultinginoverï¬tting(refer\ntoï¬gure).Forexample,wecanalwaysï¬tthetrainingsetbetterwithahigher 5.3\ndegreepolynomialandaweightdecaysettingofÎ»= 0thanwecouldwithalower\ndegreepolynomialandapositiveweightdecaysetting.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "degreepolynomialandapositiveweightdecaysetting.\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\nalgorithmdoesnotobserve.\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\nitshyperparameters .Â Forthisreason,noexamplefromthetestsetcanbeused",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\ntrainingdata.Speciï¬cally,wesplitthetrainingdataintotwodisjointsubsets.One\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\n20%forvalidation.Sincethevalidationsetisusedtoâ€œtrainâ€thehyperparameters ,\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\nInpractice,Â when thesametestsethasbeenusedrepeatedlytoevaluate",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "performanceofdiï¬€erentalgorithmsovermanyyears,andespeciallyifweconsider\nalltheattemptsfromthescientiï¬ccommunityatbeatingthereportedstate-of-\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreï¬‚ectthe\ntrueï¬eldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\n1 2 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.3.1Cross-Validation\nDividingthedatasetintoaï¬xedtrainingsetandaï¬xedtestsetcanbeproblematic\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\naroundtheestimatedaveragetesterror,makingitdiï¬ƒculttoclaimthatalgorithm\nAworksbetterthanalgorithmonthegiventask. B\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "tousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\nthetrainingandtestingcomputationondiï¬€erentrandomlychosensubsetsorsplits\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "dataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\nused.\n5.4Estimators,BiasandVariance\nTheï¬eldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "toformallycharacterizenotionsofgeneralization, underï¬ttingandoverï¬tting.\n5.4.1PointEstimation\nPointestimationistheattempttoprovidethesingleâ€œbestâ€predictionofsome\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\nconventionwillbetodenoteapointestimateofaparameterbyÎ¸ Ë†Î¸.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed\n1 2 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "inthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\nofunsupervisedlearning.Â The algorithmreturnsthevectoroferrorseforeach\nexamplein D,whosemeanistheestimatedgeneralization error.Â Theerrorson\nindividualexamplescanbeusedtocomputeaconï¬denceintervalaroundthemean\n(equation).Â Whiletheseconï¬denceintervalsarenotwell-justiï¬edafterthe 5.47\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\nalgorithmAisbetterthanalgorithmBonlyiftheconï¬denceintervaloftheerror",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "ofalgorithmAliesbelowanddoesnotintersecttheconï¬denceintervalofalgorithm\nB.\nDeï¬neKFoldXV(): D,A,L,k\nRequire: D,thegivendataset,withelementsz() i\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\ninputandoutputsalearnedfunction\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\nanexamplez() iâˆˆ âˆˆ Dtoascalar R\nRequire:k,thenumberoffolds\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\nfordoikfromto1\nf i= (A D D\\ i)\nforz() jin D ido\ne j= (Lf i,z() j)\nendfor",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "forz() jin D ido\ne j= (Lf i,z() j)\nendfor\nendfor\nReturne\n1 2 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\nË†Î¸ m= (gx(1),...,x() m). (5.19)\nThedeï¬nitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\nÎ¸oreventhattherangeofgisthesameasthesetofallowablevaluesofÎ¸.\nThisdeï¬nitionofapointestimatorisverygeneralandallowsthedesignerofan\nestimatorgreatï¬‚exibility.Whilealmostanyfunctionthusqualiï¬esasanestimator,\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingÎ¸that\ngeneratedthetrainingdata.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "generatedthetrainingdata.\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\nthatthetrueparametervalueÎ¸isï¬xedbutunknown,whilethepointestimate\nË†Î¸isafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\nfunctionofthedataisrandom.Therefore Ë†Î¸isarandomvariable.\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\nestimators.\nFunctionEstimationAswementionedabove,sometimesweareinterestedin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "performingfunctionestimation(orfunctionapproximation).Herewearetryingto\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\nwemayassumethaty=f(x)+î€,whereî€standsforthepartofythatisnot\npredictablefromx.Â Infunctionestimation,weareinterestedinapproximating\nfwithamodelorestimate Ë†f.Functionestimationisreallyjustthesameas\nestimatingaparameterÎ¸;thefunctionestimator Ë†fissimplyapointestimatorin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "functionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\nafunction Ë†f y mappingfromtox.\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\ndiscusswhattheytellusabouttheseestimators.\n5.4.2Bias\nThebiasofanestimatorisdeï¬nedas:\nbias(Ë†Î¸ m) = ( EË†Î¸ m)âˆ’Î¸ (5.20)\n1 2 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\nandÎ¸isthetrueunderlyingvalueofÎ¸usedtodeï¬nethedatageneratingdistri-\nbution.Anestimator Ë†Î¸ missaidtobeunbiasedifbias(Ë†Î¸ m) = 0,whichimplies\nthat E(Ë†Î¸ m)=Î¸.Anestimator Ë†Î¸ missaidtobeasymptoticallyunbiasedif\nlim m â†’ âˆžbias(Ë†Î¸ m) = 0,whichimpliesthatlim m â†’ âˆž E(Ë†Î¸ m) = Î¸.\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\nbutionwithmean:Î¸\nPx(() i;) = Î¸Î¸x() i(1 )âˆ’Î¸(1 âˆ’ x() i). (5.21)\nAcommonestimatorfortheÎ¸parameterofthisdistributionisthemeanofthe\ntrainingsamples:\nË†Î¸ m=1\nmmî˜\ni=1x() i. (5.22)\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\nintoequation:5.20\nbias(Ë†Î¸ m) = [ EË†Î¸ m]âˆ’Î¸ (5.23)\n= Eî€¢\n1\nmmî˜\ni=1x() iî€£\nâˆ’Î¸ (5.24)\n=1\nmmî˜\ni=1Eî¨\nx() iî©\nâˆ’Î¸ (5.25)\n=1\nmmî˜\ni=11î˜\nx() i=0î€\nx() iÎ¸x() i(1 )âˆ’Î¸(1 âˆ’ x() i)î€‘\nâˆ’Î¸(5.26)\n=1\nmmî˜",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "x() iÎ¸x() i(1 )âˆ’Î¸(1 âˆ’ x() i)î€‘\nâˆ’Î¸(5.26)\n=1\nmmî˜\ni=1()Î¸âˆ’Î¸ (5.27)\n= = 0Î¸Î¸âˆ’ (5.28)\nSince bias(Ë†Î¸) = 0,wesaythatourestimator Ë†Î¸isunbiased.\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\naccordingtoaGaussiandistributionp(x() i) =N(x() i;Âµ,Ïƒ2),whereiâˆˆ{1,...,m}.\n1 2 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\npx(() i;Âµ,Ïƒ2) =1âˆš\n2Ï€Ïƒ2expî€ \nâˆ’1\n2(x() iâˆ’Âµ)2\nÏƒ2î€¡\n.(5.29)\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\nmean:\nË†Âµ m=1\nmmî˜\ni=1x() i(5.30)\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\nitsexpectation:\nbias(Ë†Âµ m) = [Ë† EÂµ m]âˆ’Âµ (5.31)\n= Eî€¢\n1\nmmî˜\ni=1x() iî€£\nâˆ’Âµ (5.32)\n=î€ \n1\nmmî˜\ni=1Eî¨\nx() iî©î€¡\nâˆ’Âµ (5.33)\n=î€ \n1\nmmî˜\ni=1Âµî€¡\nâˆ’Âµ (5.34)\n= = 0ÂµÂµâˆ’ (5.35)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "=î€ \n1\nmmî˜\ni=1Âµî€¡\nâˆ’Âµ (5.34)\n= = 0ÂµÂµâˆ’ (5.35)\nThusweï¬ndthatthesamplemeanisanunbiasedestimatorofGaussianmean\nparameter.\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\nexample,wecomparetwodiï¬€erentestimatorsofthevarianceparameterÏƒ2ofa\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\nTheï¬rstestimatorofÏƒ2weconsiderisknownasthesamplevariance:\nË†Ïƒ2\nm=1\nmmî˜\ni=1î€\nx() iâˆ’Ë†Âµ mî€‘2\n, (5.36)\nwhere Ë†Âµ misthesamplemean,deï¬nedabove.Moreformally,weareinterestedin\ncomputing\nbias(Ë†Ïƒ2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "computing\nbias(Ë†Ïƒ2\nm) = [Ë† EÏƒ2\nm]âˆ’Ïƒ2(5.37)\n1 2 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nWebeginbyevaluatingtheterm E[Ë†Ïƒ2\nm]:\nE[Ë†Ïƒ2\nm] = Eî€¢\n1\nmmî˜\ni=1î€\nx() iâˆ’Ë†Âµ mî€‘2î€£\n(5.38)\n=mâˆ’1\nmÏƒ2(5.39)\nReturningtoequation,weconcludethatthebiasof 5.37 Ë†Ïƒ2\nmisâˆ’Ïƒ2/m.Therefore,\nthesamplevarianceisabiasedestimator.\nTheunbiasedsamplevarianceestimator\nËœÏƒ2\nm=1\nmâˆ’1mî˜\ni=1î€\nx() iâˆ’Ë†Âµ mî€‘2\n(5.40)\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\nThatis,weï¬ndthat E[ËœÏƒ2\nm] = Ïƒ2:\nE[ËœÏƒ2\nm] = Eî€¢\n1\nmâˆ’1mî˜\ni=1î€\nx() iâˆ’Ë†Âµ mî€‘2î€£\n(5.41)\n=m\nmâˆ’1E[Ë†Ïƒ2\nm] (5.42)\n=m\nmâˆ’1î€’mâˆ’1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "(5.41)\n=m\nmâˆ’1E[Ë†Ïƒ2\nm] (5.42)\n=m\nmâˆ’1î€’mâˆ’1\nmÏƒ2î€“\n(5.43)\n= Ïƒ2. (5.44)\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\nestimatorsareclearlydesirable,theyarenotalwaystheâ€œbestâ€estimators.Aswe\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\n5.4.3VarianceandStandardError\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "Thevarianceofanestimatorissimplythevariance\nVar(Ë†Î¸) (5.45)\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\nvarianceiscalledthe ,denotedstandarderror SE(Ë†Î¸).\n1 2 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\nlowvariance.\nWhenwecomputeanystatisticusingaï¬nitenumberofsamples,ourestimate\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\nbeendiï¬€erent.Theexpecteddegreeofvariationinanyestimatorisasourceof\nerrorthatwewanttoquantify.\nThestandarderrorofthemeanisgivenby\nSE(Ë†Âµ m) =î¶îµîµî´Varî€¢\n1\nmmî˜\ni=1x() iî€£\n=Ïƒâˆšm, (5.46)\nwhereÏƒ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\nestimatedbyusinganestimateofÏƒ.Unfortunately,neitherthesquarerootof\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "provideanunbiasedestimateofthestandarddeviation.Bothapproachestend\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\nForlarge,theapproximation isquitereasonable. m\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\nfallsinanychoseninterval.Forexample,the95%conï¬denceintervalcenteredon\nthemean Ë†Âµ mis\n(Ë†Âµ mâˆ’196SE( Ë†.Âµ m)Ë†,Âµ m+196SE( Ë†.Âµ m)), (5.47)\nunderthenormaldistributionwithmean Ë†Âµ mandvariance SE(Ë†Âµ m)2.Inmachine\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "Biftheupperboundofthe95%conï¬denceintervalfortheerrorofalgorithmAis\nlessthanthelowerboundofthe95%conï¬denceintervalfortheerrorofalgorithm\nB.\n1 2 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nExample:Â BernoulliDistributionWeonceagainconsiderasetofsamples\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\n(recallP(x() i;Î¸) =Î¸x() i(1âˆ’Î¸)(1 âˆ’ x() i)).Thistimeweareinterestedincomputing\nthevarianceoftheestimator Ë†Î¸ m=1\nmîm\ni=1x() i.\nVarî€\nË†Î¸ mî€‘\n= Varî€ \n1\nmmî˜\ni=1x() iî€¡\n(5.48)\n=1\nm2mî˜\ni=1Varî€\nx() iî€‘\n(5.49)\n=1\nm2mî˜\ni=1Î¸Î¸ (1âˆ’) (5.50)\n=1\nm2mÎ¸Î¸ (1âˆ’) (5.51)\n=1\nmÎ¸Î¸ (1âˆ’) (5.52)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "=1\nm2mÎ¸Î¸ (1âˆ’) (5.51)\n=1\nmÎ¸Î¸ (1âˆ’) (5.52)\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\nreturntowhenwediscussconsistency(seesection).5.4.5\n5.4.4Tradingoï¬€BiasandVariancetoMinimizeMeanSquared\nError\nBiasandvariancemeasuretwodiï¬€erentsourcesoferrorinanestimator.Bias\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "estimatorvaluethatanyparticularsamplingofthedataislikelytocause.\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\nï¬gureandweareonlyoï¬€eredthechoicebetweenamodelwithlargebiasand 5.2\nonethatsuï¬€ersfromlargevariance.Howdowechoosebetweenthem?\nThemostcommonwaytonegotiatethistrade-oï¬€istousecross-validation.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\nMSE = [( EË†Î¸ mâˆ’Î¸)2] (5.53)\n= Bias(Ë†Î¸ m)2+Var(Ë†Î¸ m) (5.54)\n1 2 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheMSEmeasurestheoverallexpecteddeviationâ€”in asquarederrorsenseâ€”\nbetweentheestimatorandthetruevalueoftheparameterÎ¸.Asisclearfrom\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\nC apac i t yB i as Ge ne r al i z at i on\ne r r orV ar i anc e\nO pt i m al\nc apac i t yO v e r ï¬t t i ngÂ z o n e U nde r ï¬t t i ngÂ z o n e",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderï¬tting\nwhenthecapacityisbelowthisoptimumandoverï¬ttingwhenitisabove.Thisrelationship\nissimilartotherelationshipbetweencapacity,underï¬tting,andoverï¬tting,discussedin\nsectionandï¬gure. 5.2 5.3\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "learningconceptsofcapacity,underï¬ttingandoverï¬tting.Inthecasewheregen-\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\nanddecreasebias.Thisisillustratedinï¬gure,whereweseeagaintheU-shaped 5.6\ncurveofgeneralization errorasafunctionofcapacity.\n5.4.5Consistency\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\nï¬xedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\n1 3 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\nplimm â†’ âˆžË†Î¸ m= Î¸. (5.55)\nThesymbolplimindicatesconvergenceinprobability,meaningthatforanyî€>0,\nP(|Ë†Î¸ mâˆ’|Î¸>î€)â†’0asmâ†’âˆž.Theconditiondescribedbyequationis5.55\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\nstrongconsistencyreferringtothealmostsureconvergenceofË†Î¸toÎ¸.Almost\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\noccurswhenp(lim m â†’ âˆž x() m= ) = 1x.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "occurswhenp(lim m â†’ âˆž x() m= ) = 1x.\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\nnumberofdataexamplesgrows.However,thereverseisnottrueâ€”asymptotic\nunbiasednessdoesnotimplyconsistency.Â Forexample,considerestimatingthe\nmeanparameterÂµofanormaldistributionN(x;Âµ,Ïƒ2),withadatasetconsisting\nofmsamples:{x(1),...,x() m}.Wecouldusetheï¬rstsamplex(1)ofthedataset\nasanunbiasedestimator:Ë†Î¸=x(1).Inthatcase, E(Ë†Î¸ m)=Î¸sotheestimator",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\nestimatorasitisthecasethat not Ë†Î¸ mâ†’ â†’âˆžÎ¸mas.\n5.5MaximumLikelihoodEstimation\nPreviously,wehaveseensomedeï¬nitionsofcommonestimatorsandanalyzed\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciï¬c",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "functionsthataregoodestimatorsfordiï¬€erentmodels.\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\nthetruebutunknowndatageneratingdistributionpdata() x.\nLetpmodel( x;Î¸)beaparametricfamilyofprobabilitydistributionsoverthe\nsamespaceindexedbyÎ¸.Inotherwords,pmodel(x;Î¸)mapsanyconï¬gurationx\ntoarealnumberestimatingthetrueprobabilitypdata()x.\nThemaximumlikelihoodestimatorforisthendeï¬nedas Î¸\nÎ¸ML= argmax\nÎ¸pmodel(;) XÎ¸ (5.56)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "Î¸ML= argmax\nÎ¸pmodel(;) XÎ¸ (5.56)\n= argmax\nÎ¸mî™\ni=1pmodel(x() i;)Î¸ (5.57)\n1 3 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\nForexample,itispronetonumericalunderï¬‚ow.Toobtainamoreconvenient\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\nintoasum:\nÎ¸ML= argmax\nÎ¸mî˜\ni=1logpmodel(x() i;)Î¸. (5.58)\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "withrespecttotheempiricaldistributionË†pdatadeï¬nedbythetrainingdata:\nÎ¸ML= argmax\nÎ¸E x âˆ¼Ë† pdatalogpmodel(;)xÎ¸. (5.59)\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\nthedissimilaritybetweentheempiricaldistributionË†pdatadeï¬nedbythetraining\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\nDKL(Ë†pdataî«pmodel) = E x âˆ¼Ë† pdata[log Ë†pdata()logxâˆ’pmodel()]x.(5.60)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\nneedonlyminimize\nâˆ’ E x âˆ¼Ë† pdata[logpmodel()]x (5.61)\nwhichisofcoursethesameasthemaximization inequation.5.59\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\nentropybetweenthedistributions.Manyauthorsusethetermâ€œcross-entropyâ€to\nidentifyspeciï¬callythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\nentropybetweentheempiricaldistributiondeï¬nedbythetrainingsetandthe\nprobabilitydistributiondeï¬nedbymodel.Forexample,meansquarederroristhe\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\ntributionmatchtheempiricaldistributionË†pdata.Ideally,wewouldliketomatch\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\ndistribution.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "distribution.\nWhiletheoptimalÎ¸isthesameregardlessofwhetherwearemaximizingthe\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\n1 3 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\narediï¬€erent.Insoftware,weoftenphrasebothasminimizingacostfunction.\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "5.5.1ConditionalLog-LikelihoodandMeanSquaredError\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\nourgoalistoestimateaconditionalprobabilityP( y x|;Î¸)inordertopredict y\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\ntargets,thentheconditionalmaximumlikelihoodestimatoris\nÎ¸ML= argmax\nÎ¸P. ( ;)YX|Î¸ (5.62)\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\nÎ¸ML= argmax\nÎ¸mî˜",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "Î¸ML= argmax\nÎ¸mî˜\ni=1log(Py() i|x() i;)Î¸. (5.63)\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\nintroducedearlierinsection,maybejustiï¬edasamaximumlikelihood 5.1.4\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\ntotakeaninputxandproduceanoutputvalue Ë†y.ThemappingfromxtoË†yis\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "likelihoodestimation.Insteadofproducingasingleprediction Ë†y,wenowthink\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\nthatwithaninï¬nitelylargetrainingset,wemightseeseveraltrainingexamples\nwiththesameinputvaluexbutdiï¬€erentvaluesofy.Â Thegoalofthelearning\nalgorithmisnowtoï¬tthedistributionp(y|x)toallofthosediï¬€erentyvalues\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\nweobtainedbefore,wedeï¬nep(y|x) =N(y;Ë†y(x;w),Ïƒ2).Thefunction Ë†y(x;w)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\nthevarianceisï¬xedtosomeconstantÏƒ2chosenbytheuser.Wewillseethatthis\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\n1 3 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\ngivenby\nmî˜\ni=1log(py() i|x() i;)Î¸ (5.64)\n= log âˆ’mÏƒâˆ’m\n2log(2)Ï€âˆ’mî˜\ni=1î€î€Ë†y() iâˆ’y() iî€î€2\n2Ïƒ2,(5.65)\nwhere Ë†y() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\nsquarederror,\nMSEtrain=1\nmmî˜\ni=1||Ë†y() iâˆ’y() i||2, (5.66)\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "thesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\nThetwocriteriahavediï¬€erentvaluesbutthesamelocationoftheoptimum.This\njustiï¬estheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\n5.5.2PropertiesofMaximumLikelihood\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\nbethebestestimatorasymptotically,asthenumberofexamplesmâ†’âˆž,interms\nofitsrateofconvergenceasincreases.m",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "ofitsrateofconvergenceasincreases.m\nUnderappropriateÂ conditions,Â the maximumlikelihoodÂ estimatorhasÂ the\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\noftrainingexamplesapproachesinï¬nity,themaximumlikelihoodestimateofa\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\nâ€¢Thetruedistributionpdatamustliewithinthemodelfamilypmodel(Â·;Î¸).\nOtherwise,noestimatorcanrecoverpdata.\nâ€¢ThetruedistributionpdatamustcorrespondtoexactlyonevalueofÎ¸.Other-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. Î¸\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\ntor,manyofwhichsharethepropertyofbeingconsistentestimators.Â However,\n1 3 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nconsistentestimatorscandiï¬€erintheirstatisticeï¬ƒciency,meaningthatone\nconsistentestimatormayobtainlowergeneralization errorforaï¬xednumberof\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaï¬xedlevelof\ngeneralization error.\nStatisticaleï¬ƒciencyistypicallystudiedintheparametriccase(likeinlinear\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "measurehowclosewearetothetrueparameterisbytheexpectedmeansquared\nerror,computingthesquareddiï¬€erencebetweentheestimatedandtrueparameter\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\nformlarge,theCramÃ©r-Raolowerbound(,;,)showsthatno Rao1945CramÃ©r1946\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\nestimator.\nForthesereasons(consistencyandeï¬ƒciency),maximumlikelihoodisoften",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "consideredthepreferredestimatortouseformachinelearning.Whenthenumber\nofexamplesissmallenoughtoyieldoverï¬ttingbehavior,regularizationstrategies\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\nthathaslessvariancewhentrainingdataislimited.\n5.6BayesianStatistics\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\ningasinglevalueofÎ¸,thenmakingallpredictionsthereafterbasedonthatone\nestimate.AnotherapproachistoconsiderallpossiblevaluesofÎ¸whenmakinga",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "prediction.ThelatteristhedomainofBayesianstatistics.\nAsdiscussedÂ insectionÂ ,Â theÂ frequen tistÂ perspectiveÂ isthatÂ thetrue 5.4.1\nparametervalueÎ¸isï¬xedbutunknown,whilethepointestimate Ë†Î¸isarandom\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\nTheBayesianperspectiveonstatisticsisquitediï¬€erent.Â The Bayesianuses\nprobabilitytoreï¬‚ectdegreesofcertaintyofstatesofknowledge.Thedatasetis\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterÎ¸",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "isunknownoruncertainandthusisrepresentedasarandomvariable.\nBeforeobservingthedata,werepresentourknowledgeofÎ¸usingtheprior\nprobabilitydistribution,p(Î¸)(sometimesreferredtoassimplyâ€œthepriorâ€).\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\nquitebroad(i.e.withhighentropy)toreï¬‚ectahighdegreeofuncertaintyinthe\n1 3 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nvalueofÎ¸beforeobservinganydata.Forexample,onemightassume that apriori\nÎ¸liesinsomeï¬niterangeorvolume,withauniformdistribution.Â Manypriors\ninsteadreï¬‚ectapreferenceforâ€œsimplerâ€Â solutions(suchassmallermagnitude\ncoeï¬ƒcients,orafunctionthatisclosertobeingconstant).\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\nrecovertheeï¬€ectofdataonourbeliefaboutÎ¸bycombiningthedatalikelihood\npx((1),...,x() m|Î¸)withthepriorviaBayesâ€™rule:",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "px((1),...,x() m|Î¸)withthepriorviaBayesâ€™rule:\npx(Î¸|(1),...,x() m) =px((1),...,x() m|Î¸Î¸)(p)\npx((1),...,x() m)(5.67)\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\nfewhighlylikelyvaluesoftheparameters.\nRelativetomaximumlikelihoodestimation,Bayesianestimationoï¬€erstwo",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "importantdiï¬€erences.First,unlikethemaximumlikelihoodapproachthatmakes\npredictionsusingapointestimateofÎ¸,theBayesianapproachistomakepredictions\nusingafulldistributionoverÎ¸.Forexample,afterobservingmexamples,the\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\npx((+1) m|x(1),...,x() m) =îš\npx((+1) m| |Î¸Î¸)(px(1),...,x() m)d.Î¸(5.68)\nHereeachvalueofÎ¸withpositiveprobabilitydensitycontributestotheprediction\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\nvalueofÎ¸,thenthisuncertaintyisincorporated directlyintoanypredictionswe\nmightmake.\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\ntaintyinagivenpointestimateofÎ¸byevaluatingitsvariance.Thevarianceof\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "protectwellagainstoverï¬tting.Â Thisintegralisofcoursejustanapplicationof\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\nestimate.\nThesecondimportantdiï¬€erencebetweentheBayesianapproachtoestimation\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\n1 3 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\npriordistribution.Thepriorhasaninï¬‚uencebyshiftingprobabilitymassdensity\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\njudgmentimpactingthepredictions.\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\nisavailable,buttypicallysuï¬€erfromhighcomputational costwhenthenumberof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "trainingexamplesislarge.\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\nwelearnalinearmappingfromaninputvectorxâˆˆ Rntopredictthevalueofa\nscalar.Thepredictionisparametrized bythevector yâˆˆ R wâˆˆ Rn:\nË†y= wî€¾x. (5.69)\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\nofovertheentiretrainingsetas: y\nË†y()train= X()trainw. (5.70)\nExpressedasaGaussianconditionaldistributionony()train,wehave",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "p(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\nâˆexpî€’\nâˆ’1\n2(y()trainâˆ’X()trainw)î€¾(y()trainâˆ’X()trainw)î€“\n,\n(5.72)\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\n(X()train,y()train) ( ) assimplyXy,.\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\nï¬rstneedtospecifyapriordistribution.Thepriorshouldreï¬‚ectournaivebelief\naboutthevalueoftheseparameters.Whileitissometimesdiï¬ƒcultorunnatural",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\naboutÎ¸.Â Forreal-valuedparametersitiscommontouseaGaussianasaprior\ndistribution:\np() = (;w NwÂµ0, Î›0) expâˆî€’\nâˆ’1\n2(wÂµâˆ’0)î€¾Î›âˆ’1\n0(wÂµâˆ’0)î€“\n,(5.73)\n1 3 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nwhereÂµ0and Î›0arethepriordistributionmeanvectorandcovariancematrix\nrespectively.1\nWiththepriorthusspeciï¬ed,wecannowproceedindeterminingtheposterior\ndistributionoverthemodelparameters.\np,p,p (wX|y) âˆ(yX|w)()w (5.74)\nâˆexpî€’\nâˆ’1\n2( )yXwâˆ’î€¾( )yXwâˆ’î€“\nexpî€’\nâˆ’1\n2(wÂµâˆ’0)î€¾Î›âˆ’1\n0(wÂµâˆ’0)î€“\n(5.75)\nâˆexpî€’\nâˆ’1\n2î€\nâˆ’2yî€¾Xww+î€¾Xî€¾Xww+î€¾Î›âˆ’1\n0wÂµâˆ’2î€¾\n0 Î›âˆ’1\n0wî€‘î€“\n.\n(5.76)\nWenowdeï¬ne Î› m=î€€\nXî€¾X+ Î›âˆ’1\n0î€ âˆ’1andÂµ m= Î› mî€€\nXî€¾y+ Î›âˆ’1\n0Âµ0î€\n.Using\nthesenewvariables,weï¬ndthattheposteriormayberewrittenasaGaussian",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "distribution:\np, (wX|y) expâˆî€’\nâˆ’1\n2(wÂµâˆ’ m)î€¾Î›âˆ’1\nm(wÂµâˆ’ m)+1\n2Âµî€¾\nm Î›âˆ’1\nmÂµ mî€“\n(5.77)\nâˆexpî€’\nâˆ’1\n2(wÂµâˆ’ m)î€¾Î›âˆ’1\nm(wÂµâˆ’ m)î€“\n. (5.78)\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\neï¬€ectofBayesianinference.Inmostsituations,wesetÂµ0to 0.Ifweset Î›0=1\nÎ±I,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "Î±I,\nthenÂµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\nweightdecaypenaltyofÎ±wî€¾w.Onediï¬€erenceisthattheBayesianestimateis\nundeï¬nedifÎ±issettozeroâ€”-wearenotallowedtobegintheBayesianlearning\nprocesswithaninï¬nitelywideprioronw.Themoreimportantdiï¬€erenceisthat\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\ndiï¬€erentvaluesofare,ratherthanprovidingonlytheestimate w Âµ m.\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\nposteriordistributionovertheparameterÎ¸,itisstilloftendesirabletohavea\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\nd i a g o n a l c o v a ria n c e m a t rix Î›0= diag( Î»0) .\n1 3 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nsinglepointestimate.Â Onecommonreasonfordesiringapointestimateisthat\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\nintractable,andapointestimateoï¬€ersatractableapproximation.Ratherthan\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\nthebeneï¬toftheBayesianapproachbyallowingthepriortoinï¬‚uencethechoice\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\ncaseofcontinuous):Î¸\nÎ¸MAP= argmax\nÎ¸p( ) = argmaxÎ¸x|\nÎ¸log( )+log() pxÎ¸|pÎ¸.(5.79)\nWerecognize,aboveontherighthandside,logp(xÎ¸|),i.e.thestandardlog-\nlikelihoodterm,and,correspondingtothepriordistribution. log()pÎ¸\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\nÎ»I2),thenthelog-priortermin\nequationisproportional tothefamiliar 5.79 Î»wî€¾wweightdecaypenalty,plusa",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "termthatdoesnotdependonwanddoesnotaï¬€ectthelearningprocess.MAP\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\ndecay.\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\nthepriceofincreasedbias.\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(Î¸).Not\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\ndistributionisnotallowedtodo.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "distributionisnotallowedtodo.\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\ndistribution,astheprior(NowlanandHinton1992,).\n1 3 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.7SupervisedLearningAlgorithms\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\ntrainingsetofexamplesofinputsxandoutputsy.Â Inmanycasestheoutputs\nymaybediï¬ƒculttocollectautomatically andmustbeprovidedbyahuman\nâ€œsupervisor,â€butthetermstillappliesevenwhenthetrainingsettargetswere\ncollectedautomatically .\n5.7.1ProbabilisticSupervisedLearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "5.7.1ProbabilisticSupervisedLearning\nMostÂ supervisedÂ learningÂ algorithmsÂ inthisÂ bookÂ areÂ basedÂ on estimatingÂ a\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\nlikelihoodestimationtoï¬ndthebestparametervectorÎ¸foraparametricfamily\nofdistributions .py(|xÎ¸;)\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\npyy (| NxÎ¸;) = (;Î¸î€¾xI,). (5.80)\nWecangeneralizelinearregressiontotheclassiï¬cationscenariobydeï¬ninga\ndiï¬€erentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\nmustaddupto1.\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "interval(0,1)andinterpretthatvalueasaprobability:\npy Ïƒ (= 1 ;) = |xÎ¸ (Î¸î€¾x). (5.81)\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\nweusethemodelforclassiï¬cationratherthanregression).\nInthecaseoflinearregression,wewereabletoï¬ndtheoptimalweightsby\nsolvingthenormalequations.Logisticregressionissomewhatmorediï¬ƒcult.There\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "log-likelihood(NLL)usinggradientdescent.\n1 4 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\ntherightkindofinputandoutputvariables.\n5.7.2SupportVectorMachines\nOneofthemostinï¬‚uentialapproachestosupervisedlearningisthesupportvector\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\nlogisticregressioninthatitisdrivenbyalinearfunctionwî€¾x+b.Unlikelogistic",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "regression,thesupportvectormachinedoesnotprovideprobabilities, butonly\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\nwî€¾x+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\nwî€¾x+bisnegative.\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "bere-writtenas\nwî€¾x+= +bbmî˜\ni=1Î± ixî€¾x() i(5.82)\nwherex() iisatrainingexampleandÎ±isavectorofcoeï¬ƒcients.Rewritingthe\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\nfunctionÏ†(x) andthedotproductwithafunctionk(xx,() i) =Ï†(x)Â·Ï†(x() i) called\nakernel.The Â·operatorrepresentsaninnerproductanalogoustoÏ†(x)î€¾Ï†(x() i).\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\nsomeinï¬nitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "example,innerproductsbasedonintegrationratherthansummation.Acomplete\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\nusingthefunction\nfb () = x +î˜\niÎ± ik,(xx() i). (5.83)\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenÏ†(x)\nandf(x)islinear.Also,therelationshipbetweenÎ±andf(x)islinear.The\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "Ï†()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\n1 4 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nguaranteedtoconvergeeï¬ƒciently.ThisispossiblebecauseweconsiderÏ†ï¬xedand\noptimizeonlyÎ±,i.e.,theoptimization algorithmcanviewthedecisionfunction\nasbeinglinearinadiï¬€erentspace.Second,thekernelfunctionkoftenadmits\nanimplementationthatissigniï¬cantlymorecomputational eï¬ƒcientthannaively\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. Ï†()x\nInsomecases,Ï†(x)canevenbeinï¬nitedimensional,whichwouldresultin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "aninï¬nitecomputational costforthenaive,explicitapproach.Inmanycases,\nk(xx,î€°)isanonlinear,tractablefunctionofxevenwhenÏ†(x)isintractable.As\nanexampleofaninï¬nite-dimens ionalfeaturespacewithatractablekernel,we\nconstructafeaturemappingÏ†(x)overthenon-negativeintegersx.Supposethat\nthismappingreturnsavectorcontainingxonesfollowedbyinï¬nitelymanyzeros.\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\ntothecorrespondinginï¬nite-dimens ionaldotproduct.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "tothecorrespondinginï¬nite-dimens ionaldotproduct.\nThemostcommonlyusedkernelistheGaussiankernel\nk, ,Ïƒ (uvuv ) = (N âˆ’;02I) (5.84)\nwhere N(x;Âµ, Î£)isthestandardnormaldensity.Thiskernelisalsoknownas\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\nproductinaninï¬nite-dimens ionalspace,butthederivationofthisspaceisless\nstraightforwardthaninourexampleofthekernelovertheintegers. min",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "WecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\nforclassy.Whenatestpointxî€°isnearxaccordingtoEuclideandistance,the\nGaussiankernelhasalargeresponse,indicatingthatxî€°isverysimilartothex\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\nsimilarityofthecorrespondingtrainingexamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "similarityofthecorrespondingtrainingexamples.\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\norkernelmethods( ,; WilliamsandRasmussen1996SchÃ¶lkopf1999etal.,).\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "contributesatermÎ± ik(xx,() i)tothedecisionfunction.Supportvectormachines\nareabletomitigatethisbylearninganÎ±vectorthatcontainsmostlyzeros.\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\nthetrainingexamplesthathavenon-zeroÎ± i.Thesetrainingexamplesareknown\n1 4 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nassupportvectors.\nKernelmachinesalsosuï¬€erfromahighcomputational costoftrainingwhen\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\nontheMNISTbenchmark.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "ontheMNISTbenchmark.\n5.7.3OtherSimpleSupervisedLearningAlgorithms\nWehavealreadybrieï¬‚yencounteredanothernon-probabilis ticsupervisedlearning\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\nafamilyoftechniquesthatcanbeusedforclassiï¬cationorregression.Asa\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaï¬xed\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\nweï¬ndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\nanykindofsupervisedlearningwherewecandeï¬neanaverageoveryvalues.In\nthecaseofclassiï¬cation,wecanaverageoverone-hotcodevectorscwithc y= 1\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\nsupposewehaveamulticlassclassiï¬cationtaskandmeasureperformancewith0-1\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\nnumberoftrainingexamplesapproachesinï¬nity.TheerrorinexcessoftheBayes\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\ndistantneighborsrandomly.Whenthereisinï¬nitetrainingdata,alltestpointsx",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "willhaveinï¬nitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\nofthem,theprocedureconvergestotheBayeserrorrate.Â Thehighcapacityof\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\ngivenasmall,ï¬nitetrainingset.Oneweaknessofk-nearestneighborsisthatit\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "imaginewehavearegressiontaskwithxâˆˆ R100drawnfromanisotropicGaussian\n1 4 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\nfeaturesx2throughx100,notbythelonefeaturex1.Â Thustheoutputonsmall\ntrainingsetswillessentiallyberandom.\n1 4 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n0\n101\n1110 1\n011\n1111 1110110\n10010\n001110 11111101001 00\n010 01111\n111\n11\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\ndisplayedwithabinarystringidentiï¬ercorrespondingtoitspositioninthetree,obtained\nbyappendingabittoitsparentidentiï¬er(0=chooseleftortop,1=chooserightorbottom).",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeï¬ne,soitis\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\nnumberoftrainingexamples.\n1 4 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\n1984)anditsmanyvariants.Asshowninï¬gure,eachnodeofthedecision 5.7\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\ncut).Â Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\nwithaxis-alignedsteps.\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\nlearningalgorithmsbythinkingaboutthesimilaritiesanddiï¬€erencesbetween\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\nSee (),Â (),Â  ()orothermachine Murphy2012Bishop2006Hastieetal.2001\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\n5.8UnsupervisedLearningAlgorithms\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "onlyâ€œfeaturesâ€butnotasupervisionsignal.Thedistinctionbetweensupervised\nandunsupervisedalgorithmsisnotformallyandrigidlydeï¬nedbecausethereisno\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "ï¬ndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\n1 4 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nrelatedexamples.\nAclassicunsupervisedlearningtaskistoï¬ndtheâ€œbestâ€representationofthe\ndata.Byâ€˜bestâ€™wecanmeandiï¬€erentthings,butgenerallyspeakingwearelooking\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\nmoreaccessiblethanitself.x\nTherearemultiplewaysofdeï¬ningarepresentation.Threeofthe simplerÂ \nmostcommonincludelowerdimensionalrepresentations,sparserepresentations",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "andindependentrepresentations.Low-dimensionalrepresentationsattemptto\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "overallstructureoftherepresentationthattendstodistributedataalongtheaxes\noftherepresentationspace.Independentrepresentationsattempttodisentangle\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\noftherepresentationarestatisticallyindependent.\nOfÂ coursetheseÂ threeÂ criteriaareÂ certainlyÂ notmutuallyexclusive.Low-\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "reducethesizeofarepresentationistoï¬ndandremoveredundancies.Identifying\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\nachievemorecompressionwhilediscardinglessinformation.\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\ndevelopthesecriteriaindiï¬€erentwaysorintroduceothercriteria.\n5.8.1PrincipalComponentsAnalysis\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\n1 4 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nâˆ’ âˆ’ 2 0 1 0 0 1 0 2 0\nx 1âˆ’ 2 0âˆ’ 1 001 02 0x 2\nâˆ’ âˆ’ 2 0 1 0 0 1 0 2 0\nz 1âˆ’ 2 0âˆ’ 1 001 02 0z 2\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.Â  ( R i g h t )The\ntransformeddataz=xî€¾Wnowvariesmostalongtheaxisz 1.Thedirectionofsecond\nmostvarianceisnowalongz 2.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "mostvarianceisnowalongz 2.\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\nisaï¬rststeptowardthecriterionoflearningrepresentationswhoseelementsare\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "inputxtoarepresentationzasshowninï¬gure.Insection,wesawthat 5.8 2.12\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\ncorrespondstotheï¬rstprincipalcomponentofthedata.ThuswecanusePCA\nasasimpleandeï¬€ectivedimensionalityreductionmethodthatpreservesasmuch\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "decorrelatestheoriginaldatarepresentation.X\nLetusconsiderthemnÃ—-dimensionaldesignmatrixX.Wewillassumethat\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\nVar[] =x1\nmâˆ’1Xî€¾X. (5.85)\n1 4 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nPCAï¬ndsarepresentation(throughlineartransformation)z=xî€¾Wwhere\nVar[]zisdiagonal.\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\naregivenbytheeigenvectorsofXî€¾X.Fromthisview,\nXî€¾XWW = Î›î€¾. (5.86)\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\nSpeciï¬cally,theyaretherightsingularvectorsofX.Toseethis,letWbethe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "rightsingularvectorsinthedecompositionX=UW Î£î€¾.Â Wethenrecoverthe\noriginaleigenvectorequationwithastheeigenvectorbasis: W\nXî€¾X=î€\nUW Î£î€¾î€‘î€¾\nUW Î£î€¾= W Î£2Wî€¾.(5.87)\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\nSVDof,wecanexpressthevarianceofas: X X\nVar[] =x1\nmâˆ’1Xî€¾X (5.88)\n=1\nmâˆ’1(UW Î£î€¾)î€¾UW Î£î€¾(5.89)\n=1\nmâˆ’1W Î£î€¾Uî€¾UW Î£î€¾(5.90)\n=1\nmâˆ’1W Î£2Wî€¾, (5.91)\nwhereweusethefactthatUî€¾U=IbecausetheUmatrixofthesingularvalue\ndecompositionisdeï¬nedtobeorthogonal.Thisshowsthatifwetakez=xî€¾W,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "wecanensurethatthecovarianceofisdiagonalasrequired: z\nVar[] =z1\nmâˆ’1Zî€¾Z (5.92)\n=1\nmâˆ’1Wî€¾Xî€¾XW (5.93)\n=1\nmâˆ’1Wî€¾W Î£2Wî€¾W (5.94)\n=1\nmâˆ’1Î£2, (5.95)\nwherethistimeweusethefactthatWî€¾W=I,againfromthedeï¬nitionofthe\nSVD.\n1 4 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\n(asgivenby Î£2)whichimmediatelyimpliesthattheindividualelementsofzare\nmutuallyuncorrelated.\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "variationunderlyingthedata.Â InthecaseofPCA,thisdisentanglingtakesthe\nformofï¬ndingarotationoftheinputspace(describedbyW)thatalignsthe\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\nwith.z\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\ncanbedonewithasimplelineartransformation.\n5.8.2-meansClustering k",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "5.8.2-meansClustering k\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\nThek-meansclusteringalgorithmdividesthetrainingsetintokdiï¬€erentclusters\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\nzero.\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "representation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\nwewilldevelopotheralgorithmsthatlearnmoreï¬‚exiblesparserepresentations,\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\nareanextremeexampleofsparserepresentationsthatlosemanyofthebeneï¬ts\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\nsimilartoeachother)anditconfersthecomputational advantagethattheentire",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "representationmaybecapturedbyasingleinteger.\nThek-meansalgorithmworksbyinitializingkdiï¬€erentcentroids{Âµ(1),...,Âµ() k}\ntodiï¬€erentvalues,thenalternatingbetweentwodiï¬€erentstepsuntilconvergence.\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\nthenearestcentroidÂµ() i.Intheotherstep,eachcentroidÂµ() iisupdatedtothe\nmeanofalltrainingexamplesx() jassignedtocluster.i\n1 5 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nOnediï¬ƒcultypertainingtoclusteringisthattheclusteringproblemisinherently\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "clusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\nmaybemanydiï¬€erentclusteringsthatallcorrespondwelltosomepropertyof\ntherealworld.Wemayhopetoï¬ndaclusteringthatrelatestoonefeaturebut\nobtainadiï¬€erent,equallyvalidclusteringthatisnotrelevanttoourtask.For\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\ncars.Ifweaskeachclusteringalgorithmtoï¬ndtwoclusters,onealgorithmmay",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "ï¬ndaclusterofcarsandaclusteroftrucks,whileanothermayï¬ndaclusterof\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\nlostinformationaboutsimilarity.Redcarsareinadiï¬€erentclusterfromgray\ncars,justastheyareinadiï¬€erentclusterfromgraytrucks.Â Theoutputofthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\nthantheyaretograytrucks.Theyarediï¬€erentfromboththings,andthatisall\nweknow.\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\ntwoattributesforeachvehicleâ€”onerepresentingitscolorandonerepresenting\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "twoattributesweareinterestedinarecolorandcar-versus-truckratherthan\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\nsimilaritybetweenobjectsinaï¬ne-grainedwaybycomparingmanyattributes\ninsteadofjusttestingwhetheroneattributematches.\n5.9StochasticGradientDescent\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\n1 5 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\ngradientdescentalgorithmintroducedinsection.4.3\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\nexpensive.\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\nJ() = Î¸ E x ,y âˆ¼Ë† pdataL,y,(xÎ¸) =1\nmmî˜\ni=1L(x() i,y() i,Î¸)(5.96)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "mmî˜\ni=1L(x() i,y() i,Î¸)(5.96)\nwhereistheper-exampleloss L L,y,py. (xÎ¸) = logâˆ’ (|xÎ¸;)\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\nâˆ‡ Î¸J() =Î¸1\nmmî˜\ni=1âˆ‡ Î¸L(x() i,y() i,.Î¸) (5.97)\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\nlong.\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "Speciï¬cally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\nB={x(1),...,x( mî€°)}drawnuniformlyfromthetrainingset.Theminibatchsize\nmî€°istypicallychosentobearelativelysmallnumberofexamples,rangingfrom\n1toafewhundred.Crucially,mî€°isusuallyheldï¬xedasthetrainingsetsizem\ngrows.Wemayï¬tatrainingsetwithbillionsofexamplesusingupdatescomputed\nononlyahundredexamples.\nTheestimateofthegradientisformedas\ng=1\nmî€°âˆ‡ Î¸mî€°î˜\ni=1L(x() i,y() i,.Î¸) (5.98)",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "g=1\nmî€°âˆ‡ Î¸mî€°î˜\ni=1L(x() i,y() i,.Î¸) (5.98)\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\nthenfollowstheestimatedgradientdownhill:\nÎ¸Î¸g â† âˆ’î€, (5.99)\nwhereisthelearningrate. î€\n1 5 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\nlocalminimuminareasonableamountoftime,butitoftenï¬ndsaverylowvalue\nofthecostfunctionquicklyenoughtobeuseful.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "ofthecostfunctionquicklyenoughtobeuseful.\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\ndatasets.Foraï¬xedmodelsize,thecostperSGDupdatedoesnotdependonthe\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\nconvergenceusuallyincreaseswithtrainingsetsize.Â However,asmapproaches",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "inï¬nity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\nextendtheamountoftrainingtimeneededtoreachthemodelâ€™sbestpossibletest\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\namodelwithSGDisasafunctionof. O(1) m\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "algorithmsrequireconstructinganmmÃ—matrixG i , j=k(x() i,x() j).Constructing\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\nwithÂ billions ofÂ examples. InÂ academia, startingÂ in2006,deepÂ learning was\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\ndatasets.\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\ninchapter.8\n5.10BuildingaMachineLearningAlgorithm\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\nafairlysimplerecipe:combineaspeciï¬cationofadataset,acostfunction,an\noptimization procedureandamodel.\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\n1 5 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nXyand,thecostfunction\nJ,b(w) = âˆ’ E x ,y âˆ¼Ë† pdatalogpmodel( )y|x, (5.100)\nthemodelspeciï¬cationpmodel(y|x) =N(y;xî€¾w+b,1),and,inmostcases,the\noptimization algorithmdeï¬nedbysolvingforwherethegradientofthecostiszero\nusingthenormalequations.\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "processtoperformstatisticalestimation.Themostcommoncostfunctionisthe\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\nlikelihoodestimation.\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\ntoobtain\nJ,bÎ» (w) = ||||w2\n2âˆ’ E x ,y âˆ¼Ë† pdatalogpmodel( )y|x.(5.101)\nThisstillallowsclosed-formoptimization.\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\noptimization procedure,suchasgradientdescent.\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\nlearningcanbesupportedbydeï¬ningadatasetthatcontainsonlyXandproviding\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheï¬rst",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "PCAvectorbyspecifyingthatourlossfunctionis\nJ() = w E x âˆ¼Ë† pdata||âˆ’ ||xr(;)xw2\n2 (5.102)\nwhileourmodelisdeï¬nedtohavewwithnormoneandreconstructionfunction\nr() = xwî€¾xw.\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\napproximatingitsgradients.\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "immediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\n1 5 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\ntheircostfunctionshaveï¬‚atregionsthatmaketheminappropriate forminimization\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\ncanbedescribedusingthisrecipehelpstoseethediï¬€erentalgorithmsaspartofa\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "thanasalonglistofalgorithmsthateachhaveseparatejustiï¬cations.\n5.11ChallengesMotivatingDeepLearning\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymorediï¬ƒcultwhenworkingwithhigh-dimensionaldata,andhow\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\nareinsuï¬ƒcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\novercometheseandotherobstacles.\n5.11.1TheCurseofDimensionality\nManymachinelearningproblemsbecomeexceedinglydiï¬ƒcultwhenthenumber\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\nconï¬gurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\nincreases.\n1 5 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\nright),thenumberofconï¬gurationsofinterestmaygrowexponentially. ( L e f t )Inthis\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "eachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\ndimensionsitismorediï¬ƒculttodistinguish10diï¬€erentvaluesofeachvariable.Â Weneed\ntokeeptrackofupto10Ã—10=100regions,andweneedatleastthatmanyexamplesto\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\naxis,weseemtoneedO(vd)regionsandexamples.Â Thisisaninstanceofthecurseof",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "dimensionality.FiguregraciouslyprovidedbyNicolasChapados.\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\nespeciallysoinmachinelearning.\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\nAsillustratedinï¬gure,astatisticalchallengearisesbecausethenumberof 5.9\npossibleconï¬gurations ofxismuchlargerthanthenumberoftrainingexamples.\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\ngrid,likeintheï¬gure.Wecandescribelow-dimensional spacewithalownumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\nexamplesinthesamecell.Â Ifwearedoingregressionwecanaveragethetarget",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\nconï¬gurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\n1 5 6",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nmeaningfulaboutthesenewconï¬gurations? Manytraditionalmachinelearning\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\nthesameastheoutputatthenearesttrainingpoint.\n5.11.2LocalConstancyandSmoothnessRegularization\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 273,
      "type": "default"
    }
  },
  {
    "content": "overparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\ndirectlyinï¬‚uencingtheitselfandonlyindirectlyactingontheparameters function\nviatheireï¬€ectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\ndegreeofbeliefinvariousfunctions.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 274,
      "type": "default"
    }
  },
  {
    "content": "degreeofbeliefinvariousfunctions.\nAmongthemostwidelyusedoftheseimplicitâ€œpriorsâ€Â isthesmoothness\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\nshouldnotchangeverymuchwithinasmallregion.\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\nadditional(explicitÂ andimplicit)priorsinorderÂ toreducethegeneralization",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 275,
      "type": "default"
    }
  },
  {
    "content": "erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\ninsuï¬ƒcientforthesetasks.\nTherearemanydiï¬€erentwaystoimplicitlyorexplicitlyexpressapriorbelief\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediï¬€erent\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionfâˆ—that\nsatisï¬esthecondition\nfâˆ—() xâ‰ˆfâˆ—(+)xî€ (5.103)\nformostconï¬gurationsxandsmallchangeî€.Inotherwords,ifweknowagood\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 276,
      "type": "default"
    }
  },
  {
    "content": "answerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\npossible.\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\n1 5 7",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 277,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\nthanthenumberoftrainingexamples.\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 278,
      "type": "default"
    }
  },
  {
    "content": "apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\nresembleseachtrainingexamplex() i.Â Muchofthemodernmotivationfordeep\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\n( ,). Bengioetal.2006b\nDecisiontreesalsosuï¬€erfromthelimitationsofexclusivelysmoothness-based\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 279,
      "type": "default"
    }
  },
  {
    "content": "leavesanduseaseparateparameter(orsometimesmanyparametersforextensions\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\nrequiredtoï¬tthetree.Amultipleofnisneededtoachievesomelevelofstatistical\nconï¬denceinthepredictedoutput.\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 280,
      "type": "default"
    }
  },
  {
    "content": "associatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\nwhereeachtrainingexamplecanbeusedtodeï¬neatmostoneregion,isillustrated\ninï¬gure.5.10\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\nForÂ example,Â imagine thatÂ thetargetfunctionisÂ akindÂ ofcheckerboard.A\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 281,
      "type": "default"
    }
  },
  {
    "content": "Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 282,
      "type": "default"
    }
  },
  {
    "content": "exampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\n1 5 8",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 283,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.10:Â Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintoregions.Â Anexample(representedherebyacircle)withineachregiondeï¬nesthe\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\ndeï¬neswhattheoutputshouldbeforallpointswithinthecorrespondingregion.Â The\nregionsdeï¬nedbynearestneighbormatchingformageometricpatterncalledaVoronoi\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 284,
      "type": "default"
    }
  },
  {
    "content": "oftrainingexamples.Whilethisï¬gureillustratesthebehaviorofthenearestneighbor\nalgorithmspeciï¬cally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\nsurroundingthatexample.\n1 5 9",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 285,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 286,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentwayalongeachdimension.Ifthefunctionadditionallybehavesdiï¬€erently\nindiï¬€erentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\ngeneralizewell?\nTheanswertobothofthesequestionsâ€”whetheritispossibletorepresent\nacomplicatedfunctioneï¬ƒciently,andwhetheritispossiblefortheestimated\nfunctiontogeneralizewelltonewinputsâ€”isyes.Thekeyinsightisthatavery",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 287,
      "type": "default"
    }
  },
  {
    "content": "largenumberofregions,e.g.,O(2k),canbedeï¬nedwithO(k)examples,solong\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\ndiï¬€erentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\nOtherapproachestomachinelearningoftenmakestronger,task-speciï¬cas-",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 288,
      "type": "default"
    }
  },
  {
    "content": "sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\nstrong,task-speciï¬cassumptionsintoneuralnetworkssothattheycangeneralize\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\ncomplextobelimitedtosimple,manuallyspeciï¬edpropertiessuchasperiodicity,\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 289,
      "type": "default"
    }
  },
  {
    "content": "thecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\ngorithms.Â Theseapparentlymildassumptionsallowanexponentialgaininthe\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 290,
      "type": "default"
    }
  },
  {
    "content": "ofdimensionality.\n1 6 0",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 291,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\n5.11.3ManifoldLearning\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\nmanifold.\nAmanifoldisaconnectedÂ region. Mathematically ,Â itÂ isasetofpoints,\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\n3-Dspace.\nThedeï¬nitionofaneighborhoodsurroundingeachpointimpliestheexistence",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 292,
      "type": "default"
    }
  },
  {
    "content": "oftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\ntoaneighboringone.Intheexampleoftheworldâ€™ssurfaceasamanifold,onecan\nwalknorth,south,east,orwest.\nAlthoughthereisaformalmathematical meaningtothetermâ€œmanifold,â€in\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\ndimensioncorrespondstoalocaldirectionofvariation.Seeï¬gureforan5.11",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 293,
      "type": "default"
    }
  },
  {
    "content": "exampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\nofthemanifoldtovaryfromonepointtoanother.Â This oftenhappenswhena\nmanifoldintersectsitself.Forexample,aï¬gureeightisamanifoldthathasasingle\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .âˆ’ 1 0 .âˆ’ 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 294,
      "type": "default"
    }
  },
  {
    "content": "Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\ntheunderlyingmanifoldthatthelearnershouldinfer.\n1 6 1",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 295,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nManymachinelearningproblemsseemhopelessifweexpectthemachine\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\nof Rnconsistsofinvalidinputs,Â andthatinterestinginputsoccuronlyalong\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 296,
      "type": "default"
    }
  },
  {
    "content": "thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\nhighlyconcentrated.\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 297,
      "type": "default"
    }
  },
  {
    "content": "thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\noftwocategoriesofobservations.\nTheï¬rstobservationinfavorofthemanifoldhypothesisisthattheproba-\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\nfromthesedomains.Â Figureshowshow,instead,uniformlysampledpoints 5.12",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 298,
      "type": "default"
    }
  },
  {
    "content": "looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\n1 6 2",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 299,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\nvolumeofimagespace.\nOfcourse,concentratedprobabilitydistributionsarenotsuï¬ƒcienttoshow",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 300,
      "type": "default"
    }
  },
  {
    "content": "thatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\n1 6 3",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 301,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 302,
      "type": "default"
    }
  },
  {
    "content": "moveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\nconnectedtothemanifoldofimagesofcatfaces.\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\ntuitivereasonssupportingit.MorerigorousexperimentsÂ (Cayton2005Narayanan,;\nandMitter2010SchÃ¶lkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 303,
      "type": "default"
    }
  },
  {
    "content": "2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\ninterestinAI.\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\nspeciï¬caddressesintermsofaddressnumbersalongthese1-Droads,notinterms",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 304,
      "type": "default"
    }
  },
  {
    "content": "ofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\nmethodsnecessarytolearnsuchamanifoldstructure.Inï¬gure,wewillsee 20.6\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 305,
      "type": "default"
    }
  },
  {
    "content": "andmachinelearningwhichareemployedthroughouttheremainingpartsofthe\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\n1 6 4",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 306,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER5.MACHINELEARNINGBASICS\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\nfeat.\n1 6 5",
    "metadata": {
      "source": "[9]part-1-chapter-5.pdf",
      "chunk_id": 307,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 3\nL i n e ar F act or Mo d e l s\nManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel\noftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto\npredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many\nofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|.\nTheselatentvariablesprovideanothermeansofrepresentingthedata.Distributed",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationsbasedÂ onlatentÂ variablescanobtainÂ alloftheadvantagesof\nrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrent\nnetworks.\nInthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\nlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\nblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;\nRoweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "alsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat\nthemoreadvanceddeepmodelswillextendfurther.\nAlinearfactormodelisdeï¬nedbytheuseofastochastic,lineardecoder\nfunctionthatgeneratesbyaddingnoisetoalineartransformationof. x h\nThesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\nfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\nmadethesemodelssomeoftheï¬rstlatentvariablemodelstobeextensivelystudied.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Alinearfactormodeldescribesthedatagenerationprocessasfollows.First,\nwesampletheexplanatoryfactorsfromadistribution h\nhâˆ¼ p ,() h (13.1)\nwhere p( h)isafactorialdistribution,with p( h) =î‘\ni p( h i),sothatitiseasyto\n489",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:\nx W h b = ++noise (13.2)\nwherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\nThisisillustratedinï¬gure.13.1\nh 1 h 1 h 2 h 2 h 3 h 3\nx 1 x 1 x 2 x 2 x 3 x 3\nx h n  o i s  e x h n  o i s  e = W + + b = W + + b\nFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\nwhichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "independentlatentfactors h,plussomenoise.Diï¬€erentmodels,suchasprobabilistic\nPCA,factoranalysisorICA,makediï¬€erentchoicesabouttheformofthenoiseandof\ntheprior. p() h\n13.1ProbabilisticPCAandFactorAnalysis\nProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\nfactormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\ndiï¬€erinthechoicesmadeforthenoisedistributionandthemodelâ€™spriorover\nlatentvariablesbeforeobserving. h x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "latentvariablesbeforeobserving. h x\nIn f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994\npriorisjusttheunitvarianceGaussian\nh 0 âˆ¼N(; h , I) (13.3)\nwhiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,\ngiven h.Speciï¬cally,Â theÂ noiseisassumedÂ tobedrawnfromaÂ diagonalco-\nvarianceÂ Gaussian distribution,withÂ covariancematrix Ïˆ=diag( Ïƒ2),with\nÏƒ2= [ Ïƒ2\n1 , Ïƒ2\n2 , . . . , Ïƒ2\nn]î€¾avectorofper-variablevariances.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "2 , . . . , Ïƒ2\nn]î€¾avectorofper-variablevariances.\nTheroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween\nthediï¬€erentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta\nmultivariatenormalrandomvariable,with\nxâˆ¼N(; x b W W ,î€¾+) Ïˆ . (13.4)\n490",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nInordertocastPCAinaprobabilisticframework,Â wecanmakeaslight\nmodiï¬cationtothefactoranalysismodel,makingtheconditionalvariances Ïƒ2\ni\nequaltoeachother.Inthatcasethecovarianceof xisjust W Wî€¾+ Ïƒ2I,where\nÏƒ2isnowascalar.Thisyieldstheconditionaldistribution\nxâˆ¼N(; x b W W ,î€¾+ Ïƒ2I) (13.5)\norequivalently\nx h z = W ++ b Ïƒ (13.6)\nwhere zâˆ¼N( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999\niterativeEMalgorithmforestimatingtheparameters and W Ïƒ2.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "This pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost\nvariationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall\nresidual r e c o nst r u c t i o n e r r o r Ïƒ2.Asshownby (), TippingandBishop1999\nprobabilisticPCAbecomesPCAas Ïƒâ†’0.Inthatcase,theconditionalexpected\nvalueof hgiven xbecomesanorthogonalprojectionof x bâˆ’ontothespace\nspannedbythecolumnsof,likeinPCA. d W\nAs Ïƒâ†’0,thedensitymodeldeï¬nedbyprobabilisticPCAbecomesverysharp",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "aroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe\nmodelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster\nnearahyperplane.\n13.2IndependentComponentAnalysis(ICA)\nIndependentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\nalgorithms( ,; ,;Â ,; HeraultÂ andAns1984JuttenÂ andHerault1991Comon1994\nHyvÃ¤rinen1999HyvÃ¤rinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,).\nItisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\ntheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\nmerelydecorrelatedfromeachother.1\nManydiï¬€erentspeciï¬cmethodologiesarereferredtoasICA.Thevariant\nthatismostsimilartotheothergenerativemodelswehavedescribedhereisa\nvariant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992\npriordistributionovertheunderlyingfactors, p( h),mustbeï¬xedaheadoftimeby",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma\n1Seesectionforadiscussionofthediï¬€erencebetweenuncorrelatedvariablesandindepen- 3.8\ndentvariables.\n491",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nnonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning\nthemodelthenproceedsasusual,usingmaximumlikelihood.\nThemotivationforthisapproachisthatbychoosing p( h)tobeindependent,\nwecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.\nThisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto\nrecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each\ntrainingexampleisonemomentintime,each x iisonesensorâ€™sobservationof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "themixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For\nexample,wemighthave npeoplespeakingsimultaneously.Ifwehave ndiï¬€erent\nmicrophonesplacedindiï¬€erentlocations,ICAcandetectthechangesinthevolume\nbetweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso\nthateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused\ninneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical\nsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubjectâ€™s",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "headareusedtomeasuremanyelectricalsignalscomingfromthebody.The\nexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom\nthesubjectâ€™sheartandeyesarestrongenoughtoconfoundmeasurementstaken\natthesubjectâ€™sscalp.Thesignalsarriveattheelectrodesmixedtogether,so\nICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals\noriginatinginthebrain,andtoseparatesignalsindiï¬€erentbrainregionsfrom\neachother.\nAsmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "inthegenerationof xratherthanusingadeterministicdecoder.Mostdonot\nusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\nh= Wâˆ’ 1xindependentfromeachother.Manycriteriathataccomplishthisgoal\narepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe\nanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\nproblematicoperationbyconstrainingtobeorthogonal. W\nAllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "isanindependentpriorwithGaussiancomponents,then Wisnotidentiï¬able.\nWecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery\ndiï¬€erentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\nthatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe\nmodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe\nuserexplicitlyspeciï¬esthedistribution,atypicalchoiceistouse p( h i) =d\nd h iÏƒ( h i).",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "d h iÏƒ( h i).\nTypicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\ndoestheGaussiandistribution,sowecanalsoseemostimplementations ofICA\naslearningsparsefeatures.\n492",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\nphrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples\nfromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but\ndonothaveanywayofrepresenting p( h),andthusdonotimposeadistribution\nover p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\nh= Wâˆ’ 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "accomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore\noftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\ndataorestimatingitsdensity.\nJustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\nchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\nweuseanonlinearfunction ftogeneratetheobserveddata.SeeHyvÃ¤rinenand\nPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "ensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000\nAnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent\nc o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014\nofinvertibletransformations(encoderstages)thathavethepropertythatthe\ndeterminantoftheJacobianofeachtransformationcanbecomputedeï¬ƒciently.\nThismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "totransformthedataintoaspacewhereithasafactorizedmarginaldistribution,\nbutismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\nisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto\ngeneratesamplesfromthemodel(byï¬rstsamplingfrom p( h)andthenapplying\nthedecoder).\nAnothergeneralization ofICAistolearngroupsoffeatures,withstatistical\ndependenceallowedwithinagroupbutdiscouragedbetweengroups(HyvÃ¤rinenand",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Hoyer1999HyvÃ¤rinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen\ntobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso\npossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\ngroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\nfeatures.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns\nGaborï¬lters,suchthatneighboringfeatureshavesimilarorientation,locationor",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "frequency.Manydiï¬€erentphaseoï¬€setsofsimilarGaborfunctionsoccurwithin\neachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\n13.3SlowFeatureAnalysis\nSl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom\n493",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ntimesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\nSlowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\nprinciple.Theideaisthattheimportantcharacteristicsofsceneschangevery\nslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\nscene.Forexample,incomputervision,individualpixelvaluescanchangevery\nrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "willrapidlychangefromblacktowhiteandbackagainasthezebraâ€™sstripespass\noverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe\nimagewillnotchangeatall,andthefeaturedescribingthezebraâ€™spositionwill\nchangeslowly.Â Wethereforemaywishtoregularizeourmodeltolearnfeatures\nthatchangeslowlyovertime.\nTheslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\ntoawidevarietyofmodels(,;,; ,; Hinton1989FÃ¶ldiÃ¡k1989Mobahi e t a l .2009",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\ndiï¬€erentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\nintroducedbyaddingatermtothecostfunctionoftheform\nÎ»î˜\ntL f(( x( + 1 ) t)( , f x( ) t)) (13.7)\nwhere Î»isahyperparameter determiningthestrengthoftheslownessregularization\nterm, tistheindexintoatimesequenceofexamples, fisthefeatureextractor\ntoberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "and f( x( + 1 ) t).Acommonchoiceforisthemeansquareddiï¬€erence. L\nSlowfeatureanalysisisaparticularlyeï¬ƒcientapplicationoftheslowness\nprinciple.Itiseï¬ƒcientbecauseitisappliedtoalinearfeatureextractor,andcan\nthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\ngenerativemodelperse,inthesensethatitdeï¬nesalinearmapbetweeninput\nspaceandfeaturespacebutdoesnotdeï¬neaprioroverfeaturespaceandthus\ndoesnotimposeadistributiononinputspace. p() x",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "doesnotimposeadistributiononinputspace. p() x\nTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeï¬ning f( x; Î¸)\ntobealineartransformation,andsolvingtheoptimization problem\nmin\nÎ¸E t(( f x( + 1 ) t) iâˆ’ f( x( ) t) i)2(13.8)\nsubjecttotheconstraints\nE t f( x( ) t) i= 0 (13.9)\nand\nE t[( f x( ) t)2\ni] = 1 . (13.10)\n494",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\nproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\nvaluesandobtainadiï¬€erentsolutionwithequalvalueoftheslownessobjective.\nTheconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\npathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\nareordered,withtheï¬rstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\nmustalsoaddtheconstraint",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "mustalsoaddtheconstraint\nâˆ€ i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11)\nThisspeciï¬esthatthelearnedfeaturesmustbelinearlydecorrelated fromeach\nother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe\noneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\nreconstructionerror,Â toÂ forcetheÂ featurestodiversify,Â butÂ thisdecorrelation\nmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\nproblemmaybesolvedinclosedformbyalinearalgebrapackage.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\nexpansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe\nquadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear\nSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors\nbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis\nexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron\ntopofthatexpansion.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "topofthatexpansion.\nWhentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\nquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\nthoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\nonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep\nSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\nbyneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA\nthusseemstobeareasonablybiologicallyplausiblemodel.",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich\nfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical\npredictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\nconï¬guration spaceÂ (e.g.,Â intheÂ caseofrandomÂ motion intheÂ 3-Drendered\nenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\ndistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\ntheunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\nappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\n495",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nThisisincomparisontootherlearningalgorithmswherethecostfunctiondepends\nhighlyonspeciï¬cpixelvalues,makingitmuchmorediï¬ƒculttodeterminewhat\nfeaturesthemodelwilllearn.\nDeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\nestimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome\nthebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited\nitsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\nitwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\nonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\nwhethertheobjectâ€™svelocityishighorlow,buttheslownessprincipleencourages\nthemodeltoignorethepositionofobjectsthathavehighvelocity.\n13.4SparseCoding\nSpar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996\nbeenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "mechanism.Â Strictlyspeaking,thetermâ€œsparsecodingâ€referstotheprocessof\ninferringthevalueof hinthismodel,whileâ€œsparsemodelingâ€referstotheprocess\nofdesigningandlearningthemodel,butthetermâ€œsparsecodingâ€isoftenusedto\nrefertoboth.\nLikemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\nobtainreconstructionsof x,asspeciï¬edinequation.Morespeciï¬cally,sparse 13.2\ncodingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\nisotropicprecision: Î²\np , ( ) = (; + x h| N x W h b1\nÎ²I) . (13.12)",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "p , ( ) = (; + x h| N x W h b1\nÎ²I) . (13.12)\nThedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen\nandField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\nStudent- tdistributions.Forexample,theLaplacepriorparametrized intermsof\nthesparsitypenaltycoeï¬ƒcientisgivenby Î»\np h( i) = Laplace( h i;0 ,2\nÎ») =Î»\n4eâˆ’1\n2Î» h| i|(13.13)\nandtheStudent-priorby t\np h( i) âˆ1\n(1+h2\ni\nÎ½)Î½ +1\n2. (13.14)\n496",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\ntrainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\nreconstructthedatagiventheencoding.Thisapproachwillbejustiï¬edfurtheras\naprincipledapproximation tomaximumlikelihoodlater,insection.19.3\nFormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\nthatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\nisanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek\nthesinglemostlikelycodevalue:\nhâˆ—= () = argmax f x\nhp . ( ) h x| (13.15)\nWhencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12\noptimization problem:\nargmax\nhp( ) h x| (13.16)\n=argmax\nhlog( ) p h x| (13.17)\n=argmin\nhÎ»|||| h 1+ Î²||âˆ’ || x W h2\n2 , (13.18)\nwherewehavedroppedtermsnotdependingon handdividedbypositivescaling",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "factorstosimplifytheequation.\nDuetotheimpositionofan L1normon h,thisprocedurewillyieldasparse\nhâˆ—(Seesection).7.1.2\nTotrainthemodelratherthanjustperforminference,wealternatebetween\nminimization withrespectto handminimization withrespectto W.Inthis\npresentation,wetreat Î²asahyperparameter.Typicallyitissetto1becauseits\nroleinthisoptimization problemissharedwith Î»andthereisnoneedforboth\nhyperparameters.Inprinciple,wecouldalsotreat Î²asaparameterofthemodel",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\non hbutdodependon Î².Tolearn Î²,thesetermsmustbeincluded,or Î²will\ncollapseto.0\nNotallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|).\nOftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\nvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\nIfwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor\nanelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "sparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d\n497",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\ninferenceinadiï¬€erentmodelfamily,thespikeandslabsparsecodingmodel,for\nwhichsamplesfromthepriorusuallycontaintruezeros.\nThesparsecodingapproachcombinedwiththeuseofthenon-parametric\nencodercaninprincipleminimizethecombinationofreconstructionerrorand\nlog-priorbetterthananyspeciï¬cparametricencoder.Anotheradvantageisthat\nthereisnogeneralization errortotheencoder.Aparametricencodermustlearn\nhowtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "thetrainingdata,alearned,parametricencodermayfailtoï¬ndan hthatresults\ninaccuratereconstructionorasparsecode.Forthevastmajorityofformulations\nofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\nprocedurewillalwaysï¬ndtheoptimalcode(unlessdegeneratecasessuchas\nreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\ncanstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe\ndecoderweights,ratherthangeneralization errorintheencoder.Thelackof",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "generalization errorinsparsecodingâ€™soptimization-based encodingprocessmay\nresultinbettergeneralization whensparsecodingisusedasafeatureextractorfor\naclassiï¬erthanwhenaparametricfunctionisusedtopredictthecode.Coates\nandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\nobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\nencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l .",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\nextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\nlabelsperclass).\nTheprimarydisadvantageofthenon-parametric encoderisthatitrequires\ngreatertimetocompute hgiven xbecausethenon-parametric approachrequires\nrunninganiterativealgorithm.Theparametricautoencoderapproach,developed\ninÂ chapterÂ ,usesonlyÂ aÂ ï¬xedÂ n umberÂ ofÂ layers,Â oftenÂ onlyÂ one.Another 14",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe\nnon-parametric encoder,whichmakesitdiï¬ƒculttopretrainasparsecodingmodel\nwithanunsupervisedcriterionandthenï¬ne-tuneitusingasupervisedcriterion.\nModiï¬edversionsofsparsecodingthatpermitapproximate derivativesdoexist\nbutarenotwidelyused( ,). BagnellandBradley2009\nSparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\nshowninï¬gure.Thishappensevenwhenthemodelisabletoreconstruct 13.2",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thedatawellandprovideusefulfeaturesforaclassiï¬er.Thereasonisthateach\nindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\nresultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated\nsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-\n498",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nFigure13.2:Â Example samplesandweightsfromaspikeandslabsparsecodingmodel\ntrainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe\ntrainingexamples.Atï¬rstglance,onemightassumethemodelispoorlyï¬t.The ( R i g h t )\nweightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\ndigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\noverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\ngenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\nreproducedwithpermissionfromGoodfellow2013d e t a l .().\nfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore\nsophisticatedshallowmodels.\n13.5ManifoldInterpretationofPCA\nLinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\nlearningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "deï¬ningathinpancake-shapedregionofhighprobabilityâ€”aGaussiandistribution\nthatisverynarrowalongsomeaxes,justasapancakeisveryï¬‚atalongitsvertical\naxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\naxes.Â Thisisillustratedinï¬gure.Â PCAcanbeinterpretedasaligningthis 13.3\npancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation\nappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\nmatrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "xaspossible,\nLettheencoderbe\nh x W = ( f) = î€¾( ) x Âµâˆ’ . (13.19)\n499",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\nTheencodercomputesalow-dimensional representationof h.Withtheautoencoder\nview,wehaveadecodercomputingthereconstruction\nË† x h b V h = ( g) = + . (13.20)\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\nmanifold.Theï¬gureshowstheupperhalfoftheâ€œpancakeâ€abovetheâ€œmanifoldplaneâ€\nwhichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis\nverysmall(arrowpointingoutofplane)andcanbeconsideredlikeâ€œnoise,â€whiletheother",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "variancesarelarge(arrowsintheplane)andcorrespondtoâ€œsignal,â€andacoordinate\nsystemforthereduced-dimensiondata.\nThechoicesoflinearencoderanddecoderthatminimizereconstructionerror\nE[||âˆ’ xË† x||2] (13.21)\ncorrespondto V= W, Âµ= b= E[ x]andthecolumnsof Wformanorthonormal\nbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\nmatrix\nC x Âµ x Âµ = [( E âˆ’)(âˆ’)î€¾] . (13.22)\nInthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\nOnecanalsoshowthateigenvalue Î» iof Ccorrespondstothevarianceof x\ninthedirectionofeigenvector v( ) i.If xâˆˆ RDand hâˆˆ Rdwith d < D,thenthe\n500",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER13.LINEARFACTORMODELS\noptimalreconstructionerror(choosing,,andasabove)is Âµ b V W\nmin[ E||âˆ’ xË† x||2] =D î˜\ni d = + 1Î» i . (13.23)\nHence,ifthecovariancehasrank d,theeigenvalues Î» d + 1to Î» Dare0andrecon-\nstructionerroris0.\nFurthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\nmaximizingthevariancesoftheelementsof h,underorthogonal W,insteadof\nminimizingreconstructionerror.\nLinearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "simplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiï¬ersand\nlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\nfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\nmodelsthatperformthesametasksbutwithamuchmorepowerfulandï¬‚exible\nmodelfamily.\n501",
    "metadata": {
      "source": "[19]part-3-chapter-13.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "Deep L ea r ni n g\nI a n G o o d f e l l o w\nY o s h u a B e n g i o\nA a r o n C o u r v i l l e",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "C on t e n t s\nWebsite vii\nAcknowledgments viii\nNotation xi\n1Introduction 1\n1.1WhoShouldReadThisBook?.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .8\n1.2HistoricalTrendsinDeepLearning.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 11\nIAppliedMathandMachineLearningBasics 29\n2LinearAlgebra 31\n2.1Scalars,Vectors,MatricesandTensors.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .31\n2.2MultiplyingMatricesandVectors.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .34\n2.3IdentityandInverseMatrices.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .36",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "2.4LinearDependenceandSpan.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .37\n2.5Norms.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 39\n2.6SpecialKindsofMatricesandVectors.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . 40\n2.7Eigendecomposition.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 42\n2.8SingularValueDecomposition.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .44\n2.9TheMoore-PenrosePseudoinverse.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .45",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "2.10TheTraceOperator.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 46\n2.11TheDeterminant.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . 47\n2.12Example:PrincipalComponentsAnalysis.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .48\n3ProbabilityandInformationTheory 53\n3.1WhyProbability?.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .54\ni",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n3.2RandomVariables.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .56\n3.3ProbabilityDistributions.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .56\n3.4MarginalProbability.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â . 58\n3.5ConditionalProbability.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .59\n3.6TheChainRuleofConditionalProbabilities.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .59\n3.7IndependenceandConditionalIndependence.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .60",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "3.8Expectation,VarianceandCovariance.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .60\n3.9CommonProbabilityDistributions.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .62\n3.10UsefulPropertiesofCommonFunctions.Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .67\n3.11Bayesâ€™Rule.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .70\n3.12TechnicalDetailsofContinuousVariables.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â . 71\n3.13InformationTheory.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 73\n3.14StructuredProbabilisticModels.Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â . 75",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "4NumericalComputation 80\n4.1Overï¬‚owandUnderï¬‚ow.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .80\n4.2PoorConditioning .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . 82\n4.3Gradient-BasedOptimization .Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .82\n4.4ConstrainedOptimization .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .93\n4.5Example:LinearLeastSquares.Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .96\n5MachineLearningBasics 98\n5.1LearningAlgorithms.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .99",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "5.2Capacity,Overï¬ttingandUnderï¬tting.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .110\n5.3HyperparametersandValidationSets..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .120\n5.4Estimators,BiasandVariance.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .122\n5.5MaximumLikelihoodEstimation.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .131\n5.6BayesianStatistics.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .135\n5.7SupervisedLearningAlgorithms.Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 140\n5.8UnsupervisedLearningAlgorithms.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .146",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "5.9StochasticGradientDescent.Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 151\n5.10BuildingaMachineLearningAlgorithm.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .153\n5.11ChallengesMotivatingDeepLearning.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .155\nIIDeepNetworks:ModernPractices 166\n6DeepFeedforwardNetworks 168\n6.1Example:LearningXOR..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .171\n6.2Gradient-BasedLearning..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .177\ni i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n6.3HiddenUnits.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .191\n6.4ArchitectureDesign.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .197\n6.5Back-PropagationandOtherDiï¬€erentiationAlgorithms.Â .Â .Â .Â .204\n6.6HistoricalNotes.Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .224\n7RegularizationforDeepLearning 228\n7.1ParameterNormPenalties.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 230\n7.2NormPenaltiesasConstrainedOptimization .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .237",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "7.3RegularizationandUnder-ConstrainedProblems.Â ..Â .Â .Â .Â .Â .Â .239\n7.4DatasetAugmentation.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .240\n7.5NoiseRobustness.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .242\n7.6Semi-SupervisedLearning.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .243\n7.7Multi-TaskLearning.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .244\n7.8EarlyStopping.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .246",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "7.9ParameterTyingandParameterSharingÂ .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . 253\n7.10SparseRepresentations.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .254\n7.11BaggingandOtherEnsembleMethods..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .256\n7.12Dropout.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .258\n7.13AdversarialTraining.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 268\n7.14TangentDistance,TangentProp,andManifoldTangentClassiï¬er270\n8OptimizationforTrainingDeepModels 274",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "8OptimizationforTrainingDeepModels 274\n8.1HowLearningDiï¬€ersfromPureOptimization .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . 275\n8.2ChallengesinNeuralNetworkOptimization .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .282\n8.3BasicAlgorithms.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .294\n8.4ParameterInitialization Strategies..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .301\n8.5AlgorithmswithAdaptiveLearningRates.Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .306\n8.6ApproximateSecond-Order Methods.Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .310",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "8.7Optimization StrategiesandMeta-Algorithms.Â .Â .Â .Â ..Â .Â .Â .Â .Â .317\n9ConvolutionalNetworks 330\n9.1TheConvolutionOperation.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .331\n9.2Motivation.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .335\n9.3Pooling.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .339\n9.4ConvolutionandPoolingasanInï¬nitelyStrongPrior.Â ..Â .Â .Â .Â .345\n9.5VariantsoftheBasicConvolutionFunction.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . 347",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "9.6StructuredOutputs..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 358\n9.7DataTypes.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 360\n9.8Eï¬ƒcientConvolutionAlgorithms.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .362\n9.9RandomorUnsupervisedFeatures.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .363\ni i i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n9.10TheNeuroscientiï¬cBasisforConvolutionalNetworks.Â .Â .Â .Â .Â ..364\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning.Â .Â .Â .371\n10Â SequenceModeling:RecurrentandRecursiveNets373\n10.1UnfoldingComputational Graphs.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .375\n10.2RecurrentNeuralNetworks.Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .378\n10.3BidirectionalRNNsÂ .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .394\n10.4Encoder-DecoderSequence-to-SequenceArchitectures.Â .Â .Â .Â .Â ..396",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "10.5DeepRecurrentNetworks.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .398\n10.6RecursiveNeuralNetworks.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 400\n10.7TheChallengeofLong-TermDependencies.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .401\n10.8EchoStateNetworks.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .404\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales.Â .Â ..406\n10.10Â TheLongShort-TermMemoryandOtherGatedRNNs.Â ..Â .Â .Â .408\n10.11Â Optimization forLong-TermDependencies.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .413",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "10.12Â Explicit Memory.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 416\n11Â PracticalMethodology 421\n11.1PerformanceMetrics.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .422\n11.2DefaultBaselineModels.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .425\n11.3DeterminingWhethertoGatherMoreData.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . 426\n11.4SelectingHyperparameters.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .427\n11.5DebuggingStrategies.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .436",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "11.6Example:Multi-DigitNumberRecognition.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . 440\n12Â Applications 443\n12.1Large-ScaleDeepLearning..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .443\n12.2ComputerVision.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .452\n12.3SpeechRecognitionÂ .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .458\n12.4NaturalLanguageProcessing.Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .461\n12.5OtherApplications.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .478\nIIIDeepLearningResearch 486",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "IIIDeepLearningResearch 486\n13Â LinearFactorModels 489\n13.1ProbabilisticPCAandFactorAnalysis.Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . 490\n13.2IndependentComponentAnalysis(ICA).Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .491\n13.3SlowFeatureAnalysis.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .493\n13.4SparseCoding.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .496\ni v",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n13.5ManifoldInterpretation ofPCA.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .499\n14Â Autoencoders 502\n14.1Undercomplete Autoencoders.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .503\n14.2RegularizedAutoencoders.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .504\n14.3RepresentationalPower,LayerSizeandDepth.Â .Â .Â .Â .Â ..Â .Â .Â .Â .508\n14.4StochasticEncodersandDecoders.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .509\n14.5DenoisingAutoencoders.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .510",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "14.6LearningManifoldswithAutoencoders.Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . 515\n14.7ContractiveAutoencoders..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .521\n14.8PredictiveSparseDecomposition.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .523\n14.9ApplicationsofAutoencoders.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .524\n15Â RepresentationLearning 526\n15.1GreedyLayer-WiseUnsupervisedPretraining.Â .Â .Â .Â .Â ..Â .Â .Â .Â .528\n15.2TransferLearningandDomainAdaptation.Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .536",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "15.3Semi-SupervisedDisentanglingofCausalFactors.Â .Â .Â .Â ..Â .Â .Â .541\n15.4DistributedRepresentation.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .546\n15.5ExponentialGainsfromDepth.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .553\n15.6ProvidingCluestoDiscoverUnderlyingCauses.Â .Â .Â ..Â .Â .Â .Â .Â .554\n16Â StructuredProbabilisticModelsforDeepLearning558\n16.1TheChallengeofUnstructuredModeling..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .559\n16.2UsingGraphstoDescribeModelStructure.Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .563",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "16.3SamplingfromGraphicalModels.Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .580\n16.4AdvantagesofStructuredModelingÂ ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .582\n16.5LearningaboutDependencies.Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 582\n16.6InferenceandApproximateInference.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .584\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\n17Â MonteCarloMethods 590\n17.1SamplingandMonteCarloMethods.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 590",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "17.2ImportanceSampling.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .592\n17.3MarkovChainMonteCarloMethods.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .595\n17.4GibbsSamplingÂ .Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .599\n17.5TheChallengeofMixingbetweenSeparatedModes.Â .Â .Â .Â .Â .Â ..599\n18Â ConfrontingthePartitionFunction 605\n18.1TheLog-LikelihoodGradient..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .606\n18.2StochasticMaximumLikelihoodandContrastiveDivergence.Â .Â .607\nv",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\n18.3Pseudolikelihood.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .615\n18.4ScoreMatchingandRatioMatching.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 617\n18.5DenoisingScoreMatching.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .619\n18.6Noise-ContrastiveEstimation.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .620\n18.7EstimatingthePartitionFunction.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .623\n19Â ApproximateInference 631\n19.1InferenceasOptimization ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .633",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "19.2ExpectationMaximization .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .634\n19.3MAPInferenceandSparseCoding..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .635\n19.4VariationalInferenceandLearning.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .638\n19.5LearnedApproximateInference.Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .651\n20Â DeepGenerativeModels 654\n20.1BoltzmannMachines.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .654\n20.2RestrictedBoltzmannMachines.Â .Â .Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .656",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "20.3DeepBeliefNetworks..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 660\n20.4DeepBoltzmannMachines.Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .663\n20.5BoltzmannMachinesforReal-ValuedData.Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .676\n20.6ConvolutionalBoltzmannMachinesÂ .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .683\n20.7BoltzmannMachinesforStructuredorSequentialOutputs.Â .Â .Â .685\n20.8OtherBoltzmannMachines.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 686\n20.9Back-PropagationthroughRandomOperations.Â .Â .Â .Â .Â ..Â .Â .Â .687",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "20.10Â DirectedGenerativeNets.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .692\n20.11Â DrawingSamplesfromAutoencoders.Â .Â .Â .Â ..Â .Â .Â .Â .Â .Â .Â .Â . .Â .711\n20.12Â Generativ eStochasticNetworks.Â .Â ..Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . 714\n20.13Â OtherGenerationSchemes.Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . 716\n20.14Â EvaluatingGenerativeModelsÂ .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .717\n20.15Â Conclus ion.Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .Â .Â . .Â .Â .Â .Â .Â .720\nBibliography 721\nIndex 777\nv i",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "W e b s i t e\nwww.deeplearningb ook.org\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\nvii",
    "metadata": {
      "source": "[1]table-of-contents.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 2\nL i n e ar A l ge b ra\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\nthekeylinearalgebraprerequisites.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "thekeylinearalgebraprerequisites.\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Shilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\ntopicsthatarenotessentialforunderstandingdeeplearning.\n2.1Scalars,Vectors,MatricesandTensors\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\nâ€¢Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\n31",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nexample,wemightsayâ€œLet sâˆˆ Rbetheslopeoftheline,â€whiledeï¬ninga\nreal-valuedscalar,orâ€œLet nâˆˆ Nbethenumberofunits,â€whiledeï¬ninga\nnaturalnumberscalar.\nâ€¢Vectors:Â Avectorisanarrayofnumbers.Thenumbersarearrangedin\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\nasx.Theelementsofthevectorareidentiï¬edbywritingitsnameinitalic\ntypeface,withasubscript.Theï¬rstelementofxis x 1,thesecondelement",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "is x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\nwewritethemasacolumnenclosedinsquarebrackets:\nx=ï£®\nï£¯ï£¯ï£¯ï£°x 1\nx 2\n...\nx nï£¹\nï£ºï£ºï£ºï£». (2.1)\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\ngivingthecoordinatealongadiï¬€erentaxis.\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "deï¬neasetcontainingtheindicesandwritethesetasasubscript.For\nexample,toaccess x 1, x 3and x 6,wedeï¬netheset S={1 ,3 ,6}andwrite\nx S.Weusetheâˆ’signtoindexthecomplementofaset.Forexamplex âˆ’ 1is\nthevectorcontainingallelementsofxexceptfor x 1,andx âˆ’ Sisthevector\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\nâ€¢Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiï¬ed\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "aheightof mandawidthof n,thenwesaythatAâˆˆ Rm n Ã—.Â Weusually\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\nofthenumberswithverticalcoordinate ibywritingaâ€œâ€forthehorizontal :\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\n3 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nA =ï£®\nï£°A 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,\nA 3 1 , A 3 2 ,ï£¹\nï£» â‡’ Aî€¡=î€¥A 1 1 , A 2 1 , A 3 1 ,\nA 1 2 , A 2 2 , A 3 2 ,î€¦\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\nmaindiagonal.\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\nî€”A 1 1 , A 1 2 ,\nA 2 1 , A 2 2 ,î€•\n. (2.2)\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\nofthematrixcomputedbyapplyingthefunctionto. fA\nâ€¢Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamedâ€œAâ€\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\nbywriting A i , j , k.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "bywriting A i , j , k.\nOneimportantoperationonmatricesisthetranspose.Â Thetransposeofa\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\nï¬gureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\nmatrixasAAî€¾,anditisdeï¬nedsuchthat\n(Aî€¾) i , j= A j , i . (2.3)\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\n3 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ndeï¬neavectorbywritingoutitselementsinthetextinlineasarowmatrix,\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\nx= [ x 1 , x 2 , x 3]î€¾.\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\ncanseethatascalarisitsowntranspose: a a= î€¾.\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "byperformingthatoperationoneachelementofamatrix:D= aÂ·B+ cwhere\nD i , j= a BÂ· i , j+ c.\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\nmatrix.Thisshorthandeliminatestheneedtodeï¬neamatrixwithbcopiedinto\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\niscalled .broadcasting\n2.2MultiplyingMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "2.2MultiplyingMatricesandVectors\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\norderforthisproducttobedeï¬ned,Amusthavethesamenumberofcolumnsas\nBhasrows.IfAisofshape m nÃ—andBisofshape n pÃ—,thenCisofshape\nm pÃ—.Wecanwritethematrixproductjustbyplacingtwoormorematrices\ntogether,e.g.\nCAB= . (2.4)\nTheproductoperationisdeï¬nedby\nC i , j=î˜\nkA i , k B k, j . (2.5)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "C i , j=î˜\nkA i , k B k, j . (2.5)\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\nelement-wiseproductHadamardproduct or ,andisdenotedas.ABî€Œ\nThedotproductbetweentwovectorsxandyofthesamedimensionality\nisthematrixproductxî€¾y.WecanthinkofthematrixproductC=ABas\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\n3 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\nanalysisÂ ofmatricesÂ moreconvenient.ForÂ example,Â matrixÂ m ultiplicationÂ is\ndistributive:\nABCABAC (+) = + . (2.6)\nItisalsoassociative:\nABCABC ( ) = ( ) . (2.7)\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\nvectorsiscommutative:\nxî€¾yy= î€¾x . (2.8)\nThetransposeofamatrixproducthasasimpleform:\n( )ABî€¾= Bî€¾Aî€¾. (2.9)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "( )ABî€¾= Bî€¾Aî€¾. (2.9)\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\nxî€¾y=î€\nxî€¾yî€‘î€¾\n= yî€¾x . (2.10)\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\nthereadershouldbeawarethatmanymoreexist.\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\nequations:\nAxb= (2.11)\nwhereAâˆˆ Rm n Ã—isaknownmatrix,bâˆˆ Rmisaknownvector,andxâˆˆ Rnisa",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "vectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\nconstraint.Wecanrewriteequationas:2.11\nA 1 : ,x= b 1 (2.12)\nA 2 : ,x= b 2 (2.13)\n. . . (2.14)\nA m , :x= b m (2.15)\nor,evenmoreexplicitly,as:\nA 1 1 , x 1+A 1 2 , x 2+ +Â·Â·Â·A 1 , n x n= b 1 (2.16)\n3 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nï£®\nï£°100\n010\n001ï£¹\nï£»\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\nA 2 1 , x 1+A 2 2 , x 2+ +Â·Â·Â·A 2 , n x n= b 2 (2.17)\n. . . (2.18)\nA m , 1 x 1+A m , 2 x 2+ +Â·Â·Â·A m , n x n= b m . (2.19)\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\nequationsofthisform.\n2.3IdentityandInverseMatrices\nLinearalgebraoï¬€ersapowerfultoolcalledmatrixinversionthatallowsusto\nanalyticallysolveequationformanyvaluesof. 2.11 A",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "analyticallysolveequationformanyvaluesof. 2.11 A\nTodescribematrixinversion,weï¬rstneedtodeï¬netheconceptofanidentity\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\nn-dimensionalvectorsasI n.Formally,I nâˆˆ Rn n Ã—,and\nâˆ€âˆˆx Rn,I nxx= . (2.20)\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\ndiagonalare1,whilealloftheotherentriesarezero.Seeï¬gureforanexample.2.2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "ThematrixinverseofAisdenotedasAâˆ’ 1,anditisdeï¬nedasthematrix\nsuchthat\nAâˆ’ 1AI= n . (2.21)\nWecannowsolveequationbythefollowingsteps: 2.11\nAxb= (2.22)\nAâˆ’ 1AxA= âˆ’ 1b (2.23)\nI nxA= âˆ’ 1b (2.24)\n3 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nxA= âˆ’ 1b . (2.25)\nOfcourse,thisprocessdependsonitbeingpossibletoï¬ndAâˆ’ 1.Wediscuss\ntheconditionsfortheexistenceofAâˆ’ 1inthefollowingsection.\nWhenAâˆ’ 1exists,severaldiï¬€erentalgorithmsexistforï¬ndingitinclosedform.\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\ntimesfordiï¬€erentvaluesofb.However,Aâˆ’ 1isprimarilyusefulasatheoretical\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "BecauseAâˆ’ 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\nestimatesof.x\n2.4LinearDependenceandSpan\nInorderforAâˆ’ 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\nsolutionsorinï¬nitelymanysolutionsforsomevaluesofb.Â Itisnotpossibleto\nhavemorethanonebutlessthaninï¬nitelymanysolutionsforaparticularb;if\nbothandaresolutionsthen xy\nzxy = Î±+(1 )âˆ’ Î± (2.26)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "bothandaresolutionsthen xy\nzxy = Î±+(1 )âˆ’ Î± (2.26)\nisalsoasolutionforanyreal. Î±\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\nofAasspecifyingdiï¬€erentdirectionswecantravelfromtheorigin(thepoint\nspeciï¬edbythevectorofallzeros),anddeterminehowmanywaysthereareof\nreachingb.Inthisview,eachelementofxspeciï¬eshowfarweshouldtravelin\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\ncolumn: i\nAx=î˜\nix iA : , i . (2.27)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "column: i\nAx=î˜\nix iA : , i . (2.27)\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\neachvectorv( ) ibyacorrespondingscalarcoeï¬ƒcientandaddingtheresults:\nî˜\nic iv( ) i. (2.28)\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\noftheoriginalvectors.\n3 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\nspacerangeortheof.A\nInorderforthesystemAx=btohaveasolutionforallvaluesofbâˆˆ Rm,\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\nimmediately thatAmusthaveatleast mcolumns,i.e., n mâ‰¥.Â Otherwise, the",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "dimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\n3Ã—2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\nifandonlyifliesonthatplane.b\nHaving n mâ‰¥isonlyanecessaryconditionforeverypointtohaveasolution.\nItisnotasuï¬ƒcientcondition,becauseitispossibleforsomeofthecolumnsto\nberedundant.Considera2Ã—2matrixwherebothofthecolumnsareidentical.\nThishasthesamecolumnspaceasa2Ã—1matrixcontainingonlyonecopyofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "replicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\nencompassallof R2,eventhoughtherearetwocolumns.\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\noftheothervectors.Â Ifweaddavectortoasetthatisalinearcombinationof\ntheothervectorsintheset,thenewvectordoesnotaddanypointstothesetâ€™s\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\nconditionisbothnecessaryandsuï¬ƒcientforequationtohaveasolutionfor 2.11\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\nmcolumnsmayhavemorethanonesuchset.\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "equationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\nwayofparametrizing eachsolution.\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\nwithlinearlydependentcolumnsisknownas.singular\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\nequation.However,wecannotusethemethodofmatrixinversiontoï¬ndthe\n3 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nsolution.\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\nalsopossibletodeï¬neaninversethatismultipliedontheright:\nAAâˆ’ 1= I . (2.29)\nForsquarematrices,theleftinverseandrightinverseareequal.\n2.5Norms\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\nisgivenby\n||||x p=î€ î˜\ni| x i|pî€¡ 1\np\n(2.30)\nfor p , p . âˆˆ Râ‰¥1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "||||x p=î€ î˜\ni| x i|pî€¡ 1\np\n(2.30)\nfor p , p . âˆˆ Râ‰¥1\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatisï¬es\nthefollowingproperties:\nâ€¢ â‡’ f() = 0 xx= 0\nâ€¢ â‰¤ f(+) xy f f ()+x ()y(thetriangleinequality)\nâ€¢âˆ€âˆˆ || Î± R , f Î±(x) = Î± f()x\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\nEuclideandistancefromtheorigintothepointidentiï¬edbyx.The L2normis",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "usedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\nthesquared L2norm,whichcanbecalculatedsimplyasxî€¾x.\nThesquared L2normismoreconvenienttoworkwithmathematically and\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\n3 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\nthe L1norm.The L1normmaybesimpliï¬edto\n||||x 1=î˜\ni| x i| . (2.31)\nThe L1normiscommonlyusedinmachinelearningwhenthediï¬€erencebetween\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\nawayfrom0by,the î€ L1normincreasesby. î€",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "awayfrom0by,the î€ L1normincreasesby. î€\nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\nelements.Someauthorsrefertothisfunctionastheâ€œ L0norm,â€butthisisincorrect\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\nscalingthevectorby Î±doesnotchangethenumberofnonzeroentries.Â The L1\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\nOneothernormthatcommonlyarisesinmachinelearningisthe Lâˆžnorm,\nalsoknownasthemaxnorm.Thisnormsimpliï¬estotheabsolutevalueofthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "elementwiththelargestmagnitudeinthevector,\n||||x âˆž= max\ni| x i| . (2.32)\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\nFrobeniusnorm:\n|||| A F=î³î˜\ni , jA2\ni , j , (2.33)\nwhichisanalogoustothe L2normofavector.\nThedotproductoftwovectorscanberewrittenintermsofnorms.Speciï¬cally,\nxî€¾yx= |||| 2||||y 2cos Î¸ (2.34)\nwhereistheanglebetweenand. Î¸ xy\n2.6SpecialKindsofMatricesandVectors",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2.6SpecialKindsofMatricesandVectors\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\nthemaindiagonal.Â Formally,amatrixDisdiagonalifandonlyif D i , j=0for\n4 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nall iî€¶= j.Â Wehavealreadyseenoneexampleofadiagonalmatrix:Â theidentity\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\nisverycomputationally eï¬ƒcient.Tocomputediag(v)x,weonlyneedtoscaleeach\nelement x iby v i.Inotherwords,diag(v)x=vxî€Œ.Invertingasquarediagonal",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "matrixisalsoeï¬ƒcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\nandinthatcase,diag(v)âˆ’ 1=diag([1 /v 1 , . . . ,1 /v n]î€¾).Inmanycases,wemay\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\nmatricestobediagonal.\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "possibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\nelementsofthevectorifiswiderthanitistall. D\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\nAA= î€¾. (2.35)\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\nA isavectorwith : unitvectorunitnorm\n||||x 2= 1 . (2.36)\nAvectorxandavectoryareorthogonaltoeachotherifxî€¾y= 0.Ifboth\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\northonormal.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "orthonormal.\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\nmalandwhosecolumnsaremutuallyorthonormal:\nAî€¾AAA= î€¾= I . (2.37)\n4 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThisimpliesthat\nAâˆ’ 1= Aî€¾, (2.38)\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\nPaycarefulattentiontothedeï¬nitionoforthogonalmatrices.Counterintuitively,\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\n2.7Eigendecomposition\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\nconstituentparts,orï¬ndingsomepropertiesofthemthatareuniversal,notcaused",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "bythewaywechoosetorepresentthem.\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\norinbinary,butitwillalwaysbetruethat12 = 2Ã—2Ã—3.Fromthisrepresentation\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\nintegermultipleofwillbedivisibleby. 12 3\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\nrepresentationofthematrixasanarrayofelements.\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\neigenvalues.\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\nplicationbyaltersonlythescaleof: A v\nAvv= Î» . (2.39)\nThescalar Î»isknownastheeigenvaluecorrespondingtothiseigenvector.(One",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "canalsoï¬ndalefteigenvectorsuchthatvî€¾A= Î»vî€¾,Â butweareusually\nconcernedwithrighteigenvectors).\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s âˆˆ Rî€¶= 0.\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\nforuniteigenvectors.\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\nv( ) n},withcorrespondingeigenvalues { Î» 1 , . . . , Î» n}.Wemayconcatenateallofthe\n4 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n\u0000î€³ \u0000î€² \u0000î€± î€° î€± î€² î€³\nî¸î€°\u0000î€³\u0000î€²\u0000î€±î€°î€±î€²î€³î¸î€±î¶î€¨î€± î€©\nî¶î€¨î€² î€©î‚ î¥ î¦ î¯ î² î¥ î€  î­ îµ î¬ î´ î© î° î¬ î© î£ î¡ î´ î© î¯ î®\n\u0000î€³ \u0000î€² \u0000î€± î€° î€± î€² î€³\nî¸î€°\nî€°\u0000î€³\u0000î€²\u0000î€±î€°î€±î€²î€³î¸î€°\nî€±î¶î€¨î€± î€©î‚¸î€± î¶î€¨î€± î€©\nî¶î€¨î€² î€©î‚¸î€²î¶î€¨î€² î€©î î¦ î´ î¥ î² î€  î­ îµ î¬ î´ î© î° î¬ î© î£ î¡ î´ î© î¯ î®î… î¦ î¦ î¥ î£ î´ î€  î¯î¦ î€  î¥ î© î§ î¥ î® î¶ î¥ î£ î´ î¯î² î³ î€  î¡î® î¤ î€  î¥ î© î§î¥ î® î¶ î¡î¬ îµ î¥ î³\nFigure2.3:Anexampleoftheeï¬€ectofeigenvectorsandeigenvalues.Here,wehave\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue Î» 1andv( 2 )with",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "eigenvalue Î» 2. ( L e f t )Weplotthesetofallunitvectorsuâˆˆ R2asaunitcircle. ( R i g h t )We\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\ncanseethatitscalesspaceindirectionv( ) iby Î» i.\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorÎ»= [ Î» 1 , . . . ,\nÎ» n]î€¾.The ofisthengivenby eigendecompositionA\nAVÎ»V = diag()âˆ’ 1. (2.40)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "AVÎ»V = diag()âˆ’ 1. (2.40)\nWehaveseenthatconstructingmatriceswithspeciï¬ceigenvaluesandeigenvec-\ntorsallowsustostretchspaceindesireddirections.Â Ho wever,weoftenwantto\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\n4 3",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciï¬cclassof\nmatricesthathaveasimpledecomposition.Speciï¬cally,everyrealsymmetric\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\nandeigenvalues:\nAQQ = Î›î€¾, (2.41)\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Î›isa\ndiagonalmatrix.TheeigenvalueÎ› i , iisassociatedwiththeeigenvectorincolumn i",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "ofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\nscalingspaceby Î» iindirectionv( ) i.Seeï¬gureforanexample.2.3\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Î›",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\nifalloftheeigenvaluesareunique.\nTheeigendecompositionofÂ amatrixÂ tellsusÂ manyÂ usefulfactsaboutÂ the\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\nquadraticexpressionsoftheform f(x) =xî€¾Axsubjectto||||x 2= 1.Wheneverx\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivedeï¬nite.A\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideï¬-\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeï¬nite,and\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemideï¬nite.Positive\nsemideï¬nitematricesareinterestingbecausetheyguaranteethatâˆ€xx ,î€¾Axâ‰¥0.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "Positivedeï¬nitematricesadditionallyguaranteethatxî€¾Axx = 0 â‡’ = 0.\n2.8SingularValueDecomposition\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\n4 4",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdeï¬ned,andwe\nmustuseasingularvaluedecompositioninstead.\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\namatrixVofeigenvectorsandavectorofeigenvaluesÎ»suchthatwecanrewrite\nAas\nAVÎ»V = diag()âˆ’ 1. (2.42)\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "asaproductofthreematrices:\nAUDV = î€¾. (2.43)\nSupposethatAisan m nÃ—matrix.ThenUisdeï¬nedtobean m mÃ—matrix,\nD V tobeanmatrix,and m nÃ— tobeanmatrix. n nÃ—\nEachofthesematricesisdeï¬nedtohaveaspecialstructure.ThematricesU\nandVarebothdeï¬nedtobeorthogonalmatrices.ThematrixDisdeï¬nedtobe\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\ncolumnsofareknownasasthe V right-singularvectors.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "columnsofareknownasasthe V right-singularvectors.\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\neigenvectorsofAAî€¾.Theright-singularvectorsofAaretheeigenvectorsofAî€¾A.\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAî€¾A.\nThesameistrueforAAî€¾.\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\nsection.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "section.\n2.9TheMoore-PenrosePseudoinverse\nMatrixinversionisnotdeï¬nedformatricesthatarenotsquare.Supposewewant\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\nAxy= (2.44)\n4 5",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nbyleft-multiplyingeachsidetoobtain\nxBy= . (2.45)\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\nuniquemappingfromto.AB\nIfAistallerthanitiswide,Â thenitispossibleforthisequationtohave\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\nsolutions.\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\nthesecases.Thepseudoinverseofisdeï¬nedasamatrix A\nA+=lim\nÎ± î€¦ 0(Aî€¾AI+ Î±)âˆ’ 1Aî€¾. (2.46)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "A+=lim\nÎ± î€¦ 0(Aî€¾AI+ Î±)âˆ’ 1Aî€¾. (2.46)\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeï¬ni-\ntion,butrathertheformula\nA+= VD+Uî€¾, (2.47)\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\nelementsthentakingthetransposeoftheresultingmatrix.\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\npseudoinverseprovidesoneofthemanypossiblesolutions.Speciï¬cally,itprovides",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\nsolutions.\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\npossibletointermsofEuclideannorm y ||âˆ’||Axy 2.\n2.10TheTraceOperator\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\nTr() =Aî˜\niA i , i . (2.48)\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\ndiï¬ƒculttospecifywithoutresortingtosummationnotationcanbespeciï¬edusing",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "4 6",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\nanalternativewayofwritingtheFrobeniusnormofamatrix:\n|||| A F=î±\nTr(AAî€¾) . (2.49)\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\nmanipulatetheexpressionusingmanyusefulidentities.Â Forexample,thetrace\noperatorisinvarianttothetransposeoperator:\nTr() = Tr(AAî€¾) . (2.50)\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "movingthelastfactorintotheï¬rstposition,iftheshapesofthecorresponding\nmatricesallowtheresultingproducttobedeï¬ned:\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\normoregenerally,\nTr(nî™\ni = 1F( ) i) = Tr(F( ) nn âˆ’ 1î™\ni = 1F( ) i) . (2.52)\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\ndiï¬€erentshape.Forexample,forAâˆˆ Rm n Ã—andBâˆˆ Rn m Ã—,wehave\nTr( ) = Tr( )ABBA (2.53)\neventhoughABâˆˆ Rm m Ã—andBAâˆˆ Rn n Ã—.\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\n2.11TheDeterminant",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "2.11TheDeterminant\nThedeterminant ofaÂ squarematrix,Â denoted det(A),Â isaÂ functionmapping\nmatricestoÂ realscalars.Thedeterminant isequalÂ totheproductofÂ allthe\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\nthetransformationpreservesvolume.\n4 7",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n2.12Example:PrincipalComponentsAnalysis\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\nWewouldliketoloseaslittleprecisionaspossible.",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Wewouldliketoloseaslittleprecisionaspossible.\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\nofthem.Foreachpointx( ) iâˆˆ Rnwewillï¬ndacorrespondingcodevectorc( ) iâˆˆ Rl.\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\noriginaldata.Wewillwanttoï¬ndsomeencodingfunctionthatproducesthecode\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\ninputgivenitscode, .xx â‰ˆ g f(())\nPCAisdeï¬nedbyourchoiceofthedecodingfunction.Speciï¬cally,tomakethe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\ninto Rn.Let,where g() = cDcDâˆˆ Rn l Ã—isthematrixdeï¬ningthedecoding.\nComputingtheoptimalcodeforthisdecodercouldbeadiï¬ƒcultproblem.To\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\ntoeachother.(NotethatDisstillnottechnicallyâ€œanorthogonalmatrixâ€unless\nl n= )\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "theproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\nnorm.\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,theï¬rst\nthingweneedtodoisï¬gureouthowtogeneratetheoptimalcodepointcâˆ—for\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\ninputpointxanditsreconstruction, g(câˆ—).Wecanmeasurethisdistanceusinga\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\ncâˆ—= argmin\nc||âˆ’ ||x g()c 2 . (2.54)\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "bothareminimizedbythesamevalueofc.Bothareminimizedbythesame\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\n4 8",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nmonotonically increasingfornon-negative arguments.\ncâˆ—= argmin\nc||âˆ’ ||x g()c2\n2 . (2.55)\nThefunctionbeingminimizedsimpliï¬esto\n( ())xâˆ’ gcî€¾( ())xâˆ’ gc (2.56)\n(bythedeï¬nitionofthe L2norm,equation)2.30\n= xî€¾xxâˆ’î€¾g g ()câˆ’()cî€¾xc+( g)î€¾g()c (2.57)\n(bythedistributiveproperty)\n= xî€¾xxâˆ’2î€¾g g ()+c ()cî€¾g()c (2.58)\n(becausethescalar g()cî€¾xisequaltothetransposeofitself).\nWecannowchangethefunctionbeingminimizedagain,toomittheï¬rstterm,\nsincethistermdoesnotdependon:c\ncâˆ—= argmin",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "sincethistermdoesnotdependon:c\ncâˆ—= argmin\ncâˆ’2xî€¾g g ()+c ()cî€¾g .()c (2.59)\nTomakefurtherprogress,wemustsubstituteinthedeï¬nitionof: g()c\ncâˆ—= argmin\ncâˆ’2xî€¾Dcc+î€¾Dî€¾Dc (2.60)\n= argmin\ncâˆ’2xî€¾Dcc+î€¾I lc (2.61)\n(bytheorthogonalityandunitnormconstraintson)D\n= argmin\ncâˆ’2xî€¾Dcc+î€¾c (2.62)\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\nyoudonotknowhowtodothis):\nâˆ‡ c(2âˆ’xî€¾Dcc+î€¾c) = 0 (2.63)\nâˆ’2Dî€¾xc+2= 0 (2.64)\ncD= î€¾x . (2.65)\n4 9",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nThismakesthealgorithmeï¬ƒcient:Â wecanoptimallyencodexjustusinga\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\nf() = xDî€¾x . (2.66)\nUsingafurthermatrixmultiplication, wecanalsodeï¬nethePCAreconstruction\noperation:\nr g f () = x (()) = xDDî€¾x . (2.67)\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\noferrorscomputedoveralldimensionsandallpoints:\nDâˆ—= argmin\nDî³î˜\ni , jî€\nx( ) i\njâˆ’ r(x( ) i) jî€‘2\nsubjecttoDî€¾DI= l(2.68)\nToderivethealgorithmforï¬ndingDâˆ—,wewillstartbyconsideringthecase\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\ndâˆ—= argmin\ndî˜\ni||x( ) iâˆ’ddî€¾x( ) i||2\n2subjectto||||d 2= 1 .(2.69)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "2subjectto||||d 2= 1 .(2.69)\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\nscalarvaluedî€¾x( ) iontherightofthevectord.Itismoreconventionaltowrite\nscalarcoeï¬ƒcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\nsuchaformulaas\ndâˆ—= argmin\ndî˜\ni||x( ) iâˆ’dî€¾x( ) id||2\n2subjectto||||d 2= 1 ,(2.70)\nor,exploitingthefactthatascalarisitsowntranspose,as\ndâˆ—= argmin\ndî˜\ni||x( ) iâˆ’x( ) i î€¾dd||2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "dâˆ—= argmin\ndî˜\ni||x( ) iâˆ’x( ) i î€¾dd||2\n2subjectto||||d 2= 1 .(2.71)\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\n5 0",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\nThiswillallowustousemorecompactnotation.LetXâˆˆ Rm n Ã—bethematrix\ndeï¬nedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) iî€¾.\nWecannowrewritetheproblemas\ndâˆ—= argmin\nd||âˆ’XXddî€¾||2\nFsubjecttodî€¾d= 1 .(2.72)\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\nportionasfollows:\nargmin\nd||âˆ’XXddî€¾||2\nF (2.73)\n= argmin\ndTrî€’î€",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "argmin\nd||âˆ’XXddî€¾||2\nF (2.73)\n= argmin\ndTrî€’î€\nXXdd âˆ’î€¾î€‘î€¾î€\nXXdd âˆ’î€¾î€‘î€“\n(2.74)\n(byequation)2.49\n= argmin\ndTr(Xî€¾XXâˆ’î€¾Xddî€¾âˆ’ddî€¾Xî€¾Xdd+î€¾Xî€¾Xddî€¾)(2.75)\n= argmin\ndTr(Xî€¾X)Tr(âˆ’Xî€¾Xddî€¾)Tr(âˆ’ddî€¾Xî€¾X)+Tr(ddî€¾Xî€¾Xddî€¾)\n(2.76)\n= argmin\ndâˆ’Tr(Xî€¾Xddî€¾)Tr(âˆ’ddî€¾Xî€¾X)+Tr(ddî€¾Xî€¾Xddî€¾)(2.77)\n(becausetermsnotinvolvingdonotaï¬€ectthe) d argmin\n= argmin\ndâˆ’2Tr(Xî€¾Xddî€¾)+Tr(ddî€¾Xî€¾Xddî€¾)(2.78)\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\n= argmin\ndâˆ’2Tr(Xî€¾Xddî€¾)+Tr(Xî€¾Xddî€¾ddî€¾)(2.79)\n(usingthesamepropertyagain)",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "(usingthesamepropertyagain)\nAtthispoint,were-introducetheconstraint:\nargmin\ndâˆ’2Tr(Xî€¾Xddî€¾)+Tr(Xî€¾Xddî€¾ddî€¾)subjecttodî€¾d= 1(2.80)\n= argmin\ndâˆ’2Tr(Xî€¾Xddî€¾)+Tr(Xî€¾Xddî€¾)subjecttodî€¾d= 1(2.81)\n(duetotheconstraint)\n= argmin\ndâˆ’Tr(Xî€¾Xddî€¾)subjecttodî€¾d= 1(2.82)\n5 1",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER2.LINEARALGEBRA\n= argmax\ndTr(Xî€¾Xddî€¾)subjecttodî€¾d= 1(2.83)\n= argmax\ndTr(dî€¾Xî€¾Xdd )subjecttoî€¾d= 1(2.84)\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciï¬cally,\ntheoptimaldisgivenbytheeigenvectorofXî€¾Xcorrespondingtothelargest\neigenvalue.\nThisderivationisspeciï¬ctothecaseof l=1andrecoversonlytheï¬rst\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\nwritingthisproofasanexercise.\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\n5 2",
    "metadata": {
      "source": "[6]part-1-chapter-2.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 2\nA p p l i cat i on s\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\nspeciï¬capplicationareasthatdeeplearninghasbeenusedtosolve.Â Whileone\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\nLanguagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\nvocabulary)perinputfeature.\n12. 1 L arge- S c a l e D eep L earni n g\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\nimprovementinneuralnetworkâ€™saccuracyandtheimprovementofthecomplexity\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\ngrownexponentiallyforthepastthreedecades,yetartiï¬cialneuralnetworksare\nonlyaslargeasthenervoussystemsofinsects.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "onlyaslargeasthenervoussystemsofinsects.\nBecausethesizeofneuralnetworksisofparamountimportance,deeplearning\n443",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nrequireshighperformancehardwareandsoftwareinfrastructure.\n12.1.1FastCPUImplementations\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\nToday,thisapproachisgenerallyconsideredinsuï¬ƒcient.WenowmostlyuseGPU\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Adescriptionofhowtoimplementeï¬ƒcientnumericalCPUcodeisbeyond\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\nspeciï¬cCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingï¬xed-point\narithmeticratherthanï¬‚oating-pointarithmetic.Bycreatingacarefullytunedï¬xed-\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\nastrongï¬‚oating-pointsystem.EachnewmodelofCPUhasdiï¬€erentperformance",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "characteristics,sosometimesï¬‚oating-pointimplementations canbefastertoo.\nTheimportantprincipleisthatcarefulspecializationofnumericalcomputation\nroutinescanyieldalargepayoï¬€.Otherstrategies,besideschoosingwhethertouse\nï¬xedorï¬‚oatingpoint,includeoptimizingdatastructurestoavoidcachemisses\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\nimplementationdetails,butwhentheperformanceofanimplementation restricts\nthesizeofthemodel,theaccuracyofthemodelsuï¬€ers.\n12.1.2GPUImplementations",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "12.1.2GPUImplementations\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\nbeneï¬cialforneuralnetworksaswell.\nVideogamerenderingrequiresperformingmanyoperationsinparallelquickly.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "ModelsofÂ charactersÂ and environmentsÂ arespeciï¬edÂ intermsofÂ listsofÂ 3-D\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\nateachpixelinparalleltodeterminethecolorofeachpixel.Â Inbothcases,the\n4 4 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuï¬€ersof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "memory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\nCPUs.\nNeuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\nlargeandnumerousbuï¬€ersofparameters,activationvalues,andgradientvalues,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\nbuï¬€ersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.\nGPUsoï¬€eracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.\nNeuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\nnetworkscanbedividedintomultipleindividualâ€œneuronsâ€thatcanbeprocessed",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "independentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\nbeneï¬tfromtheparallelismofGPUcomputing.\nGPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\ngraphicstasks.Overtime,GPUhardwarebecamemoreï¬‚exible,allowingcustom\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\nbasedonarenderingtask.TheseGPUscouldbeusedforscientiï¬ccomputingby",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "writingtheoutputofacomputationtoabuï¬€erofpixelvalues.Steinkrau e t a l .\n()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\nacceleratesupervisedconvolutionalnetworks.\nThepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "code,notjustrenderingsubroutines.Â NVIDIAâ€™sCUDAprogramming language\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\n4 4 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nbandwidth,GP-GPUsnowoï¬€eranidealplatformforneuralnetworkprogramming.\nThisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\nWritingeï¬ƒcientcodeforGP-GPUsremainsadiï¬ƒculttaskbestlefttospe-\ncialists.Â ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\ndiï¬€erentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "writablememorylocationsarenotcached,soitcanactuallybefastertocompute\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.\nGPUcodeisalsoinherentlymulti-threaded andthediï¬€erentthreadsmustbe\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\ntransaction.Diï¬€erentmodelsofGPUsareabletocoalescediï¬€erentkindsofread",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "orwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\nof2.Â Theexactspeciï¬cationsdiï¬€erbetweenmodelsofGPU.Anothercommon\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\nsameinstructionsimultaneously.Thismeansthatbranchingcanbediï¬ƒculton\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\nexecutesthesameinstructionduringeachcycle,soifdiï¬€erentthreadswithinthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "samewarpneedtoexecutediï¬€erentcodepaths,thesediï¬€erentcodepathsmust\nbetraversedsequentiallyratherthaninparallel.\nDuetothediï¬ƒcultyofwritinghighperformanceGPUcode,researchersshould\nstructuretheirworkï¬‚owtoavoidneedingtowritenewGPUcodeinordertotest\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "machinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciï¬esallofits\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l .\n2011b)providesimilarfeatures.\n4 4 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.1.3Large-ScaleDistributedImplementations\nInmanycases,thecomputational resourcesavailableonasinglemachineare\ninsuï¬ƒcient.Wethereforewanttodistributetheworkloadoftrainingandinference\nacrossmanymachines.\nDistributinginferenceissimple,becauseeachinputexamplewewanttoprocess\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\ntogetheronasingledatapoint,witheachmachinerunningadiï¬€erentpartofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "model.Thisisfeasibleforbothinferenceandtraining.\nDataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\nthestandarddeï¬nitionofgradientdescentisasacompletelysequentialalgorithm:\nthegradientatstepisafunctionoftheparametersproducedbystep. t tâˆ’1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.\nThisreducestheaverageamountofimprovementthateachgradientdescentstep\nyields,becausesomeofthecoresoverwriteeachotherâ€™sprogress,buttheincreased\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\n2015).Academicdeeplearningresearcherstypicallycannotaï¬€ordthesamescale\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "distributednetworkswithrelativelylow-costhardwareavailableintheuniversity\nsetting( ,). Coates e t a l .2013\n12.1.4ModelCompression\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\n4 4 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\npowerfulcomputercluster,thendeployitonmobilephones.\nAkeystrategyforreducingthecostofinferenceismodelcompression(Bu-\nciluË‡a2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "storeandevaluate.\nModelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\nprimarilybyaneedtopreventoverï¬tting.Inmostcases,themodelwiththe\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.\nEvaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\nTheselargemodelslearnsomefunction f(x),butdosousingmanymore\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thelimitednumberoftrainingexamples.Assoonaswehaveï¬tthisfunction\nf(x),wecangenerateatrainingsetcontaininginï¬nitelymanyexamples,simply\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\nmodeltomatch f(x)onthesepoints.Inordertomosteï¬ƒcientlyusethecapacity\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "modeltrainedontheoriginaltrainingset.\nAlternatively,onecantrainthesmallermodelonlyontheoriginaltraining\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).\n12.1.5DynamicStructure\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\ncomputation(,; ,).Â Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\n4 4 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\nDynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\ngenerallythroughoutthesoftwareengineeringdiscipline.Â Thesimplestversions\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\nbeappliedtoaparticularinput.\nAvenerablestrategyforacceleratinginferenceinaclassiï¬eristouseacascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "ofclassiï¬ers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\nwemustuseasophisticatedclassiï¬erwithhighcapacity,thatisexpensivetorun.\nHowever,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\nasequenceofclassiï¬ers.Theï¬rstclassiï¬ersinthesequencehavelowcapacity,\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "wedonotwronglyrejectaninputwhentheobjectispresent.Theï¬nalclassiï¬er\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\nclassiï¬ersinasequence,abandoninganyexampleassoonasanyoneelementin\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\nhighconï¬dence,usingahighcapacitymodel,butdoesnotforceustopaythecost\noffullinferenceforeveryexample.Therearetwodiï¬€erentwaysthatthecascade\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\nhighcapacity,becausesomeofitsindividualmembersdo.Â Itisalsopossibleto\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiï¬er\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\nthatï¬rstlocatestheaddressnumberwithonemachinelearningmodelandthen\ntranscribesitwithanother(Goodfellow2014d e t a l .,).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\nAsimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\n4 4 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\ndonewiththeprimarygoalofacceleratinginferencecomputations.\nInthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\ngiventhecurrentinput.Theï¬rstversionofthisideaiscalledthemixtureof\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "ofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\nandtheï¬naloutputisobtainedbytheweightedcombinationoftheoutputof\ntheexperts.Inthatcase,Â theuseofthegaterdoesnotoï¬€erareductionin\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "whenwewanttoselectdiï¬€erentsubsetsofunitsorparameters,itisnotpossible\ntouseaâ€œsoftswitchâ€becauseitrequiresenumerating(andcomputingoutputsfor)\nallthegaterconï¬gurations. Todealwiththisproblem,severalapproacheshave\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l .\n()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\nqualityoftheapproximation.\nAnotherÂ kindofÂ dynamicstructureÂ isaÂ switch,Â whereÂ ahiddenÂ unit can\nreceiveinputfromdiï¬€erentunitsdependingonthecontext.Thisdynamicrouting\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\nSofar,theuseofahardswitchhasnotproveneï¬€ectiveonlarge-scaleapplications.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\nandthusdonotachieveallofthepossiblecomputational beneï¬tsofdynamic\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\ndegreeofparallelismthatresultsfromthesystemfollowingdiï¬€erentcodebranches\nfordiï¬€erentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "canwritemorespecializedsub-routinesthatconvolveeachexamplewithdiï¬€erent\nkernelsormultiplyeachrowofadesignmatrixbyadiï¬€erentsetofcolumns\nofweights.Unfortunately,Â thesemorespecializedsubroutinesarediï¬ƒcultto\nimplementeï¬ƒciently.CPUimplementations willbeslowduetothelackofcache\n4 5 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\ndiï¬€erentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\nofexamplessimultaneously.Â Thiscanbeanacceptablestrategyforminimizing\nthetimerequiredtoprocessaï¬xedamountofexamplesinanoï¬„inesetting.In",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "areal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\nmachinetoprocesstheï¬rststepinacascadeandanothermachinetoprocess\nthelaststepinacascade,thentheï¬rstwilltendtobeoverloadedandthelast\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\nimplementdiï¬€erentnodesofaneuraldecisiontree.\n12.1.6SpecializedHardwareImplementationsofDeepNetworks",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l .\n2003MisraandSaha2010 ; ,).\nDiï¬€erentformsofspecializedhardware(GrafandJackel1989Meadand ,;\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "beendevelopedoverthelastdecades,eitherwithASICs(application-speciï¬cinte-\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\n(combiningdigitalandanalogcomponents).Inrecentyearsmoreï¬‚exibleFPGA\n(ï¬eldprogrammable gatedarray)implementations(wheretheparticularsofthe\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\nandGPUs)typicallyuse32or64bitsofprecisiontorepresentï¬‚oatingpoint\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\n4 5 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\nGPUs).Thisisverydiï¬€erentfromthesituationofthe1990s(thepreviousneural\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\nnaturallanguage).\nRecentworkonlow-precisionimplementationsofbackprop-based neuralnets\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\nthatbetween8and16bitsofprecisioncansuï¬ƒceforusingortrainingdeep\nneuralnetworkswithback-propagation.Â Whatisclearisthatmoreprecisionis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\nï¬xedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\nrequiredpernumber.Traditionalï¬xedpointnumbersarerestrictedtoaï¬xed\nrange(whichcorrespondstoagivenexponentinaï¬‚oatingpointrepresentation).\nDynamicï¬xedpointrepresentationssharethatrangeamongasetofnumbers\n(suchasalltheweightsinonelayer).Usingï¬xedpointratherthanï¬‚oatingpoint\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "powerrequirementsandcomputingtimeneededforperformingmultiplications,\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\ntrainamoderndeepnetworkwithbackprop.\n12. 2 C om p u t er V i s i on\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\ndeeplearningapplications,becausevisionisataskthatiseï¬€ortlessforhumans\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "ofobjectrecognitionoropticalcharacterrecognition.\nComputervisionisaverybroadï¬eldencompassingawidevarietyofways\nofprocessingimages,andanamazingdiversityofapplications.Â Applicationsof\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\n4 5 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\nremovingobjectsfromimages.\n12.2.1Preprocessing\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\ninputcomesinaformthatisdiï¬ƒcultformanydeeplearningarchitecturesto",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\nsame,reasonablerange,like[0,1]or[-1,1].Â Mixingimagesthatliein[0,1]with\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\ncroppedorscaledtoï¬tthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\nimage( ,). Hadsell e t a l .2007\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\nthemodelmanydiï¬€erentversionsofthesameinput(forexample,thesameimage\ncroppedatslightlydiï¬€erentlocations)andhavethediï¬€erentinstantiationsofthe\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\nensembleapproach,andhelpstoreducegeneralization error.\nOtherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\n4 5 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\nthemodelneededtoï¬tthetrainingset.Simplertasksmaybesolvedbysmaller\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\nisconï¬denthasnorelevancetothetask.Whentrainingwithlargedatasetsand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "largemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\ne t a l .,).2012\n12.2.1.1ContrastNormalization\nOneofthemostobvioussourcesofvariationthatcanbesafelyremovedÂ for\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "magnitudeofthediï¬€erencebetweenthebrightandthedarkpixelsinanimage.\nTherearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\nXâˆˆ Rr cÃ—Ã—3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\nentireimageisgivenby\nî¶îµîµî´1\n3 r cr î˜\ni=1c î˜\nj=13 î˜\nk=1î€€",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "î¶îµîµî´1\n3 r cr î˜\ni=1c î˜\nj=13 î˜\nk=1î€€\nX i , j , kâˆ’Â¯ Xî€2(12.1)\nwhere Â¯ Xisthemeanintensityoftheentireimage:\nÂ¯ X=1\n3 r cr î˜\ni=1c î˜\nj=13 î˜\nk=1X i , j , k . (12.2)\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage,Â then\nrescalingitsothattheÂ standarddeviationÂ across itsÂ pixelsisÂ equaltosome\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "intensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\n4 5 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\nmotivatesintroducingasmall,positiveregularizationparameter Î»tobiasthe\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\ntobeatleast î€.Givenaninputimage X,GCNproducesanoutputimage Xî€°,\ndeï¬nedsuchthat\nXî€°\ni , j , k= sX i , j , kâˆ’Â¯ X\nmaxî€š\nî€ ,î±\nÎ»+1\n3 r cîr\ni=1îc\nj=1î3\nk=1î€€\nX i , j , kâˆ’Â¯ Xî€2î€› .(12.3)\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\ntopracticallyignorethesmalldenominator problembysetting Î»= 0andavoid\ndivisionby0inextremelyrarecasesbysetting î€toanextremelylowvaluelike\n10âˆ’8.Â Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\nî€ Î» = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\npreferabletodeï¬neGCNintermsofstandarddeviationratherthan L2norm\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "basedonstandarddeviationallowsthesame stobeusedregardlessofimage\nsize.However,theobservationthatthe L2normisproportionaltothestandard\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\nexamplestoasphericalshell.Seeï¬gureforanillustration.Thiscanbea 12.1\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiï¬€erent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "biases.Suchcoordinationcanbediï¬ƒcultforthelearningalgorithmtodiscover.\nAdditionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\nreducingeachexampletoadirectionratherthanadirectionandadistance.\nCounterintuitively,thereisapreprocessingoperationknownasspheringand\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\n4 5 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nâˆ’ 1 5 0 0 1 5 . . .\nx 0âˆ’ 1 5 .0 0 .1 5 .x 1Rawinput\nâˆ’ 1 5 0 0 1 5 . . .\nx 0GCN, = 10 Î»âˆ’ 2\nâˆ’ 1 5 0 0 1 5 . . .\nx 0GCN, = 0 Î»\nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\n( C e n t e r )GCNwith Î»= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\ns= 1and î€= 10âˆ’ 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "GCN,with Î» >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\nvariationintheirnorm.Weleaveandthesameasbefore. s î€\nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\nsphericalcontours.Spheringismorecommonlyknownas .whitening\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\nlargediï¬€erencebetweenthebrightnessofthedarkareaandthebrightnessofthe\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.\nThismotivateslocalcontrastnormalization.Localcontrastnormalization\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\ntheimageasawhole.Seeï¬gureforacomparisonofglobalandlocalcontrast 12.2\nnormalization.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "normalization.\nVariousdeï¬nitionsoflocalcontrastnormalization arepossible.Inallcases,\nonemodiï¬eseachpixelbysubtractingameanofnearbypixelsanddividingby\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\npixeltobemodiï¬ed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\nbemodiï¬ed.Â Inthecaseofcolorimages,somestrategiesprocessdiï¬€erentcolor\n4 5 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInputimage GCN LCN\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeï¬€ects\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\ncontrastnormalizationmodiï¬estheimagemuchmore,discardingallregionsofconstant\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofï¬netexture,\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "normalizationkernelbeingtoohigh.\nchannelsseparatelywhileotherscombineinformationfromdiï¬€erentchannelsto\nnormalizeeachpixel( ,). Sermanet e t a l .2012\nLocalcontrastnormalization canusuallybeimplemented eï¬ƒcientlybyusing\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\ndivisionondiï¬€erentfeaturemaps.\nLocalcontrastnormalization isadiï¬€erentiable operationandcanalsobeusedas",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\noperationappliedtotheinput.\nAswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.\n4 5 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.2.1.2DatasetAugmentation\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassiï¬er 7.4\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\nexamplesthathavebeenmodiï¬edwithtransformationsthatdonotchangethe\nclass.Objectrecognitionisaclassiï¬cationtaskthatisespeciallyamenableto\nthisformÂ ofdatasetÂ augmentationbecauseÂ theclassÂ isinvariantÂ tosoÂ many\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "operations.Asdescribedbefore,classiï¬erscanbeneï¬tfromrandomtranslations,\nrotations,andinsomecases,ï¬‚ipsoftheinputtoaugmentthedataset.Inspecialized\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\ntheinput( ,). LeCun e t a l .1998b\n12. 3 S p eec h R ec ogn i t i o n\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "asequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\ntaskconsistsofcreatingafunction fâˆ—\nASRthatcomputesthemostprobablelinguistic\nsequencegiventheacousticsequence: y X\nfâˆ—\nASR() = argmaxX\nyPâˆ—( = ) y X|X (12.4)\nwhere Pâˆ—isthetrueconditionaldistributionrelatingtheinputsXtothetargets\ny.\nSincethe1980sanduntilabout2009â€“2012,state-of-theartspeechrecognition\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "phonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987\nTheGMM-HMMÂ modelfamilytreatsÂ acousticwaveformsasbeinggenerated\nbythefollowingprocess:Â ï¬rstanHMMgeneratesasequenceofphonemesand\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\n4 5 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\nspeechrecognitionwasactuallyoneoftheï¬rstareaswhereneuralnetworkswere\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performanceofASRbasedonneuralnetsapproximately matchedtheperformance\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\nphonemestodiscriminatebetween),Â whichwasbetterthanorcomparableto\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\nHowever,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "speechrecognitionandtheeï¬€ortthathadbeeninvestedinbuildingthesesystems\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\nLater,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "forthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).\nStartingin2009,speechresearchersappliedaformofdeeplearningbasedon\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "Thesenetworkstakespectralacousticrepresentationsinaï¬xed-sizeinputwindow\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniï¬cantlyimprove\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "recognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\n4 5 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\nontechniquessuchasrectiï¬edlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\ne t a l .,).Â Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\narenowdeployedinproductssuchasmobilephones.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "arenowdeployedinproductssuchasmobilephones.\nLater,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\nunnecessaryordidnotbringanysigniï¬cantimprovement.\nThesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "longperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\ntrainingsets(seeï¬gure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\nalgorithmsandarchitectures forASR,whichisstillongoingtoday.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l .\n2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\nfrequencyofspectralcomponents.\nAnotherimportantpush,Â stillongoing,hasbeentowardsend-to-enddeep",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "learningspeechrecognitionsystemsthatcompletelyremovetheHMM.Theï¬rst\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding.Â This",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\nappliedinothersettings.\nAnothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\nsystemlearnhowtoâ€œalignâ€theacoustic-levelinformationwiththephonetic-level\n4 6 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\nspecializedlanguagesdesignedtoalloweï¬ƒcientandunambiguousparsingbysimple\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\ndescription.Â Naturallanguageprocessingincludesapplicationssuchasmachine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "translation,inwhichthelearnermustreadasentenceinonehumanlanguageand\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\narebasedonlanguagemodelsthatdeï¬neaprobabilitydistributionoversequences\nofwords,charactersorbytesinanaturallanguage.\nAswiththeotherapplicationsdiscussedinthischapter,verygenericneural\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\nHowever,toachieveexcellentperformanceandtoscalewelltolargeapplications,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "somedomain-speciï¬cstrategiesbecomeimportant.Tobuildaneï¬ƒcientmodelof\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\nbeendevelopedtomakemodelsofsuchaspaceeï¬ƒcient,bothinacomputational",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "andinastatisticalsense.\n12.4.1-grams n\nAlanguagemodeldeï¬nesaprobabilitydistributionoversequencesoftokens\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\nearliestsuccessfullanguagemodelswerebasedonmodelsofï¬xed-lengthsequences\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\nModelsbasedon n-gramsdeï¬netheconditionalprobabilityofthe n-thtoken\ngiventhepreceding nâˆ’1tokens.Themodelusesproductsoftheseconditional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "distributionstodeï¬netheprobabilitydistributionoverlongersequences:\nP x(1 , . . . , x Ï„) = ( P x1 , . . . , x nâˆ’1)Ï„î™\nt n=P x( t| x t nâˆ’+1 , . . . , x tâˆ’1) .(12.5)\n4 6 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nThisdecompositionisjustiï¬edbythechainruleofprobability.Theprobability\ndistributionovertheinitialsequence P( x1 , . . . , x nâˆ’1)maybemodeledbyadiï¬€erent\nmodelwithasmallervalueof. n\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "Mercer1980Katz1987ChenandGoodman1999 ,;,; ,).\nForsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\nfor n=2,andtrigramfor n=3.Â ThesenamesderivefromtheLatinpreï¬xesfor\nthecorrespondingnumbersandtheGreeksuï¬ƒxâ€œ-gramâ€denotingsomethingthat\niswritten.\nUsuallywetrainbothan n-grammodelandan nâˆ’1 grammodelsimultaneously.\nThismakesiteasytocompute\nP x( t| x t nâˆ’+1 , . . . , x tâˆ’1) =P n( x t nâˆ’+1 , . . . , x t)\nP nâˆ’1( x t nâˆ’+1 , . . . , x tâˆ’1)(12.6)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "P nâˆ’1( x t nâˆ’+1 , . . . , x tâˆ’1)(12.6)\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\ninferencein P n,wemustomittheï¬nalcharacterfromeachsequencewhenwe\ntrain P nâˆ’1.\nAsanexample,wedemonstratehowatrigrammodelcomputestheprobability\nofthesentenceâ€œTHEDOGRANAWAY.â€Theï¬rstwordsofthesentencecannotbe\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "abilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N).\nFinally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\nweobtain:\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N .\n(12.7)\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thoughthetuple ( x t nâˆ’+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\ndiï¬€erentkindsofcatastrophicoutcomes.When P nâˆ’1iszero,theratioisundeï¬ned,\nsothemodeldoesnotevenproduceasensibleoutput.When P nâˆ’1isnon-zerobut\nP niszero,thetestlog-likelihoodisâˆ’âˆž.Â Toavoidsuchcatastrophicoutcomes,\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\n4 6 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\nSee ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\nsymbolvalues.Thismethodcanbejustiï¬edasBayesianinferencewithauniform\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "higher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\nmorelikelytoavoidcountsofzero.Back-oï¬€methodslook-upthelower-order\nn-gramsifthefrequencyofthecontext x tâˆ’1 , . . . , x t nâˆ’+1istoosmalltousethe\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\ncontexts x t n k âˆ’+ , . . . , x tâˆ’1,forincreasing k,untilasuï¬ƒcientlyreliableestimateis\nfound.\nClassical n-grammodelsareparticularlyvulnerabletothecurseofdimension-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "ality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset.\nOnewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\nisevenmoreseverethanusual,becauseanytwodiï¬€erentwordshavethesamedis-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "tancefromeachotherinone-hotvectorspace.Itisthusdiï¬ƒculttoleveragemuch\ninformationfromanyâ€œneighborsâ€â€”onlytrainingexamplesthatrepeatliterallythe\nsamecontextareusefulforlocalgeneralization.Â T oovercometheseproblems,a\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\nsemanticallysimilarwords.\nToimprovethestatisticaleï¬ƒciencyof n-grammodels,class-basedlanguage\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-oï¬€are\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\nlostinthisrepresentation.\n4 6 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12.4.2NeuralLanguageModels\nNeurallanguagemodelsorNLMsareÂ aclassoflanguagemodeldesigned\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\nsequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "word(anditscontext)andothersimilarwordsandcontexts.Thedistributed\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "transferringinformationfromeachtrainingsentencetoanexponentiallylarge\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\nexponentialnumberofsimilarsentences.\nWesometimescallthesewordrepresentationswordembeddings.Inthis\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\naone-hotvector,soeverypairofwordsisatEuclideandistanceâˆš\n2fromeach\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\n(oranypairofwordssharingsomeâ€œfeaturesâ€learnedbythemodel)arecloseto\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\nFigurezoomsinonspeciï¬careasofalearnedwordembeddingspacetoshow 12.3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\nNeuralnetworksinotherdomainsalsodeï¬neembeddings.Forexample,a\nhiddenlayerofaconvolutionalnetworkprovidesanâ€œimageembedding.â€Usually\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\nrepresented.\nThebasicideaofusingdistributedrepresentationstoimprovemodelsfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\n4 6 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmultiplelatentvariables(MnihandHinton2007,).\nâˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 3432302826âˆ’14âˆ’13âˆ’12âˆ’11âˆ’10âˆ’9âˆ’8âˆ’7âˆ’6\nCanadaEuropeOntario\nNorthEnglish\nCanadianUnionAfricanAfrica\nBritishFrance\nRussianChina\nGermanyFrench\nAssemblyEU JapanIraq\nSouthEuropean\n350355360365370375380 . . . . . . .171819202122\n1995199619971998199920002001\n200220032004\n20052006200720082009\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "machinetranslationmodel( ,),zoominginonspeciï¬careaswhere Bahdanau e t a l .2015\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\n12.4.3High-DimensionalOutputs\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\nrepresentingsuchadistributionistoapplyanaï¬ƒnetransformationfromahidden\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "componentofthisaï¬ƒnetransformationisverylarge,becauseitsoutputdimension\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\ntimeaswellastesttimeâ€”wecannotcalculateonlythedotproductwiththeweight\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "attesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\n4 6 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlossfunctions,thegradientcanbecomputedeï¬ƒciently( ,),but Vincent e t a l .2015\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\nmanydiï¬ƒculties.\nSupposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\nË†y.IfweparametrizethetransformationfromhtoË†ywithlearnedweightsW\nandlearnedbiasesb,thentheaï¬ƒne-softmaxoutputlayerperformsthefollowing\ncomputations:\na i= b i+î˜\njW i j h jâˆ€âˆˆ{ ||} i1 , . . . , V , (12.8)\nË† y i=ea i\nî|| V\niî€°=1 eai î€°. (12.9)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "Ë† y i=ea i\nî|| V\niî€°=1 eai î€°. (12.9)\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\ncomputationofmostneurallanguagemodels.\n12.4.3.1UseofaShortList\nTheï¬rstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "builtuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\nfrequentwords(handledbytheneuralnet)andatail T= V L\\ofmorerarewords\n(handledbyan n-grammodel).Â Tobeabletocombinethetwopredictions,the\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\nunittoprovideanestimateof P( i C âˆˆ| T ).Theextraoutputcanthenbeusedto\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "P y i C (= |) =1 iâˆˆ L P y i C, i P i C (= | âˆˆ âˆ’ L)(1 (âˆˆ| T ))\n+1 iâˆˆ T P y i C, i P i C (= | âˆˆ T)(âˆˆ| T )(12.10)\nwhere P( y= i C, i| âˆˆ L)isprovidedbytheneurallanguagemodeland P( y= i|\nC, iâˆˆ T) isprovidedbythe n-grammodel.Withslightmodiï¬cation,thisapproach\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodelâ€™ssoftmax\nlayer,ratherthanaseparatesigmoidunit.\nAnobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "4 6 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nwords,where,arguably,itistheleastuseful.Â Thisdisadvantagehasstimulated\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\ndescribedbelow.\n12.4.3.2HierarchicalSoftmax\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "the|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\nmodels.\nOnecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.\nThesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\nthetreehasdepth O(log|| V).Â Theprobabilityofachoosingawordisgivenby\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "everynodeonapathfromtherootofthetreetotheleafcontainingtheword.\nFigureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\nsummationoverallofthepathsthatleadtothatword.\nTopredicttheconditionalprobabilities requiredateachnodeofthetree,we\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "samecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\nBecausetheoutputlog-likelihoodcanbecomputedeï¬ƒciently(aslowaslog|| V\nratherthan|| V),itsgradientsmayalsobecomputedeï¬ƒciently.Thisincludesnot\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "withrespecttothehiddenlayeractivations.\nItispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\n4 6 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n( 1) ( 0)\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciï¬cwords.\nInternalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "ofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissuï¬ƒcientlybalanced,\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\nthenumberofwords|| V:Â thechoiceofoneoutof|| Vwordscanbeobtainedbydoing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "O(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "nodeindexedbythepreï¬xofthesebits.Forexample,node(1 ,0)correspondstothe\npreï¬x( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\n4 6 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npractice,thecomputational savingsaretypicallynotworththeeï¬€ortbecausethe\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\nactivationsgrowsasas O( l n2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "activationsgrowsasas O( l n2\nh)whiletheoutputcomputations growas O( n h n b).\nAslongas n bâ‰¤ l n h,wecanreducecomputationmorebyshrinking n hthanby\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\nexceedsamillionwordsandlog2(106)â‰ˆ20,itispossibletoreduce n btoabout,20\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\natreewithabranchingfactorof,onecaninsteaddeï¬neatreewithdepthtwo 2\nandabranchingfactorofî°\n|| V.Suchatreecorrespondstosimplydeï¬ningaset",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "|| V.Suchatreecorrespondstosimplydeï¬ningaset\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\ntwocapturesmostofthecomputational beneï¬tofthehierarchicalstrategy.\nOnequestionthatremainssomewhatopenishowtobestdeï¬netheseword\nclasses,orhowtodeï¬nethewordhierarchyingeneral.Earlyworkusedexisting\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\njointlywiththeneurallanguagemodel.Learningthehierarchyisdiï¬ƒcult.Anexact",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "optimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization. However,\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\nwordsintowordclasses.\nAnimportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\ntionalbeneï¬tsbothattrainingtimeandattesttime,ifattesttimewewantto\ncomputetheprobabilityofspeciï¬cwords.\nOfcourse,computingtheprobabilityofall|| Vwordswillremainexpensive",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "evenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\nprovideaneï¬ƒcientandexactsolutiontothisproblem.\nAdisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\ntoapoorchoiceofwordclasses.\n12.4.3.3ImportanceSampling\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "computingthecontributionofthegradientfromallofthewordsthatdonotappear\n4 6 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\ninequation,thegradientcanbewrittenasfollows: 12.8\nâˆ‚ P y C log(|)\nâˆ‚ Î¸=âˆ‚logsoftmax y()a\nâˆ‚ Î¸(12.13)\n=âˆ‚\nâˆ‚ Î¸logea y\nî\ni ea i(12.14)\n=âˆ‚\nâˆ‚ Î¸( a yâˆ’logî˜\niea i) (12.15)\n=âˆ‚ a y\nâˆ‚ Î¸âˆ’î˜\niP y i C (= |)âˆ‚ a i\nâˆ‚ Î¸(12.16)",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "=âˆ‚ a y\nâˆ‚ Î¸âˆ’î˜\niP y i C (= |)âˆ‚ a i\nâˆ‚ Î¸(12.16)\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\nperword.Theï¬rsttermisthepositivephaseterm(pushing a yup)whilethe\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\nSamplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\nwhichispreciselywhatwearetryingtoavoid.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "whichispreciselywhatwearetryingtoavoid.\nInsteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\nSÃ©nÃ©cal2003BengioandSÃ©nÃ©cal2008 ,; ,).Thisisanapplicationofamoregeneral\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\ninsection.Unfortunately,evenexactimportancesamplingisnoteï¬ƒcient 17.2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "becauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\nassociatedgradientisweightedby\nw i=p n i /q n iîN\nj=1 p n j /q n j. (12.17)\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\n4 7 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ngradient:\n|| Vî˜\ni=1P i C(|)âˆ‚ a i\nâˆ‚ Î¸â‰ˆ1\nmm î˜\ni=1w iâˆ‚ a n i\nâˆ‚ Î¸. (12.18)\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\ntheparameters,itisalsopossibletosamplefromsuchadistributionveryeï¬ƒciently.\nImportancesamplingisnotonlyusefulforspeedingupmodelswithlarge\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.\nMachinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\ncomputational beneï¬ttousingsparseoutputs,becausethemodelmaychooseto\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\nDauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\nimportancesampling.Theeï¬ƒcientalgorithmminimizesthelossreconstructionfor",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "theâ€œpositivewordsâ€(thosethatarenon-zerointhetarget)andanequalnumber\nofâ€œnegativewords.â€Thenegativewordsarechosenrandomly,usingaheuristicto\nsamplewordsthataremorelikelytobemistaken.Â Thebiasintroducedbythis\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.\nInallofthesecases,thecomputational complexityofgradientestimationfor\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\nratherthanproportionaltothesizeoftheoutputvector.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "ratherthanproportionaltothesizeoftheoutputvector.\n12.4.3.4Noise-ContrastiveEstimationandRankingLoss\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\n4 7 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nscores a i.Therankinglossproposedthenis\nL=î˜\nimax(01 ,âˆ’ a y+ a i) . (12.19)\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\n(includingconditionaltextgenerationtaskssuchastranslation).",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\nandKavukcuoglu2013,).\n12.4.4CombiningNeuralLanguageModelswith-grams n\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\nofcapacity.Incomparison,doublinganeuralnetworkâ€™snumberofparameters\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "networks,canaddparameterswhilereducingthedegreeofparametersharing\ninordertomaintainthesameamountofcomputation. However,typicalneural\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\nproportionaltothenumberofparameters.\nOneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "theensemblemembersmakeindependentmistakes.Theï¬eldofensemblelearning\nprovidesmanywaysofcombiningtheensemblemembersâ€™predictions,including\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\nItisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\n4 7 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\nishugeâ€”thenewportionofthearchitecturecontainsupto|| s Vnparametersâ€”but\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\ntheextrainputsareverysparse.\n12.4.5NeuralMachineTranslation",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "12.4.5NeuralMachineTranslation\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\nemittingasentencewiththeequivalentmeaninginanotherlanguage.Â Mac hine\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\ntranslationswillnotbegrammaticalduetodiï¬€erencesbetweenthelanguages.For\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "directlytheyyieldphrasessuchasâ€œapplered.â€Theproposalmechanismsuggests\nmanyvariantsofthesuggestedtranslation,ideallyincludingâ€œredapple.â€Asecond\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\ntranslations,andcanscoreâ€œredappleâ€asbetterthanâ€œapplered.â€\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "usedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\nmachinetranslationincludenotjusttraditionalback-oï¬€ n-grammodels(Jelinek\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\nentropylanguagemodels(,),inwhichanaï¬ƒne-softmaxlayer Berger e t a l .1996\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\nthatdeï¬nesamarginaldistributionoversomevariabletodeï¬neaconditional\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "inthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\nreplacestheestimateprovidedbyconditional-grammodels. n\n4 7 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nD e c ode rO ut putÂ ob j e c t Â  ( E ngl i s hÂ \ns e nt e nc e )\nI nt e r m e di at e , Â  s e m a n t i c Â  r e pr e s e nt a t i o n\nSourc e Â  ob j e c t Â  ( F r e nc hÂ  s e n t e nc e Â  or Â  i m a g e )E nc ode r\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\nByusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\ntomachinetranslationbutalsotocaptiongenerationfromimages.\nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\npreprocessedtobeofï¬xedlength.Tomakethetranslationmoreï¬‚exible,wewould",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "liketouseamodelthatcanaccommodatevariablelengthinputsandvariable\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\nwhentheinputisasequence.Inallcases,onemodelï¬rstreadstheinputsequence\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\nsummarytheâ€œcontextâ€ C.Thecontext Cmaybealistofvectors,oritmaybea",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "vectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\nnetwork(KalchbrennerandBlunsom2013,).Â Asecondmodel,usuallyanRNN,\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\ninï¬gure.12.5\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\nmodelmusthaveawaytorepresenttheentiresourcesentence.Â Earliermodels",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "wereonlyabletorepresentindividualwordsorphrases.Â Fromarepresentation\n4 7 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\nexploredï¬rstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\nÎ±( t âˆ’ 1 )Î±( t âˆ’ 1 )Î±( ) tÎ±( ) tÎ±( + 1 ) tÎ±( + 1 ) t\nh( t âˆ’ 1 )h( t âˆ’ 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\nÃ— Ã— Ã— Ã— Ã— Ã—+\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\noffeaturevectorsh( ) twithweights Î±( ) t.Insomeapplications,thefeaturevectorshare",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\nweights Î±( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\napproximatesreadingthatonespeciï¬ctimestepprecisely.Theweights Î±( ) tareusually\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "indexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\nattentionmechanismbasedonweightedaveragesisasmooth,diï¬€erentiableapproximation\nthatcanbetrainedwithexistingoptimizationalgorithms.\nUsingaï¬xed-sizerepresentationtocaptureallthesemanticdetailsofavery\nlongsentenceofsay60wordsisverydiï¬ƒcult.Â Itcanbeachievedbytraininga\nsuï¬ƒcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\ne t a l .()and2014aSutskever2014 e t a l .().However,amoreeï¬ƒcientapproachis",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\n4 7 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\nfocusingonadiï¬€erentpartoftheinputsentenceinordertogatherthesemantic\ndetailsthatarerequiredtoproducethenextoutputword.Â Thatisexactlythe\nideathat ()ï¬rstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\ntofocusonspeciï¬cpartsoftheinputsequenceateachtimestepisillustratedin\nï¬gure.12.6\nWecanthinkofanattention-basedsystemashavingthreecomponents:",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "1.Aprocessthatâ€œ r e a d sâ€rawdata(suchassourcewordsinasourcesentence),\nandconvertsthemintodistributedrepresentations,withonefeaturevector\nassociatedwitheachwordposition.\n2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\nunderstoodasaâ€œâ€Â containingasequenceoffacts,whichcanbe m e m o r y\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\nofthem.\n3.Aprocessthatâ€œâ€thecontentofthememorytosequentiallyperform e x p l o i t s",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "atask,ateachtimestephavingtheabilityputattentiononthecontentof\nonememoryelement(orafew,withadiï¬€erentweight).\nThethirdcomponentgeneratesthetranslatedsentence.\nWhenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "wordembeddingsinanother(KoÄiskÃ½2014 e t a l .,),yieldingloweralignmenterror\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\nThereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\n2012).Manyextensionstothisapproacharepossible.Forexample,moreeï¬ƒcient\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\n12.4.6HistoricalPerspective\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "e t a l .()inoneoftheï¬rstexplorationsofback-propagation, withsymbols 1986a\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\nsuchas(Colin,Mother,Victoria).Â The ï¬rstlayeroftheneuralnetworklearned\narepresentationofeachfamilymember.Forexample,Â thefeaturesforColin\n4 7 6",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\nthemotherofColin.\nTheideaofforminganembeddingforasymbolwasextendedtotheideaofan\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\nThehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\npopularityofdiï¬€erentwaysofrepresentingtheinputtothemodel.Following\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\ntheinputasasequenceofcharacters.\nBengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "neuralmodelshavescaledupfromdeï¬ningrepresentationsofasmallsetofsymbols\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\nmodernapplications.Thiscomputational scalingeï¬€ortledtotheinventionofthe\ntechniquesdescribedaboveinsection.12.4.3\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\nimprovedlanguageÂ modeling performance( ,).Tothisday, Bengio e t a l .2001\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\nmodelingindividualbytesofUnicodecharacters.\nTheideasbehindneurallanguagemodelshavebeenextendedintoseveral\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\ntasks.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "tasks.\nTwo-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proï¬leappli-\ncationtovisualizationwordembeddingsbyJosephTurianin2009.\n4 7 7",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\n12. 5 O t h er A p p l i c a t i o n s\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\narediï¬€erentfromthestandardobjectrecognition,speechrecognitionandnatural\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\nscopeevenfurthertotasksthatremainprimarilyresearchareas.\n12.5.1RecommenderSystems\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\nmaderegardingthatproducttothatuser.Theinternetiscurrentlyï¬nancedin",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "greatpartbyvariousformsofonlineadvertising.Â Therearemajorpartsofthe\neconomythatrelyononlineshopping.Â CompaniesincludingAmazonandeBay\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .\nSometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\nforvideogames,ormatchingpeopleindatingservices.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "forvideogames,ormatchingpeopleindatingservices.\nOften,thisassociationproblemishandledlikeasupervisedlearningproblem:\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\ninterest(userclicksonad,userentersarating,userclicksonaâ€œlikeâ€button,user\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "classiï¬cationproblem(predictingtheconditionalprobabilityofsomediscrete\nevent).\nTheearlyworkonrecommendersystemsreliedonminimalinformationas\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\nthetargetvariablefordiï¬€erentusersorfordiï¬€erentitems.Supposethatuser1\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\n4 7 8",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\nthenameofcollaborativeï¬ltering.Bothnon-parametric approaches(suchas\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\ndependonlyoneithertheuserIDortheitemID).LetË†Rbethematrixcontaining\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\npredictionisthusobtainedasfollows:\nË† R u , i= b u+ c i+î˜\njA u , j B j , i . (12.20)\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\nË† R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\nconvenientlyvisualizedwhentheyareï¬rstreducedtoalowdimension(twoor\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "likewordembeddings.Â One waytoobtaintheseembeddingsisbyperforminga\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings).\nThiscorrespondstofactorizingR=UDVî€°(oranormalizedvariant)intothe\nproductoftwofactors,thelowerrankmatricesA=UDandB=Vî€°.One\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\nbasedoptimization. TheSVDandthebilinearpredictionofequation both12.20\nperformedverywellinthecompetitionfortheNetï¬‚ixprize( , BennettandLanning\n2007),aimingatpredictingratingsforï¬lms,basedonlyonpreviousratingsby\nalargesetofanonymoususers.Â Manymachinelearningexpertsparticipatedin\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "improvementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\n4 7 9",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\npresentedbymostofthecompetitors,includingthewinners( ,; TÃ¶scher e t a l .2009\nKoren2009,).\nBeyondthesebilinearmodelswithdistributedrepresentations,oneoftheï¬rst\nusesofneuralnetworksforcollaborativeï¬lteringisbasedontheRBMundirected\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\noftheensembleofmethodsthatwontheNetï¬‚ixcompetition(TÃ¶scher2009 e t a l .,;\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\nMnih2008,).\nHowever,thereisabasiclimitationofcollaborativeï¬lteringsystems:whena\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\nthecold-startrecommendation problemistointroduceextrainformationabout",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "theindividualusersanditems.Forexample,thisextrainformationcouldbeuser\nproï¬leinformationorfeaturesofeachitem.Â Systems thatusesuchinformation\narecalledcontent-basedrecommendersystems.Themappingfromarich\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,).\nSpecializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "audiotracks,formusicrecommendation (vandenOÃ¶rd2013 e t a l .,).Inthatwork,\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.\n12.5.1.1ExplorationVersusExploitation\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\ntheywererecommendedandnottotheotheritems.Â Inaddition,insomecases\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\n4 8 0",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\nwouldbeliketrainingaclassiï¬erbypickingoneclassË† yforeachtrainingexample\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "label yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "someinputvariablethatcaninformthedecision.Forexample,weatleastknow\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\nissueinthereinforcementlearningandbanditsliterature.\nReinforcementlearningrequireschoosingatradeoï¬€betweenexplorationand\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "bestversionofthelearnedpolicyâ€”actionsthatweknowwillachieveahighreward.\nExplorationreferstotakingactionsspeciï¬callyinordertoobtainmoretraining\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\nrewardof1.However,wemayalsowanttoexplorebytryingaction aî€°.Wedonot\nknowwhatwillhappenifwetryaction aî€°.Wehopetogetarewardof,butwe 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\nrewardandthemodelâ€™samountofuncertaintyaboutthatreward.\n4 8 1",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\nOneofthemostprominentfactorsisthetimescaleweareinterestedin.Â Ifthe\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\nmoreexplorationsothatfutureactionscanbeplannedmoreeï¬€ectivelywithmore\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\nmoreexploitation.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "moreexploitation.\nSupervisedÂ learninghasÂ notradeoï¬€Â betweenÂ explorationandÂ exploitation\nbecausethesupervisionsignalalwaysspeciï¬eswhichoutputiscorrectforeach\ninput.Thereisnoneedtotryoutdiï¬€erentoutputstodetermineifoneisbetter\nthanthemodelâ€™scurrentoutputâ€”wealwaysknowthatthelabelisthebestoutput.\nAnotherdiï¬ƒcultyarisinginthecontextofreinforcementlearning,besidesthe\nexploration-exploitationtrade-oï¬€,isthediï¬ƒcultyofevaluatingandcomparing",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\nevaluatethelearnerâ€™sperformanceusingaï¬xedsetoftestsetinputvalues.The\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\ntechniquesforevaluatingcontextualbandits.\n12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\nswering\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "translationandnaturallanguageprocessingduetotheuseofembeddingsfor\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\nrepresentations.\n12.5.2.1Knowledge,RelationsandQuestionAnswering",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "12.5.2.1Knowledge,RelationsandQuestionAnswering\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\nInmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\n4 8 2",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\ndonot.Forexample,wecandeï¬netherelationâ€œislessthanâ€onthesetofentities\n{1 ,2 ,3}bydeï¬ningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\nrelationisdeï¬ned,wecanuseitlikeaverb.Because(1 ,2)âˆˆ S,wesaythat1is\nlessthan2.Because(2 ,1)î€¶âˆˆ S,wecannotsaythat2islessthan1.Ofcourse,the\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeï¬nea\nrelation containingtupleslike(,). is_a_type_of dogmammal\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\nsentencestaketheformofatripletoftokens\n(subjectverbobject) , , (12.21)\nwithvalues\n(entityi ,relation j ,entityk) . (12.22)\nWecanalsodeï¬neanattribute,aconceptanalogoustoarelation,buttaking\nonlyoneargument:\n(entity i ,attribute j) . (12.23)\nForexample,wecoulddeï¬nethehas_furattribute,andapplyittoentitieslike\ndog.",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "dog.\nManyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\nHowshouldwebestdothiswithinthecontextofneuralnetworks?\nMachinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\nTherearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\nkindofinformation,Â alb eitÂ notformattedasthreetokensentences.Whena",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "databaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\nexpertknowledgeaboutanapplicationareatoanartiï¬cialintelligencesystem,\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).2013a\n1R e s p e c t i v e l y a v a i l a b l e Â  f ro m t h e s e Â  w e b Â  s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t .\np r i n c e t o n . e d u w i k i b a . s e ,\n2g e n e o n t o l o g y . o r g\n4 8 3",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nInadditiontotrainingdata,wealsoneedtodeï¬neamodelfamilytotrain.\nAcommonapproachistoextendneurallanguagemodelstomodelentitiesand\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.\nEarlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "2000)positedhighlyconstrainedparametricforms(â€œlinearrelationalembeddingsâ€),\noftenusingadiï¬€erentformofrepresentationfortherelationthanfortheentities.\nForexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\ne t a l .,),allowingustomakestatementsaboutrelations,butmoreï¬‚exibilityis 2012",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "putinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\nApracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .\n(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "application.\nEvaluatingtheperformanceofamodelonalinkpredictiontaskisdiï¬ƒcult\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\nbetrue).Â Ifthemodelproposesafactthatisnotinthedataset,weareunsure\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\nintherelationwithadiï¬€erententityselectedatrandom.Thepopularprecisionat\n10%metriccountshowmanytimesthemodelranksaâ€œcorrectâ€factamongthe\ntop10%ofallcorruptedversionsofthatfact.\n4 8 4",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER12.APPLICATIONS\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\none,insomecontext.\nEventually,knowledgeofrelationscombinedwithareasoningprocessand\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "inputinformationandrememberimportantfacts,organizedinawaythatenables\nittoretrieveandreasonaboutthemlater.Thisremainsadiï¬ƒcultopenproblem\nwhichcanonlybesolvedinrestrictedâ€œtoyâ€environments.Currently,thebest\napproachtorememberingandretrievingspeciï¬cdeclarativefactsistousean\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\nï¬rstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).\ne t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "theinputintothememoryandtoproducetheanswergiventhecontentsofthe\nmemory.\nDeeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\nasofthiswriting.\nThisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "networks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\nmethodsinvolveusingthegradientofacostfunctiontoï¬ndtheparametersofa\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\nterritoryofresearchâ€”methodsthataredesignedtoworkwithlesstrainingdata\nortoperformagreatervarietyoftasks,wherethechallengesaremorediï¬ƒcult\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\n4 8 5",
    "metadata": {
      "source": "[17]part-2-chapter-12.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "P a rt I\nAppliedMathandMachine\nLearningBasics\n29",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\nallo w us t o deï¬ne f unctions of many v ariables , ï¬ nd t he highes t and low e s t p oints\non t hes e f unctions and q uantify degrees of b e lief.\nN e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "t o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction.\nThis e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\nalgorithms , including approac hes t o machine learning t hat are not deep.Â In t he",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "s ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\nf r amew ork.\n3 0",
    "metadata": {
      "source": "[5]part-1-basics.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 4\nA u t o e n co d e rs\nAn aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput\ntoitsoutput.Â Internally ,ithasahiddenlayer hthatdescribesa c o deusedto\nrepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\nencoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h).\nThisarchitectureispresentedinï¬gure.Ifanautoencodersucceedsinsimply 14.1\nlearningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\nrestrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly\ninputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\nwhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\ndata.\nModernÂ autoencodersÂ havegeneralizedÂ theÂ idea ofÂ anencoderÂ andÂ ade-\ncoderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and\npdecoder( ) x h|.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "pdecoder( ) x h|.\nTheideaofautoencodershasbeenpartofthehistoricallandscapeofneural\nnetworksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\n1994).Traditionally,Â autoencoderswereusedÂ fordimensionalityreductionor\nfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersand\nlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\nmodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\naspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "techniques,typicallyminibatchgradientdescentfollowinggradientscomputed\nbyback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\nalsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning\nalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\n502",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\ntotheactivationsonthereconstructedinput.Recirculationisregardedasmore\nbiologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning\napplications.\nxx rrh h\nf g\nFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\n(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder\nhastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto\nr).\n14.1UndercompleteAutoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "r).\n14.1UndercompleteAutoencoders\nCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\ninterestedintheoutputoftheÂ decoder. Instead,Â wehopeÂ thattrainingthe\nautoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful\nproperties.\nOnewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto\nhavesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless\nthantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "representationforcestheautoencodertocapturethemostsalientfeaturesofthe\ntrainingdata.\nThelearningprocessisdescribedsimplyasminimizingalossfunction\nL , g f ( x(())) x (14.1)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthemeansquarederror.\nWhenthedecoderislinearand Listhemeansquarederror,anundercomplete\nautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\ntrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\ntrainingdataasaside-eï¬€ect.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "trainingdataasaside-eï¬€ect.\nAutoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-\ntions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-\n5 0 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\ncanlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\nthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\nwithaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto\nrepresenteachtrainingexample x() iwiththecode i.Thedecodercouldlearnto\nmaptheseintegerindicesbacktothevaluesofspeciï¬ctrainingexamples.This",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "speciï¬cscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\ncodertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\nthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\n14.2RegularizedAutoencoders\nUndercomplete autoencoders,withcodedimensionlessthantheinputdimension,\ncanlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\ntheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\ngiventoomuchcapacity.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "giventoomuchcapacity.\nAsimilarproblemoccursifthehiddencodeisallowedtohavedimension\nequaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas\ndimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear\ndecodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\naboutthedatadistribution.\nIdeally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\nthecodedimensionandthecapacityoftheencoderanddecoderbasedonthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\nabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\nanddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\nfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheability\ntocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\nrepresentation,smallnessofthederivativeoftherepresentation,androbustness\ntonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "overcompletebutstilllearnsomethingusefulaboutthedatadistributionevenif\nthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.\nInadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted\nasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\nandequippedwithaninferenceprocedure(forcomputinglatentrepresentations\ngiveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\nmodelingapproachesthatemphasizethisconnectionwithautoencodersarethe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "descendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b\n5 0 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12\nThesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\nanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\narenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\ntheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\n1 4 . 2 . 1 S p a rse A u t o en co d ers\nAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "sparsitypenaltyâ„¦( h)onthecodelayer h,inadditiontothereconstructionerror:\nL , g f ( x(()))+â„¦() x h (14.2)\nwhere g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder\noutput.\nSparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch\nasclassiï¬cation.Anautoencoderthathasbeenregularizedtobesparsemust\nrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\nthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\nfeaturesasabyproduct.\nWecanthinkÂ ofthepenalty â„¦( h)simplyasaregularizertermaddedto\nafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\n(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\n(withÂ asupervisedÂ learningÂ ob jective)Â thatdependsÂ onÂ thesesparsefeatures.\nUnlikeotherregularizerssuchasweightdecay,thereisnotastraightforward",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Bayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1\nwithweightdecayandotherregularizationpenaltiescanbeinterpretedasa\nMAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\ncorrespondingtoapriorprobabilitydistributionoverthemodelparameters.In\nthisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( Î¸ x|),\nwhichisequivalenttomaximizing log p( x Î¸|)+log p( Î¸).Â The log p( x Î¸|)term\nistheusualdatalog-likelihoodtermandthelog p( Î¸)term,thelog-priorover",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "parameters,incorporatesthepreferenceoverparticularvaluesof Î¸.Thisviewwas\ndescribedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\nbecausetheregularizerdependsonthedataandisthereforebydeï¬nitionnota\npriorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\ntermsasimplicitlyexpressingapreferenceoverfunctions.\nRatherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\ntask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\n5 0 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nmaximumlikelihoodÂ trainingofagenerativemodelÂ thathaslatentvariables.\nSupposewehaveamodelwithvisiblevariables xandlatentvariables h,with\nanexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto\npmodel( h)asthemodelâ€™spriordistributionoverthelatentvariables,representing\nthemodelâ€™sbeliefspriortoseeing x.Thisisdiï¬€erentfromthewaywehave\npreviouslyusedthewordâ€œprior,â€torefertothedistribution p( Î¸)encodingour",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "beliefsaboutthemodelâ€™sparametersbeforewehaveseenthetrainingdata.The\nlog-likelihoodcanbedecomposedas\nlog pmodel() = log xî˜\nhpmodel( ) h x , . (14.3)\nWecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\nforjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative\nmodel(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather\nthantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof\nview,withthischosen,wearemaximizing h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "view,withthischosen,wearemaximizing h\nlog pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)\nThelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,\npmodel( h i) =Î»\n2eâˆ’| Î» h i|, (14.5)\ncorrespondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\nabsolutevaluepenalty,weobtain\nâ„¦() = h Î»î˜\ni| h i| (14.6)\nâˆ’log pmodel() = hî˜\niî€’\nÎ» h| i|âˆ’logÎ»\n2î€“\n= â„¦()+const h (14.7)\nwheretheconstanttermdependsonlyon Î»andnot h.Wetypicallytreat Î»asa",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "hyperparameteranddiscardtheconstanttermsinceitdoesnotaï¬€ecttheparameter\nlearning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From\nthispointofviewofsparsityasresultingfromtheeï¬€ectof pmodel( h)onapproximate\nmaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\nall.Â Itisjustaconsequenceofthemodelâ€™sdistributionoveritslatentvariables.\nThisviewprovidesadiï¬€erentmotivationfortraininganautoencoder:itisaway",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ofapproximately trainingagenerativemodel.Italsoprovidesadiï¬€erentreasonfor\n5 0 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\nvariablesthatexplaintheinput.\nEarlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008\nvariousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\nandthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected\nprobabilisticmodel p( x) =1\nZËœ p( x).Theideaisthatminimizing log Zpreventsa\nprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "onÂ anautoencoderÂ preventstheautoencoderfromÂ havingÂ lowreconstruction\nerroreverywhere.Inthiscase,Â theconnectionisonthelevelofanintuitive\nunderstandingofageneralmechanismratherthanamathematical correspondence.\nTheinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina\ndirectedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward.\nOnewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "wasintroducedin ().Theideaistouserectiï¬edlinearunitsto Glorot e t a l .2011b\nproducethecodelayer.Withapriorthatactuallypushestherepresentationsto\nzero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\nnumberofzerosintherepresentation.\n1 4 . 2 . 2 D en o i s i n g A u t o en co d ers\nRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder â„¦ \nthatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\nfunction.\nTraditionally,autoencodersminimizesomefunction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Traditionally,autoencodersminimizesomefunction\nL , g f ( x(())) x (14.8)\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\nthe L2normoftheirdiï¬€erence.Â This encourages g fâ—¦tolearntobemerelyan\nidentityfunctioniftheyhavethecapacitytodoso.\nA orDAEinsteadminimizes denoising aut o e nc o der\nL , g f ( x((Ëœ x))) , (14.9)\nwhere Ëœ xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising\nautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\ninput.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "input.\nDenoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),\nasshownÂ byÂ  ()Â and ().Denoising AlainandBengio2013BengioÂ  e t a l .2013c\n5 0 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\nasabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\nhowovercomplete,high-capacity modelsmaybeusedasautoencoderssolong\nascareistakentopreventthemfromlearningtheidentityfunction.Â Denoising\nautoencodersarepresentedinmoredetailinsection.14.5\n1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es\nAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse â„¦",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "autoencoders,\nL , g f , , ( x(()))+â„¦( x h x) (14.10)\nbutwithadiï¬€erentformof:â„¦\nâ„¦( ) = h x , Î»î˜\ni||âˆ‡ x h i||2. (14.11)\nThisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\nchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\ntheautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\ndistribution.\nAnautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der\norCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail\ninsection.14.7\n14.3RepresentationalPower,LayerSizeandDepth\nAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\ndecoder.However,thisisnotarequirement.Infact,usingdeepencodersand\ndecodersoï¬€ersmanyadvantages.\nRecallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\nwardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork\nasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\nbeneï¬tfromdepth.\nOnemajoradvantageofnon-trivialdepthisthattheuniversalapproximator\ntheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\nlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan\n5 0 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\narbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\nthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\nfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrom\ninputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\nconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\nleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any\nmappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Depthcanexponentiallyreducethecomputational costofrepresentingsome\nfunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\nneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\ndepthinfeedforwardnetworks.\nExperimentally,deepautoencodersyieldmuchbettercompressionthancorre-\nspondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\nAcommonstrategyfortrainingadeepautoencoderistogreedilypretrain\nthedeeparchitecturebytrainingastackofshallowautoencoders,soweoften",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "encountershallowautoencoders,evenwhentheultimategoalistotrainadeep\nautoencoder.\n14.4StochasticEncodersandDecoders\nAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\nunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\nautoencoders.\nAsdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\nandthelossfunctionofafeedforwardnetworkistodeï¬neanoutputdistribution\np( y x|)andminimizethenegativelog-likelihoodâˆ’log p( y x|).Inthatsetting, y",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "wasavectoroftargets,suchasclasslabels.\nInthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,\nwecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay\nthinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|).Â We\nmaythentraintheautoencoderbyminimizing âˆ’log pdecoder( ) x h|.Theexact\nformofthislossfunctionwillchangedependingontheformof pdecoder.Aswith\ntraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "themeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative\nlog-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues\ncorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid\noutputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon.\n5 0 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nTypically,theoutputvariablesaretreatedasbeingconditionallyindependent\ngiven hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome\ntechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs\nwithcorrelations.\nxx rrh h\np e n c o d e r ( ) h x| p d e c o d e r ( ) x h|\nFigure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\ndecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "theiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder\nand pdecoder( ) x h|forthedecoder.\nTomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\npreviously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to\nan e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedinï¬gure.14.2\nAnylatentvariablemodel pmodel( ) h x ,deï¬nesastochasticencoder\npencoder( ) = h x| pmodel( ) h x| (14.12)\nandastochasticdecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andastochasticdecoder\npdecoder( ) = x h| pmodel( ) x h| . (14.13)\nIngeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\ndistributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l .\n()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\nwilltendtomakethemcompatibleasymptotically(withenoughcapacityand\nexamples).\n14.5DenoisingAutoencoders\nThe denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\nasitsoutput.\nTheDAEtrainingprocedureisillustratedinï¬gure.Weintroducea 14.3\ncorruptionprocess C(Ëœx x|)whichrepresentsaconditionalÂ distrib utionover\n5 1 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nËœ x Ëœ x L Lh h\nfg\nxxC ( Ëœ x x| )\nFigure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder,\nwhichistrainedtoreconstructthecleandatapoint xfromitscorruptedversionËœ x.\nThisisaccomplishedbyminimizingtheloss L=âˆ’log pdecoder( x h|= f(Ëœ x)),where\nËœ xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption\nprocess C(Ëœ x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean\nparametersareemittedbyafeedforwardnetwork. g",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "parametersareemittedbyafeedforwardnetwork. g\ncorruptedsamples Ëœ x,givenadatasample x.Theautoencoderthenlearnsa\nr e c o nst r u c t i o n di st r i but i o n preconstruct( x|Ëœ x)estimatedfromtrainingpairs\n( x ,Ëœ x),asfollows:\n1.Â Sampleatrainingexamplefromthetrainingdata. x\n2.Â SampleacorruptedversionËœ xfrom C(Ëœ x x|= ) x.\n3.Use( x ,Ëœ x)asatrainingexampleforestimatingtheautoencoderreconstruction\ndistribution preconstruct( x|Ëœx) = pdecoder( x h|)with htheoutputofencoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "f(Ëœ x)and pdecodertypicallydeï¬nedbyadecoder. g() h\nTypicallywecansimplyperformgradient-basedapproximate minimization (such\nasminibatchgradientdescent)onthenegativelog-likelihoodâˆ’log pdecoder( x h|).\nSolongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\nnetworkÂ andmayÂ beÂ trainedwithÂ exactlythesameÂ techniquesÂ asÂ anyother\nfeedforwardnetwork.\nWecanthereforeviewtheDAEasperformingstochasticgradientdescenton\nthefollowingexpectation:",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "thefollowingexpectation:\nâˆ’ E xâˆ¼Ë† p d a t a() x EËœ xâˆ¼ C(Ëœx| x)log pdecoder( = ( x h| fËœ x))(14.14)\nwhere Ë† pdata() xisthetrainingdistribution.\n5 1 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nxËœ x\ng fâ—¦\nËœ x\nC ( Ëœ x x| )\nx\nFigure14.4:AdenoisingautoencoderistrainedtomapacorrupteddatapointËœxbackto\ntheoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara\nlow-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption\nprocess C(Ëœx x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\nhowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\n|| g( f(Ëœ x))âˆ’|| x2,thereconstruction g( f(Ëœ x)) estimates E x ,Ëœ xâˆ¼ p dat a()( x CËœx x|)[ x|Ëœ x].Thevector\ng( f(Ëœx))âˆ’Ëœ xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(Ëœx))\nestimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenrisetoËœ x.The\nautoencoderthuslearnsavectorï¬eld g( f( x))âˆ’ xindicatedbythegreenarrows.This\nvectorï¬eldestimatesthescoreâˆ‡ xlog pdata( x)uptoamultiplicativefactorthatisthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "averagerootmeansquarereconstructionerror.\n5 1 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n1 4 . 5 . 1 E s t i m a t i n g t h e S co re\nScorematching(,)isanalternativetomaximumlikelihood.It HyvÃ¤rinen2005\nprovidesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\nthemodeltohavethesame sc o r easthedatadistributionateverytrainingpoint\nx.Inthiscontext,thescoreisaparticulargradientï¬eld:\nâˆ‡ xlog() p x . (14.15)\nScorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4\nregardingautoencoders,itissuï¬ƒcienttounderstandthatlearningthegradient",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "ï¬eldoflog pdataisonewaytolearnthestructureof pdataitself.\nAveryimportantpropertyofDAEsisthatÂ theirtrainingcriterion(with\nconditionallyGaussian p( x h|))makesÂ theautoencoderÂ learnavectorï¬eld\n( g( f( x))âˆ’ x)thatestimatesthescoreofthedatadistribution.Thisisillustrated\ninï¬gure.14.4\nDenoisingtrainingofaspeciï¬ckindofautoencoder(sigmoidalhiddenunits,\nlinearÂ reconstr uctionÂ units) usingGaussiannoiseandÂ meansquaredÂ erroras\nthereconstructioncostisequivalent(,)totrainingaspeciï¬ckind Vincent2011",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.\nThiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1\ndiscussionitsuï¬ƒcestoknowthatitisamodelthatprovidesanexplicit pmodel( x; Î¸).\nWhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun\n2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\nautoencoder.Withaï¬xednoiselevel,regularizedscorematchingisnotaconsistent",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "estimator;itinsteadrecoversablurredversionofthedistribution.However,if\nthenoiselevelischosentoapproach0whenthenumberofexamplesapproaches\ninï¬nity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\nmoredetailinsection.18.5\nOtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\nappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\ncombinedwitharegularizationtermsimilartothecontractivepenaltyofthe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen-\ncodergradientprovidesanapproximationtocontrastivedivergencetrainingof\nRBMs.\nForcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand\nreconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\ngeneralencoderanddecoderparametrizations ( ,).This AlainandBengio2013\nmeansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\n5 1 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nbytrainingwiththesquarederrorcriterion\n|| g f((Ëœ x x))âˆ’||2(14.16)\nandcorruption\nC(Ëœ x=Ëœx x|) = (NËœ x x ;= Âµ , ÏƒÎ£ = 2I) (14.17)\nwithnoisevariance Ïƒ2.Seeï¬gureforanillustrationofhowthisworks. 14.5\nFigure14.5:Vectorï¬eldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold\nnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe\nreconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "accordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorï¬eldhaszeros\natbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima\nofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof\nlocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\nthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength\nofthearrows)islarge,itmeansthatprobabilitycanbesigniï¬cantlyincreasedbymoving",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "inthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.\nTheautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.\nWhereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore\naccurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013\nIngeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe\ninput xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis\n5 1 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nwhytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011\nwhere g( f( x))âˆ’ xmaybeobtainedbytakingthederivativeofanotherfunction.\nKamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by\nidentifyingafamilyofshallowautoencoderssuchthat g( f( x))âˆ’ xcorrespondsto\nascoreforallmembersofthefamily.\nSofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\naprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "agenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed\nlater,insection.20.11\n1 4 . 5 . 1 . 1 Hi st o r i c a l P e r spec t i v e\nTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987\nand ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001\nimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\nHowever,thenameâ€œdenoisingautoencoderâ€referstoamodelthatisintendednot\nmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "asÂ asideeï¬€ectÂ oflearningtoÂ denoise.ThisÂ ideacameÂ muchlaterÂ (Vincent\ne t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\ndeeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\nsparsecoding,contractiveautoencodersandotherregularizedautoencoders,the\nmotivationforDAEswastoallowthelearningofaveryhigh-capacity encoder\nwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\nPriortotheintroduction ofthemodernDAE,InayoshiandKurita2005()",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\nminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\nnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\ngeneralization byintroducingÂ the reconstructionerrorÂ andtheinjectednoise.\nHowever,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction\nfamiliesaspowerfulascanthemodernDAE.\n14.6LearningManifoldswithAutoencoders\nLikeÂ manyÂ otherÂ machineÂ learningÂ algorithms,Â auto encodersÂ exploittheidea",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\nmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3\nthisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe\nmanifoldbutmayhaveunusualbehaviorifgivenaninputthatisoï¬€themanifold.\n5 1 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\nTounderstandhowautoencodersdothis,wemustpresentsomeimportant\ncharacteristicsofmanifolds.\nAnimportantcharacterization ofamanifoldisthesetofits t angen t pl anes.\nAtapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis\nvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\nillustratedinï¬gure,theselocaldirectionsspecifyhowonecanchange 14.6 x",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "inï¬nitesimallywhilestayingonthemanifold.\nAllautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\n1.Learningarepresentation hofatrainingexample xsuchthat xcanbe\napproximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn\nfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed\nnotsuccessfullyreconstructinputsthatarenotprobableunderthedata\ngeneratingdistribution.\n2.Â Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\naregularizationtermaddedtothereconstructioncost.Thesetechniques\ngenerallyprefersolutionsthatarelesssensitivetotheinput.\nClearly,neitherforcealonewouldbeusefulâ€”copyingtheinputtotheoutput\nisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\nareusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\naboutthestructureofthedatageneratingdistribution.Theimportantprinciple",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "isthattheautoencodercanaï¬€ordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d\nt o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates\nnearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture\nalocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\nmanifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder\nlearnsamappingfromtheinputspace xtoarepresentationspace,amappingthat",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\nchangesorthogonaltothemanifold.\nAone-dimensional exampleisillustratedinï¬gure,showingthat,bymaking 14.7\nthereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\ndatapoints,wecausetheautoencodertorecoverthemanifoldstructure.\nTounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\nstructivetocomparethemtootherapproaches.Whatismostcommonlylearned",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "tocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)\n5 1 6",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.6:Â Anillustrationoftheconceptofatangenthyperplane.Herewecreatea\none-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784\npixelsandtransformitbytranslatingitvertically.Â Theamountofverticaltranslation\ndeï¬nesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath\nthroughimagespace.Thisplotshowsafewpointsalongthismanifold.Â Forvisualization,\nwehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "manifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches\nthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.\nItdeï¬nesthespaceofdirectionsinwhichitispossibletomovewhileremainingon\nthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean\nexampletangentlineatonepoint,withanimageshowinghowthistangentdirection\nappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "thetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels\nthatdarken.\n5 1 7",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nx 0 x 1 x 2\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y\nO p t i m a l r e c o n s t r u c t i o n\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\nperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\nthemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\nlineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\narrowsatthebottomoftheplotindicatethe r( x)âˆ’ xreconstructiondirectionvector\natthebaseofthearrow,ininputspace,alwayspointingtowardsthenearestâ€œmanifoldâ€\n(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake\nthederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The\ncontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\nspacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\nthereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints\nbackontothemanifold.\nthemanifold.Sucharepresentationforaparticularexampleisalsocalledits\nembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions\nthantheâ€œambientâ€spaceofwhichthemanifoldisalow-dimensionalsubset.Some",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "algorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly\nlearnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\nmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\npointintheambientspace(theinputspace)toitsembedding.\nManifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\nattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\nonlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "onthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample\nandedgesconnectingnearneighborstoeachother.Thesemethods(SchÃ¶lkopf\ne t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;\n5 1 8",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph\ninwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor\nrelationships.Â Variousprocedurescanthusobtainthetangentplaneassociatedwitha\nneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\nexamplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize\nsucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\napproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l .\n2000).\nandNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\nandRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha\ntangentplanethatspansthedirectionsofvariationsassociatedwiththediï¬€erence\nvectorsbetweentheexampleanditsneighbors,asillustratedinï¬gure.14.8\nAglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "solvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9\nlargenumberoflocallylinearGaussian-likepatches(orâ€œpancakes,â€becausethe\nGaussiansareï¬‚atinthetangentdirections).\nHowever,thereisafundamentaldiï¬ƒcultywithsuchlocalnon-parametric\napproachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005\nmanifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),\nonemayneedaverylargenumberoftrainingexamplestocovereachoneof\n5 1 9",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nFigure14.9:Ifthetangentplanes(seeï¬gure)ateachlocationareknown,thenthey 14.6\ncanbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\ncanbethoughtofasalocalEuclideancoordinatesystemorasalocallyï¬‚atGaussian,or\nâ€œpancake,â€withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\nverylargevarianceinthedirectionsdeï¬ningthecoordinatesystemonthepancake.A\nmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Parzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003\nvariant( ,). Bengio e t a l .2006c\nthesevariations,withnochancetogeneralizetounseenvariations.Indeed,these\nmethodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between\nneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan\nhaveverycomplicatedstructurethatcanbediï¬ƒculttocapturefromonlylocal\ninterpolation.Considerforexamplethemanifoldresultingfromtranslationshown",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "inï¬gure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe\nimageistranslated,wewillobservethatonecoordinateencountersapeakora\ntroughinitsvalueonceforeverypeakortroughinbrightnessintheimage.Â In\notherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage\ntemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming\nsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentations\nanddeeplearningforcapturingmanifoldstructure.\n5 2 0",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\n14.7ContractiveAutoencoders\nThecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab\nonthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:\nâ„¦() = h Î»î€î€î€î€âˆ‚ f() x\nâˆ‚ xî€î€î€î€2\nF. (14.18)\nThepenaltyâ„¦( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\nJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\nThereisaconnectionbetweenthedenoisingautoencoderandthecontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\ninputÂ noise,Â theÂ denoisingÂ reconstruction errorisÂ equivalentÂ toacontractive\npenaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother\nwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\nï¬nite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\nfeatureextractionfunctionresistinï¬nitesimalperturbationsoftheinput.When\nusingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "withaclassiï¬er,thebestclassiï¬cationaccuracyusuallyresultsfromapplyingthe\ncontractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)\nalsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1\nThename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speciï¬-\ncally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\ntomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\nWecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "neighborhood.\nToclarify,theCAEiscontractiveonlylocallyâ€”allperturbationsofatraining\npoint xaremappednearto f( x).Globally,twodiï¬€erentpoints xand xî€°maybe\nmappedto f( x)and f( xî€°)pointsthatarefartherapartthantheoriginalpoints.\nItisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see\nforexamplewhathappensinthe1-Dtoyexampleofï¬gure).Whenthe 14.7 â„¦( h)\npenaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto\nmakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa\nbinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\nmostofthehypercubethatitssigmoidalhiddenunitscanspan.\nWecanthinkoftheJacobianmatrix Jatapoint xasapproximating the\nnonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword\nâ€œcontractiveâ€moreformally.Â Inthetheoryoflinearoperators,alinearoperator\n5 2 1",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nissaidtobecontractiveifthenormof J xremainslessthanorequaltofor1\nallunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere.\nWecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear\napproximationof f( x)ateverytrainingpoint xinordertoencourageeachof\ntheselocallinearoperatortobecomeacontraction.\nAsdescribedÂ insection,Â regularized autoencoderslearnmanifoldsby 14.6\nbalancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "reconstructionerrorandthecontractivepenaltyâ„¦( h).Reconstructionerroralone\nwouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\nalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\nThecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\nâˆ‚ f() x\nâˆ‚ xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\nsmallnumberofdirectionsintheinput,mayhavesigniï¬cantderivatives.\nThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "xwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich\napproximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a\nand ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b\nof Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1\nsomesingularvaluesremainabove,becausethereconstructionerrorpenalty 1\nencouragestheCAEtoencodethedirectionswiththemostlocalvariance.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "directionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent\ndirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent\ndirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE\nappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas\nobjectsintheimagegraduallychangepose,asshowninï¬gure.Visualizations 14.6\noftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful\ntransformationsoftheinputimage,asshowninï¬gure.14.10",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\nischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\nmuchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\nRifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each\ntrainedtoreconstructthepreviousautoencoderâ€™shiddenlayer.Thecomposition\noftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\nseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\ntheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\ncapturesmanyofthedesirablequalitativecharacteristics.\nAnotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\n5 2 2",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nInput\npointTangentvectors\nLocalPCA(nosharingacrossregions)\nContractiveautoencoder\nFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\nandbyacontractiveautoencoder.Thelocationonthemanifoldisdeï¬nedbytheinput\nimageofadogdrawnfromtheCIFAR-10dataset.Â Thetangentvectorsareestimated\nbytheleadingsingularvectorsoftheJacobianmatrixâˆ‚ h\nâˆ‚ xoftheinput-to-codemapping.\nAlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\nsharingacrossdiï¬€erentlocationsthatshareasubsetofactivehiddenunits.Â TheCAE\ntangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\ntheheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c\nifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\ncouldconsistofmultiplyingtheinputbyasmallconstant î€andthedecoder",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "couldconsistofdividingthecodeby î€.As î€approaches,theencoderdrivesthe 0\ncontractivepenaltyâ„¦( h)toapproachwithouthavinglearnedanythingaboutthe 0\ndistribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\ne t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare\nstandardneuralnetworklayersconsistingofanaï¬ƒnetransformationfollowedby\nanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g\ntobethetransposeoftheweightmatrixof. f",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "tobethetransposeoftheweightmatrixof. f\n14.8PredictiveSparseDecomposition\nP r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse\ncodingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric\nencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\nappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\n(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "asforaudio( ,).Themodelconsistsofanencoder Henaï¬€ e t a l .2011 f( x)anda\ndecoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe\n5 2 3",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\noptimization algorithm.Trainingproceedsbyminimizing\n||âˆ’ || x g() h2+ Î»|| h1+ () Î³ f ||âˆ’ h x||2. (14.19)\nLikeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with\nrespectto handminimization withrespecttothemodelparameters.Minimization\nwithrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe\ncostfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent\ncanobtainreasonablevaluesofinasfewastensteps. h",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "canobtainreasonablevaluesofinasfewastensteps. h\nThetrainingprocedureusedbyPSDisdiï¬€erentfromï¬rsttrainingasparse\ncodingmodelandthentraining f( x)topredictthevaluesofthesparsecoding\nfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparameters\nforwhichcaninfergoodcodevalues. f() x\nPredictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e.\nInsection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\nprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\nmodel.\nInpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring\ntraining.Theparametricencoder fisusedtocomputethelearnedfeatureswhen\nthemodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto\ninferring hviagradientdescent.Because fisadiï¬€erentiableparametricfunction,\nPSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "withanothercriterion.\n14.9ApplicationsofAutoencoders\nAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\nmationretrievaltasks.Dimensionalityreductionwasoneoftheï¬rstapplications\nofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\nforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\nastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\nwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand\nthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\nunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\nLower-dimensionalrepresentationscanimproveperformanceonmanytasks,\nsuchasclassiï¬cation.Modelsofsmallerspacesconsumelessmemoryandruntime.\n5 2 4",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER14.AUTOENCODERS\nManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\neachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l .\n().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\ngeneralization.\nOnetaskthatbeneï¬tsevenmorethanusualfromdimensionalityreductionis\ni nf o r m at i o n r e t r i e v al,thetaskofï¬ndingentriesinadatabasethatresemblea\nqueryentry.Â Thistaskderivestheusualbeneï¬tsfromdimensionalityreduction",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "thatothertasksdo,butalsoderivestheadditionalbeneï¬tthatsearchcanbecome\nextremelyeï¬ƒcientincertainkindsoflowdimensionalspaces.Speciï¬cally,Â if\nwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\ndimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary\nmappingbinarycodevectorstoentries.Thishashtableallowsustoperform\ninformationretrievalbyreturningalldatabaseentriesthathavethesamebinary\ncodeastheÂ query.WecanalsoÂ searchÂ overslightlylesssimilarÂ entriesÂ very",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "eï¬ƒciently,justbyï¬‚ippingindividualbitsfromtheencodingofthequery.Â This\napproachtoinformationretrievalviadimensionalityreductionandbinarization\niscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas\nbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\nimages(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,).\nToproducebinarycodesforsemantichashing,onetypicallyusesanencoding\nfunctionwithsigmoidsontheï¬nallayer.Thesigmoidunitsmustbetrainedtobe",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\nthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\ntraining.Themagnitudeofthenoiseshouldincreaseovertime.Toï¬ghtthat\nnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\nmagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\nTheideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\ndirections,includingtheideaoftrainingtherepresentationssoastooptimize",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "alossmoredirectlylinkedtothetaskofï¬ndingnearbyexamplesinthehash\ntable( ,). NorouziandFleet2011\n5 2 5",
    "metadata": {
      "source": "[20]part-3-chapter-14.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "N ot at i o n\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\nconcepts,wedescribemostoftheseideasinchapters2â€“4.\nNum b e r s and Ar r a y s\naAscalar(integerorreal)\naAvector\nAAmatrix\nAAtensor\nI nIdentitymatrixwithrowsandcolumns n n\nIIdentitymatrixwithdimensionalityimpliedby\ncontext\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\n1atposition i\ndiag()aAsquare,diagonalmatrixwithdiagonalentries",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "diag()aAsquare,diagonalmatrixwithdiagonalentries\ngivenbya\naAscalarrandomvariable\naAvector-valuedrandomvariable\nAAmatrix-valuedrandomvariable\nxi",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nSet s and G r aphs\nAAset\nRThesetofrealnumbers\n{}01 ,Thesetcontaining0and1\n{ } 01 , , . . . , nThesetofallintegersbetweenand0 n\n[] a , bTherealintervalincludingand a b\n(] a , bTherealintervalexcludingbutincluding a b\nA B\\Setsubtraction,i.e.,Â thesetcontainingtheele-\nmentsofthatarenotin A B\nGAgraph\nP a G(x i)Theparentsofx iinG\nI ndexing\na iElement iofvectora,withindexingstartingat1\na âˆ’ iAllelementsofvectorexceptforelementa i\nA i , jElementofmatrix i , jA\nA i , :Rowofmatrix iA",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "A i , :Rowofmatrix iA\nA : , iColumnofmatrix iA\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\nA : : , , i2-Dsliceofa3-Dtensor\na iElementoftherandomvector i a\nL i near Al g e br a O p e r at i o ns\nAî€¾TransposeofmatrixA\nA+Moore-PenrosepseudoinverseofA\nABî€ŒElement-wise(Hadamard)productofandAB\ndet()ADeterminantofA\nx i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCal c ul usd y\nd xDerivativeofwithrespectto y x\nâˆ‚ y\nâˆ‚ xPartialderivativeofwithrespectto y x\nâˆ‡ x yGradientofwithrespectto y x\nâˆ‡ X yMatrixderivativesofwithrespectto y X\nâˆ‡ X yTensorcontainingderivativesof ywithrespectto\nX\nâˆ‚ f\nâˆ‚xJacobianmatrixJâˆˆ Rm n Ã—of f: Rnâ†’ Rm\nâˆ‡2\nx f f f () (xorH)()xTheHessianmatrixofatinputpointxîš\nf d()xxDeï¬niteintegralovertheentiredomainofx\nîš\nSf d()xx x Deï¬niteintegralwithrespecttoovertheset S\nP r o babil i t y and I nf o r m at i o n T heor y",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "abTherandomvariablesaandbareindependent âŠ¥\nabcTheyareconditionallyindependentgivenc âŠ¥|\nP()aAprobabilitydistributionoveradiscretevariable\np()aAprobabilitydistributionoveracontinuousvari-\nable,oroveravariablewhosetypehasnotbeen\nspeciï¬ed\na Randomvariableahasdistribution âˆ¼ P P\nE x âˆ¼ P[()] () () () f xor E f xExpectationof f xwithrespectto Px\nVar(()) f xVarianceofunderx f x() P()\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\nH()xShannonentropyoftherandomvariablex",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "H()xShannonentropyoftherandomvariablex\nD K L( ) P Qî«Kullback-LeiblerdivergenceofPandQ\nN(; )xÂµ ,Î£GaussiandistributionoverxwithmeanÂµand\ncovarianceÎ£\nx i i i",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nF unc t i o ns\nf f : A Bâ†’Thefunctionwithdomainandrange A B\nf g f g â—¦Compositionofthefunctionsand\nf(;)xÎ¸Afunctionofxparametrized byÎ¸.Â (Sometimes\nwewrite f(x)andomittheargumentÎ¸tolighten\nnotation)\nlog x x Naturallogarithmof\nÏƒ x()Logisticsigmoid,1\n1+exp()âˆ’ x\nÎ¶ x x () log(1+exp( Softplus, ))\n||||x p Lpnormofx\n||||x L2normofx\nx+Positivepartof,i.e., x max(0) , x\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "vector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\ntothearrayelement-wise. Forexample,if C= Ïƒ( X),then C i , j , k= Ïƒ( X i , j , k)forall\nvalidvaluesof,and. i j k\nD at aset s and D i st r i but i o n s\np da t aThedatageneratingdistribution\nË† p da t aTheempiricaldistributiondeï¬nedbythetraining\nset\nXAsetoftrainingexamples\nx( ) iThe-thexample(input)fromadataset i\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\ning\nXThe m nÃ—matrixwithinputexamplex( ) iinrow",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "ing\nXThe m nÃ—matrixwithinputexamplex( ) iinrow\nX i , :\nx i v",
    "metadata": {
      "source": "[3]notation.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 5\nRepresen t at i on L e ar n i n g\nInthischapter,weï¬rstdiscusswhatitmeanstolearnrepresentationsandhow\nthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss\nhowlearningalgorithmssharestatisticalstrengthacrossdiï¬€erenttasks,including\nusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\nrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\nlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "representationexists.Finally,westepbackandargueaboutthereasonsforthe\nsuccessofrepresentationlearning,startingwiththetheoreticaladvantagesof\ndistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand\nendingwiththemoregeneralideaofunderlyingassumptionsaboutthedata\ngeneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.\nManyinformationprocessingtaskscanbeveryeasyorverydiï¬ƒcultdepending\nonhowtheinformationisrepresented.Thisisageneralprincipleapplicableto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "dailylife,computerscienceingeneral,andtomachinelearning.Forexample,it\nisstraightforwardforapersontodivide210by6usinglongdivision.Â Thetask\nbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\nnumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\nbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\npermittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\nconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "appropriateorinappropriate representations.Forexample,insertinganumber\nintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe\nlistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa\nred-blacktree.\nInthecontextofmachinelearning,whatmakesonerepresentationbetterthan\n526",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\nlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\nofthesubsequentlearningtask.\nWecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\nformingakindofrepresentationlearning.Speciï¬cally,thelastlayerofthenetwork\nistypicallyalinearclassiï¬er,suchasasoftmaxregressionclassiï¬er.Therestof\nthenetworklearnstoprovidearepresentationtothisclassiï¬er.Trainingwitha",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\nmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiï¬cation\ntaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\nfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\nlastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiï¬er\n(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\nlearndiï¬€erentpropertiesdependingonthetypeofthelastlayer.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\nanyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\nlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\nsomeparticularway.Forexample,supposewewanttolearnarepresentationthat\nmakesdensityestimationeasier.Distributionswithmoreindependencesareeasier\ntomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\noftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\nlearnarepresentationasasideeï¬€ect.Regardlessofhowarepresentationwas\nobtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\nsupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\nrepresentation.\nMostrepresentationlearningproblemsfaceatradeoï¬€betweenpreservingas\nmuchinformationabouttheinputaspossibleandattainingniceproperties(such\nasindependence).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "asindependence).\nRepresentationlearningisparticularlyinterestingbecauseitprovidesone\nwaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\nlargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\ndata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\nresultsinsevereoverï¬tting.Semi-supervisedlearningoï¬€ersthechancetoresolve\nthisoverï¬ttingproblembyalsolearningfromtheunlabeleddata.Speciï¬cally,\nwecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "representationstosolvethesupervisedlearningtask.\nHumansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\n5 2 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\nperformanceâ€”forexample,thebrainmayuseverylargeensemblesofclassiï¬ers\norBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\nabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\ntoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\nunlabeleddatacanbeusedtolearnagoodrepresentation.\n15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\nnetworks,enablingresearchersfortheï¬rsttimetotrainadeepsupervisednetwork\nwithoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We\ncallthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -\nwi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow\narepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "theshapeoftheinputdistribution)cansometimesbeusefulforanothertask\n(supervisedlearningwiththesameinputdomain).\nGreedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\ntationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\ncodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\npretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\nandproducingasoutputanewrepresentationofthedata,whosedistribution(or",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "itsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.\nSeealgorithm foraformaldescription. 15.1\nGreedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\nbeenusedtosidestepthediï¬ƒcultyofjointlytrainingthelayersofadeepneuralnet\nforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron\n(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\nthatthisgreedylearningprocedurecouldbeusedtoï¬ndagoodinitialization for",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\ntosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;\nandSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,).\nPriortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\nresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\nthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "architectures,buttheunsupervisedpretrainingapproachwastheï¬rstmethodto\nsucceed.\nGreedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -\n5 2 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nr i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one\npieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se\nbecausetheseindependentpiecesarethelayersofthenetwork.Speciï¬cally,greedy\nlayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile\nkeepingthepreviousonesï¬xed.Inparticular,thelowerlayers(whicharetrained\nï¬rst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "v i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning\nalgorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe\nonlyaï¬rststepbeforeajointtrainingalgorithmisappliedto ï¬ne-t uneallthe\nlayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\nasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\ndecreasingtrainingerror)andaformofparameterinitialization.\nItiscommontousethewordâ€œpretrainingâ€torefernotonlytothepretraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "stageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining\nphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\ntrainingasimpleclassiï¬erontopofthefeatureslearnedinthepretrainingphase,\noritmayinvolvesupervisedï¬ne-tuningoftheentirenetworklearnedinthe\npretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\nwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining\nschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "willobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining\nfollowthisbasicprotocol.\nGreedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\nforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\nandSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent\nvariables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006\nBoltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\nmodelswillbedescribedinchapter.20",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "modelswillbedescribedinchapter.20\nAsdiscussedinsection,Â itisalsopossibletohavegreedylayer-wise 8.7.4\nsupervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\niseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\ncontexts(,). Erhanetal.2010\n1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk?\nOnmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\nimprovementsintesterrorforclassiï¬cationtasks.Thisobservationwasresponsible",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\n5 2 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nAl g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol.\nGiventhefollowing:Â Unsupervisedfeaturelearningalgorithm L,whichtakesa\ntrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw\ninputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftheï¬rst\nstageencoderonX.Inthecasewhereï¬ne-tuningisperformed,weusealearner\nTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "ï¬ne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber\nofstagesis.m\nfâ†Identityfunction\nËœXX= \nf o r dok,...,m = 1\nf( ) k= (LËœX)\nffâ†( ) kâ—¦f\nËœXâ†f( ) k(ËœX)\ne nd f o r\ni fï¬ne-tuning t he n\nff,, â†T(XY)\ne nd i f\nRet ur nf\n2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,\nunsupervisedpretrainingeitherdoesnotconferabeneï¬torevencausesnoticeable\nharm. ()studiedtheeï¬€ectofpretrainingonmachinelearning Maetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\nslightlyharmful,butformanytaskswassigniï¬cantlyhelpful.Becauseunsupervised\npretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand\nwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\ntask.\nAttheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\ntogreedyunsupervisedpretraininginparticular.Thereareother,completely",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\nsuchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13\ntrainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\nExamplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\nandBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\nobjectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly\nusingtheinput).",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "usingtheinput).\nUnsupervisedpretrainingcombinestwodiï¬€erentideas.First,itmakesuseof\n5 3 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\nasigniï¬cantregularizingeï¬€ectonthemodel(and,toalesserextent,thatitcan\nimproveoptimization). Second,itmakesuseofthemoregeneralideathatlearning\nabouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto\noutputs.\nBothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral\npartsofthemachinelearningalgorithmthatarenotentirelyunderstood.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "Theï¬rstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\ncanhaveastrongregularizingeï¬€ectonitsperformance, istheleastwellunderstood.\nAtthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\nmodelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\nanother.Â Today,localminimaarenolongerconsideredtobeaseriousproblem\nforneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork\ntrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\nbeinaccessibleâ€”forexample,aregionthatissurroundedbyareaswherethecost\nfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\naverynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\nHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\nverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\npretrainedparametersareretainedduringthesupervisedtrainingstageislimited.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\nlearningandsupervisedlearningratherthantwosequentialstages.Onemay\nalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe\nsupervisedlearningstagepreservesinformationfromtheunsupervisedlearning\nstagebysimplyfreezingtheÂ parameters forÂ thefeatureÂ extractorsandÂ using\nsupervisedlearningonlytoaddaclassiï¬erontopofthelearnedfeatures.\nTheotheridea,thatalearningalgorithmcanuseinformationlearnedinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\nunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\ntaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\nagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout\nwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\ntherepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\nlearnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "soitisnotalwayspossibletopredictwhichtaskswillbeneï¬tfromunsupervised\nlearninginthisway.Manyaspectsofthisapproacharehighlydependenton\nthespeciï¬cmodelsused.Forexample,ifwewishtoaddalinearclassiï¬eron\n5 3 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ntopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\nseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\nisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\npreferableâ€”theconstraintsimposedbytheoutputlayerarenaturallyincluded\nfromthestart.\nFromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\nwecanexpectunsupervisedpretrainingtobemoreeï¬€ectivewhentheinitial",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "representationispoor.Â Onekeyexampleofthisistheuseofwordembeddings.\nWordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo\ndistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance\nof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2\ndistancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially\nusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps\nbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealow",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "qualitysimilaritymetric.\nFromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\nexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\nexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\npretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\ntoperformbestÂ whentheÂ numberÂ ofunlabeledÂ examples isÂ veryÂ large.The\nadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\nunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\ncompetitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011\nnumberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\nofexamplesperclass).Theseeï¬€ectswerealsodocumentedincarefullycontrolled\nexperimentsbyPaine2014etal.().\nOtherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\nislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Unsupervisedlearningdiï¬€ersfromregularizerslikeweightdecaybecauseitdoesnot\nbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering\nfeaturefunctionsthatareusefulfortheunsupervisedlearningtask.Â Ifthetrue\nunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput\ndistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\nThesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\npretrainingisknowntocauseanimprovement,andexplainwhatisknownabout",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused\ntoimproveclassiï¬ers,andisusuallymostinterestingfromthepointofviewof\n5 3 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n\u0000î€´ î€° î€° î€° \u0000î€³ î€° î€° î€° \u0000î€² î€° î€° î€° \u0000î€± î€° î€° î€° î€° î€± î€° î€° î€° î€² î€° î€° î€° î€³ î€° î€° î€° î€´ î€° î€° î€°\u0000î€± î€µ î€° î€°\u0000î€± î€° î€° î€°\u0000î€µ î€° î€°î€°î€µ î€° î€°î€± î€° î€° î€°î€± î€µ î€° î€°\nî— î© î´ î¨ î€  î° î² î¥ î´ î² î¡ î© î® î© î® î§\nî— î© î´ î¨ î¯ îµ î´ î€  î° î² î¥ î´ î² î¡ î© î® î© î® î§\nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiï¬€erent\nneuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one\nmappingsfromparametervectorstofunctions),withdiï¬€erentrandominitializations",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "andwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiï¬€erent\nneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisï¬gureisadapted\nwithpermissionfrom ().Acoordinateinfunctionspaceisaninï¬nite- Erhan e t a l .2010\ndimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010\nalinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciï¬cx\npoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\n(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\novertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to\npointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\nusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.\nIsomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\nhasreducedvariance.\n5 3 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nreducingtestseterror.However,unsupervisedpretrainingcanhelptasksother\nthanclassiï¬cation,andcanacttoimproveoptimization ratherthanbeingmerely\naregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror\nfordeepautoencoders(HintonandSalakhutdinov2006,).\nErhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof\nunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "totesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe\nparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\ntrainingisnon-determinis tic,andconvergestoadiï¬€erentfunctioneverytimeit\nisrun.Â Trainingmayhaltatapointwherethegradientbecomessmall,apoint\nwhereearlystoppingendstrainingtopreventoverï¬tting,oratapointwherethe\ngradientislargebutitisdiï¬ƒculttoï¬ndadownhillstepduetoproblemssuchas\nstochasticityorpoorconditioningoftheHessian.Â Neuralnetworksthatreceive",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\nwhileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\nï¬gureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1\nnetworksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\nestimationprocess,whichcaninturnreducetheriskofsevereover-ï¬tting.In\notherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\naregionthattheydonotescape,andtheresultsfollowingthisinitialization are",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\nErhan2010etal.()alsoprovidesomeanswersastopretrainingworks when\nbestâ€”themeanandvarianceofthetesterrorweremostreducedbypretrainingfor\ndeepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\ninventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks\n(rectiï¬edlinearunits,dropoutandbatchnormalization) solessisknownaboutthe\neï¬€ectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "Animportantquestionishowunsupervisedpretrainingcanactasaregularizer.\nOnehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\nfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\nThisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\npretraining,andisdescribedfurtherinsection.15.3\nComparedtootherformsofunsupervisedlearning,unsupervisedpretraining\nhasthedisadvantagethatitoperateswithtwoseparatetrainingphases.Â Many",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\nstrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\nUnsupervisedpretrainingdoesnotoï¬€eraclearwaytoadjustthethestrength\noftheregularizationÂ arisi ngfromtheunsupervisedÂ stage.Instead,Â thereare\n5 3 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nverymanyhyperparameters ,whoseeï¬€ectmaybemeasuredafterthefactbut\nisoftendiï¬ƒculttopredictaheadoftime.Whenweperformunsupervisedand\nsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there\nisasinglehyperparameter,usuallyacoeï¬ƒcientattachedtotheunsupervised\ncost,Â thatdetermineshowstronglytheunsupervisedobjectivewillregularize\nthesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasingthiscoeï¬ƒcient.Inthecaseofunsupervisedpretraining,thereisnota\nwayofï¬‚exiblyadaptingthestrengthoftheregularizationâ€”either thesupervised\nmodelisinitializedtopretrainedparameters,oritisnot.\nAnotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\nhasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\nbepredictedduringtheï¬rstphase,sothereisalongdelaybetweenproposing\nhyperparametersfortheï¬rstphaseandbeingabletoupdatethemusingfeedback",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "fromthesecondphase.Themostprincipledapproachistousevalidationseterror\ninthesupervisedphaseinordertoselectthehyperparameters ofthepretraining\nphase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009\nlikethenumberofpretrainingiterations,aremoreconvenientlysetduringthe\npretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis\nnotidealbutcomputationally muchcheaperthanusingthesupervisedobjective.\nToday,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "ï¬eldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\none-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\nsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\nonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\nwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\nthenusethisrepresentationorï¬ne-tuneitforasupervisedtaskforwhichthe\ntrainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal.\n()andremainsincommonusetoday. 2011a\nDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\norbatchnormalization, areabletoachievehuman-levelperformanceonverymany\ntasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-\nperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and\nMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "datasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform\nmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,\nthepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\npretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\n5 3 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nandcontinuestoinï¬‚uencecontemporaryapproaches.Theideaofpretraininghas\nbeengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4\ncommonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\nispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\nnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparameters\nofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\n15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n\nTransferlearninganddomainadaptationrefertothesituationwherewhathasbeen\nlearnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization\ninanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe\nprevioussection,wherewetransferredrepresentationsbetweenanunsupervised\nlearningtaskandasupervisedlearningtask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "learningtaskandasupervisedlearningtask.\nIn t r ansf e r l e ar ni ng,thelearnermustperformtwoormorediï¬€erenttasks,\nbutweassumethatmanyofthefactorsthatexplainthevariationsinP 1are\nrelevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically\nunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\ntargetmaybeofadiï¬€erentnature.Forexample,wemaylearnaboutonesetof\nvisualcategories,suchascatsanddogs,intheï¬rstsetting,thenlearnabouta",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If\nthereissigniï¬cantlymoredataintheï¬rstsetting(sampledfromP 1),thenthat\nmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly\nveryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions\nofedgesandvisualshapes,theeï¬€ectsofgeometricchanges,changesinlighting,\netc.Â Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7\nadaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatareusefulforthediï¬€erentsettingsortasks,correspondingtounderlying\nfactorsthatappearinmorethanonesetting.Thisisillustratedinï¬gure,with7.2\nsharedlowerlayersandtask-dependentupperlayers.\nHowever,Â sometimes,Â whatissharedÂ amongtheÂ diï¬€erentÂ tasksisnotthe\nsemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\nrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,but\ntheearlierlayersneartheinputmayneedtorecognizeverydiï¬€erentversionsof",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "thesamephonemesorsub-phonemicvocalizationsdependingonwhichperson\nisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\n(neartheoutput)oftheneuralnetwork,andhaveatask-speciï¬cpreprocessing,as\n5 3 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nillustratedinï¬gure.15.2\nSe l e c t i onÂ sw i t c h\nh(1)h(1)h(2)h(2)h(3)h(3)yy\nh(shared)h(shared)\nx(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )\nFigure15.2:Â Example architectureformulti-taskortransferlearningwhentheoutput\nvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiï¬€erent y x \nmeaning(andpossiblyevenadiï¬€erentdimension)foreachtask(or,forexample,each\nuser),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "switch)aretask-speciï¬c,whiletheupperlevelsareshared.Thelowerlevelslearnto\ntranslatetheirtask-speciï¬cinputintoagenericsetoffeatures.\nIntherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-\noutputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\nisslightlydiï¬€erent.Forexample,considerthetaskofsentimentanalysis,which\nconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\nCommentspostedonthewebcomefrommanycategories.Adomainadaptation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\nmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments\naboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine\nthatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\nneutralornegative,butofcoursethevocabularyandstylemayvaryfromone\ndomaintoanother,makingitmorediï¬ƒculttogeneralizeacrossdomains.Simple\nunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "successfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\nArelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform\noftransferlearningduetogradualchangesinthedatadistributionovertime.\nBothconceptdriftandtransferlearningcanbeviewedasparticularformsof\n5 3 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nmulti-tasklearning.Whilethephraseâ€œmulti-tasklearningâ€Â typicallyrefersto\nsupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\ntounsupervisedlearningandreinforcementlearningaswell.\nInallofthesecases,theobjectiveistotakeadvantageofdatafromtheï¬rst\nsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\ndirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\nsamerepresentationinbothsettingsallowstherepresentationtobeneï¬tfromthe\ntrainingdatathatisavailableforbothtasks.\nAsmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\nsuccessinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\netal.,).Intheï¬rstofthesecompetitions,theexperimentalsetupisthe 2011\nfollowing.Eachparticipantisï¬rstgivenadatasetfromtheï¬rstsetting(from",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "distributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants\nmustusethistolearnagoodfeaturespace(mappingtherawinputtosome\nrepresentation),suchthatwhenweapplythislearnedtransformationtoinputs\nfromthetransfersetting(distributionP 2),alinearclassiï¬ercanbetrainedand\ngeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults\nfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand\ndeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "intheï¬rstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond\n(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled\nexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\ngeneralization performance.\nTwoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t\nl e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample\nofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "givenatallforthezero-shotlearningtask.\nOne-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\nlearnstocleanlyseparatetheunderlyingclassesduringtheï¬rststage.Duringthe\ntransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\npossibletestexamplesthatallclusteraroundthesamepointinrepresentation\nspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\ntheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "representationspace,andwehavesomehowlearnedwhichfactorsdoanddonot\nmatterwhendiscriminatingobjectsofcertaincategories.\nAsanexampleofazero-shotlearningsetting,considertheproblemofhaving\nalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\n5 3 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nItmaybepossibletorecognizeaspeciï¬cobjectclassevenwithouthavingseenan\nimageofthatobject,ifthetextdescribestheobjectwellenough.Â Forexample,\nhavingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\nguessthatanimageisacat,withouthavingseenacatbefore.\nZero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\netal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\nasincludingthreerandomvariables:thetraditionalinputsx,thetraditional\noutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.\nThemodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where\nTisadescriptionofthetaskwewishthemodeltoperform.Â Inourexampleof\nrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\nwithy= 1indicatingâ€œyesâ€andy= 0indicatingâ€œno.â€ThetaskvariableTthen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "representsquestionstobeansweredsuchasâ€œIsthereacatinthisimage?â€Ifwe\nhaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\nsamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.\nInourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\nimportantthatwehavehadunlabeledtextdatacontainingsentencessuchasâ€œcats\nhavefourlegsâ€orâ€œcatshavepointyears.â€\nZero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "ofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan\nobjectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b\nofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\nwitheachcategory.\nAsimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;\nMikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and\ntherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "otherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith\nwordsintheother.Eventhoughwemaynothavelabeledexamplestranslating\nwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa\ntranslationforwordAbecausewehavelearnedadistributedrepresentationfor\nwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and\ncreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples\nconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "mostsuccessfulifallthreeingredients(thetworepresentationsandtherelations\nbetweenthem)arelearnedjointly.\nZero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\nexplainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation\n5 3 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh x = f x ( ) x\nx t e s t\ny t e s th y = f y ( ) y\ny âˆ’ s pa ce\nR e l at i onshi pÂ  b e t w e e n Â  e m be dde dÂ  p oi n t s Â  w i t hi nÂ  one Â  o f Â  t h e Â  d o m a i n s\nMapsÂ be t w e e n Â  r e p r e s e n t at i onÂ spac e s Â f x\nf y\nx âˆ’ s pa ce\n( ) pa i r s i n t he t r a i ni ng s et x y ,\nf x : enco der f unctio n f o r x\nf y : enco der f unctio n f o r y\nFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand\nsimilarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions\nappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\napplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints\ninxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance\ninh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "ofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled\nexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way\nortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe\nrepresentationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning\nisthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno\nimageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "andimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween\nrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwerenever\npaired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach\nother.FigureinspiredfromsuggestionbyHrantKhachatrian.\n5 4 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\ninonemodality,arepresentationintheother,andtherelationship(ingeneralajoint\ndistribution)betweenpairs (xy,)consistingofoneobservationxinonemodality\nandanotherobservationyintheothermodality(SrivastavaandSalakhutdino v,\n2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from\nytoitsrepresentation,andtherelationshipbetweenthetworepresentations),\nconceptsinonerepresentationareanchoredintheother,andvice-versa,allowing",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "onetomeaningfullyÂ generalizetoÂ newpairs.TheprocedureisÂ illustratedin\nï¬gure.15.3\n15. 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors\nAnimportantquestionaboutrepresentationlearningisâ€œwhatmakesonerepre-\nsentationbetterthananother?â€Onehypothesisisthatanidealrepresentation\nisoneinwhichthefeatureswithintherepresentationcorrespondtotheunder-\nlyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "spacecorrespondingtodiï¬€erentcauses,sothattherepresentationdisentanglesthe\ncausesfromoneanother.Thishypothesismotivatesapproachesinwhichweï¬rst\nseekagoodrepresentationforp(x).Â Sucharepresentationmayalsobeagood\nrepresentationforcomputingp(yx|)ifyisamongthemostsalientcausesof\nx.Â Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\nthe1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail.\nForotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006\nInotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\nwitharepresentationthatiseasytomodelâ€”forexample,onewhoseentriesare\nsparse,orindependentfromeachother.Arepresentationthatcleanlyseparates\ntheunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel.\nHowever,afurtherpartofthehypothesismotivatingsemi-supervisedlearning\nviaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "propertiescoincide:Â once weareabletoobtaintheunderlyingexplanationsfor\nwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\ntheothers.Speciï¬cally,ifarepresentationhrepresentsmanyoftheunderlying\ncausesoftheobservedx,andtheoutputsyareamongthemostsalientcauses,\nthenitiseasytopredictfrom.yh\nFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\nlearningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "wherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly,\nobservingatrainingsetofvaluesalonegivesusnoinformationabout. x p( )y x|\n5 4 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nxp x ( )y = 1 y = 2 y = 3\nFigure15.4:ExampleofadensityoverxthatisamixtureoverthreeÂ components.\nThecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture\ncomponents(e.g.,Â naturalobjectclassesinimagedata)arestatisticallysalient,just\nmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor\ny.\nNext,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\nConsiderthesituationwhere xarisesfromamixture,withonemixturecomponent",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "pervalueofy,asillustratedinï¬gure.Â Ifthemixturecomponentsarewell- 15.4\nseparated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda\nsinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).\nButmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x\nIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and\np(yx|)willÂ bestronglytied,Â andunsupervisedrepresentationlearningthat\ntriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "semi-supervisedlearningstrategy.\nConsidertheassumptionthatyisoneofthecausalfactorsofx,andlet\nhrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas\nstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\np,pp. (hx) = ( )xh|()h (15.1)\nAsaconsequence,thedatahasmarginalprobability\np() = x E hp. ( )xh| (15.2)\nFromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\nofx(fromageneralization pointofview)istheonethatuncoverstheaboveâ€œtrueâ€\n5 4 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.\nTheâ€œidealâ€representationlearningdiscussedaboveshouldthusrecovertheselatent\nfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe\nveryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe\nconditionaldistributionofygivenxistiedbyBayesâ€™ruletothecomponentsin\ntheaboveequation:\np( ) = yx|pp ( )xy|()y\np()x. (15.3)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "p( ) = yx|pp ( )xy|()y\np()x. (15.3)\nThusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge\nofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\nsituationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\nperformance.\nAnimportantresearchproblemregardsthefactthatmostobservationsare\nformedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but\ntheunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\nsalientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking\niteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y\nInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible\ntocaptureallormostofthefactorsofvariationthatinï¬‚uenceanobservation.\nForexample,inavisualscene,shouldtherepresentationalwaysencodeallof\nthesmallestobjectsinthebackground? Itisawell-documented psychological",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "phenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat\narenotimmediately relevanttothetasktheyareperformingâ€”see,e.g.,Simons\nandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\ndetermining toencodeineachsituation.Currently,twoofthemainstrategies what\nfordealingwithalargenumberofunderlyingcausesaretouseasupervised\nlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\nmodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "largerrepresentationsifusingpurelyunsupervisedlearning.\nAnemergingstrategyforunsupervisedlearningistomodifythedeï¬nitionof\nwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\nmodelshavebeentrainedtooptimizeaï¬xedcriterion,oftensimilartomean\nsquarederror.Theseï¬xedcriteriadeterminewhichcausesareconsideredsalient.\nForexample,meansquarederrorappliedtothepixelsofanimageimplicitly\nspeciï¬esthatanunderlyingcauseisonlysalientifitsigniï¬cantlychangesthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\ntosolveinvolvesinteractingwithsmallobjects.Seeï¬gureforanexample 15.5\n5 4 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInput Reconstruction\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\nfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits\nspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\narerelevanttotheroboticstask.Â Unfortunately,theautoencoderhaslimitedcapacity,\nandthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\nsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\npingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\nobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\nOtherdeï¬nitionsofsaliencearepossible.Forexample,ifagroupofpixels\nfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\nbrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\nOnewaytoimplementsuchadeï¬nitionofsalienceistousearecentlydeveloped",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "approachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c\nInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiï¬er.\nThefeedforwardclassiï¬erattemptstorecognizeallsamplesfromthegenerative\nmodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis\nframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis\nhighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "insection.Forthepurposesofthepresentdiscussion,itissuï¬ƒcientto 20.10.4\nunderstandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015\nshowedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect\ntogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully\ngeneratetheearswhentrainedwiththeadversarialframework.Becausethe\nearsarenotextremelybrightordarkcomparedtothesurroundingskin,they\narenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly\n5 4 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nGroundTruth MSE Adversarial\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\nlearningwhichfeaturesaresalient.Â Inthisexample,thepredictivegenerativenetwork\nhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciï¬c\nviewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould\nemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "squarederroralone.Becausetheearsdonotcauseanextremediï¬€erenceinbrightness\ncomparedtotheneighboringskin,theywerenotsuï¬ƒcientlysalientforthemodeltolearn\ntorepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof\nmeansquarederrorandadversarialloss.Â Usingthislearnedcostfunction,theearsare\nsalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\nimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\ngraciouslyprovidedby (). Lotter e t a l .2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "graciouslyprovidedby (). Lotter e t a l .2015\nrecognizableshapeandconsistentpositionmeansthatafeedforwardnetwork\ncaneasilylearntodetectthem,makingthemhighlysalientunderthegenerative\nadversarialframework.Seeï¬gureforexampleimages.Generativeadversarial 15.6\nnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.\nWeexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich\nfactorstorepresent,anddevelopmechanismsforrepresentingdiï¬€erentfactors\ndependingonthetask.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "dependingonthetask.\nAbeneï¬toflearningtheunderlyingcausalfactors,aspointedoutbySchÃ¶lkopf\netal.(),isthatifthetruegenerativeprocesshas 2012 xasaneï¬€ectandyas\nacause,thenmodelingp(x y|)isrobusttochangesinp(y).Â Ifthecause-eï¬€ect\nrelationshipwasreversed,thiswouldnotbetrue,sincebyBayesâ€™rule,p(x y|)\nwouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin\ndistributionduetodiï¬€erentdomains,temporalnon-stationarity,orchangesin\nthenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "universeareconstant)whilethemarginaldistributionovertheunderlyingcauses\ncanchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan\n5 4 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\nfactorsand. h p( )xh|\n15. 4 D i s t ri b u t ed R ep res en t at i on\nDistributedrepresentationsofconceptsâ€”representationscomposedofmanyele-\nmentsthatcanbesetseparatelyfromeachotherâ€”areoneofthemostimportant\ntoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\ntheycanusenfeatureswithkvaluestodescribekndiï¬€erentconcepts.Aswe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "haveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits\nandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof\ndistributedrepresentation.Â Wenowintroduceanadditionalobservation.Â Many\ndeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\ncanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\ndiscussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\nbecauseeachdirectioninrepresentationspacecancorrespondtothevalueofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentunderlyingconï¬gurationvariable.\nAnexampleofadistributedrepresentationisavectorofnbinaryfeatures,\nwhichcantake2nconï¬gurations, eachpotentiallycorrespondingtoadiï¬€erent\nregionininputspace,asillustratedinï¬gure.Thiscanbecomparedwith 15.7\nasymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\ncategory.Iftherearensymbolsinthedictionary,onecanimaginenfeature\ndetectors,eachcorrespondingtothedetectionofthepresenceoftheassociated",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "category.Inthatcaseonlyndiï¬€erentconï¬gurations oftherepresentationspace\narepossible,carvingndiï¬€erentregionsininputspace,asillustratedinï¬gure.15.8\nSuchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\nbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone\nofthemcanbeactive).Asymbolicrepresentationisaspeciï¬cexampleofthe\nbroaderclassofnon-distributedrepresentations,whicharerepresentationsthat\nmaycontainmanyentriesbutwithoutsigniï¬cantmeaningfulseparatecontrolover",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "eachentry.\nExamplesoflearningalgorithmsÂ basedonnon-distributedrepresentations\ninclude:\nâ€¢Clusteringmethods,includingthek-meansalgorithm:eachinputpointis\nassignedtoexactlyonecluster.\nâ€¢k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\nareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple\n5 4 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nh 1h 2 h 3\nh = [ 1 , , 1 1 ]î€¡\nh = [ 0 , , 1 1 ]î€¡h = [ 1 , , 0 1 ]î€¡h = [ 1 , , 1 0 ]î€¡\nh = [ 0 , , 1 0 ]î€¡h = [ 0 , , 0 1 ]î€¡h = [ 1 , , 0 0 ]î€¡\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\nbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\nh 1,h 2,andh 3.Â Eachfeatureisdeï¬nedbythresholdingtheoutputofalearned,linear\ntransformation.Eachfeaturedivides R2intotwohalf-planes.Leth+\nibethesetofinput",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ibethesetofinput\npointsforwhichh i=1andhâˆ’\nibethesetofinputpointsforwhichh i=0.Inthis\nillustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding\narrowpointingtotheh+\nisideoftheboundary.Therepresentationasawholetakes\nonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\nrepresentationvalue[1,1,1]î€¾correspondstotheregionh+\n1âˆ©h+\n2âˆ©h+\n3.Comparethistothe\nnon-distributedrepresentationsinï¬gure.Inthegeneralcaseof 15.8 dinputdimensions,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "adistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes.\nThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diï¬€erent\nregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly\nnregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\nmoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible\n(thereisnoh=0inthisexample)andthatalinearclassiï¬erontopofthedistributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "representationisnotabletoassigndiï¬€erentclassidentitiestoeveryneighboringregion;\nevenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew\nisthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\nlayerandaweakclassiï¬erlayercanbeastrongregularizer;aclassiï¬ertryingtolearn\ntheconceptofâ€œpersonâ€versusâ€œnotapersonâ€doesnotneedtoassignadiï¬€erentclassto\naninputrepresentedasâ€œwomanwithglassesâ€thanitassignstoaninputrepresentedas",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "â€œmanwithoutglasses.â€Thiscapacityconstraintencourageseachclassiï¬ertofocusonfew\nh iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h\n5 4 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom\neachother,sothisdoesnotqualifyasatruedistributedrepresentation.\nâ€¢Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\nactivatedwhenaninputisgiven.\nâ€¢Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\nexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest\nneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "thosevaluescannotreadilybecontrolledseparatelyfromeachother.\nâ€¢KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\nalthoughthedegreeofactivationofeachâ€œsupportvectorâ€ortemplateexample\nisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\nâ€¢Languageortranslationmodelsbasedonn-grams.Thesetofcontexts\n(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuï¬ƒxes.\nAleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Separateparametersareestimatedforeachleafofthetree(withsomesharing\nbeingpossible).\nForsomeofthesenon-distributedalgorithms,theoutputisnotconstantby\npartsbutinsteadinterpolatesbetweenneighboringregions.Therelationship\nbetweenthenumberofparameters(orexamples)andthenumberofregionsthey\ncandeï¬neremainslinear.\nAnimportantrelatedconceptthatdistinguishesadistributedrepresentation\nfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentconcepts.Aspuresymbols,â€œcatâ€andâ€œdogâ€areasfarfromeachother\nasanyothertwosymbols.However,ifoneassociatesthemwithameaningful\ndistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\ncangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation\nmaycontainentriessuchasâ€œhas_furâ€orâ€œnumber_of_legsâ€thathavethesame\nvaluefortheembeddingofbothâ€œcatâ€andâ€œdog.â€Neurallanguagemodelsthat\noperateondistributedrepresentationsofwordsgeneralizemuchbetterthanother",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\nsection.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\nsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\nabsentfrompurelysymbolicrepresentations.\nWhenandwhycantherebeastatisticaladvantagefromusingadistributed\nrepresentationaspartofalearningalgorithm?Â D istributedrepresentationscan\n5 4 8",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\nintodiï¬€erentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\nalgorithmbasedonanon-distributedrepresentation.Diï¬€erentnon-distributedalgorithms\nmayhavediï¬€erentgeometry,Â butÂ theytypicallybreaktheinputÂ spaceintoregions,\nw i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "approachisthat,givenenoughparameters,itcanï¬tthetrainingsetwithoutsolvinga\ndiï¬ƒcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiï¬€erentoutput\ni n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels\ngeneralizeonlylocallyviathesmoothnessprior,makingitdiï¬ƒculttolearnacomplicated\nfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\nthiswithadistributedrepresentation,ï¬gure.15.7\n5 4 9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe\ncompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-\ndistributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,\nwhichstatesthatifuvâ‰ˆ,thenthetargetfunctionftobelearnedhasthe\npropertythatf(u)â‰ˆf(v),ingeneral.Therearemanywaysofformalizingsuchan\nassumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "knowthatf(x)â‰ˆy,thenwechooseanestimator Ë†fthatapproximatelysatisï¬es\ntheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby\ninputx+î€.Thisassumptionisclearlyveryuseful,butitsuï¬€ersfromthecurseof\ndimensionality:Â inordertolearnatargetfunctionthatincreasesanddecreases\nmanytimesinmanydiï¬€erentregions,1wemayneedanumberofexamplesthatis\natleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof\ntheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "eachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol\ntovalue.Â However,thisdoesnotallowustogeneralizetonewsymbolsfornew\nregions.\nIfwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\nsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean\nobjectregardlessofitslocationintheimage,eventhoughspatialtranslationof\ntheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\nLetusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\nbinaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces,Â as\nillustratedinï¬gure.Theexponentiallylargenumberofintersectionsof 15.7 n\nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\nrepresentationlearnercandistinguish.Howmanyregionsaregeneratedbyan\narrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe\nintersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\ndî˜\nj = 0î€’n\njî€“\n= (Ond). (15.4)\nTherefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\nthenumberofhiddenunits.\n1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "re g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i ï¬€ e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e\nm i g h t wa n t t o d i ï¬€ e r i n f 2dd i ï¬€ e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s .\n5 5 0",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nThisprovidesageometricargumenttoexplainthegeneralization powerof\ndistributedrepresentation:withO(nd)parameters(fornlinear-threshold features\nin Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade\nnoassumptionatallaboutthedata,andusedarepresentationwithoneunique\nsymbolforeachregion,andseparateparametersforeachsymboltorecognizeits\ncorrespondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "examples.Moregenerally,theargumentinfavorofthedistributedrepresentation\ncouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\nusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\nthedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\ntransformationwithkparameterscanlearnaboutrregionsininputspace,with\nkrî€œ,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\nwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "settingwherewewouldneedO(r)examplestoobtainthesamefeaturesand\nassociatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto\nrepresentthemodelmeansthatwehavefewerparameterstoï¬t,andthusrequire\nfarfewertrainingexamplestogeneralizewell.\nAfurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\ntationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\ndistinctlyencodesomanydiï¬€erentregions.Forexample,theVCdimensionofa",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber\nofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\nmanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode\nspace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\nhtotheoutputyusingalinearclassiï¬er.Theuseofadistributedrepresentation\ncombinedwithalinearclassiï¬erthusexpressesapriorbeliefthattheclassesto\nberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "capturedbyh.Â Wewilltypicallywanttolearncategoriessuchasthesetofall\nimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\nrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition\nthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\ngreencarsandredtrucksasanotherclass.\nTheideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\nvalidated. ()ï¬ndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery\nofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\nInpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\nthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\ntoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\n5 5 1",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n-+ =\nFigure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\ntheconceptofgenderfromtheconceptofwearingglasses.Â Ifwebeginwiththerepre-\nsentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\nconceptofamanwithoutglasses,andï¬nallyaddthevectorrepresentingtheconcept\nofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\nwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\npermissionfrom (). Radford e t a l .2015\ncommonisthatonecouldimagine learningabouteachofthemwithouthavingto\nseealltheconï¬gurationsofalltheothers. ()demonstratedthat Radfordetal.2015\nagenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\ndirectionsinrepresentationspacecapturingdiï¬€erentunderlyingfactorsofvariation.\nFiguredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "towhetherthepersonismaleorfemale,whileanothercorrespondstowhether\nthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not\nï¬xedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiï¬ers:\ngradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\ninterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout\nthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\nglasses,withouthavingtocharacterizealloftheconï¬gurations ofthenâˆ’1other",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "featuresbyexamplescoveringallofthesecombinationsofvalues.Â Thisformof\nstatisticalseparabilityiswhatallowsonetogeneralizetonewconï¬gurations ofa\npersonâ€™sfeaturesthathaveneverbeenseenduringtraining.\n5 5 2",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\n15. 5 E x p on en t i al Gai n s f rom D ep t h\nWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\ntors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\nnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\nimprovedstatisticaleï¬ƒciency.Inthissection,wedescribehowsimilarresultsapply\nmoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\nInsection,wesawanexampleofagenerativemodelthatlearnedabout 15.4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "theexplanatoryfactorsunderlyingimagesoffaces,includingthepersonâ€™sgender\nandwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\nthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\nashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\nbetweentheseabstractexplanatoryfactorsandthepixelsintheimage.Â Inthis\nandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\neachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "high-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\ndemands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas\nfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\nthecompositionofmanynonlinearities.\nIthasbeenproveninmanydiï¬€erentsettingsthatorganizingcomputation\nthroughthecompositionofmanynonlinearities andahierarchyofreusedfeatures\ncangiveanexponentialboosttostatisticaleï¬ƒciency,ontopoftheexponential",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\nwithsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with\nasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\nfamilythatisauniversalapproximator canapproximatealargeclassoffunctions\n(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough\nhiddenunits.Â However,therequirednumberofhiddenunitsmaybeverylarge.\nTheoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "therearefamiliesoffunctionsthatcanberepresentedeï¬ƒcientlybyanarchitecture\nofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect\ntotheinputsize)withinsuï¬ƒcientdepth(depth2ordepth).kâˆ’1\nInsection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\napproximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\nhiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\nbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "andBengio20082010MontÃºfarandAy2011MontÃºfar2014Krause ,,; ,;,; etal.,\n2013).\n5 5 3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nInsection,wesawthatasuï¬ƒcientlydeepfeedforwardnetworkcanhave 6.4.1\nanexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\nbeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\nmodelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These\nmodelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\nsetofrandomvariables. ()showedthatthereexist DelalleauandBengio2011",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\nneedinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014\nshowedthattherearesigniï¬cantdiï¬€erencesbetweeneverytwoï¬nitedepthsof\nSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\ntheirrepresentationalpower.\nAnotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\npoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\ntoonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal.\n2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\ncasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\n15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es\nToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\nrepresentationbetterthananother?Oneanswer,ï¬rstintroducedinsection,is15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\nvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\napplications.Moststrategiesforrepresentationlearningarebasedonintroducing\ncluesthathelpthelearningtoï¬ndtheseunderlyingfactorsofvariations.Theclues\ncanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\nlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "speciï¬esthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\ntomakeuseofabundantunlabeleddata,representationlearningmakesuseof\nother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof\nimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\nordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\nregularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis\nimpossibletoï¬ndauniversallysuperiorregularizationstrategy,onegoalofdeep",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "learningistoï¬ndasetoffairlygenericregularizationstrategiesthatareapplicable\ntoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\ntosolve.\n5 5 4",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nWeprovideherealistofthesegenericregularizationstrategies.Thelistis\nclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning\nalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying\nfactors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d\nbeenpartiallyexpandedhere.\nâ€¢Smoothness:Thisistheassumptionthatf(x+î€d)â‰ˆf(x)forunitdand\nsmallî€.Thisassumptionallowsthelearnertogeneralizefromtraining",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "examplestonearbypointsininputspace.Manymachinelearningalgorithms\nleveragethisidea,butitisinsuï¬ƒcienttoovercomethecurseofdimensionality.\nâ€¢Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\nvariablesarelinear.Thisallowsthealgorithmtomakepredictionseven\nveryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\npredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\nsmoothnessassumptioninsteadmakethelinearityassumption.Theseare",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "infactdiï¬€erentassumptionsâ€”linearfunctionswithlargeweightsapplied\ntohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\n()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\nâ€¢Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\nmotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\nexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\nofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "supervisedlearningviarepresentationlearning.Learningthestructureofp(x)\nrequireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|\nx)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\ndescribeshowthisviewmotivatestheuseofdistributedrepresentations,with\nseparatedirectionsinrepresentationspacecorrespondingtoseparatefactors\nofvariation.\nâ€¢Causalfactors:themodelisconstructedinsuchawaythatittreatsthe\nfactorsofvariationdescribedbythelearnedrepresentationhasthecauses",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "oftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3\nisadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\nmorerobustwhenthedistributionovertheunderlyingcauseschangesor\nwhenweusethemodelforanewtask.\nâ€¢DepthahierarchicalÂ organizationÂ ofexplanatoryÂ factors ,Â orÂ  :High-level,\nabstractconceptscanbedeï¬nedintermsofsimpleconcepts,forminga\nhierarchy.FromÂ anotherÂ point ofÂ view,Â theÂ us eÂ ofaÂ deeparchitecture\n5 5 5",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\nexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step\nprogram,Â with eachstepÂ referringbacktotheÂ outputoftheprocessing\naccomplishedviaprevioussteps.\nâ€¢SharedfactorsÂ acrossÂ tasks:InÂ thecontextwherewehavemanytasks,\ncorrespondingtodiï¬€erentyivariablessharingthesameinput xorwhere\neachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput\nx,theassumptionisthateachyiisassociatedwithadiï¬€erentsubsetfroma",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "commonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning\nalltheP(yi|x)viaasharedintermediate representationP(h x|)allows\nsharingofstatisticalstrengthbetweenthetasks.\nâ€¢Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\ncentratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\ncase,theseregionscanbeapproximatedbylow-dimensional manifoldswith\namuchsmallerdimensionalitythantheoriginalspacewherethedatalives.\nManymachinelearningalgorithmsbehavesensiblyonlyonthismanifold",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b\nautoencoders,attempttoexplicitlylearnthestructureofthemanifold.\nâ€¢Naturalclustering:Manymachinelearningalgorithmsassumethateach\nconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\ndatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\nwithineachoneofthese.Â Thisassumptionmotivatesavarietyoflearning\nalgorithms,includingtangentpropagation, doublebackprop,themanifold\ntangentclassiï¬erandadversarialtraining.",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "tangentclassiï¬erandadversarialtraining.\nâ€¢Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\nmaketheassumptionthatthemostimportantexplanatoryfactorschange\nslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\nexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\nSeesectionforfurtherdescriptionofthisapproach. 13.3\nâ€¢Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\ninputsâ€”thereisnoneedtouseafeaturethatdetectselephanttrunkswhen",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "representinganimageofacat.Itisthereforereasonabletoimposeaprior\nthatanyfeaturethatcanbeinterpretedasâ€œpresentâ€orâ€œabsentâ€shouldbe\nabsentmostofthetime.\nâ€¢SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the\nfactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\n5 5 6",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER15.REPRESENTATIONLEARNING\npossibleismarginalindependence,P(h) =î‘\niP(h i),butlineardependencies\northosecapturedbyashallowautoencoderarealsoreasonableassumptions.\nThiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga\nlinearpredictororafactorizedpriorontopofalearnedrepresentation.\nTheconceptofrepresentationlearningtiestogetherallofthemanyforms\nofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep\nprobabilisticmodelsalllearnandexploitrepresentations.LearningÂ thebest",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "possiblerepresentationremainsanexcitingavenueofresearch.\n5 5 7",
    "metadata": {
      "source": "[21]part-3-chapter-15.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 6\nS t ru ct u r e d Probabilis t i c Mo d e l s\nf or D e e p L e ar n i n g\nDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto\nguidetheirdesigneï¬€ortsanddescribetheiralgorithms.Oneoftheseformalisms\nistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed\nstructuredprobabilisticmodelsbrieï¬‚yinsection.Thatbriefpresentationwas 3.14\nsuï¬ƒcienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "describesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III\nmodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep\nlearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes\nstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended\ntobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction\nbeforecontinuingwiththischapter.\nAstructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\ninteractwitheachotherdirectly.Hereweuseâ€œgraphâ€inthegraphtheorysenseâ€”a\nsetofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\nofthemodelisdeï¬nedbyagraph,thesemodelsareoftenalsoreferredtoas\ngraphicalmodels.\nThegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\ndiï¬€erentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\nprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning\nresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,\nyoumaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert\n558",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nmaybeneï¬tfromreadingtheï¬nalsectionofthischapter,section,inwhichwe 16.7\nhighlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning\nalgorithms.Deeplearningpractitioners tendtouseverydiï¬€erentmodelstructures,\nlearningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest\nofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese\ndiï¬€erencesinpreferencesandexplainthereasonsforthem.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthischapterweï¬rstdescribethechallengesofbuildinglarge-scaleproba-\nbilisticmodels.Â Next,wedescribehowtouseagraphtodescribethestructure\nofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany\nchallenges,itisnotwithoutitsowncomplications. Oneofthemajordiï¬ƒcultiesin\ngraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract\ndirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem.Â We\noutlinetwoapproachestoresolvingthisdiï¬ƒcultybylearningaboutthedependen-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5\ndeeplearningpractitioners placeonspeciï¬capproachestographicalmodelingin\nsection.16.7\n16.1TheChallengeofUnstructuredModeling\nThegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\nneededtosolveartiï¬cialintelligence.Thismeansbeingabletounderstandhigh-\ndimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\nbeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "documentscontainingmultiplewordsandpunctuationcharacters.\nClassiï¬cationalgorithmscantakeaninputfromsucharichhigh-dimensional\ndistributionandsummarizeitwithacategoricallabelâ€”whatobjectisinaphoto,\nwhatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\nofclassiï¬cationdiscardsmostoftheinformationintheinputandproducesa\nsingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\nclassiï¬erisalsooftenabletoignoremanypartsoftheinput.Forexample,when",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "recognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof\nthephoto.\nItispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\noftenmoreexpensivethanclassiï¬cation.Someofthemrequireproducingmultiple\noutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\n1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "e n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c .\n5 5 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\nâ€¢Densityestimation:givenaninput x,themachinelearningsystemreturns\nanestimateofthetruedensity p( x)underthedatageneratingdistribution.\nThisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-\ningoftheentireinput.Ifevenoneelementofthevectorisunusual,the\nsystemmustassignitalowprobability.\nâ€¢Denoising:givenadamagedorincorrectlyobservedinput Ëœ x,themachine",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "learningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,\nthemachinelearningsystemmightbeaskedtoremovedustorscratches\nfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\nestimatedcleanexample x)andanunderstandingoftheentireinput(since\nevenonedamagedareawillstillrevealtheï¬nalestimateasbeingdamaged).\nâ€¢Missingvalueimputation:giventheobservationsofsomeelementsof x,\nthemodelisaskedtoreturnestimatesoforaprobabilitydistributionover",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "someoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs.\nBecausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it\nmustunderstandtheentireinput.\nâ€¢Sampling:themodelgeneratesnewsamplesfromthedistribution p( x).\nApplicationsincludespeechsynthesis,i.e.producingnewwaveformsthat\nsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\ngoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\nfromthewrongdistribution,thenthesamplingprocessiswrong.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "Foranexampleofasamplingtaskusingsmallnaturalimages,seeï¬gure.16.1\nModelingarichdistributionoverthousandsormillionsofrandomvariablesisa\nchallengingtask,bothcomputationally andstatistically.Supposeweonlywanted\ntomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\nseemsoverwhelming.Forasmall, 32Ã—32 2 pixelcolor(RGB)image,thereare3 0 7 2\npossiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan\ntheestimatednumberofatomsintheuniverse.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "theestimatednumberofatomsintheuniverse.\nIngeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining\nndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof\nrepresenting P(x)bystoringalookuptablewithoneprobabilityvalueperpossible\noutcomerequires knparameters!\nThisisnotfeasibleforseveralreasons:\n5 6 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example32Ã—32pixelcolor\nimagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )\ndrawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\natthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\nspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\nadjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011\n5 6 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nâ€¢ M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues\nof nand k,representingthedistributionasatablewillrequiretoomany\nvaluestostore.\nâ€¢ St a t i s t i c a l e ï¬ƒ c i e nc y:Asthenumberofparametersinamodelincreases,\nsodoestheamountoftrainingdataneededtochoosethevaluesofthose\nparametersusingastatisticalestimator.Becausethetable-basedmodel",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "hasanastronomicalnumberofparameters,itwillrequireanastronomically\nlargetrainingsettoï¬taccurately.Anysuchmodelwilloverï¬tthetraining\nsetverybadlyunlessadditionalassumptionsaremadelinkingthediï¬€erent\nentriesinthetable(forexample,likeinback-oï¬€orsmoothed n-grammodels,\nsection).12.4.1\nâ€¢ R u nt i m e : Â  t h e c o s t o f i nfe r e nc e:Â Supposewewanttoperformaninference\ntaskwhereweuseourmodelofthejointdistribution P(x)tocomputesome\notherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "distribution P(x 2|x 1).Computingthesedistributionswillrequiresumming\nacrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\nintractablememorycostofstoringthemodel.\nâ€¢ R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample\nfromthemodel.Thenaivewaytodothisistosamplesomevalueuâˆ¼ U(0 ,1),\ntheniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\nexceed uandreturntheoutcomecorrespondingtothatpositioninthetable.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "Thisrequiresreadingthroughthewholetableintheworstcase,soithas\nthesameexponentialcostastheotheroperations.\nTheproblemwiththetable-basedapproachisthatweareexplicitlymodeling\neverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\nprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\nUsually,mostvariablesinï¬‚uenceeachotheronlyindirectly.\nForexample,considermodelingtheï¬nishingtimesofateaminarelayrace.\nSupposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\nherlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\nlapandhandsthebatontoCarol,whorunstheï¬nallap.Wecanmodeleachof\ntheirï¬nishingtimesasacontinuousrandomvariable.Aliceâ€™sï¬nishingtimedoes\nnotdependonanyoneelseâ€™s,sinceshegoesï¬rst.Bobâ€™sï¬nishingtimedepends\nonAliceâ€™s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\nhascompletedhers.Â IfAliceï¬nishesfaster,Bobwillï¬nishfaster,allelsebeing\n5 6 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nequal.Finally,Carolâ€™sï¬nishingtimedependsonbothherteammates.IfAliceis\nslow,Bobwillprobablyï¬nishlatetoo.Asaconsequence,Carolwillhavequitea\nlatestartingtimeandthusislikelytohavealateï¬nishingtimeaswell.However,\nCarolâ€™sï¬nishingtimedependsonly i ndir e c t l yonAliceâ€™sï¬nishingtimeviaBobâ€™s.\nIfwealreadyknowBobâ€™sï¬nishingtime,wewillnotbeabletoestimateCarolâ€™s\nï¬nishingtimebetterbyï¬ndingoutwhatAliceâ€™sï¬nishingtimewas.Thismeans",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "wecanmodeltherelayraceusingonlytwointeractions: Aliceâ€™seï¬€ectonBoband\nBobâ€™seï¬€ectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\nandCarolfromourmodel.\nStructuredprobabilisticmodelsprovideaformalframeworkformodelingonly\ndirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\nsigniï¬cantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\nThesesmallermodelsalsohavedramatically reducedcomputational costinterms",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\nthemodel.\n16.2UsingGraphstoDescribeModelStructure\nStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseofâ€œnodesâ€or\nâ€œverticesâ€connectedbyedges)torepresentinteractionsbetweenrandomvariables.\nEachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\nThesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\ninteractionsneedtobeexplicitlymodeled.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "interactionsneedtobeexplicitlymodeled.\nThereismoreÂ thanoneÂ waytoÂ describeÂ theinteractionsinÂ aprobability\ndistributionusingagraph.Inthefollowingsectionswedescribesomeofthemost\npopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\ntwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\nundirectedgraphs.\n1 6 . 2 . 1 D i rect ed Mo d el s\nOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,\notherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,).",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Directedgraphicalmodelsarecalledâ€œdirectedâ€becausetheiredgesaredirected,\n2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm â€œ B a y e s i a n n e t wo rk â€ wh e n o n e wis h e s t o â€œ e m p h a s i z e\nt h e j u d g m e n t a l â€ n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y\nre p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s .\n5 6 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nt 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol\nFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Aliceâ€™sï¬nishing\ntimet 0inï¬‚uencesBobâ€™sï¬nishingtimet 1,becauseBobdoesnotgettostartrunninguntil\nAliceï¬nishes.Likewise,CarolonlygetstostartrunningafterBobï¬nishes,soBobâ€™s\nï¬nishingtimet 1directlyinï¬‚uencesCarolâ€™sï¬nishingtimet 2.\nthatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariableâ€™s\nprobabilitydistributionisdeï¬nedintermsoftheotherâ€™s.Drawinganarrowfrom\natobmeansthatwedeï¬netheprobabilitydistributionoverbviaaconditional\ndistribution,withaasoneofthevariablesontherightsideoftheconditioning\nbar.Inotherwords,thedistributionoverbdependsonthevalueofa.\nContinuingwiththerelayraceexamplefromsection,supposewename 16.1\nAliceâ€™sï¬nishingtimet 0,Bobâ€™sï¬nishingtimet 1,andCarolâ€™sï¬nishingtimet 2.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "Aswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends\ndirectlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected\ngraphicalmodel,illustratedinï¬gure.16.2\nFormally,adirectedgraphicalmodeldeï¬nedonvariables xisdeï¬nedbya\ndirectedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,\nandasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where\nP aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven\nby\np() = Î x i p(x i| P aG(x i)) . (16.1)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "by\np() = Î x i p(x i| P aG(x i)) . (16.1)\nInourrelayraceexample,thismeansthat,usingthegraphdrawninï¬gure,16.2\np(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2)\nThisisourï¬rsttimeseeingastructuredprobabilisticmodelinaction.We\ncanexaminethecostofusingit,inordertoobservehowstructuredmodelinghas\nmanyadvantagesrelativetounstructuredmodeling.\nSupposewerepresentedtimebydiscretizingtimerangingfromminute0to\nminute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "variablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha\ntable,itwouldneedtostore999,999values(100valuesoft 0Ã—100valuesoft 1Ã—\n100valuesoft 2,minus1,sincetheprobabilityofoneoftheconï¬gurations ismade\n5 6 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\nonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe\ndistributionovert 0requires99values,thetabledeï¬ningt 1givent 0requires9900\nvalues,andsodoesthetabledeï¬ningt 2givent 1.Thiscomestoatotalof19,899\nvalues.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\nparametersbyafactorofmorethan50!",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "parametersbyafactorofmorethan50!\nIngeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe\nsingletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose\nwebuildadirectedgraphicalmodeloverthesevariables.Â If misthemaximum\nnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\nconditionalprobabilitydistribution,thenthecostofthetablesforthedirected\nmodelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we\ngetverydramaticsavings.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "getverydramaticsavings.\nInotherwords,solongaseachvariablehasfewparentsinthegraph,the\ndistributioncanberepresentedwithveryfewparameters.Â Somerestrictionson\nthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\noperationslikecomputingmarginalorconditionaldistributionsoversubsetsof\nvariablesareeï¬ƒcient.\nItisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\nthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "areconditionallyindependentfromeachother.Itisalsopossibletomakeother\nkindsofsimplifyingassumptions.Â Forexample,supposeweassumeBobalways\nrunsthesameregardlessofhowAliceperformed.(Inreality,Aliceâ€™sperformance\nprobablyinï¬‚uencesBobâ€™sperformanceâ€”dependingonBobâ€™spersonality,ifAlice\nrunsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\nmatchherexceptionalperformance,oritmightmakehimoverconï¬dentandlazy).\nThentheonlyeï¬€ectAlicehasonBobâ€™sï¬nishingtimeisthatwemustaddAliceâ€™s",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "ï¬nishingtimetothetotalamountoftimewethinkBobneedstorun.This\nobservationallowsustodeï¬neamodelwith O( k)parametersinsteadof O( k2).\nHowever,notethatt 0andt 1arestilldirectlydependentwiththisassumption,\nbecauset 1representstheabsolutetimeatwhichBobï¬nishes,notthetotaltime\nhehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom\nt 0tot 1.TheassumptionthatBobâ€™spersonalrunningtimeisindependentfrom\nallotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "encodethisinformationinthedeï¬nitionoftheconditionaldistributionitself.The\nconditionaldistributionisnolongera k kÃ—âˆ’1elementtableindexedbyt 0andt 1\nbutisnowaslightlymorecomplicatedformulausingonly kâˆ’1parameters.The\ndirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeï¬ne\n5 6 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nourconditionaldistributions.Itonlydeï¬neswhichvariablestheyareallowedto\ntakeinasarguments.\n1 6 . 2 . 2 Un d i rec t ed Mo d el s\nDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\nticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise\nknownasMarkovrandomï¬elds(MRFs)orMarkovnetworks(Kinder-\nmann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\nareundirected.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "areundirected.\nDirectedmodelsaremostnaturallyapplicabletosituationswherethereis\naclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\nsituationswhereweunderstandthecausalityandthecausalityonlyï¬‚owsinone\ndirection.Onesuchsituationistherelayraceexample.Earlierrunnersaï¬€ectthe\nï¬nishingtimesoflaterrunners;laterrunnersdonotaï¬€ecttheï¬nishingtimesof\nearlierrunners.\nNotallsituationswemightwanttomodelhavesuchacleardirectiontotheir",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto\noperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\nAsanexampleofsuchasituation,supposewewanttomodeladistribution\noverthreebinaryvariables:whetherornotyouaresick,whetherornotyour\ncoworkerissick,andwhetherornotyourroommateissick.Asintherelayrace\nexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\ntakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\ncolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\nit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\nthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\nacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe\ncoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\nroommate.\nInthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "itisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional\nnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\nAswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\nedge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\notherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\narrow,andisnotassociatedwithaconditionalprobabilitydistribution.\n5 6 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh r h r h y h y h c h c\nFigure16.3:Anundirectedgraphrepresentinghowyourroommateâ€™shealthh r,your\nhealthh y,andyourworkcolleagueâ€™s healthh caï¬€ecteachother.Youandyourroommate\nmightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,\nbutassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan\nonlyinfecteachotherindirectlyviayou.\nWedenotetherandomvariablerepresentingyourhealthash y,therandom",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "variablerepresentingyourroommateâ€™shealthash r,andtherandomvariable\nrepresentingyourcolleagueâ€™shealthash c.Seeï¬gureforadrawingofthe 16.3\ngraphrepresentingthisscenario.\nFormally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\ndeï¬nedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor Ï†(C)\n(alsocalledacliquepotential)Â measurestheaï¬ƒnityofthevariablesinthatclique\nforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "non-negative.Togethertheydeï¬neanunnormalizedprobabilitydistribution\nËœ p() = Î x CâˆˆG Ï† .()C (16.3)\nTheunnormalized probabilitydistributioniseï¬ƒcienttoworkwithsolongas\nallthecliquesaresmall.Itencodestheideathatstateswithhigheraï¬ƒnityare\nmorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\ndeï¬nitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\ntogetherwillyieldavalidprobabilitydistribution.Seeï¬gureforanexample 16.4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "ofreadingfactorizationinformationfromanundirectedgraph.\nOurexampleofthecoldspreadingbetweenyou,yourroommate,andyour\ncolleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis\ncliquecanbedeï¬nedbyatable,andmighthavevaluesresemblingthese:\nh y= 0h y= 1\nh c= 021\nh c= 1110\n3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f\nt h e g ra p h .\n5 6 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\n(havingbeenÂ infectedwithÂ acold).BothÂ ofyouÂ areusuallyhealthy,Â sothe\ncorrespondingstatehasthehighestaï¬ƒnity.Thestatewhereonlyoneofyouis\nsickhasthelowestaï¬ƒnity,becausethisisararestate.Thestatewherebothof\nyouaresick(becauseoneofyouhasinfectedtheother)isahigheraï¬ƒnitystate,\nthoughstillnotascommonasthestatewherebotharehealthy.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "Tocompletethemodel,wewouldneedtoalsodeï¬neasimilarfactorforthe\ncliquecontainingh yandh r.\n1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n\nWhiletheunnormalized probabilitydistributionisguaranteedtobenon-negative\neverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\nprobabilitydistribution,wemustusethecorrespondingnormalizedprobability\ndistribution:4\np() =x1\nZËœ p()x (16.4)\nwhere ZisthevalueÂ thatresultsintheprobabilityÂ distributionsummingor\nintegratingto1:\nZ=îš\nËœ p d . ()xx (16.5)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "integratingto1:\nZ=îš\nËœ p d . ()xx (16.5)\nYoucanthinkof Zasaconstantwhenthe Ï†functionsareheldconstant.Note\nthatifthe Ï†functionshaveparameters,then Zisafunctionofthoseparameters.\nItiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace.\nThenormalizingconstant Zisknownasthepartitionfunction,atermborrowed\nfromstatisticalphysics.\nSince Zisanintegralorsumoverallpossiblejointassignmentsofthestatex\nitisoftenintractabletocompute.Â Inordertobeabletoobtainthenormalized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionofanundirectedmodel,Â themodelstructureandthe\ndeï¬nitionsofthe Ï†functionsmustbeconducivetocomputing Zeï¬ƒciently.In\nthecontextofdeeplearning, Zisusuallyintractable.Â Due totheintractability\nofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate\nalgorithmsarethetopicofchapter.18\nOneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\nisthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\n4A d i s t rib u t i o n d e ï¬ n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s\nd is t rib u t i on .\n5 6 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nofËœ povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\nscalarvariablexwithasinglecliquepotential âˆˆ R Ï† x x () = 2.Inthiscase,\nZ=îš\nx2d x . (16.6)\nSincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\nthischoiceof Ï†( x).Â Sometimes thechoiceofsomeparameterofthe Ï†functions\ndetermineswhethertheÂ probabilit ydistributionÂ isdeï¬ned.ForÂ example,Â for\nÏ†( x; Î²) =expî€€âˆ’ Î² x2î€",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "Ï†( x; Î²) =expî€€âˆ’ Î² x2î€\n,the Î²parameterdetermineswhether Zexists.Positive Î²\nresultsinaGaussiandistributionoverxbutallothervaluesof Î²make Ï†impossible\ntonormalize.\nOnekeydiï¬€erencebetweendirectedmodelingandundirectedmodelingisthat\ndirectedmodelsaredeï¬neddirectlyintermsofprobabilitydistributionsfrom\nthestart,whileundirectedmodelsaredeï¬nedmorelooselyby Ï†functionsthat\narethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone\nmustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\nhasdramaticeï¬€ectonthekindofprobabilitydistributionthatagivensetof Ï†\nfunctionscorrespondsto.Forexample,consideran n-dimensionalvector-valued\nrandomvariable xandanundirectedmodelparametrized byavectorofbiases\nb.Supposewehaveonecliqueforeachelementofx, Ï†( ) i(x i) =exp( b ix i).What\nkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\nnothaveenoughinformation,becausewehavenotyetspeciï¬edthedomainofx.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "Ifx âˆˆ Rn,thentheintegraldeï¬ning Zdivergesandnoprobabilitydistribution\nexists.Ifxâˆˆ{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with\np(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors\n({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge\nvalueof b iactuallyreduces p(x j=1)for jî€¶= i.Â Often,itispossibletoleverage\ntheeï¬€ectofacarefullychosendomainofavariableinordertoobtaincomplicated",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "behaviorfromarelativelysimplesetof Ï†functions.Wewillexploreapractical\napplicationofthisidealater,insection.20.6\n1 6 . 2 . 4 E n erg y-B a s ed Mo d el s\nManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\nsumptionthatâˆ€x ,Ëœ p(x) >0.Aconvenientwaytoenforcethisconditionistouse\nan (EBM)where energy-basedmodel\nËœ p E () = exp( x âˆ’())x (16.7)\n5 6 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas\n1\nZÏ† a b ,(ab ,) Ï† b c ,(bc ,) Ï† a d ,(ad ,) Ï† b e ,(be ,) Ï† e f ,(ef ,)foranappropriatechoiceofthe Ï†func-\ntions.\nand E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall\nz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\nforanystatex.BeingcompletelyÂ free toÂ chooseÂ theenergyfunctionÂ makes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\nconstrainedoptimization toarbitrarilyimposesomespeciï¬cminimalprobability\nvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\nTheprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\nbutneverreachit.\nAnydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\nmannÂ distribution.ForÂ thisÂ reason,Â manyenergy-basedÂ modelsÂ areÂ called",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Boltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,\n1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\namodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\ntermBoltzmannmachinewasï¬rstintroducedtodescribeamodelwithexclusively\nbinaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\nBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann\nmachineswereoriginallydeï¬nedtoencompassbothmodelswithandwithoutla-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\nmodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\naremoreoftencalledMarkovrandomï¬eldsorlog-linearmodels.\nCliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\nprobabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdiï¬€erent\ncliquesintheundirectedgraphcorrespondtothediï¬€erenttermsoftheenergy\nfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "network:theexponentiationmakeseachtermintheenergyfunctioncorrespond\ntoafactorforadiï¬€erentclique.Seeï¬gureforanexampleofhowtoreadthe 16.5\n5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z\n5 7 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b c\nd e f\nFigureÂ 16.5:ThisgraphÂ impliesthat E(abcdef , , , , ,)canÂ beÂ writtenas E a b ,(ab ,)+\nE b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique\nenergyfunctions.Notethatwecanobtainthe Ï†functionsinï¬gurebysettingeach 16.4 Ï†\ntotheexponentialofthecorrespondingnegativeenergy,e.g., Ï† a b ,(ab ,) =exp(()) âˆ’ Eab ,.\nformoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\nofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\nanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\nbethoughtofasanâ€œexpertâ€thatdetermineswhetheraparticularsoftconstraint\nissatisï¬ed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\nalow-dimensionalprojectionoftherandomvariables,butwhencombinedby\nmultiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "dimensionalconstraint.\nOnepartofthedeï¬nitionofanenergy-basedmodelservesnofunctionalpurpose\nfromamachinelearningpointofview:theâˆ’signinequation.This16.7 âˆ’sign\ncouldbeincorporatedintothedeï¬nitionof E.Formanychoicesofthefunction\nE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\nâˆ’signispresentprimarilytopreservecompatibilitybetweenthemachinelearning\nliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "wereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,\nphysicalenergyanddoesnothavearbitrarysign.Â Terminologysuchasâ€œenergyâ€\nandâ€œpartitionfunctionâ€remainsassociatedwiththesetechniques,eventhough\ntheirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey\nweredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986\nreferredtonegativeenergyasharmony)havechosentoemitthenegation,but\nthisisnotthestandardconvention.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thisisnotthestandardconvention.\nManyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute\np m o de l( x)butonly log Ëœ p m o de l( x).Forenergy-basedmodelswithlatentvariables h,\nthesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\n5 7 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b a s b\n(a) (b)\nFigure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis\nactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres\nisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\nthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.\ncalledthe :freeenergy\nF âˆ’ () = x logî˜\nhexp(( )) âˆ’ E x h , . (16.8)",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "F âˆ’ () = x logî˜\nhexp(( )) âˆ’ E x h , . (16.8)\nInthisbook,weusuallypreferthemoregeneral log Ëœ p m o de l() xformulation.\n1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n\nTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\nneedtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions\ncanbeenabledordisabledbyobservingothervariables.Moreformally,wewould\nliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "other,giventhevaluesofothersubsetsofvariables.\nIdentifyingtheconditionalindependencesinagraphisverysimpleinthecase\nofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph\niscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat\nAisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath\ninvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\nseparated.Werefertopathsinvolvingonlyunobservedvariablesasâ€œactiveâ€and\npathsincludinganobservedvariableasâ€œinactive.â€\nWhenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\nSeeï¬gureforadepictionofhowactiveandinactivepathsinanundirected 16.6\nmodellookwhendrawninthisway.Seeï¬gureforanexampleofreading 16.7\nseparationfromanundirectedgraph.\nSimilarÂ concepts applyÂ todirectedÂ models ,exceptÂ thatÂ intheÂ contextÂ of",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "directedmodels,theseconceptsarereferredtoasd-separation.Theâ€œdâ€stands\nforâ€œdependence.â€Â D-separati onfordirectedgraphsisdeï¬nedthesameasseparation\n5 7 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na\nb c\nd\nFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\nbisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom\natoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb\nalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.\nTherefore,aanddarenotseparatedgivenb.\nforundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies\nthatisindependentfromgiven. A B S\nAswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\ngraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\naredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch\npathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\nmorecomplicated. Seeï¬gureforaguidetoidentifyingactivepathsina 16.8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "directedmodel.Seeï¬gureforanexampleofreadingsomepropertiesfroma 16.9\ngraph.\nItisimportanttorememberthatseparationandd-separationtellusonly\naboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno\nrequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\nitisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\ntorepresentanydistribution.Infact,somedistributionscontainindependences",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "thatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\nspeciï¬cindependencesareindependencesthatarepresentdependentonthe\nvalueofsomevariablesinthenetwork.Â Forexample,consideramodelofthree\nbinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,\nbutwhenais1,bisdeterministicallyequaltoc.Â Encodingthebehaviorwhen\na= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb\nandcareindependentwhena.= 0\nIngeneral,agraphwillneverimplythatanindependenceexistswhenitdoes",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "not.However,agraphmayfailtoencodeanindependence.\n5 7 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na s b\na s b\na\nsb a s ba s b\nc( a ) ( b )\n( c ) ( d )\nFigure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\nvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa.\nThiskindofpathbecomesblockedifsisobserved.Â Wehavealreadyseenthiskindof\npathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For\nexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaand",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "bmeasurethewindspeedattwodiï¬€erentnearbyweathermonitoringoutposts.Ifwe\nobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This\nkindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we\nexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected\nwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing\nthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "pathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe\ncollidercase.Â TheV-structurecausesaandbtoberelatedbytheexplainingaway\neï¬€ect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose\nsisavariableindicatingthatyourcolleagueisnotatwork.Â Thevariablearepresents\nherbeingsick,whilebrepresentsherbeingonvacation.Â Ifyouobservethatsheisnot\natwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "likelythatbothhavehappenedatthesametime.Ifyouï¬ndoutthatsheisonvacation,\nthisfactissuï¬ƒcienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n\nsick.Theexplainingawayeï¬€ecthappensevenifanydescendantof ( d ) sisobserved!For\nexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareport\nfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases\nyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "morelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha\nV-structureistoobservenoneofthedescendantsofthesharedchild.\n5 7 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nc\nd e\nFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\ninclude:\nâ€¢aandbared-separatedgiventheemptyset.\nâ€¢aandeared-separatedgivenc.\nâ€¢dandeared-separatedgivenc.\nWecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\nvariables:\nâ€¢aandbarenotd-separatedgivenc.\nâ€¢aandbarenotd-separatedgivend.\n5 7 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s\nWeoftenrefertoaspeciï¬cmachinelearningmodelasbeingundirectedordirected.\nForexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\nThischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\nisinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\nDirectedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\nvantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\nweshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\ndependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\nuseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\ncapturethemostindependencesintheprobabilitydistributionorwhichapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "usesthefewestedgestodescribethedistribution.Thereareotherfactorsthat\ncanaï¬€ectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle\nprobabilitydistribution,wemaysometimesswitchbetweendiï¬€erentmodeling\nlanguages.Sometimesadiï¬€erentlanguagebecomesmoreappropriateifweobserve\nacertainsubsetofvariables,orifwewishtoperformadiï¬€erentcomputational\ntask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward\napproachtoeï¬ƒcientlydrawsamplesfromthemodel(describedinsection)16.3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whiletheundirectedmodelformulationisoftenusefulforderivingapproximate\ninferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19\nmodelsishighlightedinequation).19.56\nEveryprobabilitydistributioncanberepresentedbyeitheradirectedmodel\norbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany\ndistributionbyusingaâ€œcompletegraph.â€Inthecaseofadirectedmodel,the\ncompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingon",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "therandomvariables,andeachvariablehasallothervariablesthatprecedeitin\ntheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete\ngraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.\nSeeï¬gureforanexample. 16.10\nOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\nvariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\ndoesnotimplyanyindependences.\nWhenwerepresentaprobabilitydistributionwithagraph,wewanttochoose",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\nindependencesthatdonotactuallyexist.\nFromthispointofview,somedistributionscanberepresentedmoreeï¬ƒciently\n5 7 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\nHereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph.\nIntheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t )\nInthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\nvariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\nvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.\nusingdirectedmodels,whileotherdistributionscanberepresentedmoreeï¬ƒciently\nusingÂ undirectedmodels.InÂ otherÂ words,directedÂ modelsÂ canencodeÂ some\nindependencesthatundirectedmodelscannotencode,andviceversa.\nDirectedmodelsareabletouseonespeciï¬ckindofsubstructurethatundirected\nmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "Thestructureoccurswhentworandomvariablesaandbarebothparentsofa\nthirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither\ndirection.(Thenameâ€œimmoralityâ€mayseemstrange;itwascoinedinthegraphical\nmodelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\nwithgraph Dintoanundirectedmodel,weneedtocreateanewgraph U.Â For\neverypairofvariablesxandy,weaddanundirectededgeconnectingxandyto\nUifthereisadirectededge(ineitherdirection)connectingxandyinDorifx",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "andyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa\nmoralizedgraph.Seeï¬gureforexamplesofconvertingdirectedmodelsto 16.11\nundirectedmodelsviamoralization.\nLikewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\ncanrepresentperfectly.Speciï¬cally,adirectedgraphcannotcaptureallofthe D\nconditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop\noflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "asequenceofvariablesconnectedbyundirectededges,withthelastvariablein\nthesequenceconnectedbacktotheï¬rstvariableinthesequence.Â Achordisa\nconnectionbetweenanytwonon-consecutivevariablesinthesequencedeï¬ninga\nloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese\nloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\n5 7 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3a b\nca\ncb\nFigure16.11:Â Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\n(bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted\ntoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\nresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "independences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )\ntoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely\nofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive\npathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude\nacliqueencompassingallthreevariables.ThiscliquefailstoencodethefactthatabâŠ¥.\n( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\nedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\ndirectdependences.\n5 7 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\na b\nd ca b\nd ca b\nd c\nFigure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected\nmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour\nwithnochords.Speciï¬cally,theundirectedmodelencodestwodiï¬€erentindependencesthat\nnodirectedmodelcancapturesimultaneously:acbd âŠ¥|{ ,}andbdac âŠ¥|{ ,}.To ( C e n t e r )\nconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "ensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither\naddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis\nexample,wechoosetoaddtheedgeconnectingaandc.Toï¬nishtheconversion ( R i g h t )\nprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\ndirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\nandalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\nalphabeticalorder.\nthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.\nThegraphformedbyaddingchordstoUisknownasachordalortriangulated\ngraph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\nloops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign\ndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein\nD,ortheresultdoesnotdeï¬neavaliddirectedprobabilisticmodel.Oneway",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "toassigndirectionstotheedgesinDistoimposeanorderingontherandom\nvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingto\nthenodethatcomeslaterintheordering.Seeï¬gureforademonstration. 16.12\n1 6 . 2 . 7 F a ct o r G ra p h s\nFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean\nambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\nanundirectedmodel,thescopeofevery Ï†functionmustbeaofsomeclique s u b s e t",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\nacorrespondingfactorwhosescopeencompassestheentirecliqueâ€”forexample,\nacliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\normaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\n5 7 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach Ï†\nfunction.Speciï¬cally,afactorgraphisagraphicalrepresentationofanundirected\nmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\nascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected\nmodel.Â Therestofthenodesaredrawnassquares.Â Thesenodescorrespondto\nthefactors Ï†oftheunnormalized probabilitydistribution.Variablesandfactors",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "maybeconnectedwithundirectededges.Avariableandafactorareconnected\ninthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\ntheunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother\nfactorinthegraph,norcanavariablebeconnectedtoavariable.Seeï¬gure16.13\nforanexampleofhowfactorgraphscanresolveambiguityintheinterpretation of\nundirectednetworks.\na b\nca b\ncf 1 f 1a b\ncf 1 f 1f 2 f 2\nf 3 f 3\nFigure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "ofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:\na,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )\nfactorgraphhasonefactoroverallthreevariables.Â Anothervalidfactorgraph ( R i g h t )\nforthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo\nvariables.Representation,inference,andlearningareallasymptoticallycheaperinthis\nfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "sameundirectedgraphtorepresent.\n16.3SamplingfromGraphicalModels\nGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\nOneadvantageofdirectedgraphicalmodelsisthatasimpleandeï¬ƒcientproce-\ndurecalledancestralsamplingcanproduceasamplefromthejointdistribution\nrepresentedbythemodel.\nThebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,\nsothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables\n5 8 0",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ncanthenbesampledinthisorder.Inotherwords,weï¬rstsamplex 1âˆ¼ P(x 1),\nthensample P(x 2| P aG(x 2)),andsoon,untilï¬nallywesample P(x n| P aG(x n)).\nSolongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,\nthenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\nguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\nsamplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "sampleavariablebeforeitsparentsareavailable.\nForsomegraphs,morethanonetopologicalorderingispossible.Ancestral\nsamplingmaybeusedwithanyofthesetopologicalorderings.\nAncestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\ntionaliseasy)andconvenient.\nOnedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\nmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\noperation.Whenwewishtosamplefromasubsetofthevariablesinadirected",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\ningvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\nInthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\nspeciï¬edbythemodeldistribution.Otherwise,theconditionaldistributionswe\nneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.\nTheseposteriordistributionsareusuallynotexplicitlyspeciï¬edandparametrized",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\nthisisthecase,ancestralsamplingisnolongereï¬ƒcient.\nUnfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\ncansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\noftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\ndistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\nsomanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "fromanundirectedmodelwithoutï¬rstconvertingittoadirectedmodelseemsto\nrequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\nvariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\ndrawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass\nprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewe\nhaveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We\niterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "variables,from p(x i|xâˆ’ i).Duetotheseparationpropertiesofthegraphical\nmodel,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,\nafterwehavemadeonepassthroughthegraphicalmodelandsampledall n\nvariables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe\n5 8 1",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nprocessandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors.\nAsymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\nthecorrectdistribution.Itcanbediï¬ƒculttodeterminewhenthesampleshave\nreachedasuï¬ƒcientlyaccurateapproximationofthedesireddistribution.Sampling\ntechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\nchapter.17\n16.4AdvantagesofStructuredModeling",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "chapter.17\n16.4AdvantagesofStructuredModeling\nTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\nustodramatically reducethecostofrepresentingprobabilitydistributionsaswell\naslearningandinference.Samplingisalsoacceleratedinthecaseofdirected\nmodels,whilethesituationcanbecomplicatedwithundirectedmodels.The\nprimarymechanismthatallowsalloftheseoperationstouselessruntimeand\nmemoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "informationbyleavingedgesout.Anywherethereisnotanedge,themodel\nspeciï¬estheassumptionthatwedonotneedtomodeladirectinteraction.\nAlessquantiï¬ablebeneï¬tofusingstructuredprobabilisticmodelsisthat\ntheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\nknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\ndevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\ninferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\ndata.Wecanthencombinethesediï¬€erentalgorithmsandstructuresandobtain\naCartesianproductofdiï¬€erentpossibilities.Itwouldbemuchmorediï¬ƒcultto\ndesignend-to-endalgorithmsforeverypossiblesituation.\n16.5LearningaboutDependencies\nAgoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\nobservedorâ€œvisibleâ€Â variables v.Oftenthediï¬€erentelementsofvarehighly\ndependentoneachother.Inthecontextofdeeplearning,theapproachmost",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedtomodelthesedependenciesistointroduceseverallatentor\nâ€œhiddenâ€variables,h.Themodelcanthencapturedependenciesbetweenanypair\nofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and\ndirectdependenciesbetweenandv h j.\nAgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto\n5 8 2",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\ncliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis\ncostlyâ€”bothinacomputational sense,becausethenumberofparametersthat\nmustbestoredinmemoryscalesexponentiallywiththenumberofmembersina\nclique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\nrequiresawealthofdatatoestimateaccurately.",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "requiresawealthofdatatoestimateaccurately.\nWhenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\nwithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\ngraphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\nomitedgesbetweenothervariables.Anentireï¬eldofmachinelearningcalled\nstructurelearningisdevotedtothisproblemForagoodreferenceonstructure\nlearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "aformofgreedysearch.Astructureisproposed,amodelwiththatstructure\nistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\npenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\naddedorremovedarethenproposedasthenextstepofthesearch.Thesearch\nproceedstoanewstructurethatisexpectedtoincreasethescore.\nUsinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\ndiscretesearchesandmultipleroundsoftraining.Aï¬xedstructureovervisible",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "andhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits\ntoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter\nlearningtechniqueswecanlearnamodelwithaï¬xedstructurethatimputesthe\nrightstructureonthemarginal . p()v\nLatentvariableshaveadvantagesbeyondtheirroleineï¬ƒcientlycapturing p(v).\nThenewvariables halsoprovideanalternativerepresentationforv.Forexample,\nasdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This\nmeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo\nclassiï¬cation.Â Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14\ncodinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiï¬er,\norascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,\nbutdeepermodelsandmodelswithdiï¬€erentkindsofinteractionscancreateeven\nricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "bylearninglatentvariables.Often,givensomemodelofvandh,experimental\nobservationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor\nv.\n5 8 3",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\n16.6InferenceandApproximateInference\nOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\nhowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\nwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\nextractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed\ntosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\nusingtheprincipleofmaximumlikelihood.Because",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "usingtheprincipleofmaximumlikelihood.Because\nlog()= p v E h hâˆ¼ p (| v )[log( )log( )] p h v ,âˆ’ p h v| ,(16.9)\nweoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof\ntheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof\nsomevariablesgivenothervariables,orpredicttheprobabilitydistributionover\nsomevariablesgiventhevalueofothervariables.\nUnfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\nintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\nwithareasonablenumberofparameters,butthegraphsusedfordeeplearningare\nusuallynotrestrictiveenoughtoalsoalloweï¬ƒcientinference.\nItisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\ngraphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe\ncomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\nhasasolutionandï¬ndingasolutionifoneexists.Problemsin#Prequirecounting",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\nwedeï¬neagraphicalmodeloverthebinaryvariablesina3-SATproblem.Â We\ncanimposeauniformdistributionoverthesevariables.Wecanthenaddone\nbinarylatentvariableperclausethatindicateswhethereachclauseissatisï¬ed.\nWecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare\nsatisï¬ed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\ntreeoflatentvariables,witheachnodeinthetreereportingwhethertwoother",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "variablesaresatisï¬ed.Theleavesofthistreearethevariablesforeachclause.\nTherootofthetreereportswhethertheentireproblemissatisï¬ed.Â Duetothe\nuniformdistributionovertheliterals,themarginaldistributionovertherootofthe\nreductiontreespeciï¬eswhatfractionofassignmentssatisfytheproblem.While\nthisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical\nreal-worldscenarios.\nThismotivatestheuseofapproximate inference.InÂ thecontextofÂ deep",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "learning,thisusuallyreferstovariationalinference,inwhichweapproximate the\n5 8 4",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\ntruedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas\nclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\ninchapter.19\n16.7TheDeepLearningApproachtoStructuredProb-\nabilisticModels\nDeeplearningpractitioners generallyusethesamebasiccomputational toolsas\nothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\nHowever,inthecontextofdeeplearning,weusuallymakediï¬€erentdesigndecisions",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\nhaveaverydiï¬€erentï¬‚avorfrommoretraditionalgraphicalmodels.\nDeeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\ncontextofgraphicalmodels,wecandeï¬nethedepthofamodelintermsofthe\ngraphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa\nlatentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved\nvariableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "depthofanysuch h i.Thiskindofdepthisdiï¬€erentfromthedepthinducedby\nthecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno\nlatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational\ngraphstodeï¬netheconditionaldistributionswithinamodel.\nDeeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\ntations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\nshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "haveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave\nmorelatentvariablesthanobservedvariables.Complicated nonlinearinteractions\nbetweenvariablesareaccomplishedviaindirectconnectionsthatï¬‚owthrough\nmultiplelatentvariables.\nBycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\nareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\nrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\nvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.\nThewaythatlatentvariablesaredesignedalsodiï¬€ersindeeplearning.The\ndeeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\ntakeonanyspeciï¬csemanticsaheadoftimeâ€”thetrainingalgorithmisfreeto\ninventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\n5 8 5",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\ntechniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When\nlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\noftendesignedwithsomespeciï¬csemanticsinmindâ€”thetopicofadocument,\ntheintelligenceofastudent,thediseasecausingapatientâ€™ssymptoms,etc.These\nmodelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "moretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare\nnotreusableinasmanydiï¬€erentcontextsasdeepmodels.\nAnotherobviousdiï¬€erenceisthekindofconnectivitytypicallyusedinthe\ndeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits\nthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween\ntwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels\nhaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "individuallydesigned.Thedesignofthemodelstructureistightlylinkedwith\nthechoiceofinferencealgorithm.Traditionalapproachestographicalmodels\ntypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint\nistoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled\nloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery\nsparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto\nconnecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "distributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo).\nDistributedrepresentationshavemanyadvantages,butfromthepointofview\nofgraphicalmodelsandcomputational complexity,distributedrepresentations\nhavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor\nthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe\nrelevant.Asaconsequence,oneofthemoststrikingdiï¬€erencesbetweenthelarger\ngraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "loopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels\nareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms\neï¬ƒcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge\nnumberoflatentvariables,makingeï¬ƒcientnumericalcodeessential.Thisprovides\nanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for\ngroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween\ntwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "witheï¬ƒcientmatrixproductoperations,orsparselyconnectedgeneralizations ,like\nblockdiagonalmatrixproductsorconvolutions.\nFinally,thedeeplearningapproachtographicalmodelingischaracterizedby\namarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\nallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\n5 8 6",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\nwhosemarginaldistributionscannotbecomputed,andaresatisï¬edsimplytodraw\napproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\nobjectivefunctionthatwecannotevenapproximate inareasonableamountof\ntime,butwearestillabletoapproximately trainthemodelifwecaneï¬ƒciently\nobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isoftentoï¬gureoutwhattheminimumamountofinformationweabsolutely\nneedis,andthentoï¬gureouthowtogetareasonableapproximation ofthat\ninformationasquicklyaspossible.\n1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e\nTherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium\nisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.\nTheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "thatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20\nseehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe\nRBMexempliï¬esmanyofthepracticesusedinawidevarietyofdeepgraphical\nmodels:Â itsunitsareorganizedintolargegroupscalledlayers,theconnectivity\nbetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the\nmodelisdesignedtoalloweï¬ƒcientGibbssampling,andtheemphasisofthemodel\ndesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "werenotspeciï¬edbythedesigner.Later,insection,wewillrevisittheRBM 20.2\ninmoredetail.\nThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\nunits.Itsenergyfunctionis\nE ,( v h b ) = âˆ’î€¾v câˆ’î€¾h vâˆ’î€¾W h , (16.10)\nwhere b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan\nseethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction\nbetweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically\ninï¬gure.Asthisï¬guremakesclear,animportantaspectofthismodelis 16.14",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany\ntwohiddenunits(hencetheâ€œrestricted,â€ageneralBoltzmannmachinemayhave\narbitraryconnections).\nTherestrictionsontheRBMstructureyieldtheniceproperties\np( ) = Î  hv| i p(h i|v) (16.11)\n5 8 7",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nh 1 h 1 h 2 h 2 h 3 h 3\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\nFigure16.14:AnRBMdrawnasaMarkovnetwork.\nand\np( ) = Î  vh| i p(v i|h) . (16.12)\nTheindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\nweobtain:\nP(h i= 1 ) = |v Ïƒî€\nvî€¾W : , i+ b iî€‘\n, (16.13)\nP(h i= 0 ) = 1 |v âˆ’ Ïƒî€\nvî€¾W : , i+ b iî€‘\n. (16.14)\nTogetherthesepropertiesallowforeï¬ƒcientblockGibbssampling,whichalter-\nnatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\nï¬gure.16.15\nSincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\neasytotakeitsderivatives.Forexample,\nâˆ‚\nâˆ‚ W i , jE ,(vh) = âˆ’v ih j . (16.15)\nThesetwopropertiesâ€”eï¬ƒcientGibbssamplingandeï¬ƒcientderivativesâ€”make\ntrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\ntrainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\nTrainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "E h hâˆ¼ p (| v )[] hasasetoffeaturestodescribe. v\nOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\nicalmodels:Â representationlearningaccomplishedvialayersoflatentvariables,\ncombinedwitheï¬ƒcientinteractionsbetweenlayersparametrized bymatrices.\nThelanguageofgraphicalmodelsprovidesanelegant,ï¬‚exibleandclearlanguage\nfordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\namongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\n5 8 8",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith\npermissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn\nusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow\nrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare\nhighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "thistothesamplesandweightsofalinearfactormodel,showninï¬gure.Thesamples 13.2\nherearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The\nRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,\ntheRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|\nsothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable\ntohavebothanon-factorialandanon-factorial. p() h p( ) h v|\n5 8 9",
    "metadata": {
      "source": "[22]part-3-chapter-16.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "Acknowledgments\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\nWewouldliketothankthosewhocommentedonourproposalforthebook\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\nÃ‡aÄŸlarGÃ¼lÃ§ehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\nRohÃ©e.\nWewouldliketothankthepeoplewhooï¬€eredfeedbackonthecontentofthe\nbookitself.Someoï¬€eredfeedbackonmanychapters:MartÃ­nAbadi,Guillaume\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiÃ§ici,Matko",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "BoÅ¡njak,JohnBoersma,GregBrockman,AlexandredeBrÃ©bisson,PierreLuc\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrÃ©dÃ©ricFrancis,\nNandoÂ deFreitas,Ã‡aÄŸlarÂ GÃ¼lÃ§ehre,Â JurgenÂ V anGael,JavierAlonsoÂ GarcÃ­a,\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\nMathey,MatÃ­asMattamala,AbhinavMaurya,KevinMurphy,OlegMÃ¼rk,Roman",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Novak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\nHalisSak,Â CÃ©sarSalgado,GrigorySapunov,YoshinoriSasaki,Â MikeSchuster,\nJulianSerban,NirShabat,KenShirriï¬€,AndreSimpelo,ScottStanley,David\nSussillo,IlyaSutskever,CarlesGeladaSÃ¡ez,GrahamTaylor,ValentinTolmer,\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Webb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZajÄ…candOzanÃ‡aÄŸlayan.\nWewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\nindividualchapters:\nâ€¢Notation:ZhangYuanhang.\nâ€¢Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\nviii",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPÃ¢rvulescu\nandAlfredoSolano.\nâ€¢Chapter, :AmjadAlmahairi,NikolaBaniÄ‡,KevinBennett, 2LinearAlgebra\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\nSergeyOreshkov,IstvÃ¡nPetrÃ¡s,DennisPrangle,ThomasRohÃ©e,Gitanjali\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\nâ€¢Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "AnttiRasmus,AlexeySurkovandVolkerTresp.\nâ€¢ChapterÂ ,Â  :TranÂ LamAnIanÂ FischerÂ andHu 4NumericalComputation\nYuhuang.\nâ€¢Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.\nâ€¢Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\nElizabethBurl,IshanDurugkar,Jeï¬€Hlywa,JongWookKim,DavidKrueger\nandAdityaKumarPraharaj.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "andAdityaKumarPraharaj.\nâ€¢Chapter, :MortenKolbÃ¦k,KshitijLauria, 7RegularizationforDeepLearning\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.\nâ€¢Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\nKashifRasul,KlausStroblandNicholasTurner.\nâ€¢Chapter,9ConvolutionalNetworks:MartÃ­nArjovsky,EugeneBrevdo,Kon-\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\nStoutandWentaoWu.",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "StoutandWentaoWu.\nâ€¢Chapter,10SequenceModeling:RecurrentandRecursiveNets:GÃ¶kÃ§en\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\nDmitriySerdyuk,DongyuShiandKaiyuYang.\nâ€¢Chapter, :DanielBeckstein. 11PracticalMethodology\nâ€¢Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\nRoscher.\nâ€¢Chapter,13LinearFactorModels:JayanthKoushik.\ni x",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "CO NTE NT S\nâ€¢Chapter, :KunalGhosh. 15RepresentationLearning\nâ€¢Chapter, :Â MinhLÃª 16StructuredProbabilisticModelsforDeepLearning\nandAntonVarfolom.\nâ€¢Chapter,18ConfrontingthePartitionFunction:SamBowman.\nâ€¢Chapter, :YujiaBao. 19ApproximateInference\nâ€¢Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\nWenmingMa,FadyMedhat,ShakirMohamedandGrÃ©goireMontavon.\nâ€¢Bibliography:LukasMichelbacherandLeslieN.Smith.\nWealsowanttothankthosewhoallowedustoreproduceimages,ï¬guresor",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "datafromtheirpublications.Weindicatetheircontributionsintheï¬gurecaptions\nthroughoutthetext.\nWewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\nmakethewebversionofthebook,andforoï¬€eringsupporttoimprovethequality\noftheresultingHTML.\nWeÂ wouldÂ liketothankÂ Ianâ€™swifeDanielaÂ FloriGoodfellowforpatiently\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\nWewouldliketothanktheGoogleBrainteamforprovidinganintellectual",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "environmentwhereIancoulddevoteatremendousamountoftimetowritingthis\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\ntothankIanâ€™sformermanager,GregCorrado,andhiscurrentmanager,Samy\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoï¬€rey\nHintonforencouragement whenwritingwasdiï¬ƒcult.\nx",
    "metadata": {
      "source": "[2]acknowledgements.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 4\nNumericalComputation\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\nlution.Commonoperationsincludeoptimization (ï¬ndingthevalueofanargument\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Evenjustevaluatingamathematical functiononadigitalcomputercanbediï¬ƒcult\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\nusingaï¬niteamountofmemory.\n4. 1 O v erï¬‚ o w an d Un d erï¬‚ o w\nThefundamentaldiï¬ƒcultyinperformingcontinuousmathonadigitalcomputer\nisthatweneedtorepresentinï¬nitelymanyrealnumberswithaï¬nitenumber\nofbitpatterns.Thismeansthatforalmostallrealnumbers,Â weincursome\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "cases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\nroundingerror.\nOneformofroundingerrorthatisparticularlydevastatingis under ï¬‚o w.\nUnderï¬‚owoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\nbehavequalitativelydiï¬€erentlywhentheirargumentiszeroratherthanasmall\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\n80",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\nisusuallytreatedasâˆ’âˆž,whichthenbecomesnot-a-numberifitisusedformany\nfurtherarithmeticoperations).\nAnotherhighlydamagingformofnumericalerroris o v e r ï¬‚o w.Overï¬‚owoccurs\nwhennumberswithlargemagnitudeareapproximatedasâˆžorâˆ’âˆž.Further\narithmeticwillusuallychangetheseinï¬nitevaluesintonot-a-numbervalues.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "Oneexampleofafunctionthatmustbestabilizedagainstunderï¬‚owand\noverï¬‚owisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\ndeï¬nedtobe\nsoftmax() x i=exp( x i)în\nj = 1exp( x j). (4.1)\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\nwecanseethatalloftheoutputsshouldbeequalto1\nn.Numerically,thismay\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "underï¬‚ow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheï¬nal\nresultisundeï¬ned.When cisverylargeandpositive,exp( c)willoverï¬‚ow,again\nresultingintheexpressionasawholebeingundeï¬ned.Bothofthesediï¬ƒculties\ncanberesolvedbyinsteadevaluating softmax( z)where z= xâˆ’max i x i.Simple\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverï¬‚ow.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\nthepossibilityofunderï¬‚owinthedenominator leadingtoadivisionbyzero.\nThereisstillonesmallproblem.Underï¬‚owinthenumeratorcanstillcause\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\nlogsoftmax( x)byï¬rstrunningthesoftmaxsubroutinethenpassingtheresultto\nthelogfunction,wecoulderroneouslyobtain âˆ’âˆž.Instead,wemustimplement\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "logsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\nthefunction. softmax\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "toimplementanewalgorithmandhavethenewimplementation automatically\n8 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nstabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012\nofasoftwarepackagethatautomatically detectsandstabilizesmanycommon\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\n4. 2 P o or C on d i t i o n i n g\nConditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges\ninitsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly\ncanbeproblematicforscientiï¬ccomputationbecauseroundingerrorsintheinputs",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "canresultinlargechangesintheoutput.\nConsiderthefunction f( x)= Aâˆ’ 1x.When Aâˆˆ Rn n Ã—hasaneigenvalue\ndecomposition,its c o ndi t i o n num beris\nmax\ni , jî€Œî€Œî€Œî€ŒÎ» i\nÎ» jî€Œî€Œî€Œî€Œ. (4.2)\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\nThissensitivityisanintrinsicpropertyofthematrixitself,nottheresult\nofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\n4. 3 Gradi en t - Bas e d O p t i m i z a t i o n\nMostdeeplearningalgorithmsinvolveoptimization ofsomesort.Â Optimization\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\nx.Â Weusuallyphrasemostoptimization problemsintermsofminimizing f( x).\nMaximization maybeaccomplishedviaaminimization algorithmbyminimizing\nâˆ’ f() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "âˆ’ f() x.\nThefunctionwewanttominimizeormaximizeiscalledthe o b j e c t i v e f unc -\nt i o nor c r i t e r i o n.Whenweareminimizingit,Â wemayalsocallitthe c o st\nf unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n.Â Inthisbook,weusetheseterms\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\ntosomeoftheseterms.\nWeoftendenotethevaluethatminimizesormaximizesafunctionwitha\nsuperscript.Forexample,wemightsay âˆ— xâˆ—= argmin() f x.\n8 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nâˆ’ âˆ’ âˆ’ âˆ’ 20. 15. 10. 05 00 05 10 15 20 ......\nxâˆ’20.âˆ’15.âˆ’10.âˆ’05.00.05.10.15.20.\nGlobalminimumat= 0.x\nSincefî€°() = 0,gradient x\ndescent haltshere.\nFor 0,wehave x< fî€°() 0,x<\nsowecandecreasebyf\nmoving rightward.For 0,wehave x> fî€°() 0,x>\nsowecandecreasebyf\nmoving leftward.\nf x() =1\n2x2\nfî€°() = x x\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\nreviewofhowcalculusconceptsrelatetooptimization here.\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\nThe der i v at i v eofthisfunctionisdenotedas fî€°( x)orasd y\nd x.Thederivative fî€°( x)\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeciï¬eshowtoscale\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\noutput: f x î€ f x î€ f (+) â‰ˆ()+î€°() x.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "output: f x î€ f x î€ f (+) â‰ˆ()+î€°() x.\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\nweknowthat f( x î€âˆ’sign( fî€°( x)))islessthan f( x)forsmallenough î€.Wecan\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeï¬gureforan4.1\nexampleofthistechnique.\nWhen fî€°( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "tomove.Pointswhere fî€°( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\npoints,soitisnolongerpossibletodecrease f( x)bymakinginï¬nitesimalsteps.\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\n8 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nMinimum Maximum Saddlepoint\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\nsoitisnotpossibletoincrease f( x)bymakinginï¬nitesimalsteps.Somecritical",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "pointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\nï¬gureforexamplesofeachtypeofcriticalpoint. 4.2\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "veryï¬‚atregions.Allofthismakesoptimization verydiï¬ƒcult,especiallywhenthe\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforï¬ndinga\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\nï¬gureforanexample.4.3\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rnâ†’ R.Forthe\nconceptofâ€œminimizationâ€Â to makesense,theremuststillbeonlyone(scalar)\noutput.\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\nder i v at i v e s.Thepartialderivativeâˆ‚",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "der i v at i v e s.Thepartialderivativeâˆ‚\nâˆ‚ x if( x)measureshow fchangesasonlythe\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\nvectorcontainingallofthepartialderivatives,denoted âˆ‡ x f( x).Element iofthe\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\n8 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()\nIdeally,wewouldlike\ntoarriveattheglobal\nminimum, butthis\nmight notbepossible.Thislocalminimum\nperformsnearlyaswellas\ntheglobalone,\nsoitisanacceptable\nhaltingpoint.\nThislocalminimumperforms\npoorlyandshouldbeavoided.\nFigure4.3:Optimizationalgorithmsmayfailtoï¬ndaglobalminimumwhenthereare\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "tosigniï¬cantlylowvaluesofthecostfunction.\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\nofthefunction f( x+ Î± u)withrespectto Î±,evaluatedat Î±= 0.Usingthechain\nrule,wecanseethatâˆ‚\nâˆ‚ Î±f Î± (+ x u)evaluatesto uî€¾âˆ‡ x f Î± () xwhen = 0.\nTominimize f,wewouldliketoï¬ndthedirectioninwhich fdecreasesthe",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "fastest.Wecandothisusingthedirectionalderivative:\nmin\nu u ,î€¾ u = 1uî€¾âˆ‡ x f() x (4.3)\n=min\nu u ,î€¾ u = 1|||| u 2||âˆ‡ x f() x|| 2cos Î¸ (4.4)\nwhere Î¸istheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\nignoringfactorsthatdonotdependon u,thissimpliï¬estomin ucos Î¸.Thisis\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "Thisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\nSteepestdescentproposesanewpoint\nxî€°= xâˆ’âˆ‡ î€ x f() x (4.5)\n8 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nwhere î€isthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\nWecanchoose î€inseveraldiï¬€erentways.Apopularapproachistoset î€toasmall\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\nderivativevanish.Anotherapproachistoevaluate f î€ ( xâˆ’âˆ‡ x f()) xforseveral\nvaluesof î€andchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\nThislaststrategyiscalleda l i ne se ar c h.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "Thislaststrategyiscalleda l i ne se ar c h.\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\nequation âˆ‡ x f() = 0 xfor. x\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\nsmallmove)towardsbetterconï¬gurations canbegeneralizedtodiscretespaces.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\n( ,). RusselandNorvig2003\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\nSometimesweneedtoï¬ndallofthepartialderivativesofafunctionwhoseinput\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\nknownasa J ac o bi an m at r i x.Speciï¬cally,ifwehaveafunction f: Rmâ†’ Rn,\nthentheJacobianmatrix Jâˆˆ Rn m Ã—ofisdeï¬nedsuchthat f J i , j=âˆ‚\nâˆ‚ x jf() x i.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "âˆ‚ x jf() x i.\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rnâ†’ R,thederivative\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedasâˆ‚2\nâˆ‚ x i âˆ‚ x jf.\nInasingledimension,wecandenoted2\nd x2 fby fî€° î€°( x).Thesecondderivativetells\nushowtheï¬rstderivativewillchangeaswevarytheinput.Thisisimportant\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\nthenthereisnocurvature.Itisaperfectlyï¬‚atline,anditsvaluecanbepredicted\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 î€\nalongthenegativegradient,andthecostfunctionwilldecreaseby î€.Ifthesecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\nactuallydecreasebymorethan î€.Finally,ifthesecondderivativeispositive,the\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan î€.See\n8 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxf x()N e g a t i v e c u r v a t u r e\nxf x()N o c u r v a t u r e\nxf x()P o s i t i v e c u r v a t u r e\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "thanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\nfunctioninadvertently.\nï¬guretoseehowdiï¬€erentformsofcurvatureaï¬€ecttherelationshipbetween 4.4\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\nHessian m at r i x.TheHessianmatrix isdeï¬nedsuchthat H x()( f)\nH x()( f) i , j=âˆ‚2\nâˆ‚ x i âˆ‚ x jf .() x (4.6)\nEquivalently,theHessianistheJacobianofthegradient.\nAnywherethatthesecondpartialderivativesarecontinuous,thediï¬€erential\noperatorsarecommutative,i.e.theirordercanbeswapped:\nâˆ‚2\nâˆ‚ x i âˆ‚ x jf() = xâˆ‚2\nâˆ‚ x j âˆ‚ x if .() x (4.7)\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\nHessianalmosteverywhere.Â Because theHessianmatrixisrealandsymmetric,\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\n8 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\neigenvectors.Thesecondderivativeinaspeciï¬cdirectionrepresentedbyaunit\nvector disgivenby dî€¾H d.When disaneigenvectorof H,thesecondderivative\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "derivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\ntothefunction aroundthecurrentpoint f() x x( 0 ):\nf f () xâ‰ˆ( x( 0 ))+( x xâˆ’( 0 ))î€¾g+1\n2( x xâˆ’( 0 ))î€¾H x x (âˆ’( 0 )) .(4.8)\nwhere gisthegradientand HistheHessianat x( 0 ).Â Ifweusealearningrate\nof î€,thenthenewpoint xwillbegivenby x( 0 )âˆ’ î€ g.Substitutingthisintoour\napproximation,weobtain",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "approximation,weobtain\nf( x( 0 )âˆ’ â‰ˆ î€ g) f( x( 0 ))âˆ’ î€ gî€¾g+1\n2î€2gî€¾H g . (4.9)\nTherearethreeÂ termshere:theoriginalvalueÂ ofthefunction,Â the expected\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\ngradientdescentstepcanactuallymoveuphill.When gî€¾H giszeroornegative,\ntheTaylorseriesapproximationpredictsthatincreasing î€foreverwilldecrease f\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge î€,so",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "onemustresorttomoreheuristicchoicesof î€inthiscase.When gî€¾H gispositive,\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\nthefunctionthemostyields\nî€âˆ—=gî€¾g\ngî€¾H g. (4.10)\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\nmaximaleigenvalue Î» m a x,thenthisoptimalstepsizeisgivenby1\nÎ»max.Tothe\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\nrate.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "rate.\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\npoint, fî€°( x) = 0.Whenthesecondderivative fî€° î€°( x) >0,theï¬rstderivative fî€°( x)\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\n8 8",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nfî€°( x î€âˆ’) <0and fî€°( x+ î€) >0forsmallenough î€.Inotherwords,aswemove\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\nbeginstopointuphilltotheleft.Â Thus,when fî€°( x)=0and fî€° î€°( x) >0,wecan\nconcludethat xisalocalminimum.Similarly,when fî€°( x) = 0and fî€° î€°( x) <0,we\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\nt e st.Unfortunately,when fî€° î€°( x) = 0,thetestisinconclusive.Inthiscase xmay",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "beasaddlepoint,orapartofaï¬‚atregion.\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\nâˆ‡ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\nHessianispositivedeï¬nite(allitseigenvaluesarepositive),thepointisalocal",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\nderivativetest.Likewise,whentheHessianisnegativedeï¬nite(allitseigenvalues\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\npossibletoï¬ndpositiveevidenceofsaddlepointsinsomecases.Â Whenatleast\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "crosssection.Seeï¬gureforanexample.Finally,themultidimensionalsecond 4.5\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\nInmultipledimensions,thereisadiï¬€erentsecondderivativeforeachdirection\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "howmuchthesecondderivativesdiï¬€erfromeachother.WhentheHessianhasa\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\nremainsnegativeforlonger.Italsomakesitdiï¬ƒculttochooseagoodstepsize.\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "uphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\nstepsizeistoosmalltomakesigniï¬cantprogressinotherdirectionswithless\ncurvature.Seeï¬gureforanexample.4.6\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\n8 9",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nî¸î€±\u0000î€± î€µî€°î€± î€µî¸ î€²\u0000î€± î€µî€°î€± î€µî¦î¸î€¨î€±î€» î¸î€²î€©\n\u0000î€µ î€° î€°î€°î€µ î€° î€°\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\ninthisexampleis f( x)= x2\n1âˆ’ x2\n2.Alongtheaxiscorrespondingto x 1,thefunction\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\neigenvectoroftheHessianwithnegativeeigenvalue.Thenameâ€œsaddlepointâ€derivesfrom",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\n9 0",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nâˆ’ âˆ’ âˆ’ 3 0 2 0 1 0 0 1 0 2 0\nx 1âˆ’ 3 0âˆ’ 2 0âˆ’ 1 001 02 0x 2\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\nhasï¬vetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\ncurvatureisinthedirection[1 ,1]î€¾andtheleastcurvatureisinthedirection[1 ,âˆ’1]î€¾.The",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "redlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "thisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\ndirectioninthiscontext.\n9 1",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o nâ€™ s m e t ho d.\nNewtonâ€™smethodisbasedonusingasecond-orderTaylorseriesexpansionto\napproximatenearsomepoint f() x x( 0 ):\nf f () xâ‰ˆ( x( 0 ))+( x xâˆ’( 0 ))î€¾âˆ‡ x f( x( 0 ))+1\n2( x xâˆ’( 0 ))î€¾H x()( f( 0 ))( x xâˆ’( 0 )) .(4.11)\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\nxâˆ—= x( 0 )âˆ’ H x()( f( 0 ))âˆ’ 1âˆ‡ x f( x( 0 )) . (4.12)\nWhen fisapositivedeï¬nitequadraticfunction,Newtonâ€™smethodconsistsof",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "applyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\ndeï¬nitequadratic,Newtonâ€™smethodconsistsofapplyingequationmultiple4.12\ntimes.Â Iterativelyupdatingtheapproximation andjumpingtotheminimumof\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\npropertynearasaddlepoint.Asdiscussedinsection,Newtonâ€™smethodis 8.2.3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\npointsunlessthegradientpointstowardthem.\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\narecalled ï¬r st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\nalsousetheHessianmatrix,suchasNewtonâ€™smethod,arecalled se c o nd-or d e r\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "TheÂ optimization algorithmsÂ employedinÂ mostcontextsinÂ thisÂ bookÂ are\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\nusedindeeplearningisquitecomplicated.Inmanyotherï¬elds,thedominant\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\noffunctions.\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "ingourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\nofchangeisboundedbya L i psc hi t z c o nst antL:\nâˆ€âˆ€| âˆ’ |â‰¤L||âˆ’|| x , y , f() x f() y x y 2 . (4.13)\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\n9 2",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\nwithrelativelyminormodiï¬cations.\nPerhapsthemostsuccessfulï¬eldofspecializedoptimization is c o n v e x o p-\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\napplicableonlytoconvexfunctionsâ€”functionsforwhichtheHessianispositive",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "semideï¬niteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\nproblemsindeeplearningarediï¬ƒculttoexpressintermsofconvexoptimization.\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "moreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\norRockafellar1997().\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\npossibleÂ valuesÂ of x.InsteadwemayÂ wishtoÂ ï¬ndÂ themaximalÂ orÂ minimal\nvalueÂ of f( x)forÂ valuesof xinsomeÂ set S.ThisisÂ knownÂ as c o nst r ai n e d\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\nconstrainedoptimization terminology.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "constrainedoptimization terminology.\nWeoftenwishtoï¬ndasolutionthatissmallinsomesense.Acommon\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||â‰¤ x 1\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize î€,\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\nalinesearch,wecansearchonlyoverstepsizes î€thatyieldnew xpointsthatare",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\nWhenpossible,thismethodcanbemademoreeï¬ƒcientbyprojectingthegradient\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\nthelinesearch(,).Rosen1960\nAmoresophisticatedapproachistodesignadiï¬€erent,unconstrainedopti-\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\n9 3",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nxâˆˆ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\ng( Î¸) = f([cossin Î¸ , Î¸]î€¾)withrespectto Î¸,thenreturn[cossin Î¸ , Î¸]asthesolution\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\nbetweenoptimization problemsmustbedesignedspeciï¬callyforeachcasewe\nencounter.\nThe K ar ushâ€“ K u h n â€“ T uc k e r(KKT)approach1providesaverygeneralso-\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "newfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\nf unc t i o n.\nTodeï¬netheLagrangian,weï¬rstneedtodescribe Sintermsofequations\nandinequalities.Â W ewantadescriptionof Sintermsof mfunctions g( ) iand n\nfunctions h( ) jsothat S={|âˆ€ x i , g( ) i( x) = 0andâˆ€ j , h( ) j( x)â‰¤0}.Theequations\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Weintroducenewvariables Î» iand Î± jforeachconstraint,thesearecalledthe\nKKTmultipliers.ThegeneralizedLagrangianisthendeï¬nedas\nL , , f ( x Î» Î±) = ()+ xî˜\niÎ» i g( ) i()+ xî˜\njÎ± j h( ) j() x .(4.14)\nWecannowsolveaconstrainedminimization problemusingunconstrained\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x âˆž\nmin\nxmax\nÎ»max\nÎ± Î± , â‰¥ 0L , , . ( x Î» Î±) (4.15)\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "min\nx âˆˆ Sf .() x (4.16)\nThisfollowsbecauseanytimetheconstraintsaresatisï¬ed,\nmax\nÎ»max\nÎ± Î± , â‰¥ 0L , , f , ( x Î» Î±) = () x (4.17)\nwhileanytimeaconstraintisviolated,\nmax\nÎ»max\nÎ± Î± , â‰¥ 0L , , . ( x Î» Î±) = âˆž (4.18)\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\n9 4",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\noptimumwithinthefeasiblepointsisunchanged.\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\ngrangefunctionof,whichleadstothisoptimization problem: âˆ’ f() x\nmin\nxmax\nÎ»max\nÎ± Î± , â‰¥ 0âˆ’ f()+ xî˜\niÎ» i g( ) i()+ xî˜\njÎ± j h( ) j() x .(4.19)\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\nmax\nxmin\nÎ»min\nÎ± Î± , â‰¥ 0f()+ xî˜\niÎ» i g( ) i() xâˆ’î˜\njÎ± j h( ) j() x .(4.20)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "iÎ» i g( ) i() xâˆ’î˜\njÎ± j h( ) j() x .(4.20)\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeï¬neit\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\nanysignforeach Î» i.\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\nh( ) i( x)is ac t i v eif h( ) i( xâˆ—) = 0.Ifaconstraintisnotactive,thenthesolutionto\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "othersolutions.Forexample,aconvexproblemwithanentireregionofglobally\noptimalpoints(awide,ï¬‚at,regionofequalcost)couldhaveasubsetofthis\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\nthesolutiontomin xmax Î»max Î± Î± , â‰¥ 0 L( x Î» Î± , ,)willhave Î± i=0.Wecanthus",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "observethatatthesolution, Î± hî€Œ( x)= 0.Inotherwords,forall i,weknow\nthatatleastoneoftheconstraints Î± iâ‰¥0and h( ) i( x)â‰¤0mustbeactiveatthe\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\ntoinï¬‚uencethesolutionto x,ortheinequalityhasnoinï¬‚uenceonthesolution\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\nbutnotalwayssuï¬ƒcientconditions,forapointtobeoptimal.Theconditionsare:\nâ€¢ThegradientofthegeneralizedLagrangianiszero.\nâ€¢AllconstraintsonbothandtheKKTmultipliersaresatisï¬ed. x\n9 5",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nâ€¢Theinequalityconstraintsexhibitâ€œcomplementary slacknessâ€: Î± hî€Œ( x) = 0.\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\n4. 5 E x am p l e: L i n ear L eas t S q u are s\nSupposewewanttoï¬ndthevalueofthatminimizes x\nf() = x1\n2||âˆ’|| A x b2\n2 . (4.21)\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeï¬ƒciently.\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\nasimpleexampleofhowthesetechniqueswork.",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "asimpleexampleofhowthesetechniqueswork.\nFirst,weneedtoobtainthegradient:\nâˆ‡ x f() = x Aî€¾( ) = A x bâˆ’ Aî€¾A x Aâˆ’î€¾b . (4.22)\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\nfordetails.\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\n2||âˆ’|| A x b2\n2withrespectto x\nusinggradientdescent,startingfromanarbitraryvalueof. x\nSetthestepsize()andtolerance()tosmall,positivenumbers. î€ Î´\nwhi l e|| Aî€¾A x Aâˆ’î€¾b|| 2 > Î´ do\nx xâ† âˆ’ î€î€€\nAî€¾A x Aâˆ’î€¾bî€\ne nd whi l e",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "x xâ† âˆ’ î€î€€\nAî€¾A x Aâˆ’î€¾bî€\ne nd whi l e\nOnecanalsosolvethisproblemusingNewtonâ€™smethod.Inthiscase,because\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewtonâ€™s\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\nstep.\nNowsupposeÂ weÂ wishtoÂ minimizethesameÂ function,butsubjecttoÂ the\nconstraint xî€¾xâ‰¤1.Todoso,weintroducetheLagrangian\nL , Î» f Î» ( x) = ()+ xî€\nxî€¾xâˆ’1î€‘\n. (4.23)\nWecannowsolvetheproblem\nmin\nxmax\nÎ» , Î» â‰¥ 0L , Î» . ( x) (4.24)\n9 6",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER4.NUMERICALCOMPUTATION\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustï¬nda\nsolutionwheretheconstraintisactive.Bydiï¬€erentiating theLagrangianwith\nrespectto,weobtaintheequation x\nAî€¾A x Aâˆ’î€¾b x+2 Î»= 0 . (4.25)\nThistellsusthatthesolutionwilltaketheform\nx A= (î€¾A I+2 Î»)âˆ’ 1Aî€¾b . (4.26)",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "x A= (î€¾A I+2 Î»)âˆ’ 1Aî€¾b . (4.26)\nThemagnitudeof Î»mustbechosensuchthattheresultobeystheconstraint.We\ncanï¬ndthisvaluebyperforminggradientascenton.Todoso,observe Î»\nâˆ‚\nâˆ‚ Î»L , Î»( x) = xî€¾xâˆ’1 . (4.27)\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\nuphillandincreasetheLagrangianwithrespectto Î»,weincrease Î».Becausethe\ncoeï¬ƒcientonthe xî€¾xpenaltyhasincreased,solvingthelinearequationfor xwill\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "andadjusting Î»continuesuntil xhasthecorrectnormandthederivativeon Î»is\n0.\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ï¬‚edged\nlearningsystems.\n9 7",
    "metadata": {
      "source": "[8]part-1-chapter-4.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I\nD e e p N e t w orks: Mo d e rn\nPractices\n166",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\nusedtosolvepracticalapplications.\nDeeplearninghasalonghistoryandmanyaspirations.Severalapproaches\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\ntheï¬nalpartofthebook.\nThispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\nnologiesthatarealreadyusedheavilyinindustry.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "nologiesthatarealreadyusedheavilyinindustry.\nModernÂ deeplearningÂ providesÂ averyÂ powerfulÂ frameworkÂ forsupervised\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\nbeaccomplishedviadeeplearning,givensuï¬ƒcientlylargemodelsandsuï¬ƒciently\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "asassociatingonevectortoanother,orthatarediï¬ƒcultenoughthataperson\nwouldrequiretimetothinkandreï¬‚ectinordertoaccomplishthetask,remain\nbeyondthescopeofdeeplearningfornow.\nThispartofthebookdescribesthecoreparametricfunctionapproximation\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\nWeÂ beginÂ byÂ describingtheÂ feedforwardÂ deepnetworkmodelthatisusedto\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "andoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\nforthepracticalmethodologyinvolvedindesigning,building,andconï¬guringan\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\nlearning.",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "learning.\nThesechaptersarethemostimportantforapractitionerâ€”someone whowants\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\nproblemstoday.\n1 6 7",
    "metadata": {
      "source": "[10]part-2-deep-network-modern-practices.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1 1\nPractical Methodology\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "whethertogathermoredata,increaseordecreasemodelcapacity,addorremove\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.\nMostofthisbookisaboutdiï¬€erentmachinelearningmodels,trainingalgo-\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "importantingredienttobeingamachinelearningexpertisknowingawidevariety\nofmachinelearningtechniquesandbeinggoodatdiï¬€erentkindsofmath.Inprac-\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\nrecommendations inthischapterareadaptedfrom().Ng2015\nWerecommendthefollowingpracticaldesignprocess:",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "Werecommendthefollowingpracticaldesignprocess:\nâ€¢Determineyourgoalsâ€”whaterrormetrictouse,andyourtargetvaluefor\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\nproblemthattheapplicationisintendedtosolve.\nâ€¢Establishaworkingend-to-endpipelineassoonaspossible,includingthe\n421",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nestimationoftheappropriateperformancemetrics.\nâ€¢Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\nisduetooverï¬tting,underï¬tting, oradefectinthedataorsoftware.\nâ€¢Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\nhyperparameters,orchangingalgorithms,basedonspeciï¬cï¬ndingsfrom\nyourinstrumentation.\nAsarunningexample,wewilluseStreetViewaddressnumbertranscription",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "system( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\nmethodologyweadvocate.\nWenowdescribeeachofthestepsinthisprocess.\n11.1PerformanceMetrics",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "11.1PerformanceMetrics\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryï¬rst\nstepbecauseyourerrormetricwillguideallofyourfutureactions.Â Youshould\nalsohaveanideaofwhatlevelofperformanceyoudesire.\nKeepinmindthatformostapplications,itisimpossibletoachieveabsolute\nzeroerror.TheBayeserrordeï¬nestheminimumerrorratethatyoucanhopeto\nachieve,evenifyouhaveinï¬nitetrainingdataandcanrecoverthetrueprobability\ndistribution.ThisÂ isbecauseÂ yourÂ inputfeaturesÂ maynotÂ containÂ complete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\nstochastic.Youwillalsobelimitedbyhavingaï¬niteamountoftrainingdata.\nTheamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\nmoney,orhumansuï¬€ering(forexample,ifyourdatacollectionprocessinvolves",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "performinginvasivemedicaltests).Whenyourgoalistoanswerascientiï¬cquestion\naboutwhichalgorithmperformsbetteronaï¬xedbenchmark,thebenchmark\n4 2 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nspeciï¬cationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\nmoredata.\nHowcanonedetermineareasonablelevelofperformancetoexpect?Typically,\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\ncost-eï¬€ective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\nAnotherimportantconsiderationbesidesthetargetvalueoftheperformance\nmetricisthechoiceofwhichmetrictouse.Severaldiï¬€erentperformancemetrics\nmaybeusedtomeasuretheeï¬€ectivenessofacompleteapplicationthatincludes\nmachinelearningcomponents.Theseperformancemetricsareusuallydiï¬€erent\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "However,manyapplicationsrequiremoreadvancedmetrics.\nSometimesitismuchmorecostlytomakeonekindofamistakethananother.\nForexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\nmeasuringtheerrorrateofaspamclassiï¬er,wemaywishtomeasuresomeform",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\nofallowingspammessages.\nSometimeswewishtotrainabinaryclassiï¬erthatisintendedtodetectsome\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiï¬er\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "toinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "withprecisiononthe y-axisandrecallonthe x-axis.Theclassiï¬ergeneratesascore\nthatishigheriftheeventtobedetectedoccurred.Â Forexample,afeedforward\n4 2 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnetworkdesignedtodetectadiseaseoutputs Ë† y= P( y=1| x),estimatingthe\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\nthreshold.Â Byvaryingthethreshold,wecantradeprecisionforrecall.Â Inmany\ncases,wewishtosummarizetheperformanceoftheclassiï¬erwithasinglenumber\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\nF-scor egivenby\nF=2 pr\np r+. (11.1)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "F-scor egivenby\nF=2 pr\np r+. (11.1)\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\nInsomeapplications,itispossibleforthemachinelearningsystemtorefuseto\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\nhowconï¬dentitshouldbeaboutadecision,especiallyifawrongdecisioncan\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "transcribetheaddressnumberfromaphotographinordertoassociatethelocation\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.\nOfcourse,themachinelearningsystemisonlyusefulifitisabletodramatically",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "reducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.\nItispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\nis98%accuracy.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "is98%accuracy.\nManyothermetricsarepossible.Wecanforexample,measureclick-through\nrates,collectusersatisfactionsurveys,andsoon.Â Manyspecializedapplication\nareashaveapplication-speciï¬ccriteriaaswell.\nWhatisimportantistodeterminewhichperformancemetrictoimproveahead\noftime,thenconcentrateonimprovingthismetric.Withoutclearlydeï¬nedgoals,\nitcanbediï¬ƒculttotellwhetherchangestoamachinelearningsystemmake\nprogressornot.\n4 2 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n11.2DefaultBaselineModels\nAfterchoosingperformancemetricsandgoals,Â thenextstepinanypractical\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\nthissection,weproviderecommendations forwhichalgorithmstouseastheï¬rst\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\nafterthiswriting.\nDependingonthecomplexityofyourproblem,youmayevenwanttobegin",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\nstatisticalmodellikelogisticregression.\nIfyouknowthatyourproblemfallsintoanâ€œAI-completeâ€categorylikeobject\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\ntodowellbybeginningwithanappropriatedeeplearningmodel.\nFirst,choosethegeneralcategoryofmodelbasedonthestructureofyour",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "data.Ifyouwanttoperformsupervisedlearningwithï¬xed-sizevectorsasinput,\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\nAreasonablechoiceofoptimization algorithmisSGDwithmomentumwitha",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "decayinglearningrate(populardecayschemesthatperformbetterorworseon\ndiï¬€erentproblemsincludedecayinglinearlyuntilreachingaï¬xedminimumlearning\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.\nBatchnormalization canhaveadramaticeï¬€ectonoptimization performance,\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\nWhileitisreasonabletoomitbatchnormalization fromtheveryï¬rstbaseline,it",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "shouldbeintroducedquicklyifoptimization appearstobeproblematic.\nUnlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\neachvariable.\n4 2 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\nwillprobablydowellbyï¬rstcopyingthemodelandalgorithmthatisalready\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\ntasks( ,). Girshicketal.2015\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "scribedfurtherinpart.Thisissomewhatdomainspeciï¬c.Somedomains,such III\nasnaturallanguageprocessing,areknowntobeneï¬ttremendouslyfromunsuper-\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\nnotbringabeneï¬t,exceptinthesemi-supervisedsetting,whenthenumberof\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "thenincludeitinyourï¬rstend-to-endbaseline.Otherwise,onlyuseunsupervised\nlearninginyourï¬rstattemptifthetaskyouwanttosolveisunsupervised.You\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\nbaselineoverï¬ts.\n11.3DeterminingWhethertoGatherMoreData\nAftertheï¬rstend-to-endsystemisestablished,itistimetomeasuretheperfor-\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\nnovicesaretemptedtomakeimprovementsbytryingoutmanydiï¬€erentalgorithms.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "However,itisoftenmuchbettertogathermoredatathantoimprovethelearning\nalgorithm.\nHowdoesonedecidewhethertogathermoredata?First,determinewhether\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\nAlso,tryimprovingthelearningalgorithm,forexamplebytuningthelearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "ratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\nrichersetoffeatures.\nIftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\n4 2 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\ntrainingsetperformance,thengatheringmoredataisoneofthemosteï¬€ective\nsolutions.Â Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "signiï¬cantly.Â Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "regularization, byadjustinghyperparameters suchasweightdecaycoeï¬ƒcients,\norbyaddingregularizationstrategiessuchasdropout.Ifyouï¬ndthatthegap\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\nregularizationhyperparameters ,thengatheringmoredataisadvisable.\nWhendecidingwhethertogathermoredata,itisalsonecessarytodecide\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\ntrainingsetsizeandgeneralization error,likeinï¬gure.Byextrapolatingsuch 5.4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.\nIfgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "domainofresearchandnotthedomainofadviceforappliedpractitioners.\n11.4SelectingHyperparameters\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\naspectsofthealgorithmâ€™sbehavior.Someofthesehyperparametersaï¬€ectthetime\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters aï¬€ect\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\ncorrectresultswhendeployedonnewinputs.\nTherearetwobasicapproachestochoosingthesehyperparameters :choosing",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "themmanuallyandchoosingthemautomatically .Choosingthehyperparameters\n4 2 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\nmuchmorecomputationally costly.\n1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\nhyperparameters,trainingerror,generalization errorandcomputational resources",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\ndamentalideasconcerningtheeï¬€ectivecapacityofalearningalgorithmfrom\nchapter.5\nThegoalofmanualhyperparametersearchisusuallytoï¬ndthelowestgeneral-\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\nbecausethisishighlyplatform-dependent.\nTheprimarygoalofmanualhyperparametersearchistoadjusttheeï¬€ective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "capacityofthemodeltomatchthecomplexityofthetask.Eï¬€ectivecapacity\nisconstrainedbythreefactors:Â therepresentationalcapacityofthemodel,the\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\nhigherrepresentationalcapacityâ€”itiscapableofrepresentingmorecomplicated\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "thetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\nsomeofthesefunctions.\nThegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\nafunctionofoneofthehyperparameters ,asinï¬gure.Â Atoneextreme,the 5.3\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\nbecausetrainingerrorishigh.Thisistheunderï¬ttingregime.Attheotherextreme,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "thehyperparameter valuecorrespondstohighcapacity,andthegeneralization\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\noftrainingerror.\nForsomehyperparameters,overï¬ttingoccurswhenthevalueofthehyper-\nparameterislarge.Â Thenumberofhiddenunitsinalayerisonesuchexample,\n4 2 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\nForsomehyperparameters ,overï¬ttingoccurswhenthevalueofthehyperparame-\nterissmall.Forexample,thesmallestallowableweightdecaycoeï¬ƒcientofzero\ncorrespondstothegreatesteï¬€ectivecapacityofthelearningalgorithm.\nNoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\nManyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\nareswitchesthatÂ specifyÂ whetherornottoÂ usesomeoptionalcomponentof\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "partofthecurve.Forexample,theminimumweightdecaycoeï¬ƒcientiszero.This\nmeansthatifthemodelisunderï¬ttingwhenweightdecayiszero,wecannotenter\ntheoverï¬ttingregionbymodifyingtheweightdecaycoeï¬ƒcient.Inotherwords,\nsomehyperparameters canonlysubtractcapacity.\nThelearningrateisperhapsthemostimportanthyperparameter. Ifyou\nhaveÂ timetoÂ tuneonlyÂ onehyperparameter, tuneÂ thelearningÂ rate. ItÂ con-\ntrolstheeï¬€ectivecapacityofthemodelinamorecomplicatedwaythanother",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "hyperparametersâ€”theeï¬€ectivecapacityofthemodelishighestwhenthelearning\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\nillustratedinï¬gure.Whenthelearningrateistoolarge,gradientdescent 11.1\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.\nThiseï¬€ectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\nTuningtheparametersotherthanthelearningraterequiresmonitoringboth\ntrainingandtesterrortodiagnosewhetheryourmodelisoverï¬ttingorunderï¬tting,\nthenadjustingitscapacityappropriately .\nIfyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\nconï¬dentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\nincreasesthecomputational costsassociatedwiththemodel.\nIfyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\n4 2 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\n1 0âˆ’ 21 0âˆ’ 11 00\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaï¬xed\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\nfactorproportionaltothelearningratereduction.Â Generalizationerrorcanfollowthis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "curveorbecomplicatedbyregularizationeï¬€ectsarisingoutofhavingatoolargeor\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\noverï¬tting,andevenpointswithequivalenttrainingerrorcanhavediï¬€erentgeneralization\nerror.\nnowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\noï¬€thesequantities.Neuralnetworkstypicallyperformbestwhenthetraining",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\ndrivenbythegapbetweentrainandtesterror.Â Yourgoalistoreducethisgap\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\nchangeregularizationhyperparameters toreduceeï¬€ectivemodelcapacity,suchas\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\nlargemodelthatisregularizedwell,forexamplebyusingdropout.\nMosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "decreasemodelcapacity.SomeexamplesareincludedinTable.11.1\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\n4 3 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nHyperparameterIncreases\ncapacity\nwhen...Reason Caveats\nNumberofhid-\ndenunitsincreasedIncreasingthenumberof\nhiddenunitsincreasesthe\nrepresentationalcapacity\nofthemodel.Increasingthenumber\nofhiddenunitsÂ increases\nboththetimeandmemory\ncostofessentiallyeveryop-\nerationonthemodel.\nLearningratetunedop-\ntimallyAnimproperlearningrate,\nwhetherÂ toohighÂ ortoo\nlow,resultsinamodel\nwithloweï¬€ectivecapacity\nduetooptimizationfailure\nConvolutionker-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "duetooptimizationfailure\nConvolutionker-\nnelwidthincreasedIncreasingthekernelwidth\nincreasesthenumberofpa-\nrametersinthemodelAwiderkernelresultsin\nanarroweroutputdimen-\nsion,reducingmodelca-\npacityunlessyouuseim-\nplicitzeropaddingtore-\nducethiseï¬€ect.Wider\nkernelsrequiremoremem-\noryforparameterstorage\nandincreaseruntime,but\nanarroweroutputreduces\nmemorycost.\nImplicitzero\npaddingincreasedAddingimplicitzerosbe-\nforeconvolutionkeepsthe\nrepresentationsizelargeIncreasedtimeandmem-\norycostofmostopera-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "orycostofmostopera-\ntions.\nWeightdecayco-\neï¬ƒcientdecreasedDecreasingtheweightde-\ncaycoeï¬ƒcientfreesthe\nmodelparameterstobe-\ncomelarger\nDropoutratedecreasedDroppingunitslessoften\ngivestheunitsmoreoppor-\ntunitiestoâ€œconspireâ€with\neachothertoï¬tthetrain-\ningset\nTable11.1:Theeï¬€ectofvarioushyperparametersonmodelcapacity.\n4 3 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nprinciple,thisapproachcouldfailduetooptimization diï¬ƒculties,butformany\nproblemsoptimization doesnotseemtobeasigniï¬cantbarrier,providedthatthe\nmodelischosenappropriately .\n1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\noftenbeneï¬tsigniï¬cantlyfromtuningoffortyormorehyperparameters .Manual\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "hyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\nautomatedalgorithmscanï¬ndusefulvaluesofthehyperparameters .\nIfwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\nwearetryingtoï¬ndavalueofthehyperparametersthatoptimizesanobjective\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "fortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\ntoÂ develop h y p e r par am e t e r Â  o p t i m i z a t i o nalgorithmsÂ thatwrapÂ aÂ learnin g\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\nshouldbeexploredforeachofthelearningalgorithmâ€™shyperparameters .However,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\nsecondaryhyperparameters foralltasks.\n1 1 . 4 . 3 G ri d S ea rch\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\ng r i d se ar c h.Foreachhyperparameter,Â the userselectsasmallï¬nitesetof\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\nspeciï¬cationofhyperparametervaluesintheCartesianproductofthesetofvalues",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\n4 3 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nGrid Random\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "hyperparameterconï¬gurations.Usuallymostofthesehyperparametersareindependent\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\nhyperparameterconï¬gurationsandrunstrainingwitheachofthem.Bothgridsearch\nandrandomsearchevaluatethevalidationseterrorandreturnthebestconï¬guration.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "Theï¬gureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniï¬cant\ninï¬‚uenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\nhasasigniï¬canteï¬€ect.Gridsearchwastesanamountofcomputationthatisexponential\ninthenumberofnon-inï¬‚uentialhyperparameters,whilerandomsearchtestsaunique\nvalueofeveryinï¬‚uentialhyperparameteronnearlyeverytrial.Figurereproducedwith\npermissionfrom (). BergstraandBengio2012\n4 3 3",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\nï¬gureforanillustrationofagridofhyperparameter values. 11.2\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "searchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\nratetakenwithintheset{ .1 , .01 ,10âˆ’3,10âˆ’4,10âˆ’5},oranumberofhiddenunits\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\nsupposethatweranagridsearchoverahyperparameter Î±usingvaluesof{âˆ’1 ,0 ,1}.\nIfthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 Î±\nliesandweshouldshiftthegridandrunanothersearchwith Î±in,forexample,",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "{1 ,2 ,3}.Ifweï¬ndthatthebestvalueof Î±is,thenwemaywishtoreï¬neour 0\nestimatebyzoominginandrunningagridsearchover. {âˆ’ } . , , .101\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\nparallelism(withalmostnoneedforcommunication betweendiï¬€erentmachines",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "carryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\nevenparallelization maynotprovideasatisfactorysizeofsearch.\n1 1 . 4 . 4 Ra n d o m S ea rch\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\nrandomsearch( ,). BergstraandBengio2012\nArandomsearchproceedsasfollows.Firstwedeï¬neamarginaldistribution\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\nhyperparameters.Forexample,\nl o g l e a r n i n g r a t e __ âˆ¼âˆ’âˆ’ u(1 ,5) (11.2)\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b).\nSimilarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\nlog(2000) ).\n4 3 4",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\nnotincuradditionalcomputational cost.Â Infact,asillustratedinï¬gure,a11.2\nrandomsearchcanbeexponentiallymoreeï¬ƒcientthanagridsearch,whenthere\nareseveralhyperparametersthatdonotstronglyaï¬€ecttheperformancemeasure.\nThisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "searchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\nthenumberoftrialsrunbyeachmethod.\nAswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\nsearch,toreï¬nethesearchbasedontheresultsoftheï¬rstrun.\nThemainreasonwhyrandomsearchï¬ndsgoodsolutionsfasterthangridsearch\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\nwouldusuallyhavediï¬€erentvalues.Henceifthechangebetweenthesetwovalues\ndoesnotmarginallymakemuchdiï¬€erenceintermsofvalidationseterror,grid\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters .\n1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\nThesearchforgoodhyperparameters canbecastasanoptimization problem.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\nsimpliï¬edsettingswhereitisfeasibletocomputethegradientofsomediï¬€erentiable\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal.\n2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "intrinsicallynon-diï¬€erentiable interactionswiththevalidationseterror,asinthe\ncaseofdiscrete-valuedhyperparameters .\nTocompensateforthislackofagradient,wecanbuildamodelofthevalidation\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "mizationthusinvolvesatradeoï¬€betweenexploration(proposinghyperparameters\n4 3 5",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\nisconï¬dentwillperformaswellasanyhyperparameters ithasseensofarâ€”usually\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\nTPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "Currently,wecannotunambiguously recommendBayesianhyperparameter\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\nforobtainingthoseresultswithlesseï¬€ort.Bayesianhyperparameteroptimization\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\naparticularproblembutisnotyetsuï¬ƒcientlymatureorreliable.Thatbeing\nsaid,hyperparameter optimization isanimportantï¬eldofresearchthat,while",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "oftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneï¬t\nnotonlytheentireï¬eldofmachinelearningbutthedisciplineofengineeringin\ngeneral.\nOnedrawbackcommontomosthyperparameter optimization algorithmswith\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\nfromtheexperiment.Thisismuchlesseï¬ƒcient,inthesenseofhowmuchinfor-\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\nexperiment,toâ€œfreezeâ€arunningexperimentthatisnotpromising,ortoâ€œthawâ€\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\nmoreinformation.\n11.5DebuggingStrategies",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "moreinformation.\n11.5DebuggingStrategies\nWhenamachinelearningsystemperformspoorly,itisusuallydiï¬ƒculttotell\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\nisabugintheimplementation ofthealgorithm.Â Machine learningsystemsare\ndiï¬ƒculttodebugforavarietyofreasons.\nInmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\n4 3 6",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nneuralnetworkonaclassiï¬cationtaskanditachieves5%testerror,wehave new\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\nbehavior.\nAfurtherdiï¬ƒcultyisthatmostmachinelearningmodelshavemultipleparts\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "furtherthatwehavemanuallyimplemented thegradientdescentruleforeach\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\nb bâ†âˆ’ Î± (11.4)\nwhere Î±isthelearningrate.Thiserroneousupdatedoesnotusethegradientat\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\nDependingonthedistributionoftheinput,theweightsmaybeabletoadaptto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "compensateforthenegativebiases.\nMostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\nbothofthesetwodiï¬ƒculties.Eitherwedesignacasethatissosimplethatthe\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\npartoftheneuralnetimplementationinisolation.\nSomeimportantdebuggingtestsinclude:\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "superimposedontheimage.Whentrainingagenerativemodelofspeech,listento\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "performingwellwhenitisnot.\nVisualizetheworstmistakes:Â Mostmodelsareabletooutputsomesortof\nconï¬dencemeasureforthetasktheyperform.Forexample,classiï¬ersbasedona\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\ntothemostlikelyclassthusgivesanestimateoftheconï¬dencethemodelhasin\nitsclassiï¬cationdecision.Typically,maximumlikelihoodtrainingresultsinthese\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\n4 3 7",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.\nForexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "someofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\nconï¬dentmistakesshowedthattherewasasystematicproblemwiththecropping.\nModifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\nnumbers.\nReasoningaboutsoftwareusingtrainandtesterror:Itisoftendiï¬ƒcultto",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "determinewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\nmodelisoverï¬ttingforfundamentalalgorithmicreasons.Analternativepossibility\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\nwasprepareddiï¬€erentlyfromthetrainingdata.Ifbothtrainandtesterrorare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "high,thenitisdiï¬ƒculttodeterminewhetherthereisasoftwaredefectorwhether\nthemodelisunderï¬ttingduetofundamentalalgorithmicreasons.Thisscenario\nrequiresfurthertests,describednext.\nFitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\nitisduetogenuineunderï¬ttingorduetoasoftwaredefect.Usuallyevensmall\nmodelscanbeguaranteedtobeableï¬tasuï¬ƒcientlysmalldataset.Forexample,\naclassiï¬cationdatasetwithonlyoneexamplecanbeï¬tjustbysettingthebiases",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "oftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiï¬ertocorrectly\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\nwithhighï¬delity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\nCompareback-propagatedderivativestonumericalderivatives:Ifyouareusing\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "putations,orifyouareaddinganewoperationtoadiï¬€erentiation libraryand\nmustdeï¬neitsbpropmethod,thenacommonsourceoferrorisimplementingthis\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\n4 3 8",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nistocomparethederivativescomputedbyyourimplementation ofautomatic\ndiï¬€erentiationtothederivativescomputedbya .Because ï¬ni t e di ï¬€ e r e nc e s\nfî€°() =lim x\nî€ â†’0f x î€ f x (+)âˆ’()\nî€, (11.5)\nwecanapproximate thederivativebyusingasmall,ï¬nite: î€\nfî€°() xâ‰ˆf x î€ f x (+)âˆ’()\nî€. (11.6)\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di ï¬€ e r -\ne nc e:\nfî€°() xâ‰ˆf x(+1\n2î€ f x )âˆ’(âˆ’1\n2 î€)\nî€. (11.7)",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "e nc e:\nfî€°() xâ‰ˆf x(+1\n2î€ f x )âˆ’(âˆ’1\n2 î€)\nî€. (11.7)\nTheperturbationsize î€mustchosentobelargeenoughtoensurethatthepertur-\nbationisnotroundeddowntoomuchbyï¬nite-precisionnumericalcomputations.\nUsually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\ng: Rmâ†’ Rn.Unfortunately,ï¬nitediï¬€erencingonlyallowsustotakeasingle\nderivativeatatime.Wecaneitherrunï¬nitediï¬€erencing m ntimestoevaluateall\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "randomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\nwhere uand varerandomlychosenvectors.Computing fî€°( x)correctlyrequires\nbeingabletoback-propagatethrough gcorrectly,yetiseï¬ƒcienttodowithï¬nite\ndiï¬€erencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\nprojection.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "projection.\nIfonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\naveryeï¬ƒcientwaytonumericallyestimatethegradientbyusingcomplexnumbers\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\nobservationthat\nf x i î€ f x i î€ f (+) = ()+î€°()+( x O î€2) (11.8)\nreal((+)) = ()+( f x i î€ f x O î€2)imag( ,f x i î€ (+)\nî€) = fî€°()+( x O î€2) ,(11.9)\nwhere i=âˆš\nâˆ’1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeï¬€ect\nduetotakingthediï¬€erencebetweenthevalueof fatdiï¬€erentpoints.Thisallows",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "theuseoftinyvaluesof î€like î€= 10âˆ’150,whichmakethe O( î€2)errorinsigniï¬cant\nforallpracticalpurposes.\n4 3 9",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiï¬ers,\nhowoftenaretheyoï¬€?Arethereunitsthatarealwaysoï¬€?Fortanhunits,\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\nAssuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "stalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\nevolution.\nFinally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\nproblems.Â Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\nSomeguaranteesthatsomeoptimizationalgorithmsoï¬€erincludethattheobjective",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\nUsuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter.\n11.6Example:Multi-DigitNumberRecognition\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\ndatabaseinfrastructure,andsoon,wereofparamountimportance.\nFromthepointofviewofthemachinelearningtask,theprocessbeganwith\ndatacollection.Â The carscollectedtherawdataandhumanoperatorsprovided\nlabels.Thetranscriptiontaskwasprecededbyasigniï¬cantamountofdataset",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "curation,includingusingothermachinelearningtechniquestodetectthehouse\n4 4 0",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\nnumberspriortotranscribingthem.\nThetranscriptionprojectbeganwithachoiceofperformancemetricsand\ndesiredvaluesforthesemetrics.Â Animportantgeneralprincipleistotailorthe\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\nforthisproject.Â Speciï¬cally,thegoalwastoobtainhuman-level,98%accuracy.\nThislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "thislevelofaccuracy,theStreetViewtranscriptionsystemsacriï¬cescoverage.\nCoveragethusbecamethemainperformancemetricoptimizedduringtheproject,\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\npossibletoreducetheconï¬dencethresholdbelowwhichthenetworkrefusesto\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.\nAfterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworkwithrectiï¬edlinearunits.Thetranscriptionprojectbegan\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\nbaseline,theï¬rstimplementation oftheoutputlayerofthemodelconsistedof n\ndiï¬€erentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\nweretrainedexactlythesameasifthetaskwereclassiï¬cation,witheachsoftmax\nunittrainedindependently.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "unittrainedindependently.\nOurrecommendedmethodologyistoiterativelyreï¬nethebaselineandtest\nwhethereachchangemakesanimprovement.Theï¬rstchangetotheStreetView\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\nmetricandthestructureofthedata.Speciï¬cally,thenetworkrefusestoclassify\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\nsomethreshold t.Initially,thedeï¬nitionof p( y x|)wasad-hoc,basedonsimply",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "multiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\nmuchmoreeï¬€ectively.\nAtthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "isunderï¬ttingoroverï¬tting.Inthiscase,trainandtestseterrorwerenearly\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\n4 4 1",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER11.PRACTICALMETHODOLOGY\ntounderï¬ttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\nstrategieswerecommendistovisualizethemodelâ€™sworsterrors.Inthiscase,that\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\nhighestconï¬dence.Theseprovedtomostlyconsistofexampleswheretheinput\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\nremovedbythecroppingoperation.Forexample,aphotoofanaddressâ€œ1849â€",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "mightbecroppedtootightly,withonlytheâ€œ849â€remainingvisible.Thisproblem\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\nsystemâ€™scoverage.\nFinally,thelastfewpercentagepointsofperformancecamefromadjusting",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\nremainedroughlyequal,itwasalwaysclearthatanyperformancedeï¬citsweredue\ntounderï¬tting, aswellasduetoafewremainingproblemswiththedatasetitself.\nOverall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\nhavebeenpossibleviahumaneï¬€ort.",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "havebeenpossibleviahumaneï¬€ort.\nWehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\nothersimilarsuccesses.\n4 4 2",
    "metadata": {
      "source": "[16]part-2-chapter-11.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 3\nProbabilityandInformation\nTheory\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\nnewuncertainstatements.Inartiï¬cialintelligenceapplications,weuseprobability\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "expressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\nunderstandthematerialinthisbook.\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "thepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\nofuncertaintyinaprobabilitydistribution.\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\nbesuï¬ƒcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "suggestthatyouconsultanadditionalresource,suchasJaynes2003().\n53",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.1WhyProbability?\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\nexecuteeachmachineinstructionï¬‚awlessly.Errorsinhardwaredooccur,butare\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "makesheavyuseofprobabilitytheory.\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\nbyPearl1988().\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "Infact,beyondmathematical statementsthataretruebydeï¬nition,itisdiï¬ƒcult\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\nguaranteedtooccur.\nTherearethreepossiblesourcesofuncertainty:\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "whereweassumethatthecardsaretrulyshuï¬„edintoarandomorder.\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.Â Theoutcome\ngiventhecontestantâ€™schoiceisdeterministic,butfromthecontestantâ€™spoint\nofview,theoutcomeisuncertain.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "ofview,theoutcomeisuncertain.\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\ntheÂ information wehaveÂ observed,Â theÂ discardedÂ i nformationresultsÂ in\nuncertaintyinthemodelâ€™spredictions. Forexample,supposewebuilda\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\n54",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\ntheprecisepositionofobjects:Â eachobjectcouldbeanywherewithinthe\ndiscretecellthatitwasobservedtooccupy.\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\nmodelingsystemhastheï¬delitytoaccommodateacomplexrule.Forexample,the",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "simpleruleâ€œMostbirdsï¬‚yâ€ischeaptodevelopandisbroadlyuseful,whilearule\noftheform,â€œBirdsï¬‚y,exceptforveryyoungbirdsthathavenotyetlearnedto\nï¬‚y,sickorinjuredbirdsthathavelosttheabilitytoï¬‚y,ï¬‚ightlessspeciesofbirds\nincludingthecassowary,ostrichandkiwi...â€Â isexpensivetodevelop,maintainand\ncommunicate,andafterallofthiseï¬€ortisstillverybrittleandpronetofailure.\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "allofthetoolswewantforartiï¬cialintelligenceapplications.Probabilitytheory\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.Â Whenwe\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\ntheexperiment(e.g.,drawahandofcards)inï¬nitelymanytimes,thenproportion\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheï¬‚u,\nthismeanssomethingverydiï¬€erentâ€”wecannotmakeinï¬nitelymanyreplicasof\nthepatient,noristhereanyreasontobelievethatdiï¬€erentreplicasofthepatient\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheï¬‚u",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "and0indicatingabsolutecertaintythatthepatientdoesnothavetheï¬‚u.Â The\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\nofcertainty,isknownas B ay e si an pr o babili t y.\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\n55",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\nsee(). Ramsey1926\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\n3.2RandomVariables\nA r andom v ar i abl eisavariablethatcantakeondiï¬€erentvaluesrandomly.We\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\nmustbecoupledwithaprobabilitydistributionthatspeciï¬eshowlikelyeachof\nthesestatesare.\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\nisonethathasaï¬niteorcountablyinï¬nitenumberofstates.Notethatthese\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\nassociatedwitharealvalue.\n3.3ProbabilityDistributions",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "3.3ProbabilityDistributions\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\ncontinuous.\n3.3.1DiscreteVariablesandProbabilityMassFunctions\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "acapitalP.Oftenweassociateeachrandomvariablewithadiï¬€erentprobability\n56",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\nP P ()xisusuallynotthesameas()y.\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\nexplicitly:P(x=x).Sometimeswedeï¬neavariableï¬rst,thenuseâˆ¼notationto\nspecifywhichdistributionitfollowslater:xx. âˆ¼P()\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "satisfythefollowingproperties:\nâ€¢Thedomainofmustbethesetofallpossiblestatesofx. P\nâ€¢âˆ€âˆˆxx,0â‰¤P(x)â‰¤1.Animpossibleeventhasprobabilityandnostatecan 0 \nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\nâ€¢î\nx âˆˆ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\ntheprobabilityofoneofmanyeventsoccurring.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "theprobabilityofoneofmanyeventsoccurring.\nForexample,considerasinglediscreterandomvariablexwithkdiï¬€erent\nstates.Wecanplacea uni f o r m di st r i but i o nonxâ€”thatis,makeeachofits\nstatesequallylikelyâ€”bysettingitsprobabilitymassfunctionto\nPx (= x i) =1\nk(3.1)\nforalli.Wecanseethatthisï¬tstherequirementsforaprobabilitymassfunction.\nThevalue1\nkispositivebecauseisapositiveinteger.Wealsoseethat k\nî˜\niPx (= x i) =î˜\ni1\nk=k\nk= 1, (3.2)\nsothedistributionisproperlynormalized.\n57",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\nfollowingproperties:\nâ€¢Thedomainofmustbethesetofallpossiblestatesofx. p\nâ€¢âˆ€âˆˆ â‰¥ â‰¤ xx,px() 0 () . p Notethatwedonotrequirex 1.\nâ€¢î’\npxdx()= 1.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "â€¢î’\npxdx()= 1.\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciï¬c\nstatedirectly,insteadtheprobabilityoflandinginsideaninï¬nitesimalregionwith\nvolumeisgivenby. Î´x pxÎ´x()\nWecanintegratethedensityfunctiontoï¬ndtheactualprobabilitymassofa\nsetofpoints.Speciï¬cally,theprobabilitythatxliesinsomeset Sisgivenbythe\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\nliesintheintervalisgivenby []a,bî’\n[ ] a , bpxdx().",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "[ ] a , bpxdx().\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeciï¬c\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\nwhereaandbaretheendpointsoftheinterval,withb>a.Theâ€œ;â€notationmeans\nâ€œparametrized byâ€;weconsiderxtobetheargumentofthefunction,whileaand\nbareparametersthatdeï¬nethefunction.Toensurethatthereisnoprobability\nmassoutsidetheinterval,wesayu(x;a,b)=0forallxî€¶âˆˆ[a,b] [.Withina,b],",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "uxa,b (;) =1\nb a âˆ’.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\nbywritingx. âˆ¼Ua,b()\n3.4MarginalProbability\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\nForexample,supposewehavediscreterandomvariablesxandy,andweknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "P,(xy.Wecanï¬ndxwiththe : ) P() sum r ul e\nâˆ€âˆˆxxx,P(= ) =xî˜\nyPx,y. (= xy= ) (3.3)\n58",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThenameâ€œmarginalprobabilityâ€comesfromtheprocessofcomputingmarginal\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\ndiï¬€erentvaluesofxinrowsanddiï¬€erentvaluesofyincolumns,itisnaturalto\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\ntherightoftherow.\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\npx() =îš\npx,ydy. () (3.4)\n3.5ConditionalProbability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "px,ydy. () (3.4)\n3.5ConditionalProbability\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\nconditionalprobabilitycanbecomputedwiththeformula\nPyx (= y |x= ) =Py,x (= yx= )\nPx (= x ). (3.5)\nTheconditionalprobabilityisonlydeï¬nedwhenP(x=x)>0.Wecannotcompute\ntheconditionalprobabilityconditionedonaneventthatneverhappens.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\nwhichwedonotexploreinthisbook.\n3.6TheChainRuleofConditionalProbabilities",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "3.6TheChainRuleofConditionalProbabilities\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\nintoconditionaldistributionsoveronlyonevariable:\nP(x( 1 ),...,x( ) n) = (Px( 1 ))Î n\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i âˆ’).(3.6)\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\nItfollowsimmediatelyfromthedeï¬nitionofconditionalprobabilityinequation.3.5\n59",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nForexample,applyingthedeï¬nitiontwice,weget\nP,,P,P, (abc)= (ab|c)(bc)\nP,PP (bc)= ( )bc| ()c\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\n3.7IndependenceandConditionalIndependence\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\nonlyy:\nâˆ€âˆˆ âˆˆxx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "variableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\nwayforeveryvalueofz:\nâˆ€âˆˆ âˆˆ âˆˆ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\n(3.8)\nWeÂ candenoteindependenceÂ andconditionalindependenceÂ with compact\nnotation:xyâŠ¥meansthatxandyareindependent,whilexyz âŠ¥|meansthatx\nandyareconditionallyindependentgivenz.\n3.8Expectation,VarianceandCovariance\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\nE x âˆ¼ P[()] =fxî˜\nxPxfx, ()() (3.9)\nwhileforcontinuousvariables,itiscomputedwithanintegral:\nE x âˆ¼ p[()] =fxîš\npxfxdx. ()() (3.10)\n60",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[Â·]averagesover\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\nnoambiguity,wemayomitthesquarebrackets.\nExpectationsarelinear,forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "Expectationsarelinear,forexample,\nE x[()+ ()] = Î±fxÎ²gxÎ± E x[()]+fxÎ² E x[()]gx, (3.11)\nwhenandarenotdependenton. Î±Î² x\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\nvariablexvaryaswesamplediï¬€erentvaluesofxfromitsprobabilitydistribution:\nVar(()) = fx Eî¨\n(() [()]) fxâˆ’ Efx2î©\n. (3.12)\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "The c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\ntoeachother,aswellasthescaleofthesevariables:\nCov(()()) = [(() [()])(() [()])] fx,gy Efxâˆ’ Efxgyâˆ’ Egy.(3.13)\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\nrelated,ratherthanalsobeingaï¬€ectedbythescaleoftheseparatevariables.\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "ever,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\nzerocovariance.Forexample,supposeweï¬rstsamplearealnumberxfroma\nuniformdistributionovertheinterval[âˆ’1,1].Wenextsamplearandomvariable\n61",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ns.Withprobability1\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\nthevalueofstobeâˆ’1.Wecanthengeneratearandomvariableybyassigning\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\nthemagnitudeof.However,y Cov() = 0x,y.\nThe c o v ar i anc e m at r i xofarandomvector xâˆˆ RnisannnÃ—matrix,such\nthat\nCov() x i , j= Cov(x i,x j). (3.14)\nThediagonalelementsofthecovariancegivethevariance:\nCov(x i,x i) = Var(x i). (3.15)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "Cov(x i,x i) = Var(x i). (3.15)\n3.9CommonProbabilityDistributions\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\nlearning.\n3.9.1BernoulliDistribution\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\nItiscontrolledbyasingleparameterÏ†âˆˆ[0,1],whichgivestheprobabilityofthe\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\nP Ï† (= 1) = x (3.16)\nP Ï† (= 0) = 1x âˆ’ (3.17)\nPxÏ† (= x ) = x(1 )âˆ’Ï†1 âˆ’ x(3.18)\nE x[] = xÏ† (3.19)\nVar x() = (1 )xÏ†âˆ’Ï† (3.20)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "E x[] = xÏ† (3.19)\nVar x() = (1 )xÏ†âˆ’Ï† (3.20)\n3.9.2MultinoulliDistribution\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\nvariablewithkdiï¬€erentstates,wherekisï¬nite.1Themultinoullidistributionis\n1â€œMultinoulliâ€isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "timeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\nManytextsusethetermâ€œmultinomialâ€torefertomultinoullidistributionswithoutclarifying\nthattheyreferonlytothecase. n= 1\n62",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nparametrized byavector pâˆˆ[0,1]k âˆ’ 1,wherep igivestheprobabilityofthei-th\nstate.Theï¬nal,k-thstateâ€™sprobabilityisgivenby1âˆ’ 1î€¾p.Notethatwemust\nconstrain 1î€¾pâ‰¤1.Multinoullidistributionsareoftenusedtorefertodistributions\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\norvarianceofmultinoulli-dis tributedrandomvariables.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "TheBernoulliandmultinoullidistributionsaresuï¬ƒcienttodescribeanydistri-\nbutionovertheirdomain.Â They areabletodescribeanydistributionovertheir\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\nparametersmustimposestrictlimitsonthedistribution.\n3.9.3GaussianDistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "3.9.3GaussianDistribution\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\nt i o n,alsoknownasthe : G aussian di st r i but i o n\nN(;xÂµ,Ïƒ2) =î²\n1\n2Ï€Ïƒ2expî€’\nâˆ’1\n2Ïƒ2( )xÂµâˆ’2î€“\n.(3.21)\nSeeï¬gureforaplotofthedensityfunction. 3.1\nThetwoparameters Âµâˆˆ RandÏƒâˆˆ(0,âˆž)controlthenormaldistribution.\nTheparameterÂµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\nthedistribution: E[x] =Âµ.Thestandarddeviationofthedistributionisgivenby\nÏƒ,andthevariancebyÏƒ2.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "Ïƒ,andthevariancebyÏƒ2.\nWhenweevaluatethePDF,weneedtosquareandinvertÏƒ.Whenweneedto\nfrequentlyevaluatethePDFwithdiï¬€erentparametervalues,amoreeï¬ƒcientway\nofparametrizing thedistributionistouseaparameterÎ²âˆˆ(0,âˆž)tocontrolthe\npr e c i si o norinversevarianceofthedistribution:\nN(;xÂµ,Î²âˆ’ 1) =î²\nÎ²\n2Ï€expî€’\nâˆ’1\n2Î²xÂµ (âˆ’)2î€“\n. (3.22)\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "take,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\n63",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nâˆ’ âˆ’ âˆ’ âˆ’ 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x Âµ\nInï¬‚ectionpointsat\nx Âµ Ïƒ = Â±\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;Âµ,Ïƒ2)exhibits\naclassicâ€œbellcurveâ€shape,withthexcoordinateofitscentralpeakgivenbyÂµ,and\nthewidthofitspeakcontrolledbyÏƒ.Inthisexample,wedepictthestandardnormal\ndistribution,withand. Âµ= 0Ïƒ= 1\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "distributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\nstructuredbehavior.\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\nsection.19.4.2\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\ndeï¬nitesymmetricmatrix: Î£\nN(; ) = x Âµ, Î£î³\n1\n(2)Ï€ndet() Î£expî€’\nâˆ’1\n2( ) x Âµâˆ’î€¾Î£âˆ’ 1( ) x Âµâˆ’î€“\n.(3.23)\n64",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nTheparameter Âµstillgivesthemeanofthedistribution,thoughnowitis\nvector-valued.Theparameter Î£givesthecovariancematrixofthedistribution.\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\nmanydiï¬€erentvaluesoftheparameters,thecovarianceisnotacomputationally\neï¬ƒcientwaytoparametrizethedistribution,sinceweneedtoinvert Î£toevaluate\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x Î²\nN(; x Âµ Î²,âˆ’ 1) =î³\ndet() Î²\n(2)Ï€nexpî€’\nâˆ’1\n2( ) x Âµâˆ’î€¾Î² x Âµ (âˆ’)î€“",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "det() Î²\n(2)Ï€nexpî€’\nâˆ’1\n2( ) x Âµâˆ’î€¾Î² x Âµ (âˆ’)î€“\n.(3.24)\nWeoftenï¬xthecovariancematrixtobeadiagonalmatrix.Anevensimpler\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\ntimestheidentitymatrix.\n3.9.4ExponentialandLaplaceDistributions\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\ndi st r i but i o n:\npxÎ»Î» (;) = 1 x â‰¥ 0exp( )âˆ’Î»x. (3.25)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "pxÎ»Î» (;) = 1 x â‰¥ 0exp( )âˆ’Î»x. (3.25)\nTheexponentialdistributionusestheindicatorfunction 1 x â‰¥ 0toassignprobability\nzerotoallnegativevaluesof.x\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\nofprobabilitymassatanarbitrarypointistheÂµ L apl ac e di st r i but i o n\nLaplace(;) =xÂµ,Î³1\n2Î³expî€’\nâˆ’|âˆ’|xÂµ\nÎ³î€“\n. (3.26)\n3.9.5TheDiracDistributionandEmpiricalDistribution\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "clustersaroundasinglepoint.Thiscanbeaccomplishedbydeï¬ningaPDFusing\ntheDiracdeltafunction,:Î´x()\npxÎ´xÂµ. () = (âˆ’) (3.27)\nTheDiracdeltafunctionisdeï¬nedsuchthatitiszero-valuedeverywhereexcept\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\nassociateseachvaluexwithareal-valuedoutput,insteaditisadiï¬€erentkindof\n65",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeï¬nedintermsofits\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\nthanzero.\nBydeï¬ningp(x)tobeÎ´shiftedbyâˆ’Âµweobtainaninï¬nitelynarrowand\ninï¬nitelyhighpeakofprobabilitymasswhere.xÂµ= \nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\ndi st r i but i o n,\nË†p() = x1\nmmî˜",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "di st r i but i o n,\nË†p() = x1\nmmî˜\ni = 1Î´( x xâˆ’( ) i) (3.28)\nwhichputsprobabilitymass1\nmoneachofthempoints x( 1 ),..., x( ) mforminga\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\ntodeï¬netheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "thatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\nonthisdataset.Â Anotherimportantperspectiveontheempiricaldistributionis\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\n(seesection).5.5\n3.9.6MixturesofDistributions\nItisalsocommontodeï¬neprobabilitydistributionsbycombiningothersimpler",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "probabilitydistributions.OnecommonÂ wayofÂ combiningÂ distributionsisÂ to\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\nmultinoullidistribution:\nP() =xî˜\niPiPi (= c )( = xc| ) (3.29)\nwherecisthemultinoullidistributionovercomponentidentities. P()\nWehavealreadyseenoneexampleofamixturedistribution:theempirical",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\ncomponentforeachtrainingexample.\n66",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\nprobabilitydistributionsfromsimpleonesinmoredetail.\nThemixturemodelallowsustobrieï¬‚yglimpseaconceptthatwillbeof\nparamountimportancelaterâ€”the l at e n t v ar i abl e.Alatentvariableisarandom\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\nvariablesarediscussedfurtherinsection.16.5\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "model,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\naseparatelyparametrized mean Âµ( ) iandcovariance Î£( ) i.Somemixturescanhave\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\nviatheconstraint Î£( ) i= Î£,iâˆ€.AswithasingleGaussiandistribution,themixture\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\ndiagonalorisotropic.\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "specifythe pr i o r pr o babili t yÎ± i=P(c=i) giventoeachcomponenti.Theword\nâ€œpriorâ€indicatesthatitexpressesthemodelâ€™sbeliefsaboutc b e f o r eithasobserved\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\nspeciï¬c,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\ncomponents.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "components.\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\n3.10UsefulPropertiesofCommonFunctions\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\ntheprobabilitydistributionsusedindeeplearningmodels.\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\nÏƒx() =1\n1+exp()âˆ’x. (3.30)\nThelogisticsigmoidiscommonlyusedtoproducetheÏ†parameterofaBernoulli\n67",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nx 1x 2\nFigure3.2:Â SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\ncomponents.Fromlefttoright,theï¬rstcomponenthasanisotropiccovariancematrix,\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "separatelyalonganarbitrarybasisofdirections.\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\nfortheÏ†parameter.Seeï¬gureforagraphofthesigmoidfunction.The 3.3\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\nmeaningthatthefunctionbecomesveryï¬‚atandinsensitivetosmallchangesinits\ninput.\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\n2001):\nÎ¶x x. () = log(1+exp()) (3.31)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "2001):\nÎ¶x x. () = log(1+exp()) (3.31)\nThesoftplusfunctioncanbeusefulforproducingtheÎ²orÏƒparameterofanormal\ndistributionbecauseitsrangeis(0,âˆž).Italsoarisescommonlywhenmanipulating\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\nfactthatitisasmoothedorâ€œsoftenedâ€versionof\nx+= max(0),x. (3.32)\nSeeï¬gureforagraphofthesoftplusfunction. 3.4\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\nthem:\n68",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nâˆ’ âˆ’ 1 0 5 0 5 1 0\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .Ïƒ x ( )\nFigure3.3:Thelogisticsigmoidfunction.\nâˆ’ âˆ’ 1 0 5 0 5 1 0\nx024681 0Î¶ x ( )\nFigure3.4:Thesoftplusfunction.\n69",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nÏƒx() =exp()x\nexp()+exp(0)x(3.33)\nd\ndxÏƒxÏƒxÏƒx () = ()(1âˆ’()) (3.34)\n1 () = () âˆ’ÏƒxÏƒâˆ’x (3.35)\nlog() = () Ïƒx âˆ’Î¶âˆ’x (3.36)\nd\ndxÎ¶xÏƒx () = () (3.37)\nâˆ€âˆˆx(01),,Ïƒâˆ’ 1() = logxî€’x\n1âˆ’xî€“\n(3.38)\nâˆ€x>,Î¶0âˆ’ 1() = log(exp()1) x xâˆ’ (3.39)\nÎ¶x() =îšx\nâˆ’ âˆžÏƒydy() (3.40)\nÎ¶xÎ¶xx ()âˆ’(âˆ’) = (3.41)\nThefunctionÏƒâˆ’ 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\nusedinmachinelearning.\nEquationprovidesextrajustiï¬cationforthenameâ€œsoftplus.â€Thesoftplus 3.41",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "functionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\nfunction,xâˆ’=max{0,xâˆ’}.Toobtainasmoothfunctionthatisanalogoustothe\nnegativepart,onecanuseÎ¶(âˆ’x).Justasxcanberecoveredfromitspositivepart\nandnegativepartviatheidentityx+âˆ’xâˆ’=x,itisalsopossibletorecoverx\nusingthesamerelationshipbetweenand,asshowninequation. Î¶x()Î¶x(âˆ’) 3.41\n3.11Bayesâ€™Rule\nWeoftenï¬ndourselvesinasituationwhereweknowP(yx|)andneedtoknow",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "P(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\nusing B a y e sâ€™ r ul e:\nP( ) =xy|PP()x( )yx|\nP()y. (3.42)\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\nP() =yî\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\n70",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nBayesâ€™ruleisÂ straightforwardtoÂ derivefromÂ thedeï¬nitionofconditional\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoï¬rst\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\nindependentlydiscoveredbyPierre-SimonLaplace.\n3.12TechnicalDetailsofContinuousVariables\nAproperformalunderstandingofcontinuousrandomvariablesandprobability",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\nthistextbook,butwecanbrieï¬‚ysketchsomeoftheissuesthatmeasuretheoryis\nemployedtoresolve.\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "S 1and S 2suchthatp( xâˆˆ S 1) +p( xâˆˆ S 2)>1but S 1âˆ© S 2=âˆ….Thesesets\naregenerallyconstructedmakingveryheavyuseoftheinï¬niteprecisionofreal\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeï¬nedby\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\nprobabilityofwithoutencounteringparadoxes.Â Inthisbook,weonlyintegrate\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "becomesarelevantconcern.\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\nasetissaidtohave m e asur e z e r o.Wedonotformallydeï¬nethisconceptinthis\ntextbook.Forourpurposes,itissuï¬ƒcienttounderstandtheintuitionthataset\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "within R2,alinehasmeasurezero,whileaï¬lledpolygonhaspositivemeasure.\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\nnumbershasmeasurezero,forinstance).\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\n71",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\ntheoryholdforalldiscretevaluesbutonlyholdâ€œalmosteverywhereâ€forcontinuous\nvalues.\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "tinuous,diï¬€erentiabletransformation.Onemightexpectthatp y( y) =p x(gâˆ’ 1( y)).\nThisisactuallynotthecase.\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\ny=x\n2andxâˆ¼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\neverywhereexcepttheinterval[0,1\n2] 1 ,anditwillbeonthisinterval.Thismeans\nîš\np y()=ydy1\n2, (3.43)\nwhichviolatesthedeï¬nitionofaprobabilitydistribution.Thisisacommonmistake.\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "spaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\ninï¬nitesimallysmallregionwithvolumeÎ´ xisgivenbyp( x)Î´ x.Sincegcanexpand\norcontractspace,theinï¬nitesimalvolumesurrounding xin xspacemayhave\ndiï¬€erentvolumeinspace. y\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\npreservetheproperty\n|p y(())= gxdy||p x()xdx.| (3.44)\nSolvingfromthis,weobtain\np y() = yp x(gâˆ’ 1())yî€Œî€Œî€Œî€Œâˆ‚x\nâˆ‚yî€Œî€Œî€Œî€Œ(3.45)\norequivalently\np x() = xp y(())gxî€Œî€Œî€Œî€Œâˆ‚gx()\nâˆ‚xî€Œî€Œî€Œî€Œ. (3.46)",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "p x() = xp y(())gxî€Œî€Œî€Œî€Œâˆ‚gx()\nâˆ‚xî€Œî€Œî€Œî€Œ. (3.46)\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\nm at r i xâ€”thematrixwithJ i , j=âˆ‚ x i\nâˆ‚ y j.Thus,forreal-valuedvectorsand, x y\np x() = xp y(())g xî€Œî€Œî€Œî€Œdetî€’âˆ‚g() x\nâˆ‚ xî€“ î€Œî€Œî€Œî€Œ. (3.47)\n72",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n3.13InformationTheory\nInformationtheoryÂ isaÂ branchofÂ appliedmathematicsÂ thatrevolvesaround\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\nspeciï¬cprobabilitydistributionsusingvariousencodingschemes.Inthecontextof",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\nwheresomeofthesemessagelengthinterpretations donotapply.Thisï¬eldis\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\n().2003\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "eventhasÂ occurredismoreinformativethanlearningthataÂ likelyÂ eventhas\noccurred.Amessagesayingâ€œthesunrosethismorningâ€issouninformative as\ntobeunnecessarytosend,butamessagesayingâ€œtherewasasolareclipsethis\nmorningâ€isveryinformative.\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\nSpeciï¬cally,\nâ€¢Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\nwhatsoever.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "whatsoever.\nâ€¢Lesslikelyeventsshouldhavehigherinformationcontent.\nâ€¢Independenteventsshouldhaveadditiveinformation. Forexample,ï¬nding\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\nmuchinformationasï¬ndingoutthatatossedcoinhascomeupasheads\nonce.\nInordertosatisfyallthreeoftheseproperties,wedeï¬nethe se l f - i nf o r m a t i o n\nofaneventxtobe = x\nIxPx. () = logâˆ’ () (3.48)\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "deï¬nitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\n73",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ninformationgainedbyobservinganeventofprobability1\ne.Othertextsusebase-2\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\njustarescalingofinformationmeasuredinnats.\nWhenxiscontinuous,weusethesamedeï¬nitionofinformationbyanalogy,\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\nguaranteedtooccur.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "guaranteedtooccur.\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\nH() = x E x âˆ¼ P[()] = Ix âˆ’ E x âˆ¼ P[log()]Px. (3.49)\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\narediï¬€erent)neededonaveragetoencodesymbolsdrawnfromadistributionP.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\nï¬gureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\nknownasthe di ï¬€ e r e n t i al e nt r o p y.\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\nrandomvariablex,wecanmeasurehowdiï¬€erentthesetwodistributionsareusing\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\nD K L( ) = PQî« E x âˆ¼ Pî€”\nlogPx()\nQx()î€•",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "D K L( ) = PQî« E x âˆ¼ Pî€”\nlogPx()\nQx()î€•\n= E x âˆ¼ P[log()log()] Pxâˆ’Qx.(3.50)\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\nthecaseofdiscretevariables,orequalâ€œalmosteverywhereâ€inthecaseofcontinuous\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediï¬€erence\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\nbecauseitisnotsymmetric:D K L(PQî«)î€¶=D K L(QPî«)forsomePandQ.Â This\n74",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\nto.Theentropyisgivenby 1 (pâˆ’1)log(1âˆ’p)âˆ’pplog.Whenpisnear0,thedistribution",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "isnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\noutcomes.\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\ntouseD K L( )PQî«orD K L( )QPî«.Seeï¬gureformoredetail.3.6\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "H(P,Q) =H(P)+D K L(PQî«),whichissimilartotheKLdivergencebutlacking\nthetermontheleft:\nHP,Q( ) = âˆ’ E x âˆ¼ Plog()Qx. (3.51)\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\ntreattheseexpressionsaslim x â†’ 0xxlog= 0.\n3.14StructuredProbabilisticModels",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "3.14StructuredProbabilisticModels\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\n75",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\nxProbability Densityqâˆ—= argminq D K L() p q î«\np x()\nqâˆ—() x\nxProbability Densityqâˆ—= argminq D K L() q p î«\np() x\nqâˆ—() x\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\neitherD KL(pqî«)orD KL(qpî«).Weillustratetheeï¬€ectofthischoiceusingamixtureof\ntwoGaussiansforp,andasingleGaussianforq.Â Thechoiceofwhichdirectionofthe",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "KLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\ndirectionoftheKLdivergencereï¬‚ectswhichoftheseconsiderationstakespriorityforeach\napplication. ( L e f t )Theeï¬€ectofminimizingD KL(pqî«).Inthiscase,weselectaqthathas",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "highprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\neï¬€ectofminimizingD KL(qpî«).Inthiscase,weselectaqthathaslowprobabilitywhere\nphaslowprobability.Whenphasmultiplemodesthataresuï¬ƒcientlywidelyseparated,\nasinthisï¬gure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "illustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\narenotseparatedbyasuï¬ƒcientlystronglowprobabilityregion,thenthisdirectionofthe\nKLdivergencecanstillchoosetoblurthemodes.\n76",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\ndescribetheentirejointprobabilitydistributioncanbeveryineï¬ƒcient(both\ncomputationally andstatistically).\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\nainï¬‚uencesthevalueofbandbinï¬‚uencesthevalueofc,butthataandcare\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "variablesasaproductofprobabilitydistributionsovertwovariables:\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\nThesefactorizationscangreatlyreducethenumberofparametersneeded\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\nreducethecostofrepresentingadistributionifweareabletoï¬ndafactorization\nintodistributionsoverfewervariables.\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "â€œgraphâ€inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\ninthegraphcorrespondstoarandomvariable,Â and anedgeconnectingtwo\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "interactionsbetweenthosetworandomvariables.\nD i r e c t e dmodelsuseÂ graphswithdirectededges,Â andtheyrepresentfac-\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\nSpeciï¬cally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\ngiventheparentsofx i,denotedPa G(x i):\np() = xî™\nip(x i|Pa G(x i)). (3.53)\nSeeï¬gureforanexampleofadirectedgraphandthefactorizationofprobability 3.7",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "distributionsitrepresents.\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\n77",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "connectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\nmodelisassociatedwithafactorÏ†( ) i(C( ) i).Thesefactorsarejustfunctions,not\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\ndistribution.\nTheprobabilityofaconï¬gurationofrandomvariablesis pr o p o r t i o naltothe\nproductofallofthesefactorsâ€”assignmentsthatresultinlargerfactorvaluesare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\nthereforedividebyanormalizingconstantZ,deï¬nedtobethesumorintegral\noverallstatesoftheproductoftheÏ†functions,inordertoobtainanormalized\nprobabilitydistribution:\np() = x1\nZî™\niÏ†( ) iî€\nC( ) iî€‘\n. (3.55)\nSeeï¬gureforanexampleofanundirectedgraphandthefactorizationof 3.8\nprobabilitydistributionsitrepresents.\nKeepÂ inmindÂ thattheseÂ graphicalrepresentationsofÂ factorizations areÂ a",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\n78",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\naa\nccbb\needd\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\np,,,, (abcde) =1\nZÏ†( 1 )( )abc,,Ï†( 2 )()bd,Ï†( 3 )()ce,. (3.56)\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\nways.",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "ways.\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\ndiï¬€erentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\ndetail.\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "mostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\nremains:numericalmethods.\n79",
    "metadata": {
      "source": "[7]part-1-chapter-3.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 8\nOptimizationforTrainingDeep\nModels\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\ndiï¬ƒcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "networktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\noptimization ingeneral.\nThischapterfocusesononeparticularcaseofoptimization: ï¬ndingtheparam-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "etersÎ¸ofaneuralnetworkthatsigniï¬cantlyreduceacostfunction J(Î¸),which\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\nwellasadditionalregularizationterms.\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\nforamachinelearningtaskdiï¬€ersfrompureoptimization. Next,wepresentseveral\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdiï¬ƒcult.We\nthendeï¬neseveralpracticalalgorithms,includingbothoptimization algorithms",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\n274",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\nalgorithmsintohigher-levelprocedures.\n8.1HowLearningDiï¬€ersfromPureOptimization\nOptimization algorithmsusedfortrainingofdeepmodelsdiï¬€erfromtraditional\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "P,thatisdeï¬nedwithrespecttothetestsetandmayalsobeintractable.We\nthereforeoptimize Ponlyindirectly.Wereduceadiï¬€erentcostfunction J(Î¸)in\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\ndeepmodelsalsotypicallyincludesomespecializationonthespeciï¬cstructureof\nmachinelearningobjectivefunctions.\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\nsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "suchas\nJ() = Î¸ E ( ) Ë† x ,y âˆ¼ pdataL f , y , ((;)xÎ¸) (8.1)\nwhere Listheper-examplelossfunction, f(x;Î¸)isthepredictedoutputwhen\ntheinputisx,Ë† p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\nsupervisedcase,wheretheargumentsto Lare f(x;Î¸)and y.However,itistrivial\ntoextendthisdevelopment,forexample,toincludeÎ¸orxasarguments,orto\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\nunsupervisedlearning.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedlearning.\nEquationdeï¬nesanobjectivefunctionwithrespecttothetrainingset.We 8.1\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\novertheï¬nitetrainingset:\nJâˆ—() = Î¸ E ( ) x ,y âˆ¼ pdataL f , y . ((;)xÎ¸) (8.2)\n8.1.1EmpiricalRiskMinimization\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "thattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\n2 7 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistributionË† p(x , y)\ndeï¬nedbythetrainingset.Wenowminimizetheempiricalrisk\nE x ,y âˆ¼ Ë† pdata ( ) x , y[((;))] = L fxÎ¸ , y1\nmm î˜",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "mm î˜\ni = 1L f((x( ) i;)Î¸ , y( ) i)(8.3)\nwhereisthenumberoftrainingexamples. m\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigniï¬cantlyas\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\ncanbeexpectedtodecreasebyvariousamounts.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "canbeexpectedtodecreasebyvariousamounts.\nHowever,empiricalriskminimization ispronetooverï¬tting.Modelswith\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\nriskminimization isnotreallyfeasible.Themosteï¬€ectivemodernoptimization\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeï¬ned\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "rarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiï¬€erent\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediï¬€erent\nfromthequantitythatwetrulywanttooptimize.\n8.1.2SurrogateLossFunctionsandEarlyStopping\nSometimes,thelossfunctionweactuallycareabout(sayclassiï¬cationerror)isnot\nonethatcanbeoptimizedeï¬ƒciently.Forexample,exactlyminimizingexpected0-1\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "classiï¬er(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassiï¬cationerrorin\nexpectation.\n2 7 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\nonecanimprovetherobustnessoftheclassiï¬erbyfurtherpushingtheclassesapart\nfromeachother,obtainingamoreconï¬dentandreliableclassiï¬er,thusextracting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\nminimizingtheaverage0-1lossonthetrainingset.\nAveryimportantdiï¬€erencebetweenoptimization ingeneralandoptimization\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\nstopping(section)issatisï¬ed.Typicallytheearlystoppingcriterionisbased 7.8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\nandisdesignedtocausethealgorithmtohaltwheneveroverï¬ttingbeginstooccur.\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\nwhichisverydiï¬€erentfromthepureoptimization setting,whereanoptimization\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\n8.1.3BatchandMinibatchAlgorithms\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\nspace,decomposeintoasumovereachexample:\nÎ¸ M L= argmax\nÎ¸m î˜\ni = 1log p m o de l(x( ) i, y( ) i;)Î¸ . (8.4)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "i = 1log p m o de l(x( ) i, y( ) i;)Î¸ . (8.4)\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\nempiricaldistributiondeï¬nedbythetrainingset:\nJ() = Î¸ E x ,y âˆ¼ Ë† pdatalog p m o de l(;)x , yÎ¸ . (8.5)\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\n2 7 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmostcommonlyusedpropertyisthegradient:\nâˆ‡ Î¸ J() = Î¸ E x ,y âˆ¼ Ë† pdataâˆ‡ Î¸log p m o de l(;)x , yÎ¸ . (8.6)\nComputingÂ this expectationÂ exactlyÂ isveryÂ expensiveÂ becauseÂ itÂ requires\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "samplesisgivenby Ïƒ /âˆšn ,where Ïƒisthetruestandarddeviationofthevalueof\nthesamples.Thedenominator ofâˆšnshowsthattherearelessthanlinearreturns\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "numberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\nofthegradientratherthanslowlycomputingtheexactgradient.\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "areunlikelytotrulyencounterthisworst-casesituation,butwemayï¬ndlarge\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\nbecausethewordâ€œbatchâ€isalsooftenusedtodescribetheminibatchusedby\nminibatchstochasticgradientdescent.Typicallythetermâ€œbatchgradientdescentâ€",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "impliestheuseofthefulltrainingset,whiletheuseofthetermâ€œbatchâ€todescribe\nagroupofexamplesdoesnot.Â Forexample,itisverycommontousetheterm\nâ€œbatchsizeâ€todescribethesizeofaminibatch.\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\ncreatedexamplesratherthanfromaï¬xed-sizetrainingsetoverwhichseveral\npassesaremade.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "passesaremade.\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\n2 7 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\ncallthemstochasticmethods.\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\npresentedindetailinsection.8.3.1\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\nâ€¢Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\nlessthanlinearreturns.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "lessthanlinearreturns.\nâ€¢Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\nisnoreductioninthetimetoprocessaminibatch.\nâ€¢Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\nhardwaresetupsthisisthelimitingfactorinbatchsize.\nâ€¢Somekindsofhardwareachievebetterruntimewithspeciï¬csizesofarrays.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooï¬€er\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\nsometimesbeingattemptedforlargemodels.\nâ€¢Smallbatchescanoï¬€eraregularizingeï¬€ect( ,), WilsonandMartinez2003\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\nerrorisoftenbestforabatchsizeof1.Â Trainingwithsuchasmallbatch\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "duetotheneedtomakemoresteps,bothbecauseofthereducedlearning\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\nDiï¬€erentkindsofalgorithmsusediï¬€erentkindsofinformationfromthemini-\nbatchindiï¬€erentways.Somealgorithmsaremoresensitivetosamplingerrorthan\nothers,eitherbecausetheyuseinformationthatisdiï¬ƒculttoestimateaccurately\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "usuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\nHâˆ’ 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\nsizesarerequiredtominimizeï¬‚uctuationsintheestimatesofHâˆ’ 1g.Suppose\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\n2 7 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nHoritsinverseampliï¬espre-existingerrors,inthiscase,estimationerrorsing.\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\nHâˆ’ 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\napproximately,sotheupdateHâˆ’ 1gwillcontainevenmoreerrorthanwewould\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\nlistmightbearrangedsothatï¬rstwehaveï¬vebloodsamplestakenatdiï¬€erent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "timesfromtheï¬rstpatient,thenwehavethreebloodsamplestakenfromthe\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\nholdssomesigniï¬cance,itisnecessarytoshuï¬„etheexamplesbeforeselecting\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\nitisusuallysuï¬ƒcienttoshuï¬„etheorderofthedatasetonceandthenstoreitin\nshuï¬„edfashion.Thiswillimposeaï¬xedsetofpossibleminibatchesofconsecutive\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "signiï¬cantdetrimentaleï¬€ect.Failingtoevershuï¬„etheexamplesinanywaycan\nseriouslyreducetheeï¬€ectivenessofthealgorithm.\nManyoptimization problemsinmachinelearningdecomposeoverexamples\nwellenoughthatwecancomputeentireseparateupdatesoverdiï¬€erentexamples\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\ndiscussedfurtherinsection.12.1.3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "discussedfurtherinsection.12.1.3\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\n2 8 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ndescentshuï¬„ethedatasetonceandthenpassthroughitmultipletimes.Onthe\nï¬rstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\nnewfairsamplesfromthedatageneratingdistribution.\nThefactthatstochasticgradientdescentminimizesgeneralization erroris",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "easiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\nfromastreamofdata.Inotherwords,insteadofreceivingaï¬xed-sizetraining\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\nfrom p da t a.\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete.Â Inthis\ncase,thegeneralization error(equation)canbewrittenasasum 8.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "Jâˆ—() =Î¸î˜\nxî˜\nyp da t a()((;)) x , y L fxÎ¸ , y , (8.7)\nwiththeexactgradient\ng= âˆ‡ Î¸ Jâˆ—() =Î¸î˜\nxî˜\nyp da t a()x , yâˆ‡ Î¸ L f , y . ((;)xÎ¸)(8.8)\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\nundermildassumptionsregarding p da t aand. L\nHence,Â wecanobtainanunbiasedestimatoroftheexactgradientofÂ the",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "generalization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\nË†g=1\nmâˆ‡ Î¸î˜\niL f((x( ) i;)Î¸ , y( ) i) . (8.9)\nUpdatinginthedirectionof Î¸ Ë†gperformsSGDonthegeneralization error.\nOfcourse,Â thisinterpretation onlyÂ applies whenexamplesarenotreused.\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "unlessthetrainingsetisextremelylarge.Â When multiplesuchepochsareused,\nonlytheï¬rstepochfollowstheunbiasedgradientofthegeneralization error,but\n2 8 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofcourse,theadditionalepochsusuallyprovideenoughbeneï¬tduetodecreased\ntrainingerrortooï¬€settheharmtheycausebyincreasingthegapbetweentraining\nerrorandtesterror.\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\nset.Whenusinganextremelylargetrainingset,overï¬ttingisnotanissue,so",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "underï¬ttingandcomputational eï¬ƒciencybecomethepredominant concerns.See\nalso ()foradiscussionoftheeï¬€ectofcomputational BottouandBousquet2008\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\n8.2ChallengesinNeuralNetworkOptimization\nOptimization ingeneralisanextremelydiï¬ƒculttask.Traditionally,machine\nlearninghasavoidedthediï¬ƒcultyofgeneraloptimization bycarefullydesigning\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\nfortrainingdeepmodels.\n8.2.1Ill-Conditioning\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\nmoredetailinsection.4.3.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "moredetailinsection.4.3.1\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\nâ€œstuckâ€inthesensethatevenverysmallstepsincreasethecostfunction.\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\ncostfunctionpredictsthatagradientdescentstepofwilladd âˆ’ î€g\n1\n2î€2gî€¾Hggâˆ’ î€î€¾g (8.10)\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\n2î€2gî€¾Hg",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "2î€2gî€¾Hg\nexceeds î€gî€¾g.Â Todeterminewhetherill-conditioning isdetrimentaltoaneural\nnetworkÂ training task,Â oneÂ canmonitortheÂ squaredgradientnormgî€¾gand\n2 8 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nâˆ’50050100150200250\nTrainingtime(epochs)âˆ’20246810121416Gradient norm\n0 50100150200250\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiï¬cationerrorrate\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiï¬cation\nerrordecreasestoalowlevel.\nthegî€¾Hgterm.Inmanycases,thegradientnormdoesnotshrinksigniï¬cantly",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "throughoutlearning,butthegî€¾Hgtermgrowsbymorethananorderofmagnitude.\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\ncurvature.Figureshowsanexampleofthegradientincreasingsigniï¬cantly 8.1\nduringthesuccessfultrainingofaneuralnetwork.\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\ntraining,someofthetechniquesusedtocombatitinothercontextsareless",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "applicabletoneuralnetworks.Forexample,Newtonâ€™smethodisanexcellenttool\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\nthesubsequentsectionswewillarguethatNewtonâ€™smethodrequiressigniï¬cant\nmodiï¬cationbeforeitcanbeappliedtoneuralnetworks.\n8.2.2LocalMinima\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\ncanbereducedtotheproblemofï¬ndingalocalminimum.Anylocalminimumis\n2 8 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaï¬‚atregionat\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\naï¬‚atregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\nknowthatwehavereachedagoodsolutionifweï¬ndacriticalpointofanykind.\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "anextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\nnecessarilyamajorproblem.\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiï¬ability\nproblem.Amodelissaidtobeidentiï¬ableifasuï¬ƒcientlylargetrainingsetcan\nruleoutallbutonesettingofthemodelâ€™sparameters.Modelswithlatentvariables\nareoftennotidentiï¬ablebecausewecanobtainequivalentmodelsbyexchanging",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\nunits.Thiskindofnon-identiï¬abilit yisknownasweightspacesymmetry.\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\nadditionalcausesofnon-identiï¬abilit y.Forexample,inanyrectiï¬edlinearor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "maxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\nÎ±ifwealsoscaleallofitsoutgoingweightsby1\nÎ±.Thismeansthatâ€”ifthecost\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\nweightsratherthanthemodelsâ€™outputsâ€”everylocalminimumofarectiï¬edlinear\normaxoutnetworkliesonan( m nÃ—)-dimensionalhyperbolaofequivalentlocal\nminima.\nThesemodelidentiï¬abilityissuesmeanthattherecanbeanextremelylarge\norevenuncountablyinï¬niteamountoflocalminimainaneuralnetworkcost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "function.However,alloftheselocalminimaarisingfromnon-identiï¬abilit yare\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\nnotaproblematicformofnon-convexity.\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\noptimization algorithms.\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\n2 8 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\nnowsuspectthat,forsuï¬ƒcientlylargeneuralnetworks,mostlocalminimahavea\nlowcostfunctionvalue,andthatitisnotimportanttoï¬ndatrueglobalminimum",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "ratherthantoï¬ndapointinparameterspacethathaslowbutnotminimalcost\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\netal.,).2014\nManypractitioners attributenearlyalldiï¬ƒcultywithneuralnetworkoptimiza-\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeciï¬c\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\ninsigniï¬cantsize,theproblemisneitherlocalminimanoranyotherkindofcritical",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "point.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\nspaces,itcanbeverydiï¬ƒculttopositivelyestablishthatlocalminimaarethe\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "whileothershavealowercost.Â Atasaddlepoint,theHessianmatrixhasboth\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\nmaximumalonganothercross-section.Seeï¬gureforanillustration. 4.5\nManyclassesÂ ofrandomfunctionsexhibitthefollowingbehavior:inlow-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rnâ†’ Rof\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.Â The\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\nImaginethatthesignofeacheigenvalueisgeneratedbyï¬‚ippingacoin.Inasingle",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\n2 8 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.Â In\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\nheads ntimesifweareatacriticalpointwithlowcost.Â Thismeansthatlocal\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\nhighcostaremorelikelytobelocalmaxima.\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "proofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "deepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\nfunctionsrelatedtoneuralnetworksdoessoaswell.\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\nrithms?Forï¬rst-orderoptimization algorithmsthatuseonlygradientinformation,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\ngiveninï¬gure.Thesevisualizationsshowaï¬‚atteningofthecostfunctionnear 8.2\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\nmaybediï¬€erentformorerealisticusesofgradientdescent.\nForNewtonâ€™smethod,Â itisclearthatsaddlepointsconstituteaproblem.\n2 8 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nP r o j e c t i o n2o f Î¸\nP r o j e c t i o n 1 o f Î¸J(\n)Î¸\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\nwithpermissionfromGoodfellow2015 e t a l .().Â Thesevisualizationsappearsimilarfor\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\nvisualizationsusuallydonotshowmanyconspicuousobstacles.Â Priortothesuccessof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "stochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\nstructurethanisrevealedbytheseprojections.Â Theprimaryobstaclerevealedbythis\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\nMostoftrainingtimeisspenttraversingtherelativelyï¬‚atvalleyofthecostfunction,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\ninthisregion,orsimplytheneedtocircumnavigatethetallâ€œmountainâ€visibleinthe\nï¬gureviaanindirectarcingpath.\n2 8 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nGradientdescentisdesignedtomoveâ€œdownhillâ€andisnotexplicitlydesigned\ntoseekacriticalpoint.Newtonâ€™smethod,however,isdesignedtosolvefora\npointwherethegradientiszero.Withoutappropriatemodiï¬cation,itcanjump\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "saddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\nimprovessigniï¬cantlyoverthetraditionalversion.Second-order methodsremain\ndiï¬ƒculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\npromiseifitcouldbescaled.\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\npoints.Therearealsomaxima,Â whic haremuchlikesaddlepointsfromthe\nperspectiveofoptimizationâ€”many algorithmsarenotattractedtothem,Â but\nunmodiï¬edNewtonâ€™smethodis.Maximaofmanyclassesofrandomfunctions",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "becomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\nTheremayalsobewide,ï¬‚atregionsofconstantvalue.Intheselocations,the\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\nï¬‚atregionmustconsistentirelyofglobalminima,butinageneraloptimization\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\n8.2.4Cliï¬€sandExplodingGradients",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "8.2.4Cliï¬€sandExplodingGradients\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\ncliï¬€s,asillustratedinï¬gure.Theseresultfromthemultiplicationofseveral 8.3\nlargeweightstogether.Onthefaceofanextremelysteepcliï¬€structure,the\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoï¬€\nofthecliï¬€structurealtogether.\n2 8 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nî·\nî¢îŠî·î€» î¢\nî€¨î€©\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliï¬€region,a\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "optimizationworkthathadbeendone.Â FigureadaptedwithpermissionfromPascanu\ne t a l .().2013\nThecliï¬€canbedangerouswhetherweapproachitfromaboveorfrombelow,\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\nwithinaninï¬nitesimalregion.Whenthetraditionalgradientdescentalgorithm",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "proposestomakeaverylargestep,thegradientclippingheuristicintervenesto\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cliï¬€\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\nmultiplication.\n8.2.5Long-TermDependencies",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "multiplication.\n8.2.5Long-TermDependencies\nAnotherdiï¬ƒcultythatneuralnetworkoptimization algorithmsmustovercome\narisesÂ whenÂ thecomputationalÂ gra phÂ becomesÂ extremelyÂ deep.Feedforward\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\n2 8 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\npronounceddiï¬ƒculties.\nForexample,supposethatacomputational graphcontainsapaththatconsists\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(Î»)Vâˆ’ 1.\nInthissimplecase,itisstraightforwardtoseethat\nWt=î€€\nVÎ»Vdiag()âˆ’ 1î€t= ()VdiagÎ»tVâˆ’ 1. (8.11)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "Wt=î€€\nVÎ»Vdiag()âˆ’ 1î€t= ()VdiagÎ»tVâˆ’ 1. (8.11)\nAnyeigenvalues Î» ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\nvanishingandexplodinggradientproblemreferstothefactthatgradients\nthroughsuchagrapharealsoscaledaccordingtodiag(Î»)t.Vanishinggradients\nmakeitdiï¬ƒculttoknowwhichdirectiontheparametersshouldmovetoimprove\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliï¬€",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "structuresdescribedearlierthatmotivategradientclippingareanexampleofthe\nexplodinggradientphenomenon.\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\nsimilartothepowermethodalgorithmusedtoï¬ndthelargesteigenvalueof\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\nnotsurprisingthatxî€¾Wtwilleventuallydiscardallcomponentsofxthatare\northogonaltotheprincipaleigenvectorof.W\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\nvanishingandexplodinggradientproblem(,). Sussillo2014\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\n8.2.6InexactGradients\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\nanoisyorevenbiasedestimateofthesequantities.Â Nearlyeverydeeplearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "algorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\noftrainingexamplestocomputethegradient.\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\n2 9 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\nofaBoltzmannmachine.\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\nlossfunctionatasinglepointâ€”itcanbediï¬ƒculttomakeasinglestepif J(Î¸)is\npoorlyconditionedatthecurrentpointÎ¸,orifÎ¸liesonacliï¬€,orifÎ¸isasaddle\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\nnotpointtowarddistantregionsofmuchlowercost.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "notpointtowarddistantregionsofmuchlowercost.\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\nmountain-shapedstructure.\nMuchofresearchintothediï¬ƒcultiesofoptimization hasfocusedonwhether\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\nâˆ’log p( y|x;Î¸)canlackaglobalminimumpointandinsteadasymptotically\napproachsomevalueasthemodelbecomesmoreconï¬dent.Foraclassiï¬erwith\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "zero.Likewise,amodelofrealvalues p( y|x) =N( y; f(Î¸) , Î²âˆ’ 1)canhavenegative\nlog-likelihoodthatasymptotestonegativeinï¬nityâ€”if f(Î¸)isabletocorrectly\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\nÎ²withoutbound.Seeï¬gureforanexampleofafailureoflocaloptimization to 8.4\nï¬ndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\npoints.\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "inï¬‚uencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\n2 9 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nÎ¸J ( ) Î¸\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiï¬ƒcultyin\nthiscaseisbeinginitializedonthewrongsideoftheâ€œmountainâ€andnotbeingableto\ntraverseit.Â Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "suchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\nexcessivetrainingtime,asillustratedinï¬gure.8.2\noftheprocess.\nManyexistingresearchdirectionsareaimedatï¬ndinggoodinitialpointsfor\nproblemsthathavediï¬ƒcultglobalstructure,ratherthandevelopingalgorithms\nthatusenon-localmoves.\nGradientdescentandessentiallyalllearningalgorithmsthatareeï¬€ectivefor\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "canbediï¬ƒculttocompute.Wemaybeabletocomputesomepropertiesofthe\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\nnotdeï¬neareasonablyshortpathtoavalidsolution,butwearenotactually\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "thesecases,localdescentwithstepsofsize î€maydeï¬neareasonablyshortpath\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\nstepsofsize Î´ î€î€œ.Inthesecases,localdescentmayormaynotdeï¬neapath\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\n2 9 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\nthefunctionhasawideï¬‚atregion,orifwemanagetolandexactlyonacritical\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\nforcriticalpoints,suchasNewtonâ€™smethod).Inthesecases,localdescentdoes\nnotdeï¬neapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "ï¬gure,oralonganunnecessarilylongtrajectorytothesolution,asinï¬gure. 8.4 8.2\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\nmakingneuralnetworkoptimization diï¬ƒcult,andthisisanactiveareaofresearch.\nRegardlessofwhichoftheseproblemsaremostsigniï¬cant,allofthemmightbe\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\nwithinthatwell-behavedregion.Â Thislastviewsuggestsresearchintochoosing",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "goodinitialpointsfortraditionaloptimization algorithmstouse.\n8.2.8TheoreticalLimitsofOptimization\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\nlittlebearingontheuseofneuralnetworksinpractice.\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\nnetworkoutputÂ discretevalues.However,Â mostÂ neuralnetworkunitsoutput",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "smoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\nitcanbediï¬ƒculttotellwhetheraparticularproblemfallsintothatclass.Other\nresultsshowthatï¬ndingasolutionforanetworkofagivensizeisintractable,but\ninpracticewecanï¬ndasolutioneasilybyusingalargernetworkforwhichmany\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\ncontextofneuralnetworktraining,weusuallydonotcareaboutï¬ndingtheexact",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "minimumofafunction,butseekonlytoreduceitsvaluesuï¬ƒcientlytoobtaingood\ngeneralization error.Â Theoretical analysisofwhetheranoptimization algorithm\ncanaccomplishthisgoalisextremelydiï¬ƒcult.Developingmorerealisticbounds\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\ngoalformachinelearningresearch.\n2 9 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.3BasicAlgorithms\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\n8.3.1StochasticGradientDescent\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "optimization algorithmsformachinelearningingeneralandfordeeplearning\ninparticular.Â As discussedinsection,itispossibletoobtainanunbiased 8.1.3\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\nexamplesdrawni.i.dfromthedatageneratingdistribution.\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\nRequire:Learningrate î€ k.\nRequire:InitialparameterÎ¸\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate: Ë†gâ†+1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nApplyupdate:Î¸Î¸â† âˆ’ î€Ë†g\nendwhile\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\nhavedescribedSGDasusingaï¬xedlearningrate î€.Inpractice,itisnecessaryto\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\natiterationas k î€ k.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "atiterationas k î€ k.\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\ndescent,sobatchgradientdescentcanuseaï¬xedlearningrate.Asuï¬ƒcient\nconditiontoguaranteeconvergenceofSGDisthat\nâˆžî˜\nk = 1î€ k= and âˆž , (8.12)\n2 9 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nâˆžî˜\nk = 1î€2\nk < .âˆž (8.13)\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: Ï„\nî€ k= (1 )âˆ’ Î± î€ 0+ Î± î€ Ï„ (8.14)\nwith Î±=k\nÏ„.Afteriteration,itiscommontoleaveconstant. Ï„ î€\nThelearningratemaybechosenbytrialanderror,butitisusuallybest\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "theparameterstochooseare î€ 0, î€ Ï„,and Ï„.Usually Ï„maybesettothenumberof\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\nî€ Ï„shouldbesettoroughlythevalueof 1% î€ 0.Themainquestionishowtoset î€ 0.\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\nfunctionoftenincreasingsigniï¬cantly.Gentleoscillationsareï¬ne,especiallyif\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\nï¬nalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\naftertheï¬rst100iterationsorso.Therefore,itisusuallybesttomonitortheï¬rst\nseveraliterationsandusealearningratethatishigherthanthebest-performing\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\nconvergetowithinsomeï¬xedtoleranceofitsï¬naltestseterrorbeforeithas\nprocessedtheentiretrainingset.\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\nmeasuretheexcesserror J(Î¸)âˆ’min Î¸ J(Î¸),whichistheamountthatthecurrent",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "costfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\nproblem,theexcesserroris O(1âˆš\nk)after kiterations,whileinthestronglyconvex\ncaseitis O(1\nk).Theseboundscannotbeimprovedunlessextraconditionsare\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\ngradientdescentintheory.However,theCramÃ©r-Raobound(,;, CramÃ©r1946Rao\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\nk).Bottou\n2 9 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\nanoptimization algorithmthatconvergesfasterthan O(1\nk)formachinelearning\ntasksâ€”fasterconvergencepresumablycorrespondstooverï¬tting.Moreover,the\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\ntheremainderofthischapterachievebeneï¬tsthatmatterinpracticebutarelost\nintheconstantfactorsobscuredbythe O(1\nk)asymptoticanalysis.Onecanalso\ntradeoï¬€thebeneï¬tsofbothbatchandstochasticgradientdescentbygradually\nincreasingtheminibatchsizeduringthecourseoflearning.\nFormoreinformationonSGD,see(). Bottou1998\n8.3.2Momentum\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\nintheirdirection.Theeï¬€ectofmomentumisillustratedinï¬gure.8.5\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\nofvelocityâ€”itisthedirectionandspeedatwhichtheparametersmovethrough",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\naccordingtoNewtonâ€™slawsofmotion.Momentuminphysicsismasstimesvelocity.\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter Î±âˆˆ[0 ,1)\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "Theupdateruleisgivenby:\nvvâ† Î±âˆ’âˆ‡ î€ Î¸î€ \n1\nmm î˜\ni = 1L((fx( ) i;)Î¸ ,y( ) i)î€¡\n, (8.15)\nÎ¸Î¸v â† + . (8.16)\nThevelocityvaccumulatesthegradientelementsâˆ‡ Î¸î€€1\nmîm\ni = 1 L((fx( ) i;)Î¸ ,y( ) i)î€\n.\nThelarger Î±isrelativeto î€,themorepreviousgradientsaï¬€ectthecurrentdirection.\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\n2 9 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nâˆ’ âˆ’ âˆ’ 3 0 2 0 1 0 0 1 0 2 0âˆ’ 3 0âˆ’ 2 0âˆ’ 1 001 02 0\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\novercomestheï¬rstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\nnarrowaxisofthecanyon.Comparealsoï¬gure,whichshowsthebehaviorofgradient 4.6\ndescentwithoutmomentum.\n2 9 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\nobservesgradientg,thenitwillaccelerateinthedirectionofâˆ’g,untilreachinga\nterminalvelocitywherethesizeofeachstepis\nî€||||g\n1âˆ’ Î±. (8.17)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "î€||||g\n1âˆ’ Î±. (8.17)\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\n1 âˆ’ Î±.For\nexample, Î±= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\nthegradientdescentalgorithm.\nCommonvaluesof Î±usedinpracticeinclude .5, .9,and .99.Likethelearning\nrate, Î±mayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\nislaterraised.Itislessimportanttoadapt Î±overtimethantoshrink î€overtime.\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\nRequire:Learningrate,momentumparameter. î€ Î±",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "Require:Learningrate,momentumparameter. î€ Î±\nRequire:Initialparameter,initialvelocity. Î¸ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradientestimate:gâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nComputevelocityupdate:vvg â† Î±âˆ’ î€\nApplyupdate:Î¸Î¸v â† +\nendwhile\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "intuitionforhowthemomentumandgradientdescentalgorithmsbehave.\nThepositionoftheparticleatanypointintimeisgivenbyÎ¸( t).Theparticle\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\nf() = tâˆ‚2\nâˆ‚ t2Î¸() t . (8.18)\nRatherthanviewingthisasasecond-orderdiï¬€erentialequationoftheposition,\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\ntandrewritetheNewtoniandynamicsasaï¬rst-orderdiï¬€erentialequation:\nv() = tâˆ‚\nâˆ‚ tÎ¸() t , (8.19)\n2 9 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nf() = tâˆ‚\nâˆ‚ tv() t . (8.20)\nThemomentumalgorithmthenconsistsofsolvingthediï¬€erentialequationsvia\nnumericalsimulation.Asimplenumericalmethodforsolvingdiï¬€erentialequations\nisEulerâ€™smethod,whichsimplyconsistsofsimulatingthedynamicsdeï¬nedby\ntheequationbytakingsmall,ï¬nitestepsinthedirectionofeachgradient.\nThisexplainsthebasicformofthemomentumupdate,butwhatspeciï¬callyare\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "âˆ’âˆ‡ Î¸ J(Î¸).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\nuntilitbeginstogouphillagain.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "untilitbeginstogouphillagain.\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\notherforce,proportionaltoâˆ’v( t).Inphysicsterminology,thisforcecorresponds\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\nconvergetoalocalminimum.\nWhydoweuseâˆ’v( t)andviscousdraginparticular?Â Partofthereasonto\nuseâˆ’v( t)ismathematical convenienceâ€”anintegerpowerofthevelocityiseasy\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "velocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\nalocalminimum.Viscousdragavoidsbothoftheseproblemsâ€”itisweakenough\n2 9 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\n8.3.3NesterovMomentum\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\ninspiredbyNesterovâ€™sacceleratedgradientmethod(,,).The Nesterov19832004\nupdaterulesinthiscasearegivenby:\nvvâ† Î±âˆ’âˆ‡ î€ Î¸î€¢\n1\nmm î˜\ni = 1Lî€\nfx(( ) i;+ )Î¸ Î±v ,y( ) iî€‘î€£\n,(8.21)\nÎ¸Î¸v â† + , (8.22)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": ",(8.21)\nÎ¸Î¸v â† + , (8.22)\nwheretheparameters Î±and î€playasimilarroleasinthestandardmomentum\nmethod.Thediï¬€erencebetweenNesterovmomentumandstandardmomentumis\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "convergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\nbyNesterov1983().Unfortunately,Â inthestochasticgradientcase,Nesterov\nmomentumdoesnotimprovetherateofconvergence.\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\nRequire:Learningrate,momentumparameter. î€ Î±\nRequire:Initialparameter,initialvelocity. Î¸ v\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondinglabelsy( ) i.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "correspondinglabelsy( ) i.\nApplyinterimupdate: ËœÎ¸Î¸v â† + Î±\nComputegradient(atinterimpoint):gâ†1\nmâˆ‡ Ëœ Î¸î\ni L f((x( ) i;ËœÎ¸y) ,( ) i)\nComputevelocityupdate:vvg â† Î±âˆ’ î€\nApplyupdate:Î¸Î¸v â† +\nendwhile\n3 0 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.4ParameterInitializationStrategies\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "learningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\nmodelsisasuï¬ƒcientlydiï¬ƒculttaskthatmostalgorithmsarestronglyaï¬€ectedby\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\nencountersnumericaldiï¬ƒcultiesandfailsaltogether.Whenlearningdoesconverge,\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "convergestoapointwithhighÂ orlowcost.Also,Â pointsofcomparablecost\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaï¬€ectthe\ngeneralization aswell.\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\ninitialization strategiesisadiï¬ƒculttaskbecauseneuralnetworkoptimization is\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\nafterlearningbeginstoproceed.Afurtherdiï¬ƒcultyisthatsomeinitialpoints\nmaybebeneï¬cialfromtheviewpointofoptimization butdetrimentalfromthe\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaï¬€ects\ngeneralization isespeciallyprimitive,oï¬€eringlittletonoguidanceforhowtoselect\ntheinitialpoint.\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "parametersneedtoâ€œbreaksymmetryâ€Â betweendiï¬€erentunits.Iftwohidden\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\ntheseunitsmusthavediï¬€erentinitialparameters.Â Iftheyhavethesameinitial\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputediï¬€erent\nupdatesfordiï¬€erentunits(forexample,ifonetrainswithdropout),itisusually",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "besttoinitializeeachunittocomputeadiï¬€erentfunctionfromalloftheother\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\nofback-propagation. Thegoalofhavingeachunitcomputeadiï¬€erentfunction\n3 0 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\nforalargesetofbasisfunctionsthatareallmutuallydiï¬€erentfromeachother,\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\ndiï¬€erentfunctionfromeachotherunit.Randominitialization fromahigh-entropy",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "distributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\ntoassignanyunitstocomputethesamefunctionaseachother.\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\nchosenconstantsmuchlikethebiasesare.\nWealmostalwaysinitializealltheweightsinÂ themodelÂ tovaluesÂ drawn",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "randomlyÂ fromaÂ GaussianÂ oruniformÂ distribution.ThechoiceofÂ Gaussian\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\nlargeeï¬€ectonboththeoutcomeoftheoptimization procedureandontheability\nofthenetworktogeneralize.\nLargerinitialweightswillyieldastrongersymmetrybreakingeï¬€ect,helping\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "back-propagationthroughthelinearcomponentofeachlayerâ€”largervaluesinthe\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\nofthedeterministicforwardpropagationprocedureappearsrandom).Â Tosome\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\nTheperspectivesofregularizationandoptimization cangiveverydiï¬€erent\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\n3 0 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nduetotriggeringsomeearlystoppingcriterionbasedonoverï¬tting)expressesa\npriorthattheï¬nalparametersshouldbeclosetotheinitialparameters.Recall\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\ntheeï¬€ectofinitialization. WecanthinkofinitializingtheparametersÎ¸toÎ¸ 0as",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "beingsimilartoimposingaGaussianprior p(Î¸)withmeanÎ¸ 0.Fromthispoint\nofview,itmakessensetochooseÎ¸ 0tobenear0.Thispriorsaysthatitismore\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\npreferenceforthemtointeract.Ontheotherhand,ifweinitializeÎ¸ 0tolarge\nvalues,thenourpriorspeciï¬eswhichunitsshouldinteractwitheachother,and\nhowtheyshouldinteract.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "howtheyshouldinteract.\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\nnoutputsbysamplingeachweightfrom U(âˆ’1âˆšm,1âˆšm),whileGlorotandBengio\n()suggestusingthe 2010 normalizedinitialization\nW i , jâˆ¼ Uî€ \nâˆ’î²\n6\nm n+,î²\n6\nm n+î€¡\n. (8.23)\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "layerstohavethesamegradientvariance.Theformulaisderivedusingthe\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\nnonlinearcounterparts.\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ateachlayer.Theyderivespeciï¬cvaluesofthescalingfactorfordiï¬€erenttypesof\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "gradientsincreaseinnormastheypropagatebackward.Â  ()showed Sussillo2014\nthatsettingthegainfactorcorrectlyissuï¬ƒcienttotrainnetworksasdeepas\n3 0 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n1,000layers,withoutneedingtouseorthogonalinitializations.Â A keyinsightof\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\nbehavior.Thisisbecausefeedforwardnetworksuseadiï¬€erentweightmatrixat\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "ariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\noptimalperformance.Thismaybeforthreediï¬€erentreasons.First,wemay\nbeusingthewrongcriteriaâ€”itmaynotactuallybebeneï¬cialtopreservethe\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "increasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\nnotexactlyequaltothetheoreticalpredictions.\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\nsamestandarddeviation,suchas1âˆšm,isthateveryindividualweightbecomes\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "initializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\nvalues.Becauseittakesalongtimeforgradientdescenttoshrinkâ€œincorrectâ€large",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "values,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\nthathaveseveralï¬ltersthatmustbecarefullycoordinatedwitheachother.\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\nByrepeatedlyidentifyingtheï¬rstlayerwithunacceptably smallactivationsand\n3 0 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\nactivations.Â Thisprocedurecaninprinciplebeautomatedandisgenerallyless\ncomputationally costlythanhyperparameter optimization basedonvalidationset\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciï¬edmore\nformallyandstudiedby (). MishkinandMatas2015\nSoÂ farÂ weÂ haveÂ focusedÂ onÂ theÂ initializationÂ oftheÂ weights.Fortunately,\ninitialization ofotherparametersistypicallyeasier.\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "initialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\nnon-zerovalues:\nâ€¢Ifabiasisforanoutputunit,thenitisoftenbeneï¬cialtoinitializethebiasto\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\nonlybythebias.Thisjustiï¬essettingthebiastotheinverseoftheactivation\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\nForexample,iftheoutputisadistributionoverclassesandthisdistribution",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiï¬ersbutalsoto\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\nmatchthemarginaldistributionover.x\nâ€¢SometimeswemayÂ wanttochoosethebiastoavoidcausingtooÂ much",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "saturationatinitialization. Forexample,wemaysetthebiasofaReLU\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\nnotexpectstronginputfromthebiasesthough.Forexample,Â itisnot\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\nâ€¢Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "hâˆˆ[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h.Â We\n3 0 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ncanview hasagatethatdetermineswhether u h uâ‰ˆor u hâ‰ˆ0.Â Inthese\nsituations,wewanttosetthebiasfor hsothat hâ‰ˆ1mostofthetimeat\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\ntheLSTMmodel,describedinsection.10.10\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\nusingthemodel",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "usingthemodel\np y y (| Nx) = (|wTx+1) b , /Î² (8.24)\nwhere Î²isaprecisionparameter.Wecanusuallyinitializevarianceorprecision\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\nenoughtozerothatthebiasesmaybesetwhileignoringtheeï¬€ectoftheweights,\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\noï¬€ersfasterconvergencethanarandominitialization. Someoftheseinitialization\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "encodeinformationaboutthedistributionintheinitialparametersofthemodel.\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\ntherightscaleorsetdiï¬€erentunitstocomputediï¬€erentfunctionsfromeachother.\n8.5AlgorithmswithAdaptiveLearningRates\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\nofthehyperparameters thatisthemostdiï¬ƒculttosetbecauseithasasigniï¬cant\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\n3 0 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\nthecourseoflearning.\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "thenthelearningrateshoulddecrease.Â Ofcourse,thiskindofrulecanonlybe\nappliedtofullbatchoptimization.\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\nwillbrieï¬‚yreviewafewofthesealgorithms.\n8.5.1AdaGrad\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "parameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\nhavearelativelysmalldecreaseintheirlearningrate.Theneteï¬€ectisgreater\nprogressinthemoregentlyslopeddirectionsofparameterspace.\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthatâ€”for\ntrainingdeepneuralnetworkmodelsâ€”theaccumulation ofsquaredgradientsfrom",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "thebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\neï¬€ectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\nmodels.\n8.5.2RMSProp\nTheRMSPropalgorithm(,)modiï¬esAdaGradtoperformbetterin Hinton2012\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\ntoaconvexfunction.Â When appliedtoanon-convexfunctiontotrainaneural",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "network,thelearningtrajectorymaypassthroughmanydiï¬€erentstructuresand\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\n3 0 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.4TheAdaGradalgorithm\nRequire:Globallearningrate î€\nRequire:InitialparameterÎ¸\nRequire:Smallconstant,perhaps Î´ 10âˆ’ 7,fornumericalstability\nInitializegradientaccumulationvariabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:gâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nAccumulatesquaredgradient:rrgg â†+î€Œ\nComputeupdate: âˆ†Î¸â†âˆ’î€",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "Computeupdate: âˆ†Î¸â†âˆ’î€\nÎ´ +âˆšrî€Œg.(Divisionandsquarerootapplied\nelement-wise)\nApplyupdate:Î¸Î¸Î¸ â† +âˆ†\nendwhile\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\nextremepastsothatitcanconvergerapidlyafterï¬ndingaconvexbowl,asifit\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "movingaverageintroducesanewhyperparameter, Ï,thatcontrolsthelengthscale\nofthemovingaverage.\nEmpirically,RMSProphasbeenshowntobeaneï¬€ectiveandpracticalop-\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\n8.5.3Adam\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\nalgorithmandispresentedinalgorithm .Thenameâ€œAdamâ€Â derivesfrom 8.7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "thephraseâ€œadaptivemoments.â€Inthecontextoftheearlieralgorithms,itis\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\ndirectlyasanestimateoftheï¬rstordermoment(withexponentialweighting)of\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\n3 0 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.5TheRMSPropalgorithm\nRequire:Globallearningrate,decayrate. î€ Ï\nRequire:InitialparameterÎ¸\nRequire:Smallconstant Î´,Â usually 10âˆ’ 6,Â usedtostabilizedivisionÂ bysmall\nnumbers.\nInitializeaccumulation variablesr= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:gâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nAccumulatesquaredgradient:rrgg â† Ï+(1 )âˆ’ Ïî€Œ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "Accumulatesquaredgradient:rrgg â† Ï+(1 )âˆ’ Ïî€Œ\nComputeparameterupdate: âˆ†Î¸=âˆ’î€âˆš\nÎ´ + rî€Œg.(1âˆš\nÎ´ + rappliedelement-wise)\nApplyupdate:Î¸Î¸Î¸ â† +âˆ†\nendwhile\nbiascorrectionstotheestimatesofboththeï¬rst-ordermoments(themomentum\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\nthesuggesteddefault.\n8.5.4ChoosingtheRightOptimizationAlgorithm\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\nchoose?\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "presentedavaluablecomparisonofalargenumberofoptimization algorithms\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\n3 0 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\nRequire:Globallearningrate,decayrate,momentumcoeï¬ƒcient. î€ Ï Î±\nRequire:Initialparameter,initialvelocity. Î¸ v\nInitializeaccumulation variabler= 0\nwhile do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputeinterimupdate: ËœÎ¸Î¸v â† + Î±\nComputegradient:gâ†1\nmâˆ‡ Ëœ Î¸î\ni L f((x( ) i;ËœÎ¸y) ,( ) i)\nAccumulategradient:rrgg â† Ï+(1 )âˆ’ Ïî€Œ",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "Accumulategradient:rrgg â† Ï+(1 )âˆ’ Ïî€Œ\nComputevelocityupdate:vvâ† Î±âˆ’î€âˆšrî€Œg.(1âˆšrappliedelement-wise)\nApplyupdate:Î¸Î¸v â† +\nendwhile\nlargelyontheuserâ€™sfamiliaritywiththealgorithm(foreaseofhyperparameter\ntuning).\n8.6ApproximateSecond-OrderMethods\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\nrisk:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "risk:\nJ() = Î¸ E x ,y âˆ¼ Ë† pdata ( ) x , y[((;))] = L fxÎ¸ , y1\nmm î˜\ni = 1L f((x( ) i;)Î¸ , y( ) i) .(8.25)\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\ndiscussedinchapter.7\n8.6.1Newtonâ€™sMethod\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttoï¬rst- 4.3\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\noptimization. Themostwidelyusedsecond-ordermethodisNewtonâ€™smethod.We",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "nowdescribeNewtonâ€™smethodinmoredetail,withemphasisonitsapplicationto\nneuralnetworktraining.\n3 1 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.7TheAdamalgorithm\nRequire:Stepsize(Suggesteddefault: ) î€ 0001 .\nRequire:Exponentialdecayratesformomentestimates, Ï 1and Ï 2in[0 ,1).\n(Suggesteddefaults:andrespectively) 09 . 0999 .\nRequire:Smallconstant Î´usedfornumericalstabilization.(Suggesteddefault:\n10âˆ’ 8)\nRequire:InitialparametersÎ¸\nInitialize1stand2ndmomentvariables ,s= 0r= 0\nInitializetimestep t= 0\nwhile do stoppingcriterionnotmet",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "while do stoppingcriterionnotmet\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\ncorrespondingtargetsy( ) i.\nComputegradient:gâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nt tâ†+1\nUpdatebiasedï¬rstmomentestimate:sâ† Ï 1s+(1âˆ’ Ï 1)g\nUpdatebiasedsecondmomentestimate:râ† Ï 2r+(1âˆ’ Ï 2)ggî€Œ\nCorrectbiasinï¬rstmoment:Ë†sâ†s\n1 âˆ’ Ït\n1\nCorrectbiasinsecondmoment:Ë†râ†r\n1 âˆ’ Ït\n2\nComputeupdate: âˆ†= Î¸âˆ’ î€Ë†sâˆš\nË† r + Î´(operationsappliedelement-wise)\nApplyupdate:Î¸Î¸Î¸ â† +âˆ†\nendwhile",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "Applyupdate:Î¸Î¸Î¸ â† +âˆ†\nendwhile\nNewtonâ€™smethodisanoptimization schemebasedonusingasecond-orderTay-\nlorseriesexpansiontoapproximate J(Î¸)nearsomepointÎ¸ 0,ignoringderivatives\nofhigherorder:\nJ J () Î¸â‰ˆ(Î¸ 0)+(Î¸Î¸âˆ’ 0)î€¾âˆ‡ Î¸ J(Î¸ 0)+1\n2(Î¸Î¸âˆ’ 0)î€¾HÎ¸Î¸ (âˆ’ 0) ,(8.26)\nwhereHistheHessianof JwithrespecttoÎ¸evaluatedatÎ¸ 0.Ifwethensolvefor\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\nÎ¸âˆ—= Î¸ 0âˆ’Hâˆ’ 1âˆ‡ Î¸ J(Î¸ 0) (8.27)\nThusforalocallyquadraticfunction(withpositivedeï¬niteH),byrescaling",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "thegradientbyHâˆ’ 1,Newtonâ€™smethodjumpsdirectlytotheminimum.Â If the\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewtonâ€™s\nmethod,giveninalgorithm .8.8\n3 1 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nAlgorithm8.8Newtonâ€™smethodwithobjective J(Î¸) =\n1\nmîm\ni = 1 L f((x( ) i;)Î¸ , y( ) i).\nRequire:InitialparameterÎ¸ 0\nRequire:Trainingsetofexamples m\nwhile do stoppingcriterionnotmet\nComputegradient:gâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nComputeHessian:Hâ†1\nmâˆ‡2\nÎ¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nComputeHessianinverse:Hâˆ’ 1\nComputeupdate: âˆ†= Î¸âˆ’Hâˆ’ 1g\nApplyupdate:Î¸Î¸Î¸ = +âˆ†\nendwhile\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "deï¬nite,Newtonâ€™smethodcanbeappliediteratively.Thisimpliesatwo-step\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\ningthequadraticapproximation).Â Second, updatetheparametersaccordingto\nequation.8.27\nInsection,wediscussedhowNewtonâ€™smethodisappropriateonlywhen 8.2.3\ntheHessianispositivedeï¬nite.Indeeplearning,thesurfaceoftheobjective\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\nareproblematicforNewtonâ€™smethod.Â IftheeigenvaluesoftheHessianarenot",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "allpositive,forexample,nearasaddlepoint,thenNewtonâ€™smethodcanactually\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes Î±\nÎ¸âˆ—= Î¸ 0âˆ’[(( H fÎ¸ 0))+ ] Î±Iâˆ’ 1âˆ‡ Î¸ f(Î¸ 0) . (8.28)\nThisregularizationstrategyisusedinapproximations toNewtonâ€™smethod,such\nastheLevenbergâ€“Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\nvalueof Î±wouldhavetobesuï¬ƒcientlylargetooï¬€setthenegativeeigenvalues.\nHowever,as Î±increasesinsize,theHessianbecomesdominatedbythe Î±Idiagonal\nandthedirectionchosenbyNewtonâ€™smethodconvergestothestandardgradient\ndividedby Î±.Â Whenstrongnegativecurvatureispresent, Î±mayneedtobeso\nlargethatNewtonâ€™smethodwouldmakesmallerstepsthangradientdescentwith",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "aproperlychosenlearningrate.\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\n3 1 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nsuchassaddlepoints,theapplicationofNewtonâ€™smethodfortraininglargeneural\nnetworksislimitedbythesigniï¬cantcomputational burdenitimposes.The\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\nkcanbeinthemillions),Newtonâ€™smethodwouldrequiretheinversionofa k kÃ—\nmatrixâ€”with computational complexityof O( k3).Also,sincetheparameterswill",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "changewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\ncanbepracticallytrainedviaNewtonâ€™smethod.Intheremainderofthissection,\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewtonâ€™s\nmethodwhileside-steppingthecomputational hurdles.\n8.6.2ConjugateGradients\nConjugategradientsisamethodtoeï¬ƒcientlyavoidthecalculationoftheinverse\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineï¬€ective\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\nsearchdirection.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "searchdirection.\nLettheprevioussearchdirectionbed t âˆ’ 1.Attheminimum,wheretheline\nsearchterminates,thedirectionalderivativeiszeroindirectiond t âˆ’ 1:âˆ‡ Î¸ J(Î¸)Â·\nd t âˆ’ 1=0.Sincethegradientatthispointdeï¬nesthecurrentsearchdirection,\nd t=âˆ‡ Î¸ J(Î¸) willhavenocontributioninthedirectiond t âˆ’ 1.Thusd tisorthogonal\ntod t âˆ’ 1.Thisrelationshipbetweend t âˆ’ 1andd tisillustratedinï¬gurefor8.6\nmultipleiterationsofsteepestdescent.Asdemonstratedintheï¬gure,thechoiceof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\nseekstoaddressthisproblem.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "seekstoaddressthisproblem.\nInthemethodofconjugategradients,weseektoï¬ndasearchdirectionthat\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\n3 1 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n\u0000î€³ î€° \u0000î€² î€° \u0000î€± î€° î€° î€± î€° î€² î€°\u0000î€³ î€°\u0000î€² î€°\u0000î€± î€°î€°î€± î€°î€² î€°\nFigure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The\nmethodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline\ndeï¬nedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems\nseenwithusingaï¬xedlearningrateinï¬gure,butevenwiththeoptimalstepsize 4.6\nthealgorithmstillmakesback-and-forthprogresstowardtheoptimum.Bydeï¬nition,at",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "theminimumoftheobjectivealongagivendirection,thegradientattheï¬nalpointis\northogonaltothatdirection.\ntheform:\nd t= âˆ‡ Î¸ J Î² ()+Î¸ td t âˆ’ 1 (8.29)\nwhere Î² tisacoeï¬ƒcientwhosemagnitudecontrolshowmuchofthedirection,d t âˆ’ 1,\nweshouldaddbacktothecurrentsearchdirection.\nTwodirections,d tandd t âˆ’ 1,aredeï¬nedasconjugateifdî€¾\ntHd t âˆ’ 1= 0,where\nHistheHessianmatrix.\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\neigenvectorsofHtochoose Î² t,whichwouldnotsatisfyourgoalofdeveloping",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "amethodthatismorecomputationally viablethanNewtonâ€™smethodforlarge\nproblems.Â Canwecalculatetheconjugatedirectionswithoutresortingtothese\ncalculations?Fortunatelytheanswertothatisyes.\nTwopopularmethodsforcomputingthe Î² tare:\n1.Â Fletcher-Reeves:\nÎ² t=âˆ‡ Î¸ J(Î¸ t)î€¾âˆ‡ Î¸ J(Î¸ t)\nâˆ‡ Î¸ J(Î¸ t âˆ’ 1)î€¾âˆ‡ Î¸ J(Î¸ t âˆ’ 1)(8.30)\n3 1 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n2.Â Polak-RibiÃ¨re:\nÎ² t=(âˆ‡ Î¸ J(Î¸ t)âˆ’âˆ‡ Î¸ J(Î¸ t âˆ’ 1))î€¾âˆ‡ Î¸ J(Î¸ t)\nâˆ‡ Î¸ J(Î¸ t âˆ’ 1)î€¾âˆ‡ Î¸ J(Î¸ t âˆ’ 1)(8.31)\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\nthepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe\nminimumalongthepreviousdirections.Asaconsequence,ina k-dimensional\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\nachievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "Algorithm8.9Theconjugategradientmethod\nRequire:InitialparametersÎ¸ 0\nRequire:Trainingsetofexamples m\nInitializeÏ 0= 0\nInitialize g 0= 0\nInitialize t= 1\nwhile do stoppingcriterionnotmet\nInitializethegradientg t= 0\nComputegradient:g tâ†1\nmâˆ‡ Î¸î\ni L f((x( ) i;)Î¸ ,y( ) i)\nCompute Î² t=( g t âˆ’ g t âˆ’1 )î€¾g t\ngî€¾\nt âˆ’1g t âˆ’1(Polak-RibiÃ¨re)\n(Nonlinearconjugategradient:optionallyreset Î² ttozero,forexampleif tis\namultipleofsomeconstant,suchas) k k= 5\nComputesearchdirection:Ï t= âˆ’g t+ Î² tÏ t âˆ’ 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "Computesearchdirection:Ï t= âˆ’g t+ Î² tÏ t âˆ’ 1\nPerformlinesearchtoï¬nd: î€âˆ—= argmin î€1\nmîm\ni = 1 L f((x( ) i;Î¸ t+ î€Ï t) ,y( ) i)\n(Onatrulyquadraticcostfunction,analyticallysolvefor î€âˆ—ratherthan\nexplicitlysearchingforit)\nApplyupdate:Î¸ t + 1= Î¸ t+ î€âˆ—Ï t\nt tâ†+1\nendwhile\nNonlinearConjugateGradients:Sofarwehavediscussedthemethodof\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.Â Ofcourse,\nourprimaryinterestinthischapteristoexploreoptimization methodsfortraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\nobjectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiï¬cation.\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\n3 1 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\nsearchalongtheunalteredgradient.\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneï¬cialto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\n2011).Â Adaptationsofconjugategradientsspeciï¬callyforneuralnetworkshave\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\n1993).\n8.6.3BFGS",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "1993).\n8.6.3BFGS\nTheBroydenâ€“Fletcherâ€“Goldfarbâ€“Shanno(BFGS)algorithmattemptsto\nbringsomeoftheadvantagesofNewtonâ€™smethodwithoutthecomputational\nburden.InÂ thatrespect,Â BFGSissimilartotheconjugategradientmethod.\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewtonâ€™s\nupdate.RecallthatNewtonâ€™supdateisgivenby\nÎ¸âˆ—= Î¸ 0âˆ’Hâˆ’ 1âˆ‡ Î¸ J(Î¸ 0) , (8.32)\nwhereHistheHessianof JwithrespecttoÎ¸evaluatedatÎ¸ 0.Theprimary\ncomputational diï¬ƒcultyinapplyingNewtonâ€™supdateisthecalculationofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "inverseHessianHâˆ’ 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\namatrixM tthatisiterativelyreï¬nedbylowrankupdatestobecomeabetter\napproximationofHâˆ’ 1.\nThespeciï¬cationandderivationoftheBFGSapproximationisgiveninmany\ntextbooksonoptimization, includingLuenberger1984().\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\nÏ tisdeterminedbyÏ t=M tg t.Alinesearchisperformedinthisdirectionto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "determinethesizeofthestep, î€âˆ—,takeninthisdirection.Theï¬nalupdatetothe\nparametersisgivenby:\nÎ¸ t + 1= Î¸ t+ î€âˆ—Ï t . (8.33)\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\n3 1 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\nonthelinesearchï¬ndingapointveryclosetothetrueminimumalongtheline.\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\nlesstimereï¬ningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\nparameters.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nLimitedÂ MemoryÂ BFGSÂ (orÂ L-BFGS)TheÂ memory costsÂ oftheÂ BFGS\nalgorithmcanbesigniï¬cantlydecreasedbyavoidingstoringthecompleteinverse\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\nthatM( 1 ) t âˆ’istheidentitymatrix,ratherthanstoringtheapproximation fromone\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdeï¬nedbyL-BFGS\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "procedureremainswellbehavedwhentheminimumofthelinesearchisreached\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\n8.7OptimizationStrategiesandMeta-Algorithms\nManyoptimization techniquesarenotexactlyalgorithms,Â butrathergeneral\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\nincorporatedintomanydiï¬€erentalgorithms.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "incorporatedintomanydiï¬€erentalgorithms.\n8.7.1BatchNormalization\nBatchnormalization ( ,)isoneofthemostexcitingrecent Ioï¬€eandSzegedy2015\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\nbythediï¬ƒcultyoftrainingverydeepmodels.\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "layersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\n3 1 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\nanddoesnotuseanactivationfunctionateachhiddenlayer:Ë† y= x w 1 w 2 w 3 . . . w l.\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i âˆ’ 1 w i.\nTheoutput Ë† yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\nweights w i.Supposeourcostfunctionhasputagradientofon1 Ë† y,sowewishto\ndecreaseË† yslightly.Theback-propagationalgorithmcanthencomputeagradient",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "g=âˆ‡ wË† y.Considerwhathappenswhenwemakeanupdatewwg â† âˆ’ î€.The\nï¬rst-orderTaylorseriesapproximation ofË† ypredictsthatthevalueofË† ywilldecrease\nby î€gî€¾g.IfwewantedtodecreaseË† yby .1,thisï¬rst-orderinformationavailablein\nthegradientsuggestswecouldsetthelearningrate î€to. 1\ngî€¾g.However,theactual\nupdatewillincludesecond-orderandthird-ordereï¬€ects,onuptoeï¬€ectsoforder l.\nThenewvalueofË† yisgivenby\nx w( 1âˆ’ î€ g 1)( w 2âˆ’ î€ g 2)( . . . w lâˆ’ î€ g l) . (8.34)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "Anexampleofonesecond-ordertermarisingfromthisupdateis î€2g 1 g 2î‘l\ni = 3 w i.\nThistermmightbenegligibleifî‘l\ni = 3 w iissmall,ormightbeexponentiallylarge\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\ntochooseanappropriatelearningrate,becausetheeï¬€ectsofanupdatetothe\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\nevenhigher-orderinteractionscanbesigniï¬cant.Evensecond-orderoptimization\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\nthemfromtrulyaccountingforallsigniï¬cantsecond-orderinteractions. Building\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\ndoinstead?\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "network.Thereparametrization signiï¬cantlyreducestheproblemofcoordinating\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\nHî€°=HÂµâˆ’\nÏƒ, (8.35)\nwhereÂµisavectorcontainingthemeanofeachunitandÏƒisavectorcontaining\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 217,
      "type": "default"
    }
  },
  {
    "content": "thevectorÂµandthevectorÏƒtobeappliedtoeveryrowofthematrixH.Within\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting Âµ j\n3 1 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 218,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nanddividingby Ïƒ j.TherestofthenetworkthenoperatesonHî€°inexactlythe\nsamewaythattheoriginalnetworkoperatedon.H\nAttrainingtime,\nÂµ=1\nmî˜\niH i , : (8.36)\nand\nÏƒ=î³\nÎ´+1\nmî˜\ni( )HÂµâˆ’2\ni , (8.37)\nwhere Î´isasmallpositivevaluesuchas10âˆ’ 8imposedtoavoidencountering\ntheundeï¬nedgradientofâˆšzat z=0.Crucially,Â weback-propagatethrough\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 219,
      "type": "default"
    }
  },
  {
    "content": "anoperationÂ that actssimplytoincreasethestandardÂ deviationormeanof\nh i;thenormalization operationsremovetheeï¬€ectofsuchanactionandzero\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\nnormalization approach.Â Previous approacheshadinvolvedaddingpenaltiesto\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 220,
      "type": "default"
    }
  },
  {
    "content": "usuallyresultedinsigniï¬cantwastedtimeasthelearningalgorithmrepeatedly\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\nunitsalwaysbestandardizedbydeï¬nition,deftlysidesteppingbothproblems.\nAttesttime,ÂµandÏƒmaybereplacedbyrunningaveragesthatwerecollected\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\nwithoutneedingtousedeï¬nitionsofÂµandÏƒthatdependonanentireminibatch.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 221,
      "type": "default"
    }
  },
  {
    "content": "RevisitingtheË† y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\ndiï¬ƒcultiesinlearningthismodelbynormalizing h l âˆ’ 1.Supposethat xisdrawn\nfromaunitGaussian.Then h l âˆ’ 1willalsocomefromaGaussian,becausethe\ntransformationfrom xto h lislinear.However, h l âˆ’ 1willnolongerhavezeromean\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\nË†h l âˆ’ 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 222,
      "type": "default"
    }
  },
  {
    "content": "updatetothelowerlayers,Ë†h l âˆ’ 1willremainaunitGaussian.Theoutput Ë† ymay\nthenbelearnedasasimplelinearfunction Ë† y= w lË† h l âˆ’ 1.Learninginthismodelis\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\neï¬€ectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.Â In\nsomecornercases,thelowerlayerscanhaveaneï¬€ect.Changingoneofthelower\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\n3 1 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 223,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nofoneofthelowerweightscanï¬‚iptherelationshipbetweenË† h l âˆ’ 1and y.Â These\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\nanextremeeï¬€ectonthestatisticsof h l âˆ’ 1.Batchnormalization hasthusmade\nthismodelsigniï¬cantlyeasiertolearn.Â Inthisexample,theeaseoflearningof\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\nthelowerlayersnolongerhaveanyharmfuleï¬€ect,buttheyalsonolongerhave",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 224,
      "type": "default"
    }
  },
  {
    "content": "anybeneï¬cialeï¬€ect.Thisisbecausewehavenormalizedouttheï¬rstandsecond\norderstatistics,whichisallthatalinearnetworkcaninï¬‚uence.Inadeepneural\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\nunittochange.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 225,
      "type": "default"
    }
  },
  {
    "content": "unittochange.\nBecausetheï¬nallayerofthenetworkisabletolearnalineartransformation,\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\ntheinspirationforbatchnormalization. Unfortunately,Â eliminating alllinear\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\npracticalapproach.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 226,
      "type": "default"
    }
  },
  {
    "content": "practicalapproach.\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\npoweroftheÂ neuralnetworkcontainingthatunit.Inordertomaintainthe\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\nactivationsHwithÎ³Hî€°+Î²ratherthansimplythenormalizedHî€°.Thevariables\nÎ³andÎ²arelearnedparametersthatallowthenewvariabletohaveanymean\nandstandarddeviation.Atï¬rstglance,thismayseemuselessâ€”whydidweset\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 227,
      "type": "default"
    }
  },
  {
    "content": "anyarbitraryvalueÎ²?Theansweristhatthenewparametrization canrepresent\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\nparametrization hasdiï¬€erentlearningdynamics.Intheoldparametrization, the\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\ninthelayersbelowH.Inthenewparametrization, themeanofÎ³Hî€°+Î²is\ndeterminedsolelybyÎ².Thenewparametrization ismucheasiertolearnwith\ngradientdescent.\nMostneuralnetworklayerstaketheformof Ï†(XW+b)where Ï†issome",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 228,
      "type": "default"
    }
  },
  {
    "content": "ï¬xednonlinearactivationfunctionsuchastherectiï¬edlineartransformation.It\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\nX,ortothetransformedvalueXW+b. ()recommend Ioï¬€eandSzegedy2015\n3 2 0",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 229,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nthelatter.Morespeciï¬cally,XW+bshouldbereplacedbyanormalizedversion\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\nthe Î²parameterappliedbythebatchnormalization reparametrization. Theinput\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\nrectiï¬edlinearfunctioninapreviouslayer.Â Thestatisticsoftheinputarethus\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 230,
      "type": "default"
    }
  },
  {
    "content": "Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\nsamenormalizing Âµand Ïƒateveryspatiallocationwithinafeaturemap,sothat\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\n8.7.2CoordinateDescent\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\nvariable x i,Â thenÂ minimizeÂ it withÂ respectÂ toÂ another variable x jandÂ soon,",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 231,
      "type": "default"
    }
  },
  {
    "content": "repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\nonecoordinateatatime.Â Moregenerally,blockcoordinatedescentrefersto\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\nâ€œcoordinatedescentâ€isoftenusedtorefertoblockcoordinatedescentaswellas\nthestrictlyindividualcoordinatedescent.\nCoordinatedescentmakesthemostsensewhenthediï¬€erentvariablesinthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 232,
      "type": "default"
    }
  },
  {
    "content": "optimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\nsigniï¬cantlymoreeï¬ƒcientthanoptimization withrespecttoallofthevariables.\nForexample,considerthecostfunction\nJ ,(HW) =î˜\ni , j| H i , j|+î˜\ni , jî€\nXWâˆ’î€¾Hî€‘2\ni , j.(8.38)\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\ntoï¬ndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 233,
      "type": "default"
    }
  },
  {
    "content": "HtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\nThefunction Jisnotconvex.However,Â wecandividetheinputstothe\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\n3 2 1",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 234,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nusanoptimization strategythatallowsustouseeï¬ƒcientconvexoptimization\nalgorithms,byalternatingbetweenoptimizingWwithHï¬xed,thenoptimizing\nHWwithï¬xed.\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\nstronglyinï¬‚uencestheoptimalvalueofanothervariable,asinthefunction f(x) =\n( x 1âˆ’ x 2)2+ Î±î€€\nx2\n1+ x2\n2î€\nwhere Î±isapositiveconstant.Theï¬rsttermencourages\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 235,
      "type": "default"
    }
  },
  {
    "content": "tobenearzero.Thesolutionistosetbothtozero.Newtonâ€™smethodcansolve\ntheprobleminasinglestepbecauseitisapositivedeï¬nitequadraticproblem.\nHowever,forsmall Î±,coordinatedescentwillmakeveryslowprogressbecausethe\nï¬rsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiï¬€ers\nsigniï¬cantlyfromthecurrentvalueoftheothervariable.\n8.7.3PolyakAveraging\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\npointsÂ intheÂ trajectoryÂ through parameterÂ spacevisitedÂ byÂ anoptimization",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 236,
      "type": "default"
    }
  },
  {
    "content": "algorithm.Â If titerationsofgradientdescentvisitpointsÎ¸( 1 ), . . . ,Î¸( ) t,thenthe\noutputofthePolyakaveragingalgorithmisË†Î¸( ) t=1\ntî\niÎ¸( ) i.Â Onsomeproblem\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiï¬cation\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 237,
      "type": "default"
    }
  },
  {
    "content": "thelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\nverycomplicatedandvisitmanydiï¬€erentregions.Includingpointsinparameter\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\nexponentiallydecayingrunningaverage:\nË†Î¸( ) t= Î±Ë†Î¸( 1 ) t âˆ’+(1 )âˆ’ Î±Î¸( ) t. (8.39)",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 238,
      "type": "default"
    }
  },
  {
    "content": "Ë†Î¸( ) t= Î±Ë†Î¸( 1 ) t âˆ’+(1 )âˆ’ Î±Î¸( ) t. (8.39)\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\netal.()forarecentexample. 2015\n3 2 2",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 239,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.4SupervisedPretraining\nSometimes,directlytrainingamodeltosolveaspeciï¬ctaskcanbetooambitious\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydiï¬ƒcult.Itis\nsometimesmoreeï¬€ectivetotrainasimplermodeltosolvethetask,thenmake\nthemodelmorecomplex.Itcanalsobemoreeï¬€ectivetotrainthemodeltosolve\nasimplertask,thenmoveontoconfronttheï¬naltask.Thesestrategiesthat\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 240,
      "type": "default"
    }
  },
  {
    "content": "trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\npretraining.\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 241,
      "type": "default"
    }
  },
  {
    "content": "ï¬ne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\nï¬nds.\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\ndeeplearning.Inthissection,wedescribespeciï¬callythosepretrainingalgorithms\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\nproblems.Thisapproachisknownas . greedysupervisedpretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 242,
      "type": "default"
    }
  },
  {
    "content": "Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\nthelayersintheï¬nalneuralnetwork.Anexampleofgreedysupervisedpretraining\nisillustratedinï¬gure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 243,
      "type": "default"
    }
  },
  {
    "content": "theï¬rstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\nWhyÂ wouldÂ greedyÂ sup ervisedÂ pretraining help?TheÂ hypothesis Â initially\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\n3 2 3",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 244,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\ny y\nh( 1 )h( 1 )\nx x\n( a )U( 1 )U( 1 )\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\nx x\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\ny yh( 1 )h( 1 )\nx x\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\ny yh( 1 )h( 1 )\nx x\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\nU( 2 )U( 2 )\nW( 2 )W( 2 )\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\n( a )Westartbytrainingasuï¬ƒcientlyshallowarchitecture.Anotherdrawingofthe ( b )",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 245,
      "type": "default"
    }
  },
  {
    "content": "samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\ndiscardthehidden-to-outputlayer.Wesendtheoutputoftheï¬rsthiddenlayerasinput\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\nastheï¬rstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\nTofurtherimprovetheoptimization,wecanjointlyï¬ne-tuneallthelayers,eitheronlyat\ntheendorateachstageofthisprocess.\n3 2 4",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 246,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\ntermsofoptimization andintermsofgeneralization.\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\nandtheninitializeasame-sizenetworkwiththeï¬rst klayersoftheï¬rstnet.All",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 247,
      "type": "default"
    }
  },
  {
    "content": "thelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\nthenjointlytrainedtoperformadiï¬€erentsetoftasks(anothersubsetofthe1000\nImageNetobjectcategories),withfewertrainingexamplesthanfortheï¬rstsetof\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\nsection.15.2\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 248,
      "type": "default"
    }
  },
  {
    "content": "becomesateacherforasecondnetwork,designatedthestudent.Thestudent\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\ndiï¬ƒculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 249,
      "type": "default"
    }
  },
  {
    "content": "parametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\ntheï¬nalclassiï¬cationtarget,theobjectiveistopredictthemiddlehiddenlayer\noftheteachernetwork.Â Thelowerlayersofthestudentnetworksthushavetwo\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\nanddeepnetworkappearstobemorediï¬ƒculttotrainthanawideandshallow",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 250,
      "type": "default"
    }
  },
  {
    "content": "network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiï¬ƒcultto\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\nsolvetheproblem.\n3 2 5",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 251,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n8.7.5DesigningModelstoAidOptimization\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\ncomefromdesigningthemodelstobeeasiertooptimize.\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\ndiï¬ƒcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 252,
      "type": "default"
    }
  },
  {
    "content": "optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\nSpeciï¬cally,modernneuralnetworksreï¬‚ectadesignchoicetouselineartrans-\nformationsbetweenlayersandactivationfunctionsthatarediï¬€erentiable almost",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 253,
      "type": "default"
    }
  },
  {
    "content": "everywhereandhavesigniï¬cantslopeinlargeportionsoftheirdomain.Â Inpar-\nticular,modelinnovationsliketheLSTM,rectiï¬edlinearunitsandmaxoutunits\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\noptimization easier.Thegradientï¬‚owsthroughmanylayersprovidedthatthe\nJacobianofthelineartransformationhasreasonablesingularvalues.Â Moreover,\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodelâ€™s",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 254,
      "type": "default"
    }
  },
  {
    "content": "outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\ncorrespondsreasonablywelltomovingtowardadistantsolution.\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\ntheshortestpathfromthelowerÂ layerâ€™sparametersÂ totheoutput,Â and thus",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 255,
      "type": "default"
    }
  },
  {
    "content": "mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\nanddeeply-supervisednets(,).Theseâ€œauxiliaryheadsâ€aretrained Leeetal.2014\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\ntheauxiliaryheadsmaybediscarded.Â Thisisanalternativetothepretraining",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 256,
      "type": "default"
    }
  },
  {
    "content": "strategies,whichwereintroducedintheprevioussection.Inthisway,onecan\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\n3 2 6",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 257,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\n8.7.6ContinuationMethodsandCurriculumLearning\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\nproblemistoattempttoinitializetheparametersinaregionthatisconnected",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 258,
      "type": "default"
    }
  },
  {
    "content": "tothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\ndiscover.\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\nminimizeacostfunction J(Î¸),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 259,
      "type": "default"
    }
  },
  {
    "content": "Thesecostfunctionsaredesignedtobeincreasinglydiï¬ƒcult,with J( 0 )beingfairly\neasytominimize,and J( ) n,themostdiï¬ƒcult,being J(Î¸),thetruecostfunction\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\nmeanthatitiswellbehavedovermoreofÎ¸space.Arandominitialization ismore\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 260,
      "type": "default"
    }
  },
  {
    "content": "solvinganeasyproblemthenreï¬nethesolutiontosolveincrementally harder\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\nwhichaddsnoisetotheparameters(KirkpatrickÂ 1983etal.,).Continuation",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 261,
      "type": "default"
    }
  },
  {
    "content": "methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\novercomingthechallengeoflocalminima.Speciï¬cally,theyweredesignedto\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\nthesecontinuationmethodswouldconstructeasiercostfunctionsbyâ€œblurringâ€the\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 262,
      "type": "default"
    }
  },
  {
    "content": "J( ) i() = Î¸ EÎ¸î€°âˆ¼ N ( Î¸î€°; Î¸ , Ïƒ()2 i) J(Î¸î€°) (8.40)\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\n3 2 7",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 263,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\nenoughinformationaboutthelocationofaglobalminimumthatwecanï¬ndthe\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\napproachcanbreakdowninthreediï¬€erentways.First,itmightsuccessfullydeï¬ne\naseriesofcostfunctionswheretheï¬rstisconvexandtheoptimumtracksfrom\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 264,
      "type": "default"
    }
  },
  {
    "content": "manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(Î¸) =âˆ’Î¸î€¾Î¸.\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 265,
      "type": "default"
    }
  },
  {
    "content": "originalcostfunction.\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\neliminateï¬‚atregions,decreasevarianceingradientestimates,improveconditioning\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 266,
      "type": "default"
    }
  },
  {
    "content": "easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\nandprogresstowardaglobalsolution.\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 267,
      "type": "default"
    }
  },
  {
    "content": "training(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\nlearning(,;,;,). () Solomonoï¬€1989Elman1993Sanger1994Bengioetal.2009\njustiï¬edthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\nincreasingtheinï¬‚uenceofsimplerexamples(eitherbyassigningtheircontributions\ntothecostfunctionlargercoeï¬ƒcients,orbysamplingthemmorefrequently),and\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 268,
      "type": "default"
    }
  },
  {
    "content": "hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\ntasks.Curriculumlearningwasalsoveriï¬edasbeingconsistentwiththewayin\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\n3 2 8",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 269,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\nmoreprototypicalexamplesandthenhelpthelearnerreï¬nethedecisionsurface\nwiththelessobviouscases.Curriculum-based strategiesaremoreeï¬€ectivefor\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\nalsoincreasetheeï¬€ectivenessofotherteachingstrategies( , BasuandChristensen\n2013).\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 270,
      "type": "default"
    }
  },
  {
    "content": "ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\nstochasticcurriculum,inwhicharandommixofeasyanddiï¬ƒcultexamplesisalways\npresentedtothelearner,butwheretheaverageproportionofthemorediï¬ƒcult\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\nfromthefulltrainingset)wasobserved.\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 271,
      "type": "default"
    }
  },
  {
    "content": "regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\nlittleornomodiï¬cation.\n3 2 9",
    "metadata": {
      "source": "[13]part-2-chapter-8.pdf",
      "chunk_id": 272,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 9\nC on v ol u t i on al N e t w orks\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\ntremendouslysuccessfulinpracticalapplications.Thenameâ€œconvolutionalneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "networkâ€indicatesthatthenetworkemploysamathematical operationcalled\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\nmultiplicationinatleastoneoftheirlayers.\nInthisÂ chapter,Â wewillï¬rstÂ describewhatconvolutionis.Next,Â wewill\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "employ.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\ncorrespondpreciselytothedeï¬nitionofconvolutionasusedinotherï¬eldssuch\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\nwillalsoÂ showÂ howÂ convolutionmaybeappliedtomanykindsofdata,Â with\ndiï¬€erentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\nmoreeï¬ƒcient.Convolutionalnetworksstandoutasanexampleofneuroscientiï¬c",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "principlesinï¬‚uencingdeeplearning.Wewilldiscusstheseneuroscientiï¬cprinciples,\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\n330",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\nhere.\n9.1TheConvolutionOperation\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "valuedargument.Tomotivatethedeï¬nitionofconvolution,westartwithexamples\noftwofunctionswemightuse.\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\nt.Both xand tarereal-valued,i.e.,wecangetadiï¬€erentreadingfromthelaser\nsensoratanyinstantintime.\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\nestimateofthespaceshipâ€™sposition,wewouldliketoaveragetogetherseveral",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "measurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\ns t() =îš\nx a w t a d a ()( âˆ’) (9.1)\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\ndenotedwithanasterisk:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "denotedwithanasterisk:\ns t x w t () = ( âˆ—)() (9.2)\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeï¬ned\nforanyfunctionsforwhichtheaboveintegralisdeï¬ned,andmaybeusedforother\npurposesbesidestakingweightedaverages.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "purposesbesidestakingweightedaverages.\nInconvolutionalnetworkterminology,theï¬rstargument(inthisexample,the\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\n3 3 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\nreferredtoasthe . f e at ur e m ap\nInourexample,theideaofalasersensorthatcanprovidemeasurements\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "integervalues.Ifwenowassumethat xand waredeï¬nedonlyoninteger t,we\ncandeï¬nethediscreteconvolution:\ns t x w t () = ( âˆ—)() =âˆžî˜\na = âˆ’ âˆžx a w t a ()( âˆ’) (9.3)\nInmachinelearningapplications,theinputisusuallyamultidimensional array\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "separately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\nï¬nitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\ncanimplementtheinï¬nitesummationasasummationoveraï¬nitenumberof\narrayelements.\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\ntouseatwo-dimensionalkernel: K\nS i , j I K i , j () = ( âˆ—)() =î˜\nmî˜\nnI m , n K i m , j n . ( )( âˆ’ âˆ’)(9.4)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "mî˜\nnI m , n K i m , j n . ( )( âˆ’ âˆ’)(9.4)\nConvolutioniscommutative,meaningwecanequivalentlywrite:\nS i , j K I i , j () = ( âˆ—)() =î˜\nmî˜\nnI i m , j n K m , n . ( âˆ’ âˆ’)( )(9.5)\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\nand. n\nThecommutativepropertyofconvolutionarisesbecausewehave ï¬‚i pp e dthe\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "inputincreases,buttheindexintothekerneldecreases.Theonlyreasontoï¬‚ip\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\n3 3 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\nbutwithoutï¬‚ippingthekernel:\nS i , j I K i , j () = ( âˆ—)() =î˜\nmî˜\nnI i m , j n K m , n . (+ +)( )(9.6)\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "andspecifywhetherwemeantoï¬‚ipthekernelornotincontextswherekernel\nï¬‚ippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\nbasedonconvolutionwithkernelï¬‚ippingwilllearnakernelthatisï¬‚ippedrelative\ntothekernellearnedbyanalgorithmwithouttheï¬‚ipping.Itisalsorarefor\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "notcommuteregardlessofwhethertheconvolutionoperationï¬‚ipsitskernelor\nnot.\nSeeï¬gureforanexampleofconvolution(withoutkernelï¬‚ipping)applied 9.1\ntoa2-Dtensor.\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "m at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\nmatrixmultiplication anddoesnotdependonspeciï¬cpropertiesofthematrix\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\nfurtherspecializationsinordertodealwithlargeinputseï¬ƒciently,buttheseare\nnotstrictlynecessaryfromatheoreticalperspective.\n3 3 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\na b c d\ne f g h\ni j k lw x\ny z\na w + b x +\ne y + f za w + b x +\ne y + f zb w + c x +\nf y + g zb w + c x +\nf y + g zc w + d x +\ng y + h zc w + d x +\ng y + h z\ne w + f x +\ni y + j ze w + f x +\ni y + j zf w + g x +\nj y + k zf w + g x +\nj y + k zg w + h x +\nk y + l zg w + h x +\nk y + l zI nput\nK e r ne l\nO ut put\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ï¬‚ipping.Inthiscasewerestrict\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,calledâ€œvalidâ€",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\nupper-leftregionoftheinputtensor.\n3 3 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.2Motivation\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\nr e pr e se n t at i o ns.Moreover,Â convolutionprovidesameansforworkingwith\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "unitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "pixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\nmemoryrequirementsofthemodelandimprovesitsstatisticaleï¬ƒciency.Italso\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\nineï¬ƒciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\nmatrixmultiplication requires m n Ã—parametersandthealgorithmsusedinpractice\nhave O( m n Ã—)runtime(perexample).Ifwelimitthenumberofconnections\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "k n Ã—parametersand O( k n Ã—)runtime.Formanypracticalapplications,itis\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\nkseveralordersofmagnitudesmallerthan m.Â Forgraphicaldemonstrationsof\nsparseconnectivity,seeï¬gureandï¬gure.Inadeepconvolutionalnetwork, 9.2 9.3\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\nasshowninï¬gure.Thisallowsthenetworktoeï¬ƒcientlydescribecomplicated 9.4\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "buildingblocksthateachdescribeonlysparseinteractions.\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\noftheinput(exceptperhapssomeoftheboundarypixels,Â dependingonthe\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\n3 3 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\nandalsohighlighttheoutputunitsin sthatareaï¬€ectedbythisunit. ( T o p )When sis\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaï¬€ectedby 3 x.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\nalloftheoutputsareaï¬€ectedby x 3.\n3 3 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : Â Wehighlightoneoutputunit, s 3,\nandalsohighlighttheinputunitsin xthataï¬€ectthisunit.Theseunitsareknown\nasthereceptiveï¬eldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "width,onlythreeinputsaï¬€ect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\nconnectivityisnolongersparse,soalloftheinputsaï¬€ect s 3.\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\nx 4 x 4h 4 h 4\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\nFigure9.4:Thereceptiveï¬eldoftheunitsinthedeeperlayersofaconvolutionalnetwork\nislargerthanthereceptiveï¬eldoftheunitsintheshallowlayers.Thiseï¬€ectincreasesif\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(ï¬gure)orpooling 9.12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\ninputimage.\n3 3 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\nparameterintwodiï¬€erentmodels.Â  ( T o p )Theblackarrowsindicateusesofthecentral\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "singleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\nhasnoparametersharingsotheparameterisusedonlyonce.\nforeverylocation,welearnonlyoneset.Thisdoesnotaï¬€ecttheruntimeof\nforwardpropagationâ€”it isstill O( k n Ã—)â€”butitdoesfurtherreducethestorage\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "practicallyinsigniï¬cantcomparedto m n Ã—.Convolutionisthusdramatically more\neï¬ƒcientthandensematrixmultiplication intermsofthememoryrequirements\nandstatisticaleï¬ƒciency.Foragraphicaldepictionofhowparametersharingworks,\nseeï¬gure.9.5\nAsanexampleofbothoftheseï¬rsttwoprinciplesinaction,ï¬gureshows9.6\nhowsparseconnectivityandparametersharingcandramatically improvethe\neï¬ƒciencyofalinearfunctionfordetectingedgesinanimage.\nInthecaseofconvolution,theparticularformofparametersharingcausesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "layertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\nSpeciï¬cally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\n3 3 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nmappingoneimagefunctiontoanotherimagefunction,suchthat Iî€°= g( I)is\ntheimagefunctionwith Iî€°( x , y)= I( x âˆ’1 , y).Thisshiftseverypixelof Ione\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\ntheresultwillbethesameasifweappliedconvolutionto Iî€°,thenappliedthe\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\nthatconvolutionproducesasortoftimelinethatshowswhendiï¬€erentfeatures",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "appearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "theï¬rstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\nonanindividualâ€™sface,weprobablywanttoextractdiï¬€erentfeaturesatdiï¬€erent\nlocationsâ€”thepartofthenetworkprocessingthetopofthefaceneedstolookfor\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\nlookforachin.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "lookforachin.\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\nforhandlingthesekindsoftransformations.\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksdeï¬nedby\nmatrixmultiplication withaï¬xed-shapematrix.Convolutionenablesprocessing\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\n9.3Pooling\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeï¬gure).9.7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "Intheï¬rststage,thelayerperformsseveralconvolutionsinparalleltoproducea\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\nanonlinearactivationfunction,suchastherectiï¬edlinearactivationfunction.\nThisstageissometimescalledthe det e c t o rstage.Â Inthethirdstage,weusea\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\n3 3 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.6: E ï¬ƒ c i e n c y o f e d g e d e t e c t i o n.Â Theimageontherightwasformedbytaking\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\nleft.Â Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "requires319 Ã—280 Ã—3=267 ,960ï¬‚oatingpointoperations(twomultiplicationsand\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\ntransformationwithamatrixmultiplicationwouldtake320 Ã—280 Ã—319 Ã—280,orover\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeï¬ƒcientfor\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\nperformsoversixteenbillionï¬‚oatingpointoperations,makingconvolutionroughly60,000",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "timesmoreeï¬ƒcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\nandconvolutionwouldrequirethesamenumberofï¬‚oatingpointoperationstocompute.\nThematrixwouldstillneedtocontain2 Ã—319 Ã—280=178 ,640entries.Convolution\nisanextremelyeï¬ƒcientwayofdescribingtransformationsthatapplythesamelinear\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\nGoodfellow)\n3 4 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nConvolutionalÂ Layer\nInputÂ toÂ layerConvolutionÂ stage:\nAneÂ transform ï¬ƒDetectorÂ stage:\nNonlinearity\ne.g.,Â rectiï¬edÂ linearPoolingÂ stageNextÂ layer\nInputÂ toÂ layersConvolutionÂ layer:\nAneÂ transformÂ  ï¬ƒDetectorÂ layer:Â Nonlinearity\ne.g.,Â rectiï¬edÂ linearPoolingÂ layerNextÂ layerComplexÂ layerÂ terminology SimpleÂ layerÂ terminology\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "commonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\neachlayerhavingmanyâ€œstages.â€Inthisterminology,thereisaone-to-onemapping\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\nnoteveryâ€œlayerâ€hasparameters.\n3 4 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\nbasedonthedistancefromthecentralpixel.\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "outputsdonotchange.Seeï¬gureforanexampleofhowthisworks. 9.8 Invariance\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\nimportanttopreservethelocationofafeature.Forexample,ifwewanttoï¬nda",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "cornerdeï¬nedbytwoedgesmeetingataspeciï¬corientation,weneedtopreserve\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\nTheuseofpoolingcanbeviewedasaddinganinï¬nitelystrongpriorthat\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\nassumptioniscorrect,itcangreatlyimprovethestatisticaleï¬ƒciencyofthenetwork.\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "whichtransformationstobecomeinvariantto(seeï¬gure).9.9\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\nï¬gureforanexample.Thisimprovesthecomputational eï¬ƒciencyofthe 9.10\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\nreductionintheinputsizecanalsoresultinimprovedstatisticaleï¬ƒciencyand\nreducedmemoryrequirementsforstoringtheparameters.\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize.Â For\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiï¬cation\nlayermusthaveaï¬xedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\noï¬€setbetweenpoolingregionssothattheclassiï¬cationlayeralwaysreceivesthe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\nï¬nalpoolinglayerofthenetworkmaybedeï¬nedtooutputfoursetsofsummary\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\n3 4 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n0. 1 1. 0. 21. 1. 1.\n0. 10. 2\n. . . . . .. . . . . .\n0. 3 0. 1 1.1. 0. 3 1.\n0. 21.\n. . . . . .. . . . . .D E T E C T O R Â  S T A GEP O O L I N GÂ  ST A GE\nP O O L I N GÂ  ST A GE\nD E T E C T O R Â  S T A GE\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "andapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\n3 4 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nL ar ge Â  r e s pon s e\ni nÂ  po ol i ngÂ uni tL ar ge Â  r e s pon s e\ni nÂ  po ol i ngÂ uni t\nL ar ge\nr e s ponse\ni nÂ  de t e c t or\nuni t Â  1L ar ge\nr e s ponse\ni nÂ  de t e c t or\nuni t Â  3\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\ntheinput.Hereweshowhowasetofthreelearnedï¬ltersandamaxpoolingunitcanlearn",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "tobecomeinvarianttorotation.Allthreeï¬ltersareintendedtodetectahand-written5.\nEachï¬lterattemptstomatchaslightlydiï¬€erentorientationofthe5.Whena5appearsin\ntheinput,thecorrespondingï¬lterwillmatchitandcausealargeactivationinadetector\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\nwasactivated.Weshowherehowthenetworkprocessestwodiï¬€erentinputs,resulting\nintwodiï¬€erentdetectorunitsbeingactivated.Theeï¬€ectonthepoolingunitisroughly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\ntransformations.\n0. 1 1. 0. 21. 0. 2\n0. 10. 1\n0. 0 0. 1\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\nwanttoignoresomeofthedetectorunits.\n3 4 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\ndiï¬€erentsetofpoolingregionsforeachimage.Anotherapproachistolearna\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection.Â The20.6\ninverse-likeoperationsonpoolingunitsneededinsomediï¬€erentiablenetworkswill\nbecoveredinsection.20.10.6\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiï¬cation\nusingconvolutionandpoolingareshowninï¬gure.9.11",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "usingconvolutionandpoolingareshowninï¬gure.9.11\n9.4ConvolutionandÂ PoolingÂ asanÂ Inï¬nitelyStrong\nPrior\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\nmoreactiveroleindeterminingwheretheparametersendup.\nAninï¬nitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\nsupportthedatagivestothosevalues.\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "butwithaninï¬nitelystrongprioroveritsweights.Thisinï¬nitelystrongprior\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\nexceptforinthesmall,spatiallycontiguousreceptiveï¬eldassignedtothathidden\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinï¬nitely\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\n3 4 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInputÂ image:Â \n256x256x3OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 256x256x64OutputÂ ofÂ poolingÂ \nwithÂ strideÂ 4:Â \n64x64x64OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 64x64x64OutputÂ ofÂ poolingÂ \nwithÂ strideÂ 4:Â \n16x16x64OutputÂ ofÂ reshapeÂ toÂ \nvector:\n16,384Â unitsOutputÂ ofÂ matrixÂ \nmultiply:Â 1,000Â unitsOutputÂ ofÂ softmax:Â \n1,000Â classÂ \nprobabilities\nInputÂ image:Â \n256x256x3OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 256x256x64OutputÂ ofÂ poolingÂ \nwithÂ strideÂ 4:Â \n64x64x64OutputÂ ofÂ \nconvolutionÂ +",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "withÂ strideÂ 4:Â \n64x64x64OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 64x64x64OutputÂ ofÂ poolingÂ toÂ \n3x3Â grid:Â 3x3x64OutputÂ ofÂ reshapeÂ toÂ \nvector:\n576Â unitsOutputÂ ofÂ matrixÂ \nmultiply:Â 1,000Â unitsOutputÂ ofÂ softmax:Â \n1,000Â classÂ \nprobabilities\nInputÂ image:Â \n256x256x3OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 256x256x64OutputÂ ofÂ poolingÂ \nwithÂ strideÂ 4:Â \n64x64x64OutputÂ ofÂ \nconvolutionÂ +Â \nReLU:Â 64x64x64OutputÂ ofÂ \nconvolution:\n16x16x1,000OutputÂ ofÂ averageÂ \npooling:Â 1x1x1,000OutputÂ ofÂ softmax:Â \n1,000Â classÂ \nprobabilities",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "1,000Â classÂ \nprobabilities\nOutputÂ ofÂ poolingÂ \nwithÂ strideÂ 4:Â \n16x16x64\nFigure9.11:Examplesofarchitecturesforclassiï¬cationwithconvolutionalnetworks.The\nspeciï¬cstridesanddepthsusedinthisï¬gurearenotadvisableforrealuse;theyare\ndesignedtobeveryshallowinordertoï¬tontothepage.Â Realconvolutionalnetworks\nalsoofteninvolvesigniï¬cantamountsofbranching,unlikethechainstructuresused\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaï¬xedimagesize.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\nconvolutionalfeaturemapisreshapedtoï¬‚attenoutthespatialdimensions.Therest\nofthenetworkisanordinaryfeedforwardnetworkclassiï¬er,asdescribedinchapter.6\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\nbutaï¬xednumberofpools,inordertoprovideaï¬xed-sizevectorof576unitstothe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "fullyconnectedportionofthenetwork.Â Aconvolutionalnetworkthatdoesnot ( R i g h t )\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\ntheargumenttothesoftmaxclassiï¬eratthetop.\n3 4 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\nequivarianttotranslation.Likewise,theuseofpoolingisaninï¬nitelystrongprior\nthateachunitshouldbeinvarianttosmalltranslations.\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\ninï¬nitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\nofaconvolutionalnetasafullyconnectednetwithaninï¬nitelystrongpriorcan\ngiveussomeinsightsintohowconvolutionalnetswork.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "giveussomeinsightsintohowconvolutionalnetswork.\nOnekeyinsightisthatconvolutionandpoolingcancauseunderï¬tting. Like\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "highlyinvariantfeaturesandfeaturesthatwillnotunderï¬twhenthetranslation\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\ninappropriate.\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "areseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\nofspatialrelationshipshard-codedintothembytheirdesigner.\n9.5VariantsoftheBasicConvolutionFunction\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\nunderstoodinthemathematical literature.Thefunctionsusedinpracticediï¬€er",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "slightly.Herewedescribethesediï¬€erencesindetail,andhighlightsomeuseful\npropertiesofthefunctionsusedinneuralnetworks.\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\nnetworktoextractmanykindsoffeatures,atmanylocations.\n3 4 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\ngridofvector-valuedobservations.Â Forexample,acolorimagehasared,green\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\ntothesecondlayeristheoutputoftheï¬rstlayer,whichusuallyhastheoutput\nofmanydiï¬€erentconvolutionsateachposition.Whenworkingwithimages,we\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "oneindexintothediï¬€erentchannelsandtwoindicesintothespatialcoordinates\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdiï¬€erentexamplesin\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\nkernel-ï¬‚ippingisused.Thesemulti-channeloperationsareonlycommutativeif",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "eachoperationhasthesamenumberofoutputchannelsasinputchannels.\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\ninput,withanoï¬€setof krowsand lcolumnsbetweentheoutputunitandthe\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "acrosswithoutï¬‚ipping,then V K\nZ i , j , k=î˜\nl , m , nV l , j m , k n + âˆ’ 1 + âˆ’ 1 K i , l , m , n (9.7)\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\narraysusingafortheï¬rstentry.Thisnecessitatesthe 1 âˆ’1intheaboveformula.\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\ntheaboveexpressionevensimpler.\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "computational cost(attheexpenseofnotextractingourfeaturesasï¬nely).We\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\ndeï¬neadownsampledconvolutionfunctionsuchthat c\nZ i , j , k= ( ) c K V , , s i , j , k=î˜\nl , m , nî€‚\nVl , j s m , k s n ( âˆ’ Ã— 1 ) + ( âˆ’ Ã— 1 ) + K i , l , m , nî€ƒ\n.(9.8)\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\n3 4 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\ntodeï¬neaseparatestrideforeachdirectionofmotion.Seeï¬gureforan9.12\nillustration.\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\nateachlayer.Â Zeropaddingtheinputallowsustocontrolthekernelwidthand\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\nkernelsâ€”bothscenariosthatsigniï¬cantlylimittheexpressivepowerofthenetwork.\nSeeï¬gureforanexample. 9.13\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\nthekernelhaswidth k,theoutputwillbeofwidth m k âˆ’+1.Â Therateofthis\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "eventuallydropto1 Ã—1,atwhichpointadditionallayerscannotmeaningfully\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "inï¬‚uencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\ninanoutputimageofwidth m+ k âˆ’1.Inthiscase,theoutputpixelsnearthe\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\ncanmakeitdiï¬ƒculttolearnasinglekernelthatperformswellatallpositionsin",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\ntermsoftestsetclassiï¬cationaccuracy)liessomewherebetweenâ€œvalidâ€andâ€œsameâ€\nconvolution.\n3 4 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\nx 4 x 4 x 5 x 5s 3 s 3\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\nx 4 x 4z 4 z 4\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\nc onv ol ut i on\nD ow nsampl i n g\nC onv ol ut i on\nFigureÂ 9.12:ConvolutionÂ withaÂ stride.Inthisexample,weÂ useÂ astrideÂ oftwo.\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation.Â  ( Bot-\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\nthatarethendiscarded.\n3 5 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n. . . . . .. . .\n. . . . . .. . . . . .. . . . . .\nFigure9.13: T h e e ï¬€ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\nshrinkbyï¬vepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\nshrinkingisinevitableinthiskindofarchitecture. Byaddingï¬veimplicitzeroes ( Bottom )\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\nmakeanarbitrarilydeepconvolutionalnetwork.\n3 5 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speciï¬ed\nbya6-Dtensor W.Â Theindicesinto Warerespectively: i,theoutputchannel,\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoï¬€set\nwithintheinput,and n,thecolumnoï¬€setwithintheinput.Thelinearpartofa\nlocallyconnectedlayeristhengivenby\nZ i , j , k=î˜",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "locallyconnectedlayeristhengivenby\nZ i , j , k=î˜\nl , m , n[ V l , j m , k n + âˆ’ 1 + âˆ’ 1 w i , j , k, l , m , n] . (9.9)\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\nconnections.\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\nimage.\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\nwaytodothisistomaketheï¬rst moutputchannelsconnecttoonlytheï¬rst\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "inputchannels,andsoon.Seeï¬gureforanexample.Modelinginteractions 9.15\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\nreducememoryconsumptionandincreasestatisticaleï¬ƒciency,andalsoreduces\ntheamountofcomputationneededtoperformforwardandback-propagation. It\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\nT i l e d c o n v o l ut i o n( ,;,)oï¬€ersacom- GregorandLeCun2010aLeetal.2010\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\nneighboringlocationswillhavediï¬€erentï¬lters,likeinalocallyconnectedlayer,\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\nfeaturemap.Seeï¬gureforacomparisonoflocallyconnectedlayers,tiled 9.16\nconvolution,andstandardconvolution.\n3 5 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\naÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â aÂ  Â  Â  Â  Â  Â  Â  Â  b c Â  Â  Â  Â  Â  Â  d e Â  Â  Â  Â  Â  f gÂ  Â  Â  Â  Â  Â  h Â  i Â  Â \nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\nFigure9.14:Comparisonoflocalconnections,convolution,andfullconnections.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith\nauniquelettertoshowthateachedgeisassociatedwithitsownweightparameter.\n( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly\nthesameconnectivityasthelocallyconnectedlayer.Thediï¬€erenceliesnotinwhichunits\ninteractwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer\nhasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge.\n( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach\nedgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis\ndiagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected\nlayer.\n3 5 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nI nputÂ T e nsorO ut putÂ T e nsor\nS p a t i a l Â  c o o r d i n a t e sC h a n n e l Â  c o o r d i n a t e s\nFigure9.15:Â Aconvolutionalnetworkwiththeï¬rsttwooutputchannelsconnectedto\nonlytheï¬rsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\nthesecondtwoinputchannels.\n3 5 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\naÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â  Â  b aÂ  Â  Â  Â  Â  Â  Â aÂ  Â  Â  Â  Â  Â  Â  Â  b c Â  Â  Â  Â  Â  Â  d e Â  Â  Â  Â  Â  f gÂ  Â  Â  Â  Â  Â  h Â  i Â  Â \nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\nx 4 x 4s 4 s 4\nx 5 x 5s 5 s 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "x 4 x 4s 4 s 4\nx 5 x 5s 5 s 5\naÂ  Â  Â  Â  Â  Â  Â  Â  b c Â  Â  Â  Â  Â  Â  Â  Â  d aÂ  Â  Â  Â  Â  Â  Â  Â  b c Â  Â  Â  Â  Â  Â  Â  Â  d aÂ  Â  Â  Â  Â  Â  Â  Â \nFigure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard\nconvolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame\nsizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide.\nThediï¬€erencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight\nbylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r )\ntdiï¬€erentkernels.Hereweillustratethecaseof t= 2.Â Oneofthesekernelshasedges\nlabeledâ€œaâ€andâ€œb,â€whiletheotherhasedgeslabeledâ€œcâ€andâ€œd.â€Â Eachtimewemoveone\npixeltotherightintheoutput,wemoveontousingadiï¬€erentkernel.Thismeansthat,\nlikethelocallyconnectedlayer,neighboringunitsintheoutputhavediï¬€erentparameters.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels,\nwecyclebacktotheï¬rstkernel.Iftwooutputunitsareseparatedbyamultipleof t\nsteps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom )\nconvolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated\ninthediagrambyusingthekernelwithweightslabeledâ€œaâ€andâ€œbâ€everywhere.\n3 5 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTodeï¬netiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\nthedimensionscorrespondtodiï¬€erentlocationsintheoutputmap.Ratherthan\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\nthroughasetof tdiï¬€erentchoicesofkernelstackineachdirection.If tisequalto\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\nZ i , j , k=î˜\nl , m , nV l , j m , k n + âˆ’ 1 + âˆ’ 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "whereisÂ themodulooperation,with % t% t=0(, t+1)% t=1,etc.ItÂ is\nstraightforwardtogeneralizethisequationtouseadiï¬€erenttilingrangeforeach\ndimension.\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\ndiï¬€erentï¬lters.Iftheseï¬lterslearntodetectdiï¬€erenttransformedversionsof\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\nlearnedtransformation(seeï¬gure).Convolutionallayersarehard-codedtobe 9.9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "invariantspeciï¬callytotranslation.\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\nInsomesimplecases,Â thisoperationcanbeperformedusingtheconvolution\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\ndonothavethisproperty.\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "matrixmultiplication (ifweï¬rstreshapetheinputtensorintoaï¬‚atvector).The\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\nnetwork.\nMultiplication bythetransposeofthematrixdeï¬nedbyconvolutionisone\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\n3 5 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagationâ€™s\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\ntoldwhatthesizeoftheoriginalinputwas.\nThesethreeoperationsâ€”convolution,backpropfromoutputtoweights,and\nbackpropfromoutputtoinputsâ€”aresuï¬ƒcienttocomputeallofthegradients\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\nconvolution.Â See ()forafullderivationoftheequationsinthe Goodfellow2010",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "fullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\ndeï¬nedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "computethecostfunction J.Duringback-propagation, wewillreceiveatensor G\nsuchthat G i , j , k=âˆ‚\nâˆ‚ Z i , j , kJ , . ( V K)\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\nweightsinthekernel.Todoso,wecanuseafunction\ng , , s ( G V) i , j , k, l=âˆ‚\nâˆ‚ K i , j , k, lJ ,( V K) =î˜\nm , nG i , m , n V j , m s k, n s l ( âˆ’ Ã— 1 ) + ( âˆ’ Ã— 1 ) + .(9.11)\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "Todoso,wecanuseafunction\nh , , s ( K G) i , j , k=âˆ‚\nâˆ‚ V i , j , kJ ,( V K) (9.12)\n=î˜\nl , m\ns . t .\n( 1 ) + = l âˆ’ Ã— s m jî˜\nn , p\ns . t .\n( 1 ) + = n âˆ’ Ã— s p kî˜\nqK q , i , m , p G q , l , n .(9.13)\nAutoencodernetworks,Â describedinchapter,Â arefeedforwardnetworks 14\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\n3 5 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\nWî€¾W x.ItiscommonformoreÂ general autoencodersÂ tousemultiplication\nbythetransposeoftheweightmatrixjustasPCAdoes.Â Tomakesuchmodels\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedeï¬ne\nareconstruction\nR K H = ( h , , s .) (9.14)\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "to Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\ndiï¬€erentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\nback-propagationalgorithmonanystandardnetworkarchitectures.\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "biastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\nofhowtoshareparametersamongthebiases.Â Forlocallyconnectedlayersitis\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,ï¬xed\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "Separatingthebiasesmayslightlyreducethestatisticaleï¬ƒciencyofthemodel,but\nalsoallowsthemodeltocorrectfordiï¬€erencesintheimagestatisticsatdiï¬€erent\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\n9.6StructuredOutputs\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\nobject,ratherthanjustpredictingaclasslabelforaclassiï¬cationtaskorareal\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "standardconvolutionallayer.Forexample,themodelmightemitatensor S,where\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\nthatfollowtheoutlinesofindividualobjects.\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\n3 5 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nË† Y( 1 )Ë† Y( 1 )Ë† Y( 2 )Ë† Y( 2 )Ë† Y( 3 )Ë† Y( 3 )\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\nXXU U UV V V W W\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\nchannels(red,green,blue).ThegoalistooutputatensoroflabelsË† Y,withaprobability\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "imagecolumns,andthediï¬€erentclasses.RatherthanoutputtingË† Yinasingleshot,the\nrecurrentnetworkiterativelyreï¬nesitsestimateË† YbyusingapreviousestimateofË† Y\nasinputforcreatinganewestimate.Â Thesameparametersareusedforeachupdated\nestimate,andtheestimatecanbereï¬nedasmanytimesaswewish.Thetensorof\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "hiddenvalues.Onallbuttheï¬rststep,thekernels WareconvolvedoverË† Ytoprovide\ninputtothehiddenlayer.Ontheï¬rsttimestep,thistermisreplacedbyzero.Because\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\ndescribedinchapter.10\ninputplane,asshowninï¬gure.Inthekindsofarchitectures typicallyusedfor 9.13\nclassiï¬cationofasingleobjectinanimage,thegreatestreductioninthespatial\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\nuseapoolingoperatorwithunitstride.\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\noftheimagelabels,thenreï¬nethisinitialguessusingtheinteractionsbetween\nneighboringpixels.Repeatingthisreï¬nementstepseveraltimescorrespondsto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\nthearchitectureofsucharecurrentconvolutionalnetwork.\n3 5 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "canbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\n9.7DataTypes\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\neachchannelbeingtheobservationofadiï¬€erentquantityatsomepointinspace\nortime.Seetableforexamplesofdatatypeswithdiï¬€erentdimensionalities 9.1\nandnumberofchannels.\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\n().2010\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\nevenwhencomputational costandoverï¬ttingarenotsigniï¬cantissues.\nForexample,consideracollectionofimages,whereeachimagehasadiï¬€erent\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "ï¬xedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\ndiï¬€erentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\nmultiplication; thesameconvolutionkernelinducesadiï¬€erentsizeofdoublyblock\ncirculantmatrixforeachsizeofinput.Â Sometimes theoutputofthenetworkis\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "necessary.Inothercases,thenetworkmustproducesomeï¬xed-sizeoutput,for\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\nmaintainaï¬xednumberofpooledoutputs.Someexamplesofthiskindofstrategy\nareshowninï¬gure.9.11\n3 6 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nSinglechannel Multi-channel\n1-DAudioÂ waveform:TheÂ axisÂ we\nconvolveovercorrespondsto\ntime.Wediscretizetimeand\nmeasuretheamplitudeofthe\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\ntionsof3-Dcomputer-rendered\ncharactersaregeneratedbyalter-\ningtheposeofaâ€œskeletonâ€over\ntime.Ateachpointintime,the\nposeofthecharacterisdescribed\nbyaspeciï¬cationoftheanglesof\neachofthejointsinthecharac-\nterâ€™sskeleton.Eachchannelin\nthedatawefeedtotheconvolu-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "thedatawefeedtotheconvolu-\ntionalmodelrepresentstheangle\naboutoneaxisofonejoint.\n2-DAudiodatathathasbeenprepro-\ncessedwithaFouriertransform:\nWecantransformtheaudiowave-\nformintoa2Dtensorwithdif-\nferentrowscorrespondingtodif-\nferentfrequenciesÂ anddiï¬€erent\ncolumnscorrespondingtodiï¬€er-\nentpointsintime.Usingconvolu-\ntioninthetimemakesthemodel\nequivarianttoshiftsintime.Us-\ningconvolutionacrossthefre-\nquencyaxismakesthemodel\nequivarianttofrequency,sothat\nthesamemelodyplayedinadif-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "thesamemelodyplayedinadif-\nferentoctaveproducesthesame\nrepresentationbutatadiï¬€erent\nheightinthenetworkâ€™soutput.Colorimagedata:Onechannel\ncontainstheredpixels,onethe\ngreenÂ pixels,Â andÂ oneÂ theblue\npixels.Theconvolutionkernel\nmovesoverboththehorizontal\nandverticalaxesoftheÂ image,\nconferringtranslationequivari-\nanceinbothdirections.\n3-DVolumetricdata:Acommon\nsourceofthiskindofdataismed-\nicalimagingtechnology,suchas\nCTscans.Colorvideodata:Oneaxiscorre-\nspondstotime,onetotheheight",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "spondstotime,onetotheheight\nofthevideoframe,andoneto\nthewidthofthevideoframe.\nTable9.1:Examplesofdiï¬€erentformatsofdatathatcanbeusedwithconvolutional\nnetworks.\n3 6 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\nofobservationofthesamekindofthingâ€”diï¬€eren tlengthsofrecordingsover\ntime,diï¬€erentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludediï¬€erent\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\ncorrespondingtothetestscores.\n9.8Eï¬ƒcientConvolutionAlgorithms\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\nresources,asdiscussedinsection,areessential.Â However,inmanycasesit 12.1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\nalgorithm.\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\nofthetwosignals,Â andconvertingbacktothetimedomainusinganinverse\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\nimplementationofdiscreteconvolution.\nWhena d-dimensionalkernelcanbeexpressedasÂ theouterproductof d",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "vectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\nkernelisseparable,naiveconvolutionisineï¬ƒcient.Itisequivalenttocompose d\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\nissigniï¬cantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "convolutionrequires O( w d Ã—)runtimeandparameterstoragespace.Ofcourse,\nnoteveryconvolutioncanberepresentedinthisway.\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\nniquesthatimprovetheeï¬ƒciencyofonlyforwardpropagationareusefulbecause\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\nanetworkthantoitstraining.\n3 6 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\n9.9RandomorUnsupervisedFeatures\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "trainingistousefeaturesthatarenottrainedinasupervisedfashion.\nTherearethreebasicstrategiesforobtainingÂ con volutionkernelswithout\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.Â PartIII",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "describesmanymoreunsupervisedlearningapproaches.Learningthefeatures\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\nclassiï¬erlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\nRandomï¬ltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "etal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\naconvolutionalnetwork:ï¬rstevaluatetheperformanceofseveralconvolutional\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "Anintermediate approachistolearnthefeatures,butusingmethodsthatdo\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheï¬rstlayer\ninisolation,thenextractallfeaturesfromtheï¬rstlayeronlyonce,thentrainthe\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoï¬€er Leeetal.2009\n3 6 3",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\nWecanthenusetheparametersfromthispatch-basedmodeltodeï¬nethekernels\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "process.Usingthisapproach,wecantrainverylargemodelsandincurahigh\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\nfromroughly2007â€“2013,whenlabeleddatasetsweresmallandcomputational\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\nentirenetworkoneachtrainingiteration.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "entirenetworkoneachtrainingiteration.\nAswithotherapproachestounsupervisedpretraining,itremainsdiï¬ƒcultto\nteaseapartthecauseofsomeofthebeneï¬tsseenwiththisapproach.Unsupervised\npretrainingmayoï¬€ersomeregularizationrelativetosupervisedtraining,oritmay\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\ncostofthelearningrule.\n9.10TheNeuroscientiï¬cBasisforConvolutionalNet-\nworks\nConvolutionalÂ networksareÂ perhapsÂ the greatestÂ successstoryÂ ofbiologically",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "inspiredartiï¬cialintelligence.Thoughconvolutionalnetworkshavebeenguided\nbymanyotherï¬elds,someofthekeydesignprinciplesofneuralnetworkswere\ndrawnfromneuroscience.\nThehistoryofconvolutionalnetworksbeginswithneuroscientiï¬cexperiments\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\naNobelprize.Theirï¬ndingsthathavehadthegreatestinï¬‚uenceoncontemporary\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\ncats.Theyobservedhowneuronsinthecatâ€™sbrainrespondedtoimagesprojected\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciï¬c\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "otherpatterns.\n3 6 4",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\nfocusonasimpliï¬ed,cartoonviewofbrainfunction.\nInthissimpliï¬edview,wefocusonapartofthebraincalledV1,alsoknown\nasthe pr i m ar y v i sual c o r t e x.Â V1istheï¬rstareaofthebrainthatbeginsto\nperformsigniï¬cantlyadvancedprocessingofvisualinput.Â Inthiscartoonview,\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\ncalledthelateralgeniculatenucleus.Â Themainrole,asfarasweareconcerned\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\ntheeyetoV1,whichislocatedatthebackofthehead.\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\nmirroringÂ the structureÂ ofÂ theimageÂ inÂ the retina.ForÂ example,Â light\narrivingatthelowerhalfoftheretinaaï¬€ectsonlythecorrespondinghalfof\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\ndeï¬nedintermsoftwodimensionalmaps.\n2.V1containsmany si m pl e c e l l s.Asimplecellâ€™sactivitycantosomeextent\nbecharacterizedbyalinearÂ function oftheimageinÂ asmall,Â spatially",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "localizedreceptiveï¬eld.Thedetectorunitsofaconvolutionalnetworkare\ndesignedtoemulatethesepropertiesofsimplecells.\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\nlayersofthebrain,weeventuallyï¬ndcellsthatrespondtosomespeciï¬cconcept",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\n3 6 5",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nnicknamedâ€œgrandmother cellsâ€â€”theideaisthatapersoncouldhaveaneuronthat\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\nshadow,etc.\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\nTheyfoundwhathascometobecalledtheâ€œHalleBerryneuronâ€:anindividual\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronï¬reswhena\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\nthewordsâ€œHalleBerry.â€Ofcourse,thishasnothingtodowithHalleBerryherself;\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "convolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\nnetworkâ€™slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\n(IT).Whenviewinganobject,informationï¬‚owsfromtheretina,throughthe\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheï¬rst\n100msofglimpsinganobject.Â Ifapersonisallowedtocontinuelookingatthe\nobjectformoretime,theninformationwillbegintoï¬‚owbackwardsasthebrain",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "usestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\nHowever,ifweinterruptthepersonâ€™sgaze,andobserveonlytheï¬ringratesthat\nresultfromtheï¬rst100msofmostlyfeedforwardactivation,thenITprovestobe\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\nï¬ringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\nrecognitiontasks(,). DiCarlo2013\nThatbeingsaid,therearemanydiï¬€erencesbetweenconvolutionalnetworks",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "andthemammalianvisionsystem.Someofthesediï¬€erencesarewellknown\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\nthesediï¬€erencesarenotyetknown,becausemanybasicquestionsabouthowthe\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\nâ€¢Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\n3 6 6",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "â€¢Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\nsofararepurelyvisual.\nâ€¢Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\nabletounderstandentirescenesincludingmanyobjectsandrelationships\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "â€¢EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\nhasnotyetbeenshowntooï¬€eracompellingimprovement.\nâ€¢WhilefeedforwardITï¬ringratescapturemuchofthesameinformationas\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\ncomputations are.Thebrainprobablyusesverydiï¬€erentactivationand\npoolingfunctions.Anindividualneuronâ€™sactivationprobablyisnotwell-",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "characterizedbyasinglelinearï¬lterresponse.ArecentmodelofV1involves\nmultiplequadraticï¬ltersforeachneuron(,).Indeedour Rustetal.2005\ncartoonpictureofâ€œsimplecellsâ€Â andâ€œcomplexcellsâ€Â mightcreateanon-\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\nkindofcellbutwiththeirâ€œparametersâ€enablingacontinuumofbehaviors\nrangingfromwhatwecallâ€œsimpleâ€towhatwecallâ€œcomplex.â€\nItisÂ alsoworthmentioningthatneuroscienceÂ hastoldÂ usrelativelylittle",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\nalgorithm.\n3 6 7",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nLangÂ andHintonÂ 1988()introducedtheÂ useÂ ofback-propagationÂ totrain\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscientiï¬cobserva-\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\nconvolutionappliedtoimages.\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciï¬c\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "Inadeep,nonlinearnetwork,itcanbediï¬ƒculttounderstandthefunctionof\nindividualcells.Simplecellsintheï¬rstlayerareeasiertoanalyze,becausetheir\nresponsesaredrivenbyalinearfunction.Inanartiï¬cialneuralnetwork,wecan\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimalâ€™s",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "retina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\nthenï¬talinearmodeltotheseresponsesinordertoobtainanapproximation of\ntheneuronâ€™sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\nandShapley2004,).\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\nby G ab o r f unc t i o ns.Â TheGaborfunctiondescribestheweightata2-Dpoint\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "I( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\nlocations,deï¬nedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\nofview,theresponseofasimplecelltoanimageisgivenby\ns I() =î˜\nx âˆˆ Xî˜\ny âˆˆ Yw x , y I x , y . ()() (9.15)\nSpeciï¬cally,takestheformofaGaborfunction: w x , y()\nw x , y Î± , Î² (; x , Î² y , f , Ï† , x 0 , y 0 , Ï„ Î±) = expî€€âˆ’ Î² x xî€° 2âˆ’ Î² y yî€° 2î€\ncos( f xî€°+) Ï† ,(9.16)\nwhere",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "cos( f xî€°+) Ï† ,(9.16)\nwhere\nxî€°= ( x x âˆ’ 0)cos()+( Ï„ y y âˆ’ 0)sin() Ï„ (9.17)\n3 6 8",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nand\nyî€°= ( âˆ’ x x âˆ’ 0)sin()+( Ï„ y y âˆ’ 0)cos() Ï„ . (9.18)\nHere, Î±, Î² x, Î² y, f, Ï†, x 0, y 0,and Ï„areparametersthatcontroltheproperties\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\ndiï¬€erentsettingsoftheseparameters.\nTheparameters x 0, y 0,and Ï„deï¬neacoordinatesystem.Â Wetranslateand\nrotate xand ytoform xî€°and yî€°.Speciï¬cally,thesimplecellwillrespondtoimage\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "aswemovealongalinerotatedradiansfromthehorizontal. Ï„\nViewedasafunctionof xî€°and yî€°,thefunction wthenrespondstochangesin\nbrightnessaswemovealongthe xî€°axis.Â Ithastwoimportantfactors:oneisa\nGaussianfunctionandtheotherisacosinefunction.\nTheGaussianfactor Î±expî€€\nâˆ’ Î² x xî€° 2âˆ’ Î² y yî€° 2î€\ncanbeseenasagatingtermthat\nensuresthesimplecellwillonlyrespondtovaluesnearwhere xî€°and yî€°areboth\nzero,inotherwords,nearthecenterofthecellâ€™sreceptiveï¬eld.Thescalingfactor",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "Î±adjuststhetotalmagnitudeofthesimplecellâ€™sresponse,while Î² xand Î² ycontrol\nhowquicklyitsreceptiveï¬eldfallsoï¬€.\nThecosinefactor cos( f xî€°+ Ï†) controlshowthesimplecellrespondstochanging\nbrightnessalongthe xî€°axis.Theparameter fcontrolsthefrequencyofthecosine\nandcontrolsitsphaseoï¬€set. Ï†\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\ntoaspeciï¬cspatialfrequencyofbrightnessinaspeciï¬cdirectionataspeciï¬c\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweightsâ€”when\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\nnegative.\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\n2-Dvectorcontainingtwosimplecellsâ€™responses: c( I)=î°\ns 0() I2+ s 1() I2.Â An\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "for Ï†,and Ï†issetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeï¬nedinthisway\nrespondswhentheGaussianreweightedimage I( x , y)exp( âˆ’ Î² x xî€° 2âˆ’ Î² y yî€° 2) contains\nahighamplitudesinusoidalwavewithfrequency findirection Ï„near ( x 0 , y 0),\nregardlessofthephaseoï¬€setofthiswave.Inotherwords,thecomplexcellis\ninvarianttosmalltranslationsoftheimageindirection Ï„,ortonegatingtheimage\n3 6 9",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdiï¬€erentvaluesoftheparameters\nthatcontrolthecoordinatesystem: x 0, y 0,and Ï„.Â EachGaborfunctioninthisgridis\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and Ï„ischosenso\nthateachGaborï¬lterissensitivetothedirectionradiatingoutfromthecenterofthegrid.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots, x 0, y 0,and Ï„areï¬xedtozero.Â  Gaborfunctionswith ( C e n t e r )\ndiï¬€erentGaussianscaleparameters Î² xand Î² y.Gaborfunctionsarearrangedinincreasing\nwidth(decreasing Î² x)aswemovelefttorightthroughthegrid,andincreasingheight\n(decreasing Î² y)aswemovetoptobottom.Fortheothertwoplots,the Î²valuesareï¬xed\nto1.5 Ã—theimagewidth.Gaborfunctionswithdiï¬€erentsinusoidparameters ( R i g h t ) f\nand Ï†.Aswemovetoptobottom, fincreases,andaswemovelefttoright, Ï†increases.",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "Fortheothertwoplots,isï¬xedto0andisï¬xedto5theimagewidth. Ï† f Ã—\n(replacingblackwithwhiteandviceversa).\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\nasimpleunsupervisedlearningalgorithm,Â sparse coding,learnsfeatureswith\nreceptiveï¬eldssimilartothoseofsimplecells.Sincethen,wehavefoundthat",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\nlearningalgorithms,whichlearnthesefeaturesintheirï¬rstlayer.Figure9.19\nshowssomeexamples.Becausesomanydiï¬€erentlearningalgorithmslearnedge\ndetectors,itisdiï¬ƒculttoconcludethatanyspeciï¬clearningalgorithmisthe\nâ€œrightâ€modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "whenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydiï¬€erent\napproachestostatisticalmodeling.SeeHyvÃ¤rinen 2009etal.()forareviewofthe\nï¬eldofnaturalimagestatistics.\n3 7 0",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciï¬c\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytheï¬rstlayerofafullysupervised",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "convolutionalmaxoutnetwork.Neighboringpairsofï¬ltersdrivethesamemaxoutunit.\n9.11ConvolutionalNetworksandtheHistoryofDeep\nLearning\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\ntheï¬rstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\nconsideredviable.Convolutionalnetworkswerealsosomeoftheï¬rstneural",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "networkstosolveimportantcommercialapplicationsandremainattheforefront\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "andmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\n3 7 1",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER9.CONVOLUTIONALNETWORKS\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\nlessimpactforyearsearlier.\nConvolutionalnetsweresomeoftheï¬rstworkingdeepnetworkstrainedwith\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\nsimplybethatconvolutionalnetworksweremorecomputationally eï¬ƒcientthan\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "andtunetheirimplementation andhyperparameters.Largernetworksalsoseem\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseï¬€orttouse",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\nnetworks.\n3 7 2",
    "metadata": {
      "source": "[14]part-2-chapter-9.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 1\nI n t ro d u ct i on\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\nbacktoatleastthetimeofancientGreece.Themythicalï¬guresPygmalion,\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\nGalatea,Talos,andPandoramayallberegardedasartiï¬ciallife( , OvidandMartin\n2004Sparkes1996Tandy1997 ;,;,).\nWhenprogrammable computerswereï¬rstconceived,peoplewonderedwhether\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "built(Lovelace1842,).Today, ar t i ï¬c i al i n t e l l i g e nc e(AI)isathrivingï¬eldwith\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\ninmedicineandsupportbasicscientiï¬cresearch.\nIntheearlydaysofartiï¬cialintelligence,theï¬eldrapidlytackledandsolved\nproblemsthatareintellectually diï¬ƒcultforhumanbeingsbutrelativelystraight-\nforwardforcomputersâ€”problemsthatcanbedescribedbyalistofformal,math-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "ematicalrules.Â Thetruechallengetoartiï¬cialintelligenceprovedtobesolving\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\nformallyâ€”probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\nspokenwordsorfacesinimages.\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\nhierarchyofconcepts,witheachconceptdeï¬nedintermsofitsrelationtosimpler",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\n1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\ntheworld.Forexample,IBMâ€™sDeepBluechess-playingsystemdefeatedworld\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "inonlyrigidlycircumscribedways.DevisingasuccessfulchessstrategyisÂ a\ntremendousaccomplishment,Â butthechallengeisnotduetothediï¬ƒcultyof\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\nprovidedaheadoftimebytheprogrammer.\nIronically,abstractandformaltasksthatareamongthemostdiï¬ƒcultmental\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "havelongbeenabletodefeateventhebesthumanchessplayer,butareonly\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\norspeech.Apersonâ€™severydayliferequiresanimmenseamountofknowledge\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\ndiï¬ƒculttoarticulateinaformalway.Computersneedtocapturethissame\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\nartiï¬cialintelligenceishowtogetthisinformalknowledgeintoacomputer.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "Severalartiï¬cialintelligenceprojectshavesoughttohard-codeknowledgeabout\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\nk no wl e dge baseapproachtoartiï¬cialintelligence.Noneoftheseprojectshasled\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\ncalledCycL.Thesestatementsareenteredbyastaï¬€ofhumansupervisors.Itisan",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\nenginedetectedaninconsistencyinthestory:Â itknewthatpeopledonothave\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\nentityâ€œFredWhileShavingâ€containedelectricalparts.Itthereforeaskedwhether\nFredwasstillapersonwhilehewasshaving.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "Fredwasstillapersonwhilehewasshaving.\nThediï¬ƒcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\n2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "onthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\nvariousoutcomes.However,itcannotinï¬‚uencethewaythatthefeaturesare",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "deï¬nedinanyway.Â IflogisticregressionwasgivenanMRIscanofthepatient,\nratherthanthedoctorâ€™sformalizedreport,itwouldnotbeabletomakeuseful\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\ncomplications thatmightoccurduringdelivery.\nThisdependenceonrepresentationsisageneralphenomenon thatappears\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "thecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\narithmeticonArabicnumerals,butï¬ndarithmeticonRomannumeralsmuch\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\nenormouseï¬€ectontheperformanceofmachinelearningalgorithms.Forasimple\nvisualexample,seeï¬gure.1.1\nManyartiï¬cialintelligencetaskscanbesolvedbydesigningtherightsetof\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "learningalgorithm.Forexample,ausefulfeatureforspeakeridentiï¬cationfrom\nsoundisanestimateofthesizeofspeakerâ€™svocaltract.Itthereforegivesastrong\nclueastowhetherthespeakerisaman,woman,orchild.\nHowever,formanytasks,itisdiï¬ƒculttoknowwhatfeaturesshouldbeextracted.\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\nofawheelasafeature.Unfortunately,itisdiï¬ƒculttodescribeexactlywhata",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "wheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoï¬€\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\nobscuringpartofthewheel,andsoon.\n3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nî¸î¹îƒ î¡ î² î´ î¥ î³î©î¡î®î€  î£ î¯ î¯ î² î¤ î© î® î¡ î´ î¥ î³\nî²î‚µî î¯ î¬ î¡ î² î€  î£ î¯ î¯ î² î¤ î© î® î¡ î´ î¥ î³\nFigure1.1:Exampleofdiï¬€erentrepresentations:supposewewanttoseparatetwo\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "Onesolutiontothisproblemistousemachinelearningtodiscovernotonly\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng.Â Learnedrepresentations\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\nhumantimeandeï¬€ort;itcantakedecadesforanentirecommunityofresearchers.\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\nconvertstheinputdataintoadiï¬€erentrepresentation,anda dec o derfunction\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "throughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\nrepresentationhavevariousniceproperties.Diï¬€erentkindsofautoencodersaimto\nachievediï¬€erentkindsofproperties.\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\ncontext,weusethewordâ€œfactorsâ€simplytorefertoseparatesourcesofinï¬‚uence;\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\n4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\nobjectsorunobservedforcesinthephysicalworldthataï¬€ectobservablequantities.\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeakerâ€™s",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "age,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\nandtheangleandbrightnessofthesun.\nAmajorsourceofdiï¬ƒcultyinmanyreal-worldartiï¬cialintelligenceapplications\nisthatmanyofthefactorsofvariationinï¬‚uenceeverysinglepieceofdataweare\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\ntoblackatnight.Theshapeofthecarâ€™ssilhouettedependsontheviewingangle.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\nonesthatwedonotcareabout.\nOfcourse,itcanbeverydiï¬ƒculttoextractsuchhigh-level,abstractfeatures\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeakerâ€™saccent,\ncanbeidentiï¬edonlyusingsophisticated,nearlyhuman-levelunderstandingof\nthedata.Whenitisnearlyasdiï¬ƒculttoobtainarepresentationastosolvethe\noriginalproblem,representationlearningdoesnot,atï¬rstglance,seemtohelpus.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\nwhichareinturndeï¬nedintermsofedges.\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "networkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\napplicationofadiï¬€erentmathematical functionasprovidinganewrepresentation\noftheinput.\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "computertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\ncanbethoughtofasthestateofthecomputerâ€™smemoryafterexecutinganother\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\ninstructionsinsequence.Sequentialinstructionsoï¬€ergreatpowerbecauselater\n5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nVisibleÂ layer\n(inputÂ pixels)1stÂ hiddenÂ layer\n(edges)2ndÂ hiddenÂ layer\n(cornersÂ and\ncontours)3rdÂ hiddenÂ layer\n(objectÂ parts)CARPERSONANIMALOutput\n(objectÂ identity)\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiï¬ƒcultforacomputertounderstand\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "Deeplearningresolvesthisdiï¬ƒcultybybreakingthedesiredcomplicatedmappingintoa\nseriesofnestedsimplemappings,eachdescribedbyadiï¬€erentlayerofthemodel.The\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\nfeaturesfromtheimage.Theselayersarecalledâ€œhiddenâ€becausetheirvaluesarenotgiven\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,theï¬rstlayercaneasily\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheï¬rsthidden\nlayerâ€™sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\nlayerâ€™sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "candetectentirepartsofspeciï¬cobjects,byï¬ndingspeciï¬ccollectionsofcontoursand\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\nfromZeilerandFergus2014().\n6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nx 1 x 1Ïƒ\nw 1 w 1Ã—\nx 2 x 2 w 2 w 2Ã—+El e me n t\nS e t\n+\nÃ—\nÏƒ\nxx wwEl e me n t\nS e t\nL ogi s t i c\nR e gr e s s i onL ogi s t i c\nR e gr e s s i on\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\noutputbutdependsonthedeï¬nitionofwhatconstitutesapossiblecomputationalstep.\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "Ïƒ ( wTx ),whereÏƒisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\nviewofdeeplearning,notalloftheinformationinalayerâ€™sactivationsnecessarily\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "stateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciï¬cally,\nbutithelpsthemodeltoorganizeitsprocessing.\nTherearetwomainwaysofmeasuringthedepthofamodel.Theï¬rstviewis\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "aï¬‚owchartthatdescribeshowtocomputeeachofthemodelâ€™soutputsgiven\nitsinputs.Justastwoequivalentcomputerprogramswillhavediï¬€erentlengths\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\nbedrawnasaï¬‚owchartwithdiï¬€erentdepthsdependingonwhichfunctionswe\nallowtobeusedasindividualstepsintheï¬‚owchart.Figureillustrateshowthis 1.3\nchoiceoflanguagecangivetwodiï¬€erentmeasurementsforthesamearchitecture.\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "modelasbeingnotthedepthofthecomputational graphbutthedepthofthe\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\n7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\noftheï¬‚owchartofthecomputations neededtocomputetherepresentationof\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\nThisisbecausethesystemâ€™sunderstandingofthesimplerconceptscanbereï¬ned\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\npresentaswell.Â Inthiscase,thegraphofconceptsonlyincludestwolayersâ€”a",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "layerforeyesandalayerforfacesâ€”butthegraphofcomputations includes 2n\nlayersifwereï¬neourestimateofeachconceptgiventheothertimes. n\nBecauseitisnotalwaysclearwhichofthesetwoviewsâ€”thedepthofthe\ncomputational graph,orthedepthoftheprobabilisticmodelinggraphâ€”ismost\nrelevant,andbecausediï¬€erentpeoplechoosediï¬€erentsetsofsmallestelements\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "acomputerprogram.Â Nor isthereaconsensusabouthowmuchdepthamodel\nrequirestoqualifyasâ€œdeep.â€However,deeplearningcansafelyberegardedasthe\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\nSpeciï¬cally,itisatypeofmachinelearning,atechniquethatallowscomputer\nsystemstoimprovewithexperienceanddata.Â Accordingtotheauthorsofthis",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "book,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\nkindofmachinelearningthatachievesgreatpowerandï¬‚exibilitybylearningto\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeï¬nedin\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\noflessabstractones.Figureillustratestherelationshipbetweenthesediï¬€erent 1.4\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\narebeginningacareerindeeplearningandartiï¬cialintelligenceresearch.The\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "learningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\n8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nAIMachineÂ learningRepresentationÂ learningDeepÂ learning\nExample:\nKnowledge\nbasesExample:\nLogistic\nregressionExample:\nShallow\nautoencoders Example:\nMLPs\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\n9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nInputHand-\ndesignedÂ \nprogramOutput\nInputHand-\ndesignedÂ \nfeaturesMappingÂ fromÂ \nfeaturesOutput\nInputFeaturesMappingÂ fromÂ \nfeaturesOutput\nInputSimpleÂ \nfeaturesMappingÂ fromÂ \nfeaturesOutput\nAdditionalÂ \nlayersÂ ofÂ moreÂ \nabstractÂ \nfeatures\nRule-based\nsystemsClassic\nmachine\nlearning Representation\nlearningDeep\nlearning\nFigure1.5:Â Flowchartsshowinghowthediï¬€erentpartsofanAIsystemrelatetoeach\notherwithindiï¬€erentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\nlearnfromdata.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "learnfromdata.\n1 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\nsearchengines,onlineadvertisingandï¬nance.\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "widelybelievedtobeimportantforfutureresearchindeeplearning.\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\nchapterstoread,ï¬gureprovidesaï¬‚owchartshowingthehigh-levelorganization 1.6\nofthebook.\nWedoassumethatallreaderscomefromacomputersciencebackground. We",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "assumefamiliaritywithprogramming, abasicunderstandingofcomputational\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\nterminologyofgraphtheory.\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\nâ€¢Deeplearninghashadalongandrichhistory,buthasgonebymanynames\nreï¬‚ectingdiï¬€erentphilosophicalviewpoints,andhaswaxedandwanedin\npopularity.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "popularity.\nâ€¢Deeplearninghasbecomemoreusefulastheamountofavailabletraining\ndatahasincreased.\nâ€¢Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\n(bothhardwareandsoftware)fordeeplearninghasimproved.\nâ€¢Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\naccuracyovertime.\n1 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1.Â Introduction\nPartÂ I:Â AppliedÂ MathÂ andÂ MachineÂ LearningÂ Basics\n2.Â LinearÂ Algebra3.Â ProbabilityÂ andÂ \nInformationÂ Theory\n4.Â NumericalÂ \nComputation5.Â MachineÂ LearningÂ \nBasics\nPartÂ II:Â DeepÂ Networks:Â ModernÂ Practices\n6.Â DeepÂ FeedforwardÂ \nNetworks\n7.Â Regularization8.Â Optimization 9.Â Â CNNs10.Â Â RNNs\n11.Â PracticalÂ \nMethodology12.Â Applications\nPartÂ III:Â DeepÂ LearningÂ Research\n13.Â LinearÂ FactorÂ \nModels14.Â Autoencoders15.Â RepresentationÂ \nLearning\n16.Â Structured",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Learning\n16.Â StructuredÂ \nProbabilisticÂ Models17.Â MonteÂ CarloÂ \nMethods\n18.Â PartitionÂ \nFunction19.Â Inference\n20.Â DeepÂ GenerativeÂ \nModels\nFigure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother\nindicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter.\n1 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\nw o rks\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\nexcitingnewtechnology,andaresurprisedtoseeamentionofâ€œhistoryâ€inabook\naboutanemergingï¬eld.Infact,deeplearningdatesbacktothe1940s.Deep\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erentnames,andhasonlyrecentlybecomecalledâ€œdeeplearning.â€Theï¬eld\nhasbeenrebrandedmanytimes,reï¬‚ectingtheinï¬‚uenceofdiï¬€erentresearchers\nanddiï¬€erentperspectives.\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\nlearningÂ knownÂ as c y b e r net i c sinÂ theÂ 1940sâ€“1960s,Â deepÂ learningÂ knownas",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "c o nnec t i o n i s minthe1980sâ€“1990s,andthecurrentresurgenceunderthename\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedinï¬gure.1.7\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\nhappensorcouldhappeninthebrain.Â Asaresult,oneofthenamesthatdeep\nlearninghasgonebyis ar t i ï¬c i al neur al net w o r k s(ANNs).Thecorresponding\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "computational principlesbehindthebrainandduplicateitsfunctionality.Another\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\nlightonthesebasicscientiï¬cquestionsareusefulapartfromtheirabilitytosolve\nengineeringapplications.\nThemoderntermâ€œdeeplearningâ€goesbeyondtheneuroscientiï¬cperspective\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "principleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\nlearningframeworksthatarenotnecessarilyneurallyinspired.\n1 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1940 1950 1960 1970 1980 1990 2000\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\nc y b e r n e t i c s\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\nFigure1.7:Theï¬gureshowstwoofthethreehistoricalwavesofartiï¬cialneuralnets\nresearch,asmeasuredbythefrequencyofthephrasesâ€œcyberneticsâ€andâ€œconnectionismâ€or\nâ€œneuralnetworksâ€accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "ï¬rstwavestartedwithcyberneticsinthe1940sâ€“1960s, withthedevelopmentoftheories\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\ntheï¬rstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980â€“1995period,\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "e t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\nthecorrespondingscientiï¬cactivityoccurred.\n1 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\nmotivatedfromaneuroscientiï¬cperspective.Thesemodelsweredesignedto\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\nf ( x w, ) =x 1w 1 + Â· Â· Â· +x nw n.Thisï¬rstwaveofneuralnetworksresearchwas\nknownascybernetics,asillustratedinï¬gure.1.7\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "ofbrainfunction.Thislinearmodelcouldrecognizetwodiï¬€erentcategoriesof\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\ntocorrespondtothedesireddeï¬nitionofthecategories,theweightsneededtobe\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.Â Inthe1950s,\ntheperceptron(Rosenblatt19581962,,)becametheï¬rstmodelthatcouldlearn\ntheweightsdeï¬ningthecategoriesgivenexamplesofinputsfromeachcategory.\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "time,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\nandHoï¬€1960,),andcouldalsolearntopredictthesenumbersfromdata.\nThesesimplelearningalgorithmsgreatlyaï¬€ectedthemodernlandscapeofma-\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\nmodiï¬edversionsofthestochasticgradientdescentalgorithmremainthedominant\ntrainingalgorithmsfordeeplearningmodelstoday.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "trainingalgorithmsfordeeplearningmodelstoday.\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\nlearningmodels,thoughinmanycasestheyare t r a i ne dindiï¬€erentwaysthanthe\noriginalmodelsweretrained.\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "andf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseï¬‚awsinlinearmodelscaused\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\n1969).Thiswastheï¬rstmajordipinthepopularityofneuralnetworks.\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\nlearningresearchers,butitisnolongerthepredominant guidefortheï¬eld.\nThemainreasonforthediminishedroleÂ ofneuroscienceindeeplearning\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\n1 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\ncansolvemanydiï¬€erenttasks.Neuroscientistshavefoundthatferretscanlearnto\nâ€œseeâ€withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\ndiï¬€erenttasksthatthebrainsolves.Beforethishypothesis,machinelearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "researchwasmorefragmented,withdiï¬€erentcommunitiesofresearchersstudying\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\nthe r e c t i ï¬ed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\nfunction.Thesimpliï¬edmodernversionwasdevelopedincorporatingideasfrom",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "manyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\nneuroscienceasaninï¬‚uence,and ()citingmoreengineering- Jarrett e t a l .2009\norientedinï¬‚uences.Whileneuroscienceisanimportantsourceofinspiration,it\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\ndiï¬€erentfunctionsthanmodernrectiï¬edlinearunits,butgreaterneuralrealism\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "neurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooï¬€ermuch\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\ninï¬‚uencethanresearchersworkinginothermachinelearningï¬eldssuchaskernel",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyï¬elds,\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\n1 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nneuroscienceatall.\nItisÂ worthÂ notingthatÂ theeï¬€orttounderstandhowtheÂ brainworkson\nanÂ algorithmicÂ lev elÂ isÂ aliveÂ andwell.ThisÂ endeavorÂ isÂ primarilyÂ knownas\nâ€œcomputational neuroscienceâ€andisaseparateï¬eldofstudyfromdeeplearning.\nItiscommonforresearcherstomovebackandforthbetweenbothï¬elds.The\nï¬eldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheï¬eldof",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "computational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\nmodelsofhowthebrainactuallyworks.\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\ni ng( ,; ,).Â Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\ntounderstandingthemind,combiningmultiplediï¬€erentlevelsofanalysis.During",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\nDespitetheirpopularity,symbolicmodelswerediï¬ƒculttoexplainintermsof\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\nThecentralideainconnectionism isthatalargenumberofsimplecomputational",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\ncomputational models.\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\nthatremaincentraltotodayâ€™sdeeplearning.\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\nbird,greentruck,andsoon.Thisrequiresninediï¬€erentneurons,andeachneuron\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\n1 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\nofonespeciï¬ccategoryofobjects.Â Theconceptofdistributedrepresentationis\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "e t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\nwithneuralnetworks.()and ()identiï¬edsomeof Hochreiter1991Bengio e t a l .1994\nthefundamentalmathematical diï¬ƒcultiesinmodelinglongsequences,describedin\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\nmemoryorLSTMnetworktoresolvesomeofthesediï¬ƒculties.Today,theLSTM",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "iswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\nprocessingtasksatGoogle.\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulï¬ll\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\notherï¬eldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "1992CortesandVapnik1995SchÃ¶lkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "ThisprogramunitedmachinelearningresearchgroupsledbyGeoï¬€reyHinton\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\nandcomputervision.\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydiï¬ƒcult\ntotrain.Â Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "thesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\nwiththehardwareavailableatthetime.\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\n1 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2006.Geoï¬€reyHintonshowedthatakindofneuralnetworkcalledadeepbelief\nnetworkcouldbeeï¬ƒcientlytrainedusingastrategycalledgreedylayer-wisepre-\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\nTheotherCIFAR-aï¬ƒliatedresearchgroupsquicklyshowedthatthesamestrategy\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\ntermâ€œdeeplearningâ€toemphasizethatresearcherswerenowabletotraindeeper\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\nmodelstoleveragelargelabeleddatasets.\n1 . 2 . 2 In creasin g D a t a s et S i zes\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "crucialtechnologythoughtheï¬rstexperimentswithartiï¬cialneuralnetworkswere\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\n1 9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nintoadatasetappropriateformachinelearningapplications.Theageofâ€œBig\nDataâ€hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\nestimationâ€”generalizingwelltonewdataafterobservingonlyasmallamount\nofdataâ€”hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\nlearning.\n1 . 2 . 3 In creasin g Mo d el S i zes\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\nBiologicalneuronsarenotespeciallydenselyconnected.Asseeninï¬gure,1.10\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\nsmalluntilquiterecently,asshowninï¬gure.Sincetheintroductionofhidden 1.11",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "units,artiï¬cialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\nallowfasterscaling,artiï¬cialneuralnetworkswillnothavethesamenumberof\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\nrepresentmorecomplicatedfunctionsthancurrentartiï¬cialneurons,sobiological",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworksmaybeevenlargerthanthisplotportrays.\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\nneuronsthanaleechwereunabletosolvesophisticatedartiï¬cialintelligenceprob-\nlems.Eventodayâ€™snetworks,whichweconsiderquitelargefromacomputational\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\nprimitivevertebrateanimalslikefrogs.\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\n2 0",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1900 1950 198520002015\nYear100101102103104105106107108109Datasetsize(numberexamples)\nIrisMNISTPublicSVHN\nImageNet\nCIFAR-10ImageNet10k\nILSVRC  2014Sports-1M\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\nWMT\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "ofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\ndemonstratethatneuralnetworkswereabletolearnspeciï¬ckindsoffunctions(Widrow\nandHoï¬€1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\nofthousandsofexamplessuchastheMNISTdataset(showninï¬gure)ofscans 1.9",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "ofhandwrittennumbers( ,).Intheï¬rstdecadeofthe2000s,more LeCun e t a l .1998b\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\ntheï¬rsthalfofthe2010s,signiï¬cantlylargerdatasets,containinghundredsofthousands\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBMâ€™sdatasetconstructed\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\n2 1",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nFigure1.9:ExampleinputsfromtheMNISTdataset.Theâ€œNISTâ€standsforNational\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\nTheâ€œMâ€standsforâ€œmodiï¬ed,â€sincethedatahasbeenpreprocessedforeasierusewith\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\nandassociatedlabelsdescribingwhichdigit0â€“9iscontainedineachimage.Thissimple\nclassiï¬cationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\nGeoï¬€reyHintonhasdescribeditasâ€œthe d r o s o p h i l aofmachinelearning,â€meaningthat\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\nconditions,muchasbiologistsoftenstudyfruitï¬‚ies.\n2 2",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\nexpectedtocontinuewellintothefuture.\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "accuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\nappliedwithsuccesstobroaderandbroadersetsofapplications.\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diï¬€erent\ncategoriesofobjects.Â ThelargestcontestinobjectrecognitionistheImageNet\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\nwonthischallengefortheï¬rsttimeandbyawidemargin,bringingdownthe",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "state-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\nforeachimageandthecorrectcategoryappearedintheï¬rstï¬veentriesofthis\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\nasshowninï¬gure.1.12",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "asshowninï¬gure.1.12\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\nthishistoryinmoredetailinsection.12.3\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "imagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\n2013)andyieldedsuperhumanperformanceintraï¬ƒcsignclassiï¬cation(Ciresan\n2 3",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\n12\n34\n567\n89\n1 0\nF r ui t ï¬‚yMo useC a tH um a n\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiï¬cialneural\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\nneuronsismostlyadesignconsideration.Someartiï¬cialneuralnetworkshavenearlyas\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "tohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\nnetworksizesfrom (). Wikipedia2015\n1.Adaptivelinearelement( ,) WidrowandHoï¬€1960\n2.Neocognitron(Fukushima1980,)\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "7.Distributedautoencoder(,) Le e t al.2012\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n10.GoogLeNet( ,) Szegedy e t al.2014a\n2 4",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\ne t a l .,).2012\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\nelementsofthesequence( ,).Recurrentneuralnetworks, GÃ¼lÃ§ehreandBengio2013",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustï¬xedinputs.\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\n2015).\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "toreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\nfuturecouldinprinciplebeappliedtonearlyanytask.\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\nalsosigniï¬cantlyimprovedtheperformanceofreinforcementlearningforrobotics\n(,). Finn e t a l .2015\nManyoftheseapplicationsofdeeplearningarehighlyproï¬table.Deeplearning",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "isnowusedÂ bymanytoptechnologycompaniesÂ includi ngGoogle,Â Microsoft,\nFacebook,IBM,Baidu,Apple,Adobe,Netï¬‚ix,NVIDIAandNEC.\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\nDistBelief(,),Caï¬€e(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "TensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\ncommercialproducts.\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\n2 5",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\nscientiï¬cï¬elds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "Barley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiï¬c\nï¬eldsinthefuture.\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "bringittonewfrontiers.\n2 6",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n1950 198520002015 2056\nYear10âˆ’ 210âˆ’ 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\n123\n456\n78\n91011\n121314\n151617\n181920\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\nFigure1.11:Sincetheintroductionofhiddenunits,artiï¬cialneuralnetworkshavedoubled\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\n1.Perceptron(,,) Rosenblatt19581962\n2.Adaptivelinearelement( ,) WidrowandHoï¬€1960\n3.Neocognitron(Fukushima1980,)",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "3.Neocognitron(Fukushima1980,)\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\n7.Meanï¬eldsigmoidbeliefnetwork(,) Saul e t al.1996\n8.LeNet-5( ,) LeCun e t al.1998b\n9.Echostatenetwork( ,) JaegerandHaas2004\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\n16.OMP-1network( ,) CoatesandNg2011\n17.Distributedautoencoder(,) Le e t al.2012\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\n20.GoogLeNet( ,) Szegedy e t al.2014a\n2 7",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER1.INTRODUCTION\n2010 2011 2012 2013 2014 2015\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classiï¬cationerrorrate\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\neveryyear,andyieldedlowerandlowererrorrateseachtime.Â DatafromRussakovsky\ne t a l . e t a l . ()and2014b He().2015\n2 8",
    "metadata": {
      "source": "[4]chapter-1-introduction.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "C h a p t e r 7\nRegularization f or D e e p L e ar n i n g\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\nasregularization.Â As wewillseethereareagreatmanyformsofregularization\navailabletothedeeplearningpractitioner. Infact,Â developingmoreeï¬€ective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "regularizationstrategieshasbeenoneofthemajorresearcheï¬€ortsintheï¬eld.\nChapterintroducedthebasicconceptsofgeneralization, underï¬tting,overï¬t- 5\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\ntoformdeepmodels.\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "Ifyouarealreadyfamiliarwiththeseconcepts,Â feelfreetoskiptherelevant\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\nbasicconceptstotheparticularcaseofneuralnetworks.\nInsection,wedeï¬nedregularizationasâ€œanymodiï¬cationwemaketo 5.2.2\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\nitstrainingerror.â€Therearemanyregularizationstrategies.Someputextra\nconstraintsÂ onaÂ machineÂ learningÂ model, suchÂ asaddingÂ restrictionsonÂ the",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "parametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\n228",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\nspeciï¬ckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\nbiasforreducedvariance.Aneï¬€ectiveregularizerisonethatmakesaproï¬table\ntrade,reducingvariancesigniï¬cantlywhilenotoverlyincreasingthebias.Whenwe\ndiscussedgeneralization andoverï¬ttinginchapter,wefocusedonthreesituations, 5\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\nprocessâ€”correspondingtounderï¬ttingandinducingbias,or(2)matchedthetrue",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "datageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\notherpossiblegeneratingprocessesâ€”theoverï¬ttingregimewherevariancerather\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\nmodelfromthethirdregimeintothesecondregime.\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\nextent,wearealwaystryingtoï¬tasquarepeg(thedatageneratingprocess)into",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "aroundhole(ourmodelfamily).\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\nsimplematterofï¬ndingthemodeloftherightsize,withtherightnumberof\nparameters.Instead,wemightï¬ndâ€”andindeedinpracticaldeeplearningscenarios,\nwealmostalwaysdoï¬ndâ€”thatthebestï¬ttingmodel(inthesenseofminimizing\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\nmodel.\n2 2 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.1ParameterNormPenalties\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\nandeï¬€ectiveregularizationstrategies.\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\nrameternormpenalty â„¦(Î¸)totheobjectivefunction J.Wedenotetheregularized\nobjectivefunctionbyËœ J:",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "objectivefunctionbyËœ J:\nËœ J , J , Î± (;Î¸Xy) = (;Î¸Xy)+â„¦()Î¸ (7.1)\nwhere Î±âˆˆ[0 ,âˆž)isahyperparameter thatweightstherelativecontributionofthe\nnormpenaltyterm,,relativetothestandardobjectivefunction â„¦ J.Setting Î±to0\nresultsinnoregularization. Largervaluesof Î±correspondtomoreregularization.\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction Ëœ Jit\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\nofthesizeoftheparametersÎ¸(orsomesubsetoftheparameters).Diï¬€erent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "choicesfortheparameternormcanresultindiï¬€erentsolutionsbeingpreferred. â„¦\nInthissection,wediscusstheeï¬€ectsofthevariousnormswhenusedaspenalties\nonthemodelparameters.\nBeforedelvingintotheregularizationbehaviorofdiï¬€erentnorms,wenotethat\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatâ„¦\npenalizes oftheaï¬ƒnetransformationateachlayerandleaves onlytheweights\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatoï¬taccurately",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "thantheweights.Â Eachweightspeciï¬eshowtwovariablesinteract.Â Fittingthe\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\ncanintroduceasigniï¬cantamountofunderï¬tting. Wethereforeusethevectorw\ntoindicatealloftheweightsthatshouldbeaï¬€ectedbyanormpenalty,whilethe\nvectorÎ¸denotesalloftheparameters,includingbothwandtheunregularized\nparameters.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "parameters.\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\npenaltywithadiï¬€erent Î±coeï¬ƒcientforeachlayerofthenetwork.Becauseitcan\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\nsearchspace.\n2 3 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\nbyaddingaregularizationtermâ„¦(Î¸) =1\n2î«î«w2\n2totheobjectivefunction.Inother\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\nTikhonovregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 14,
      "type": "default"
    }
  },
  {
    "content": "Tikhonovregularization.\nWecangainsomeinsightintothebehaviorofweightdecayregularization\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\npresentation,weassumenobiasparameter,soÎ¸isjustw.Suchamodelhasthe\nfollowingtotalobjectivefunction:\nËœ J , (;wXy) =Î±\n2wî€¾wwXy +( J; ,) , (7.2)\nwiththecorrespondingparametergradient\nâˆ‡ wËœ J , Î± (;wXy) = w+âˆ‡ w J , . (;wXy) (7.3)\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\nwww â† âˆ’ î€ Î±( +âˆ‡ w J , . (;wXy)) (7.4)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 15,
      "type": "default"
    }
  },
  {
    "content": "www â† âˆ’ î€ Î±( +âˆ‡ w J , . (;wXy)) (7.4)\nWrittenanotherway,theupdateis:\nww â† âˆ’(1 î€ Î±)âˆ’âˆ‡ î€ w J , . (;wXy) (7.5)\nWecanseethattheadditionoftheweightdecaytermhasmodiï¬edthelearning\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\nasinglestep.Butwhathappensovertheentirecourseoftraining?\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 16,
      "type": "default"
    }
  },
  {
    "content": "totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\nobtainsminimalunregularized trainingcost,wâˆ—=argminw J(w).Iftheobjective\nfunctionistrulyquadratic,asinthecaseofï¬ttingalinearregressionmodelwith\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ï¬ c p o i n t i n s p a c e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 17,
      "type": "default"
    }
  },
  {
    "content": "a n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ï¬€ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 18,
      "type": "default"
    }
  },
  {
    "content": "m o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\n2 3 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 19,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nmeansquarederror,thentheapproximationisperfect.Theapproximation Ë† Jis\ngivenby\nË† J J () = Î¸ (wâˆ—)+1\n2(wwâˆ’âˆ—)î€¾Hww (âˆ’âˆ—) , (7.6)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatwâˆ—.Thereis\nnoï¬rst-orderterminthisquadraticapproximation, becausewâˆ—isdeï¬nedtobea\nminimum,wherethegradientvanishes.Likewise,becausewâˆ—isthelocationofa\nminimumof,wecanconcludethatispositivesemideï¬nite. J H\nTheminimumofË† Joccurswhereitsgradient\nâˆ‡ wË† J() = (wHwwâˆ’âˆ—) (7.7)\nisequalto. 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 20,
      "type": "default"
    }
  },
  {
    "content": "âˆ‡ wË† J() = (wHwwâˆ’âˆ—) (7.7)\nisequalto. 0\nTostudytheeï¬€ectofweightdecay,wemodifyequationbyaddingthe 7.7\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\nversionofË† J.Weusethevariable Ëœwtorepresentthelocationoftheminimum.\nÎ±ËœwH+ (Ëœwwâˆ’âˆ—) = 0 (7.8)\n(+ )H Î±IËœwHw = âˆ—(7.9)\nËœwHI = (+ Î±)âˆ’ 1Hwâˆ—. (7.10)\nAs Î±approaches0,theregularizedsolution Ëœwapproacheswâˆ—.Butwhat\nhappensas Î±grows?BecauseHisrealandsymmetric,wecandecomposeit\nintoadiagonalmatrix Î›andanorthonormal basisofeigenvectors,Q,suchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 21,
      "type": "default"
    }
  },
  {
    "content": "HQQ = Î›î€¾.Applyingthedecompositiontoequation,weobtain:7.10\nËœwQQ = ( Î›î€¾+ ) Î±Iâˆ’ 1QQ Î›î€¾wâˆ—(7.11)\n=î¨\nQIQ (+ Î› Î±)î€¾î©âˆ’ 1\nQQ Î›î€¾wâˆ—(7.12)\n= (+ )Q Î› Î±Iâˆ’ 1Î›Qî€¾wâˆ—. (7.13)\nWeseethattheeï¬€ectofweightdecayistorescalewâˆ—alongtheaxesdeï¬nedby\ntheeigenvectorsofH.Speciï¬cally,thecomponentofwâˆ—thatisalignedwiththe\ni-theigenvectorofHisrescaledbyafactorofÎ» i\nÎ» i + Î±.(Youmaywishtoreview\nhowthiskindofscalingworks,ï¬rstexplainedinï¬gure).2.3\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 22,
      "type": "default"
    }
  },
  {
    "content": "where Î» iî€ Î±,theeï¬€ectofregularizationisrelativelysmall.However,components\nwith Î» iî€œ Î±willbeshrunktohavenearlyzeromagnitude.Thiseï¬€ectisillustrated\ninï¬gure.7.1\n2 3 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 23,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2wâˆ—\nËœ w\nFigure7.1:Anillustrationoftheeï¬€ectof L2(orweightdecay)regularizationonthevalue\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\nthepointËœw,thesecompetingobjectivesreachanequilibrium.Intheï¬rstdimension,the\neigenvalueoftheHessianof Jissmall.Â Theobjectivefunctiondoesnotincreasemuch",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 24,
      "type": "default"
    }
  },
  {
    "content": "whenmovinghorizontallyawayfromwâˆ—.Becausetheobjectivefunctiondoesnotexpress\nastrongpreferencealongthisdirection,theregularizerhasastrongeï¬€ectonthisaxis.\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\nisverysensitivetomovementsawayfromwâˆ—.Thecorrespondingeigenvalueislarge,\nindicatinghighcurvature.Asaresult,weightdecayaï¬€ectsthepositionof w2relatively\nlittle.\nOnlydirectionsalongwhichtheparameterscontributesigniï¬cantlytoreducing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 25,
      "type": "default"
    }
  },
  {
    "content": "theobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\ntellsusthatmovementinthisdirectionwillnotsigniï¬cantlyincreasethegradient.\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\nSofarwehavediscussedweightdecayintermsofitseï¬€ectontheoptimization\nofanabstract,general,quadraticcostfunction.Howdotheseeï¬€ectsrelateto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 26,
      "type": "default"
    }
  },
  {
    "content": "machinelearninginparticular?Wecanï¬ndoutbystudyinglinearregression,a\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\n2 3 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 27,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthesumofsquarederrors:\n( )Xwyâˆ’î€¾( )Xwyâˆ’ . (7.14)\nWhenweadd L2regularization, theobjectivefunctionchangesto\n( )Xwyâˆ’î€¾( )+Xwyâˆ’1\n2Î±wî€¾w . (7.15)\nThischangesthenormalequationsforthesolutionfrom\nwX= (î€¾X)âˆ’ 1Xî€¾y (7.16)\nto\nwX= (î€¾XI+ Î±)âˆ’ 1Xî€¾y . (7.17)\nThematrixXî€¾Xinequationisproportionaltothecovariancematrix 7.161\nmXî€¾X.\nUsing L2regularizationreplacesthismatrixwithî€€\nXî€¾XI+ Î±î€âˆ’ 1inequation.7.17\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof Î±tothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 28,
      "type": "default"
    }
  },
  {
    "content": "diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\ntoâ€œperceiveâ€theinputXashavinghighervariance,whichmakesitshrinkthe\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\nthisaddedvariance.\n7 . 1 . 2 L1Regu l a ri z a t i o n\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\nwaystopenalizethesizeofthemodelparameters.Â Anotheroptionistouse L1\nregularization.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 29,
      "type": "default"
    }
  },
  {
    "content": "regularization.\nFormally, L1regularizationonthemodelparameter isdeï¬nedas:w\nâ„¦() = Î¸ ||||w 1=î˜\ni| w i| , (7.18)\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\nnowdiscusstheeï¬€ectof L1regularizationonthesimplelinearregressionmodel,\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\nparticular,weareinterestedindelineatingthediï¬€erencesbetween L1and L2forms",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 30,
      "type": "default"
    }
  },
  {
    "content": "2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\ni n t ro d u c e t h e t e rmâ„¦() = Î¸ ||âˆ’ w w( ) o|| 1=î\ni| w iâˆ’ w( ) o\ni| .\n2 3 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 31,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter â„¦ Î±.\nThus,theregularizedobjectivefunction Ëœ J , (;wXy)isgivenby\nËœ J , Î± (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\nwiththecorrespondinggradient(actually,sub-gradient):\nâˆ‡ wËœ J , Î± (;wXy) = sign( )+w âˆ‡ w J ,(Xyw;) (7.20)\nwhere issimplythesignofappliedelement-wise. sign( )w w",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 32,
      "type": "default"
    }
  },
  {
    "content": "Byinspectingequation,wecanseeimmediately thattheeï¬€ectof 7.20 L1\nregularizationisquitediï¬€erentfromthatof L2regularization. Speciï¬cally,wecan\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\nregularization.\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 33,
      "type": "default"
    }
  },
  {
    "content": "viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\ninthissettingisgivenby\nâˆ‡ wË† J() = (wHwwâˆ’âˆ—) , (7.21)\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J wwâˆ—.\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 34,
      "type": "default"
    }
  },
  {
    "content": "Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\naccomplishedusingPCA.\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\nposesintoasumovertheparameters:\nË† J , J (;wXy) = (wâˆ—; )+Xy ,î˜\niî€”1\n2H i , i(w iâˆ’wâˆ—\ni)2+ Î± w| i|î€•\n.(7.22)\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\n(foreachdimension),withthefollowingform: i\nw i= sign( wâˆ—\ni)maxî€š\n| wâˆ—\ni|âˆ’Î±\nH i , i,0î€›\n. (7.23)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 35,
      "type": "default"
    }
  },
  {
    "content": "i)maxî€š\n| wâˆ—\ni|âˆ’Î±\nH i , i,0î€›\n. (7.23)\n2 3 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 36,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConsiderthesituationwhere wâˆ—\ni > i 0forall.Therearetwopossibleoutcomes:\n1.Thecasewhere wâˆ—\niâ‰¤Î±\nH i , i.Heretheoptimalvalueof w iundertheregularized\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\ntotheregularizedobjectiveËœ J(w;Xy ,)isoverwhelmedâ€”indirection iâ€”by\nthe L1regularizationwhichpushesthevalueof w itozero.\n2.Thecasewhere wâˆ—\ni >Î±\nH i , i.Inthiscase,theregularizationdoesnotmovethe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 37,
      "type": "default"
    }
  },
  {
    "content": "optimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\ndistanceequaltoÎ±\nH i , i.\nAsimilarprocesshappenswhen wâˆ—\ni <0,butwiththe L1penaltymaking w iless\nnegativebyÎ±\nH i , i,or0.\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\ndiï¬€erentbehaviorthanariseswith L2regularization. Equationgavethe7.13",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 38,
      "type": "default"
    }
  },
  {
    "content": "solution Ëœ wfor L2regularization. Ifwerevisitthatequationusingtheassumption\nofadiagonalandpositivedeï¬niteHessianHthatweintroducedforouranalysisof\nL1regularization,weï¬ndthatËœ w i=H i , i\nH i , i + Î±wâˆ—\ni.If wâˆ—\niwasnonzero,then Ëœ w iremains\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\ntobecomesparse,while L1regularizationmaydosoforlargeenough. Î±\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 39,
      "type": "default"
    }
  },
  {
    "content": "asafeatureselectionmechanism.Featureselectionsimpliï¬esamachinelearning\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 40,
      "type": "default"
    }
  },
  {
    "content": "asMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\ntoMAPBayesianinferencewithaGaussianpriorontheweights.Â For L1regu-\nlarization,thepenalty Î±â„¦(w)= Î±î\ni| w i|usedtoregularizeacostfunctionis\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26wâˆˆ Rn:\nlog() = pwî˜\nilogLaplace( w i;0 ,1\nÎ±) = âˆ’|||| Î±w 1+log log2 n Î± nâˆ’ .(7.24)\n2 3 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 41,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\nignorethe termsbecausetheydonotdependon. log log2 Î±âˆ’ w\n7.2NormPenaltiesasConstrainedOptimization\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\nËœ J , J , Î± . (;Î¸Xy) = (;Î¸Xy)+â„¦()Î¸ (7.25)\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 42,
      "type": "default"
    }
  },
  {
    "content": "functionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeï¬ƒcient,\ncalledaKarushâ€“Kuhnâ€“Tucker(KKT)multiplier,andafunctionrepresenting\nwhethertheconstraintissatisï¬ed.Ifwewantedtoconstrainâ„¦(Î¸)tobelessthan\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\nL âˆ’ (; ) = (; )+(â„¦() Î¸ , Î±Xy , JÎ¸Xy , Î±Î¸ k .) (7.26)\nThesolutiontotheconstrainedproblemisgivenby\nÎ¸âˆ—= argmin\nÎ¸max\nÎ± , Î±â‰¥ 0L()Î¸ , Î± . (7.27)\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 Î¸",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 43,
      "type": "default"
    }
  },
  {
    "content": "and Î±.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\nconstraint.Manydiï¬€erentproceduresarepossibleâ€”somemayusegradientdescent,\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszeroâ€”butinall\nprocedures Î±mustincreasewheneverâ„¦(Î¸) > kanddecreasewheneverâ„¦(Î¸) < k.\nAllpositive Î±encourage â„¦(Î¸)toshrink.Theoptimalvalue Î±âˆ—willencourage â„¦(Î¸)\ntoshrink,butnotsostronglytomakebecomelessthan. â„¦()Î¸ k\nTogainsomeinsightintotheeï¬€ectoftheconstraint,wecanï¬x Î±âˆ—andview\ntheproblemasjustafunctionof:Î¸",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 44,
      "type": "default"
    }
  },
  {
    "content": "theproblemasjustafunctionof:Î¸\nÎ¸âˆ—= argmin\nÎ¸L(Î¸ , Î±âˆ—) = argmin\nÎ¸J , Î± (;Î¸Xy)+âˆ—â„¦()Î¸ .(7.28)\nThisisexactlythesameastheregularizedtrainingproblemofminimizing Ëœ J.\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\nweights.Ifisthe â„¦ L2norm,thentheweightsareconstrainedtolieinan L2\nball.Â Ifisthe â„¦ L1norm,thentheweightsareconstrainedtolieinaregionof\n2 3 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 45,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\nimposebyusingweightdecaywithcoeï¬ƒcient Î±âˆ—becausethevalueof Î±âˆ—doesnot\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\nbetween kand Î±âˆ—dependsontheformof J.Whilewedonotknowtheexactsize\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing Î±\ninordertogroworshrinktheconstraintregion.Larger Î±willresultinasmaller",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 46,
      "type": "default"
    }
  },
  {
    "content": "constraintregion.Smallerwillresultinalargerconstraintregion. Î±\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\ndescenttotakeastepdownhillon J(Î¸)andthenprojectÎ¸backtothenearest\npointthatsatisï¬esâ„¦(Î¸) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof Î±that\ncorrespondstothis. k\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 47,
      "type": "default"
    }
  },
  {
    "content": "constraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\nprocedurestogetstuckinlocalminimacorrespondingtosmallÎ¸.Whentraining\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\nâ€œdeadunits.â€Theseareunitsthatdonotcontributemuchtothebehaviorofthe\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\nallverysmall.Â Whentrainingwithapenaltyonthenormoftheweights,these\nconï¬gurations canbelocallyoptimal,evenifitispossibletosigniï¬cantlyreduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 48,
      "type": "default"
    }
  },
  {
    "content": "Jbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\nhaveaneï¬€ectwhentheweightsbecomelargeandattempttoleavetheconstraint\nregion.\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 49,
      "type": "default"
    }
  },
  {
    "content": "largegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\nconsistentlyincreasethesizeoftheweights,thenÎ¸rapidlymovesawayfrom\ntheoriginuntilnumericaloverï¬‚owoccurs.Explicitconstraintswithreprojection\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\nsomestability.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 50,
      "type": "default"
    }
  },
  {
    "content": "somestability.\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\n2 3 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 51,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 52,
      "type": "default"
    }
  },
  {
    "content": "obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\nanexplicitconstraintwithreprojection.\n7.3RegularizationandUnder-ConstrainedProblems\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\nerlydeï¬ned.Manylinearmodelsinmachinelearning,includinglinearregression\nandPCA,dependoninvertingthematrixXî€¾X.Thisisnotpossiblewhenever\nXî€¾Xissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 53,
      "type": "default"
    }
  },
  {
    "content": "butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\nXî€¾XI+ Î±instead.Thisregularizedmatrixisguaranteedtobeinvertible.\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 54,
      "type": "default"
    }
  },
  {
    "content": "theclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\nclassiï¬cation,then2wwillalsoachieveperfectclassiï¬cationandhigherlikelihood.\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\nimplementationofgradientdescentwilleventuallyreachsuï¬ƒcientlylargeweights\ntocausenumericaloverï¬‚ow,atwhichpointitsbehaviorwilldependonhowthe\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 55,
      "type": "default"
    }
  },
  {
    "content": "Mostformsofregularizationareabletoguaranteetheconvergenceofiterative\nmethodsappliedtounderdetermined problems.Â Forexample,weightdecaywill\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\nslopeofthelikelihoodisequaltotheweightdecaycoeï¬ƒcient.\nTheideaofusingregularizationtosolveunderdetermined problemsextends\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\nproblems.\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\n2 3 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 56,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMoore-Penrosepseudoinverse.Recallthatonedeï¬nitionofthepseudoinverse\nX+ofamatrixisX\nX+=lim\nÎ±î€¦ 0(Xî€¾XI+ Î±)âˆ’ 1Xî€¾. (7.29)\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\ndecay.Speciï¬cally,equationisthelimitofequationastheregularization 7.29 7.17\ncoeï¬ƒcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\nunderdetermined problemsusingregularization.\n7.4DatasetAugmentation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 57,
      "type": "default"
    }
  },
  {
    "content": "7.4DatasetAugmentation\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\nfakedata.\nThisapproachiseasiestforclassiï¬cation.Aclassiï¬erneedstotakeacompli-\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 58,
      "type": "default"
    }
  },
  {
    "content": "Thismeansthatthemaintaskfacingaclassiï¬eristobeinvarianttoawidevariety\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\ntheinputsinourtrainingset. x\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\nisdiï¬ƒculttogeneratenewfakedataforadensityestimationtaskunlesswehave\nalreadysolvedthedensityestimationproblem.\nDatasetaugmentationhasbeenaparticularlyeï¬€ectivetechniqueforaspeciï¬c\nclassiï¬cationproblem:objectrecognition.Imagesarehighdimensionalandinclude",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 59,
      "type": "default"
    }
  },
  {
    "content": "anenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\ntheimagehavealsoprovenquiteeï¬€ective.\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 60,
      "type": "default"
    }
  },
  {
    "content": "class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\ndiï¬€erencebetweenâ€˜bâ€™andâ€˜dâ€™andthediï¬€erencebetweenâ€˜6â€™andâ€˜9â€™,sohorizontal\nï¬‚ipsand180â—¦rotationsarenotappropriatewaysofaugmentingdatasetsforthese\ntasks.\n2 4 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 61,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTherearealsotransformationsthatwewouldlikeourclassiï¬erstobeinvariant\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\nbeimplementedasasimplegeometricoperationontheinputpixels.\nDatasetaugmentationiseï¬€ectiveforspeechrecognitiontasksaswell(Jaitly\nandHinton2013,).\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\ncanalsobeseenasaformofdataaugmentation.Formanyclassiï¬cationand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 62,
      "type": "default"
    }
  },
  {
    "content": "evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 63,
      "type": "default"
    }
  },
  {
    "content": "augmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\nthatthisapproachcanbehighlyeï¬€ectiveprovidedthatthemagnitudeofthe\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\nmultiplyingbynoise.\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\ntheeï¬€ectofdatasetaugmentationintoaccount.Often,hand-designeddataset",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 64,
      "type": "default"
    }
  },
  {
    "content": "augmentationschemescandramaticallyreducethegeneralization errorofamachine\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 65,
      "type": "default"
    }
  },
  {
    "content": "numeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 66,
      "type": "default"
    }
  },
  {
    "content": "whileoperationsthatarespeciï¬ctooneapplicationdomain(suchasrandomly\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\n2 4 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 67,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.5NoiseRobustness\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinï¬nitesimal\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 68,
      "type": "default"
    }
  },
  {
    "content": "appliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\nofthatapproach.\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,).Â Thiscan\nbeinterpretedasaÂ stochasticimplementation ofÂ BayesianinferenceÂ overthe\nweights.Â TheBayesiantreatmentoflearningwouldconsiderthemodelweights",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 69,
      "type": "default"
    }
  },
  {
    "content": "tobeuncertainandrepresentableviaaprobabilitydistributionthatreï¬‚ectsthis\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreï¬‚ect\nthisuncertainty.\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\nafunction Ë† y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 70,
      "type": "default"
    }
  },
  {
    "content": "costfunctionbetweenthemodelpredictions Ë† y()xandthetruevalues: y\nJ= E p x , y ( )î€‚(Ë† y y ()xâˆ’)2î€ƒ\n. (7.30)\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\nWenowassumethatwitheachinputpresentationwealsoincludearandom\nperturbation î€ Wâˆ¼N(î€; 0 , Î·I)ofthenetworkweights.Letusimaginethatwe\nhaveastandard l-layerMLP.WedenotetheperturbedmodelasË† y î€ W(x).Despite\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 71,
      "type": "default"
    }
  },
  {
    "content": "outputofthenetwork.Theobjectivefunctionthusbecomes:\nËœ J W= E p , y , ( x î€ W )î¨\n(Ë† y î€ W() )xâˆ’ y2î©\n(7.31)\n= E p , y , ( x î€ W )î€‚\nË† y2\nî€ W()2Ë†xâˆ’ y y î€ W()+x y2î€ƒ\n.(7.32)\nForsmall Î·,theminimization of Jwithaddedweightnoise(withcovariance\nÎ·I)isequivalenttominimization of Jwithanadditionalregularizationterm:\n2 4 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 72,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nÎ· E p , y ( x )î€‚î«âˆ‡ WË† y()xî«2î€ƒ\n.Thisformofregularizationencouragestheparametersto\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\narelativelysmallinï¬‚uenceontheoutput.Inotherwords,itpushesthemodel\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\nweights,ï¬ndingpointsthatarenotmerelyminima,butminimasurroundedby\nï¬‚atregions(HochreiterandSchmidhuber1995,).Inthesimpliï¬edcaseoflinear",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 73,
      "type": "default"
    }
  },
  {
    "content": "regression(where,forinstance, Ë† y(x) =wî€¾x+ b),thisregularizationtermcollapses\ninto Î· E p ( ) xî€‚\nî«î«x2î€ƒ\n,whichisnotafunctionofparametersandthereforedoesnot\ncontributetothegradientofËœ J Wwithrespecttothemodelparameters.\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 74,
      "type": "default"
    }
  },
  {
    "content": "constant î€,thetrainingsetlabel yiscorrectwithprobability 1âˆ’ î€,andotherwise\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\nsoftmaxwith koutputvaluesbyreplacingthehardandclassiï¬cationtargets 0 1\nwithtargetsofî€\nkâˆ’ 1and1âˆ’ î€,respectively.Thestandardcross-entropylossmay\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 75,
      "type": "default"
    }
  },
  {
    "content": "classiï¬erandhardtargetsmayactuallyneverconvergeâ€”thesoftmaxcannever\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\ndiscouragingcorrectclassiï¬cation.Thisstrategyhasbeenusedsincethe1980s\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 76,
      "type": "default"
    }
  },
  {
    "content": "etal.,).2015\n7.6Semi-SupervisedLearning\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\nx.\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\n2 4 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 77,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\nsimilarrepresentations.Alinearclassiï¬erinthenewspacemayachievebetter\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 78,
      "type": "default"
    }
  },
  {
    "content": "analysisasapre-processingstepbeforeapplyingaclassiï¬er(ontheprojected\ndata).\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\nthentrade-oï¬€thesupervisedcriterion âˆ’log P( y x|)withtheunsupervisedor\ngenerativeone(suchasâˆ’log P( x)orâˆ’log P( x y ,)).Thegenerativecriterionthen\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 79,
      "type": "default"
    }
  },
  {
    "content": "learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\ninthetotalcriterion,onecanï¬ndabettertrade-oï¬€thanwithapurelygenerative\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\nBengio2008,).\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 80,
      "type": "default"
    }
  },
  {
    "content": "functionofakernelmachineusedforregression,inwhichtheusageofunlabeled\nexamplesformodeling improvesquitesigniï¬cantly. P() x P( ) y x|\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\n7.7Multi-TaskLearning\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\narisingoutofseveraltasks.Â Inthesamewaythatadditionaltrainingexamples",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 81,
      "type": "default"
    }
  },
  {
    "content": "putmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\nconstrainedtowardsgoodvalues(assumingthesharingisjustiï¬ed),oftenyielding\nbettergeneralization.\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\ndiï¬€erentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\n2 4 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 82,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\nparameters:\n1.Task-speciï¬cparameters(whichonlybeneï¬tfromtheexamplesoftheirtask\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\nnetworkinï¬gure.7.2\n2.Genericparameters,sharedacrossallthetasks(whichbeneï¬tfromthe\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\ninï¬gure.7.2\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 83,
      "type": "default"
    }
  },
  {
    "content": "h( s h a r e d )h( s h a r e d )\nxx\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\nandthisï¬gureillustratesthecommonsituationwherethetasksshareacommoninputbut\ninvolvediï¬€erenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\ncanbesharedacrosssuchtasks,whiletask-speciï¬cparameters(associatedrespectively\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 84,
      "type": "default"
    }
  },
  {
    "content": "sharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 85,
      "type": "default"
    }
  },
  {
    "content": "associatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\n2 4 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 86,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n0 50 100 150 200 250\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\nV a l i d a t i o n s e t l o s s\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 87,
      "type": "default"
    }
  },
  {
    "content": "increaseagain,forminganasymmetricU-shapedcurve.\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\nthediï¬€erenttasksarevalid,meaningthatthereissomethingsharedacrosssome\nofthetasks.\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\nfollowing:amongthefactorsthatÂ explainthevariationsÂ observedÂ inthedata",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 88,
      "type": "default"
    }
  },
  {
    "content": "associatedwiththediï¬€erenttasks,somearesharedacrosstwoormoretasks.\n7.8EarlyStopping\nWhentraininglargemodelswithsuï¬ƒcientrepresentationalcapacitytooverï¬t\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\nvalidationseterrorbeginstoriseagain.Seeï¬gureforanexampleofthis 7.3\nbehavior.Thisbehavioroccursveryreliably.\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 89,
      "type": "default"
    }
  },
  {
    "content": "timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\n2 4 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 90,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\nvalidationerrorforsomepre-speciï¬ednumberofiterations.Thisprocedureis\nspeciï¬edmoreformallyinalgorithm .7.1\nAlgorithmÂ 7.1TheearlystoppingÂ meta-algorithmforÂ determiningtheÂ best\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\nvalidationset.\nLetbethenumberofstepsbetweenevaluations. n",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 91,
      "type": "default"
    }
  },
  {
    "content": "Letbethenumberofstepsbetweenevaluations. n\nLet pbetheâ€œpatience,â€thenumberoftimestoobserveworseningvalidationset\nerrorbeforegivingup.\nLetÎ¸ obetheinitialparameters.\nÎ¸Î¸â† o\niâ†0\njâ†0\nvâ†âˆž\nÎ¸âˆ—â†Î¸\niâˆ—â† i\nwhiledo j < p\nUpdatebyrunningthetrainingalgorithmforsteps. Î¸ n\ni i n â†+\nvî€°â†ValidationSetError ()Î¸\nif vî€°< vthen\njâ†0\nÎ¸âˆ—â†Î¸\niâˆ—â† i\nv vâ†î€°\nelse\nj jâ†+1\nendif\nendwhile\nBestparametersareÎ¸âˆ—,bestnumberoftrainingstepsis iâˆ—\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 92,
      "type": "default"
    }
  },
  {
    "content": "usedformofregularizationindeeplearning.Itspopularityisduebothtoits\neï¬€ectivenessanditssimplicity.\nOnewaytothinkofearlystoppingisasaveryeï¬ƒcienthyperparameter selection\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\nWecanseeinï¬gurethatthishyperparameter hasaU-shapedvalidationset 7.3\n2 4 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 93,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\nU-shapedvalidationsetperformancecurve,asillustratedinï¬gure.Inthecaseof 5.3\nearlystopping,wearecontrollingtheeï¬€ectivecapacityofthemodelbydetermining\nhowmanystepsitcantaketoï¬tthetrainingset.Mosthyperparametersmustbe\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\natthestartoftraining,thenruntrainingforseveralstepstoseeitseï¬€ect.The",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 94,
      "type": "default"
    }
  },
  {
    "content": "â€œtrainingtimeâ€Â hyperparam eterisuniqueinthatbydeï¬nitionasinglerunof\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniï¬cantcost\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 95,
      "type": "default"
    }
  },
  {
    "content": "smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 96,
      "type": "default"
    }
  },
  {
    "content": "training,theseoccasionalslowwriteshavelittleeï¬€ectonthetotaltrainingtime.\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\nsmallweights.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 97,
      "type": "default"
    }
  },
  {
    "content": "smallweights.\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\noccuratalocalminimumofthetrainingobjective.\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 98,
      "type": "default"
    }
  },
  {
    "content": "trainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\nonecanuseforthissecondtrainingprocedure.\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\n2 4 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 99,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\ntheearlystoppingproceduredeterminedwasoptimalintheï¬rstpass.Thereare\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\ntrainingsetisbigger.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 100,
      "type": "default"
    }
  },
  {
    "content": "trainingsetisbigger.\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\ntotrain,thenretrainingonallthedata.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 Î¸usingX( ) s ubtr a i nand\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 101,
      "type": "default"
    }
  },
  {
    "content": "returns iâˆ—,theoptimalnumberofsteps.\nSettorandomvaluesagain. Î¸\nTrainonX( ) t r a i nandy( ) t r a i nfor iâˆ—steps.\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\nfromtheï¬rstroundoftrainingandthencontinuetrainingbutnowusingallof\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\nofanumberofsteps.Â Instead,wecanmonitortheaveragelossfunctiononthe\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 102,
      "type": "default"
    }
  },
  {
    "content": "setobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\nThisprocedureispresentedmoreformallyinalgorithm .7.3\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 103,
      "type": "default"
    }
  },
  {
    "content": "oftrainingiterations,italsohasthebeneï¬tofprovidingregularizationwithout\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\nthegradientsofsuchadditionalterms.\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\n2 4 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 104,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\ntivevaluewestarttooverï¬t,thencontinuetraininguntilthatvalueisreached.\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\nrespectively.\nRunearlystopping(algorithm )startingfromrandom 7.1 Î¸usingX( ) s ubtr a i nand",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 105,
      "type": "default"
    }
  },
  {
    "content": "y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\nupdates.Î¸\nî€ J , â†(Î¸X( ) s ubtr a i n,y( ) s ubtr a i n)\nwhile J ,(Î¸X( v a l i d ),y( v a l i d )) > î€do\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\nendwhile\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\n()and ()arguedthatearlystoppinghastheeï¬€ectof 1995aSjÃ¶bergandLjung1995\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 106,
      "type": "default"
    }
  },
  {
    "content": "spaceintheneighborhoodoftheinitialparametervalueÎ¸ o,asillustratedin\nï¬gure.Morespeciï¬cally,imaginetaking 7.4 Ï„optimization steps(corresponding\nto Ï„trainingiterations)andwithlearningrate î€.Wecanviewtheproduct î€ Ï„\nasameasureofeï¬€ectivecapacity.Assumingthegradientisbounded,restricting\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\nspacereachablefromÎ¸ o.Inthissense, î€ Ï„behavesasifitwerethereciprocalof\nthecoeï¬ƒcientusedforweightdecay.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 107,
      "type": "default"
    }
  },
  {
    "content": "thecoeï¬ƒcientusedforweightdecay.\nIndeed,wecanshowhowâ€”inthecaseofasimplelinearmodelwithaquadratic\nerrorfunctionandsimplegradientdescentâ€”earlystoppingisequivalentto L2\nregularization.\nInordertocomparewithclassical L2regularization, weexamineasimple\nsettingwheretheonlyparametersarelinearweights(Î¸=w).Wecanmodel\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\nempiricallyoptimalvalueoftheweightswâˆ—:\nË† J J () = Î¸ (wâˆ—)+1\n2(wwâˆ’âˆ—)î€¾Hww (âˆ’âˆ—) , (7.33)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 108,
      "type": "default"
    }
  },
  {
    "content": "Ë† J J () = Î¸ (wâˆ—)+1\n2(wwâˆ’âˆ—)î€¾Hww (âˆ’âˆ—) , (7.33)\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatwâˆ—.Giventhe\nassumptionthatwâˆ—isaminimumof J(w),weknowthatHispositivesemideï¬nite.\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\nâˆ‡ wË† J() = (wHwwâˆ’âˆ—) . (7.34)\n2 5 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 109,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nw 1w 2wâˆ—\nËœ w\nw 1w 2wâˆ—\nËœ w\nFigure7.4:Anillustrationoftheeï¬€ectofearlystopping. ( L e f t )Thesolidcontourlines\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointwâˆ—that\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpointËœw.\n( R i g h t )Anillustrationoftheeï¬€ectof L2regularizationforcomparison.Thedashedcircles",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 110,
      "type": "default"
    }
  },
  {
    "content": "indicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\nnearertheoriginthantheminimumoftheunregularizedcost.\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\nanalyzinggradientdescentonË† J:\nw( ) Ï„= w( 1 ) Ï„âˆ’âˆ’âˆ‡ î€ wË† J(w( 1 ) Ï„âˆ’) (7.35)\n= w( 1 ) Ï„âˆ’âˆ’ î€Hw(( 1 ) Ï„âˆ’âˆ’wâˆ—) (7.36)\nw( ) Ï„âˆ’wâˆ—= ( )(IHâˆ’ î€w( 1 ) Ï„âˆ’âˆ’wâˆ—) . (7.37)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 111,
      "type": "default"
    }
  },
  {
    "content": "w( ) Ï„âˆ’wâˆ—= ( )(IHâˆ’ î€w( 1 ) Ï„âˆ’âˆ’wâˆ—) . (7.37)\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\ntheeigendecompositionofH:H=QQ Î›î€¾,where Î›isadiagonalmatrixandQ\nisanorthonormalbasisofeigenvectors.\nw( ) Ï„âˆ’wâˆ—= (IQQ âˆ’ î€ Î›î€¾)(w( 1 ) Ï„âˆ’âˆ’wâˆ—)(7.38)\nQî€¾(w( ) Ï„âˆ’wâˆ—) = ( )Iâˆ’ î€ Î›Qî€¾(w( 1 ) Ï„âˆ’âˆ’wâˆ—) (7.39)\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 112,
      "type": "default"
    }
  },
  {
    "content": "a l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\ni n i t i a l v a l u e w( 0 ).\n2 5 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 113,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nAssumingthatw( 0 )=0andthat î€ischosentobesmallenoughtoguarantee\n|1âˆ’ î€ Î» i| <1,theparametertrajectoryduringtrainingafter Ï„parameterupdates\nisasfollows:\nQî€¾w( ) Ï„= [ ( )Iâˆ’Iâˆ’ î€ Î›Ï„]Qî€¾wâˆ—. (7.40)\nNow,theexpressionforQî€¾Ëœwinequationfor7.13 L2regularizationcanberear-\nrangedas:\nQî€¾ËœwI = (+ Î› Î±)âˆ’ 1Î›Qî€¾wâˆ—(7.41)\nQî€¾ËœwII = [âˆ’(+ Î› Î±)âˆ’ 1Î±]Qî€¾wâˆ—(7.42)\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 î€,\nÎ± Ï„,andarechosensuchthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 114,
      "type": "default"
    }
  },
  {
    "content": "Î± Ï„,andarechosensuchthat\n( )Iâˆ’ î€ Î›Ï„= (+ ) Î› Î±Iâˆ’ 1Î± , (7.43)\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\nthatifall Î» iaresmall(thatis, î€ Î» iî€œ1and Î» i /Î±î€œ1)then\nÏ„â‰ˆ1\nî€ Î±, (7.44)\nÎ±â‰ˆ1\nÏ„ î€. (7.45)\nThatis,undertheseassumptions,thenumberoftrainingiterations Ï„playsarole",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 115,
      "type": "default"
    }
  },
  {
    "content": "inverselyproportionaltothe L2regularizationparameter,andtheinverseof Ï„ î€\nplaystheroleoftheweightdecaycoeï¬ƒcient.\nParametervaluescorrespondingtodirectionsofsigniï¬cantcurvature(ofthe\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\ntodirectionsofsigniï¬cantcurvaturetendtolearnearlyrelativetoparameters\ncorrespondingtodirectionsoflesscurvature.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 116,
      "type": "default"
    }
  },
  {
    "content": "correspondingtodirectionsoflesscurvature.\nThederivationsinthissectionhaveshownthatatrajectoryoflength Ï„ends\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 117,
      "type": "default"
    }
  },
  {
    "content": "determinesthecorrectamountofregularizationwhileweightdecayrequiresmany\ntrainingexperimentswithdiï¬€erentvaluesofitshyperparameter.\n2 5 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 118,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n7.9ParameterTyingandParameterSharing\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\ntotheparameters,wehavealwaysdonesowithrespecttoaï¬xedregionorpoint.\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\ndeviatingfromtheï¬xedvalueofzero.However,sometimeswemayneedother\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 119,
      "type": "default"
    }
  },
  {
    "content": "butweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\nshouldbesomedependencies betweenthemodelparameters.\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\nhavetwomodelsperformingthesameclassiï¬cationtask(withthesamesetof\nclasses)butwithsomewhatdiï¬€erentinputdistributions.Formally,wehavemodel\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 120,
      "type": "default"
    }
  },
  {
    "content": "maptheinputÂ totwoÂ diï¬€erent,Â butÂ related outputs:Ë† y( ) A= f(w( ) A,x)and\nË† y( ) B= ( gw( ) B,x).\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\ntoeachother: âˆ€ i, w( ) A\nishouldbecloseto w( ) B\ni.Wecanleveragethisinformation\nthroughregularization. Speciï¬cally,wecanuseaparameternormpenaltyofthe\nform: â„¦(w( ) A,w( ) B)=î«w( ) Aâˆ’w( ) Bî«2\n2.Â Hereweusedan L2penalty,butother\nchoicesarealsopossible.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 121,
      "type": "default"
    }
  },
  {
    "content": "choicesarealsopossible.\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\ntheparametersofonemodel,trainedasaclassiï¬erinasupervisedparadigm,to\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\nconstructedsuchthatmanyoftheparametersintheclassiï¬ermodelcouldbe\npairedtocorrespondingparametersintheunsupervisedmodel.\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 122,
      "type": "default"
    }
  },
  {
    "content": "closetooneanother,themorepopularwayistouseconstraints:toforcesets\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\nassharingauniquesetofparameters.Asigniï¬cantadvantageofparametersharing\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\nmodelsâ€”suchastheconvolutionalneuralnetworkâ€”thiscanleadtosigniï¬cant",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 123,
      "type": "default"
    }
  },
  {
    "content": "reductioninthememoryfootprintofthemodel.\n2 5 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 124,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\ntocomputervision.\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 125,
      "type": "default"
    }
  },
  {
    "content": "iscomputedoverdiï¬€erentlocationsintheinput.Thismeansthatwecanï¬nda\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\ni+1intheimage.\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\nmodelparametersandtosigniï¬cantlyincreasenetworksizeswithoutrequiringa\ncorrespondingincreaseintrainingdata.Â Itremainsoneofthebestexamplesof\nhowtoeï¬€ectivelyincorporatedomainknowledgeintothenetworkarchitecture.\nCNNswillbediscussedinmoredetailinchapter.9\n7.10SparseRepresentations",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 126,
      "type": "default"
    }
  },
  {
    "content": "7.10SparseRepresentations\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\npenaltyonthemodelparameters.\nWehaveÂ alreadydiscussedÂ (insection)how7.1.2 L1penalizationinduces\nasparseparametrizationâ€”meaning thatmanyoftheparametersbecomezero\n(orclosetoÂ zero).Representationalsparsity,Â onÂ theotherÂ hand,Â des cribesa",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 127,
      "type": "default"
    }
  },
  {
    "content": "representationwheremanyoftheelementsoftherepresentationarezero(orclose\ntozero).Asimpliï¬edviewofthisdistinctioncanbeillustratedinthecontextof\nlinearregression:\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°18\n5\n15\nâˆ’9\nâˆ’3ï£¹\nï£ºï£ºï£ºï£ºï£»=ï£®\nï£¯ï£¯ï£¯ï£¯ï£°400 20 0 âˆ’\n00 10 3 0 âˆ’\n050 0 0 0\n100 10 4 âˆ’ âˆ’\n100 0 50 âˆ’ï£¹\nï£ºï£ºï£ºï£ºï£»ï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°2\n3\nâˆ’2\nâˆ’5\n1\n4ï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£»\nyâˆˆ RmAâˆˆ Rm nÃ—xâˆˆ Rn(7.46)\n2 5 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 128,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°âˆ’14\n1\n19\n2\n23ï£¹\nï£ºï£ºï£ºï£ºï£»=ï£®\nï£¯ï£¯ï£¯ï£¯ï£°3 12 54 1 âˆ’ âˆ’\n4 2 3 11 3 âˆ’ âˆ’\nâˆ’ âˆ’ âˆ’ 15 4 2 3 2\n3 1 2 30 3 âˆ’ âˆ’\nâˆ’ âˆ’ âˆ’ âˆ’ 54 22 5 1ï£¹\nï£ºï£ºï£ºï£ºï£»ï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0\n2\n0\n0\nâˆ’3\n0ï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£»\nyâˆˆ RmBâˆˆ Rm nÃ—hâˆˆ Rn(7.47)\nIntheï¬rstexpression,wehaveanexampleofasparselyparametrized linear\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\ntheinformationpresentin,butdoessowithasparsevector. x",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 129,
      "type": "default"
    }
  },
  {
    "content": "Representationalregularizationisaccomplishedbythesamesortsofmechanisms\nthatwehaveusedinparameterregularization.\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\nâ„¦()h.Asbefore,wedenotetheregularizedlossfunctionbyËœ J:\nËœ J , J , Î± (;Î¸Xy) = (;Î¸Xy)+â„¦()h (7.48)\nwhere Î±âˆˆ[0 ,âˆž)weightstherelativecontributionofthenormpenaltyterm,with\nlargervaluesofcorrespondingtomoreregularization. Î±",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 130,
      "type": "default"
    }
  },
  {
    "content": "Justasan L1penaltyontheparametersinducesparametersparsity,an L1\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\nâ„¦(h) =||||h 1=î\ni| h i|.Â Ofcourse,the L1penaltyisonlyonechoiceofpenalty\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 131,
      "type": "default"
    }
  },
  {
    "content": "Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\nmî\nih( ) i,to\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\noptimization problem\nargmin\nh h ,î«î« 0 < kî«âˆ’ î«xWh2, (7.49)",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 132,
      "type": "default"
    }
  },
  {
    "content": "argmin\nh h ,î«î« 0 < kî«âˆ’ î«xWh2, (7.49)\nwhere î«î«h 0isthenumberofnon-zeroentriesofh.Â Thisproblemcanbesolved\neï¬ƒcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\n2 5 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 133,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nOMP- kwiththevalueof kspeciï¬edtoindicatethenumberofnon-zerofeatures\nallowed. ()demonstratedthatOMP-canbeaveryeï¬€ective CoatesandNg2011 1\nfeatureextractorfordeeparchitectures.\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\ncontexts.\n7.11BaggingandOtherEnsembleMethods\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 134,
      "type": "default"
    }
  },
  {
    "content": "eralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\ntrainseveraldiï¬€erentmodelsseparately,thenhaveallofthemodelsvoteonthe\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\nasensemblemethods.\nThereasonthatmodelaveragingworksisthatdiï¬€erentmodelswillusually\nnotmakeallthesameerrorsonthetestset.\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 135,
      "type": "default"
    }
  },
  {
    "content": "makesanerror î€ ioneachexample,Â withtheerrorsdrawnfromazero-mean\nmultivariatenormaldistributionwithvariances E[ î€2\ni] = vandcovariances E[ î€ i î€ j] =\nc.Â Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\n1\nkî\ni î€ i.Theexpectedsquarederroroftheensemblepredictoris\nEï£®\nï£°î€ \n1\nkî˜\niî€ iî€¡2ï£¹\nï£»=1\nk2Eï£®\nï£°î˜\niï£«\nï£­ î€2\ni+î˜\nj iî€¶=î€ i î€ jï£¶\nï£¸ï£¹\nï£»(7.50)\n=1\nkv+kâˆ’1\nkc . (7.51)\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 136,
      "type": "default"
    }
  },
  {
    "content": "errorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\nensembleisonly1\nkv.Thismeansthattheexpectedsquarederroroftheensemble\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\nindependenterrors,theensemblewillperformsigniï¬cantlybetterthanitsmembers.\nDiï¬€erentensemblemethodsconstructtheensembleofmodelsindiï¬€erentways.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 137,
      "type": "default"
    }
  },
  {
    "content": "Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\n2 5 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 138,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n8\n8F i r s t Â  e nse m b l e Â  m e m b e r\nSe c ondÂ e nse m b l e Â  m e m b e rO r i gi nal Â  data s e t\nF i r s t Â  r e s am pl e d Â  d a t a s e t\nSe c ondÂ re s am p l e d Â  d a t a s e t\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiï¬€erent\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 139,
      "type": "default"
    }
  },
  {
    "content": "bysamplingwithreplacement.Theï¬rstdatasetomitsthe9andrepeatsthe8.Onthis\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\nclassiï¬cationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\nachievingmaximalconï¬denceonlywhenbothloopsofthe8arepresent.\ndiï¬€erentkindofmodelusingadiï¬€erentalgorithmorobjectivefunction.Bagging",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 140,
      "type": "default"
    }
  },
  {
    "content": "isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\nfunctiontobereusedseveraltimes.\nSpeciï¬cally,bagginginvolvesconstructing kdiï¬€erentdatasets.Eachdataset\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 141,
      "type": "default"
    }
  },
  {
    "content": "2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\ni.Thediï¬€erencesbetweenwhichexamplesareincludedineachdatasetresultin\ndiï¬€erencesbetweenthetrainedmodels.Seeï¬gureforanexample.7.5\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\noftenbeneï¬tfrommodelaveragingevenifallofthemodelsaretrainedonthesame\ndataset.Diï¬€erencesinrandominitialization, randomselectionofminibatches,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 142,
      "type": "default"
    }
  },
  {
    "content": "diï¬€erencesinhyperparameters,ordiï¬€erentoutcomesofnon-determinis ticimple-\nmentationsofneuralnetworksareoftenenoughtocausediï¬€erentmembersofthe\n2 5 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 143,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nensembletomakepartiallyindependenterrors.\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\nforscientiï¬cpapers,becauseanymachinelearningalgorithmcanbeneï¬tsubstan-\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 144,
      "type": "default"
    }
  },
  {
    "content": "ingoverdozensofmodels.ArecentprominentexampleistheNetï¬‚ixGrand\nPrize(Koren2009,).\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 145,
      "type": "default"
    }
  },
  {
    "content": "neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\nunitstotheneuralnetwork.\n7.12Dropout\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\npowerfulmethodofregularizingabroadfamilyofmodels.Toaï¬rstapproximation,\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 146,
      "type": "default"
    }
  },
  {
    "content": "wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\nofï¬vetotenneuralnetworksâ€” ()usedsixtowintheILSVRCâ€” Szegedy etal.2014a\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\nneuralnetworks.\nSpeciï¬cally,dropouttrainstheensembleconsistingofallsub-networksthat\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 147,
      "type": "default"
    }
  },
  {
    "content": "asillustratedinï¬gure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\naï¬ƒnetransformationsandnonlinearities, wecaneï¬€ectivelyremoveaunitfroma\nnetworkbymultiplyingitsoutputvaluebyzero.Â Thisprocedurerequiressome\nslightmodiï¬cationformodelssuchasradialbasisfunctionnetworks,whichtake\n2 5 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 148,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nthediï¬€erencebetweentheunitâ€™sstateandsomereferencevalue.Here,wepresent\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\nbetriviallymodiï¬edtoworkwithotheroperationsthatremoveaunitfromthe\nnetwork.\nRecallthattolearnwithbagging,wedeï¬ne kdiï¬€erentmodels,construct k\ndiï¬€erentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 149,
      "type": "default"
    }
  },
  {
    "content": "exponentiallylargenumberofneuralnetworks.Speciï¬cally,totrainwithdropout,\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\nrandomlysampleadiï¬€erentbinarymasktoapplytoalloftheinputandhidden\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\nincluded)isahyperparameter ï¬xedbeforetrainingbegins.Â Itisnotafunction",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 150,
      "type": "default"
    }
  },
  {
    "content": "ofthecurrentvalueofthemodelparametersortheinputexample.Typically,\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\nwithdropout.\nMoreformally,supposethatamaskvectorÂµspeciï¬eswhichunitstoinclude,\nand J(Î¸Âµ ,)deï¬nesthecostofthemodeldeï¬nedbyparametersÎ¸andmaskÂµ.\nThendropouttrainingconsistsinminimizing E Âµ J(Î¸Âµ ,).Theexpectationcontains",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 151,
      "type": "default"
    }
  },
  {
    "content": "exponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\nbysamplingvaluesof.Âµ\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\nparameters,witheachmodelinheritingadiï¬€erentsubsetofparametersfromthe\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 152,
      "type": "default"
    }
  },
  {
    "content": "caseofdropout,typicallymostmodelsarenotexplicitlytrainedatallâ€”usually,\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\naretheonlydiï¬€erences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 153,
      "type": "default"
    }
  },
  {
    "content": "theoriginaltrainingsetsampledwithreplacement.\n2 5 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 154,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2\nx 2 x 2yy\nh 1 h 1 h 2 h 2\nx 1 x 1yy\nh 2 h 2\nx 1 x 1 x 2 x 2\nyy\nh 1 h 1\nx 1 x 1 x 2 x 2yy\nh 1 h 1 h 2 h 2yy\nx 1 x 1 x 2 x 2yy\nh 2 h 2\nx 2 x 2\nyy\nh 1 h 1\nx 1 x 1yy\nh 1 h 1\nx 2 x 2yy\nh 2 h 2\nx 1 x 1yy\nx 1 x 1\nyy\nx 2 x 2yy\nh 2 h 2yy\nh 1 h 1yyB ase Â  ne t w or k\nE nse m bl e Â  of Â  s u b n e t w or k s",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 155,
      "type": "default"
    }
  },
  {
    "content": "E nse m bl e Â  of Â  s u b n e t w or k s\nFigureÂ 7.6:DropoutÂ trainsanÂ ensembleÂ consistingofÂ allsub-networksÂ thatÂ canbe\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\nbydroppingoutdiï¬€erentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 156,
      "type": "default"
    }
  },
  {
    "content": "theinputtotheoutput.Thisproblembecomesinsigniï¬cantfornetworkswithwider\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\nsmaller.\n2 6 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 157,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nË† x 1Ë† x 1\nÂµ x 1 Âµ x 1 x 1 x 1Ë† x 2Ë† x 2\nx 2 x 2 Âµ x 2 Âµ x 2h 1 h 1 h 2 h 2Âµ h 1 Âµ h 1 Âµ h 2 Âµ h 2Ë† h 1Ë† h 1Ë† h 2Ë† h 2yyyy\nh 1 h 1 h 2 h 2\nx 1 x 1 x 2 x 2\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\npropagationwithdropout,werandomlysampleavectorÂµwithoneentryforeachinput",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 158,
      "type": "default"
    }
  },
  {
    "content": "orhiddenunitinthenetwork.TheentriesofÂµarebinaryandaresampledindependently\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\nï¬gureandrunningforwardpropagationthroughit. 7.6\n2 6 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 159,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\nitsmembers.Werefertothisprocessasinferenceinthiscontext.Â Sofar,our\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\nprobabilistic.Now,weassumethatthemodelâ€™sroleistooutputaprobability\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\nofthesedistributions,\n1\nkkî˜",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 160,
      "type": "default"
    }
  },
  {
    "content": "ofthesedistributions,\n1\nkkî˜\ni = 1p( ) i( ) y|x . (7.52)\nInthecaseofdropout,eachsub-modeldeï¬nedbymaskvectorÂµdeï¬nesaprob-\nabilitydistribution p( y ,|xÂµ).Thearithmeticmeanoverallmasksisgiven\nbyî˜\nÂµp p y , ()Âµ(|xÂµ) (7.53)\nwhere p(Âµ)istheprobabilitydistributionthatwasusedtosampleÂµattraining\ntime.\nBecausethissumincludesanexponentialnumberofterms,itisintractable\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\nofsimpliï¬cation.Sofar,deepneuralnetsarenotknowntopermitanytractable",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 161,
      "type": "default"
    }
  },
  {
    "content": "simpliï¬cation.Instead,Â wecanÂ approximatetheinferencewithsampling,Â by\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\nsuï¬ƒcienttoobtaingoodperformance.\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\nthearithmeticmeanoftheensemblemembersâ€™predicteddistributions.Warde-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 162,
      "type": "default"
    }
  },
  {
    "content": "Farley2014etal.()presentargumentsandempiricalevidencethatthegeometric\nmeanperformscomparablytothearithmeticmeaninthiscontext.\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\ndistributiondeï¬neddirectlybythegeometricmeanisgivenby",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 163,
      "type": "default"
    }
  },
  {
    "content": "Ëœ p e nse m bl e( ) = y|x 2dî³î™\nÂµp y , (|xÂµ) (7.54)\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\ndistributionoverÂµtosimplifythepresentation,butnon-uniformdistributionsare\n2 6 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 164,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\np e nse m bl e( ) = y|xËœ p e nse m bl e( ) y|xî\nyî€°Ëœ p e nse m bl e( yî€°|x). (7.55)\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\ni.Themotivationforthismodiï¬cationistocapturetherightexpectedvalueofthe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 165,
      "type": "default"
    }
  },
  {
    "content": "outputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\nBecauseweusuallyuseaninclusionprobabilityof1\n2,theweightscalingrule\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 166,
      "type": "default"
    }
  },
  {
    "content": "theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\nmissingonaverage.\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\nclassiï¬erwithinputvariablesrepresentedbythevector: n v\nP y (= y | v) = softmaxî€\nWî€¾v+bî€‘\ny. (7.56)\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\ninputwithabinaryvector: d",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 167,
      "type": "default"
    }
  },
  {
    "content": "inputwithabinaryvector: d\nP y (= y | v;) = dsoftmaxî€\nWî€¾( )+dî€Œ vbî€‘\ny.(7.57)\nTheensemblepredictorisdeï¬nedbyre-normalizingthegeometricmeanoverall\nensemblemembersâ€™predictions:\nP e nse m bl e(= ) =y y| vËœ P e nse m bl e(= )y y| vî\nyî€°Ëœ P e nse m bl e(= y yî€°| v)(7.58)\nwhere\nËœ P e nse m bl e(= ) =y y| v2nî³î™\ndâˆˆ{} 0 1 ,nP y . (= y | v;)d (7.59)\n2 6 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 168,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nToseethattheweightscalingruleisexact,wecansimplify Ëœ P e nse m bl e:\nËœ P e nse m bl e(= ) =y y| v2nî³î™\ndâˆˆ{} 0 1 ,nP y (= y | v;)d(7.60)\n= 2nî³î™\ndâˆˆ{} 0 1 ,nsoftmax (Wî€¾( )+)dî€Œ vby (7.61)\n= 2nî¶îµîµî´î™\ndâˆˆ{} 0 1 ,nexpî€€\nWî€¾y , :( )+dî€Œ v b yî€\nî\nyî€°expî€\nWî€¾\nyî€° , :( )+dî€Œ v b yî€°î€‘ (7.62)\n=2nî±î‘\ndâˆˆ{} 0 1 ,nexpî€€\nWî€¾y , :( )+dî€Œ v b yî€\n2nî²î‘\ndâˆˆ{} 0 1 ,nî\nyî€°expî€\nWî€¾\nyî€° , :( )+dî€Œ v b yî€°î€‘(7.63)\nBecauseËœ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 169,
      "type": "default"
    }
  },
  {
    "content": "areconstantwithrespectto: y\nËœ P e nse m bl e(= ) y y| vâˆ2nî³î™\ndâˆˆ{} 0 1 ,nexpî€€\nWî€¾y , :( )+dî€Œ v b yî€\n(7.64)\n= expï£«\nï£­1\n2nî˜\ndâˆˆ{} 0 1 ,nWî€¾\ny , :( )+dî€Œ v b yï£¶\nï£¸ (7.65)\n= expî€’1\n2Wî€¾\ny , : v+ b yî€“\n. (7.66)\nSubstitutingthisbackintoequationweobtainasoftmaxclassiï¬erwithweights 7.58\n1\n2W.\nTheweightscalingruleisalsoexactinothersettings,includingregression\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 170,
      "type": "default"
    }
  },
  {
    "content": "mationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\nbetter(intermsofclassiï¬cationaccuracy)thanMonteCarloapproximations tothe\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\nthatsomemodelsobtainbetterclassiï¬cationaccuracyusingtwentysamplesand\n2 6 4",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 171,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\napproximationisproblem-dependent.\nSrivastava2014etal.()showedthatdropoutismoreeï¬€ectivethanother\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,ï¬lter\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\nwithotherformsofregularizationtoyieldafurtherimprovement.\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 172,
      "type": "default"
    }
  },
  {
    "content": "dropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\nexamples.\nAnothersigniï¬cantadvantageofdropoutisthatitdoesnotsigniï¬cantlylimit",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 173,
      "type": "default"
    }
  },
  {
    "content": "thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\nthearchitectureofthemodel.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 174,
      "type": "default"
    }
  },
  {
    "content": "thearchitectureofthemodel.\nThoughthecostper-stepofapplyingdropouttoaspeciï¬cmodelisnegligible,\nthecostofusingdropoutinacompletesystemcanbesigniï¬cant.Becausedropout\nisaregularizationtechnique,itreducestheeï¬€ectivecapacityofamodel.Tooï¬€set\nthiseï¬€ect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 175,
      "type": "default"
    }
  },
  {
    "content": "datasets,regularizationconferslittlereductioningeneralization error.Â Inthese\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\nthebeneï¬tofregularization.\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\neï¬€ective.BayesianÂ neuralnetworks(,Â )outperformÂ dropoutÂ onthe Neal1996\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 176,
      "type": "default"
    }
  },
  {
    "content": "unsupervisedfeaturelearningcangainanadvantageoverdropout.\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\nisequivalentto L2weightdecay,withadiï¬€erentweightdecaycoeï¬ƒcientfor\n2 6 5",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 177,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\neachinputfeature.Themagnitudeofeachfeatureâ€™sweightdecaycoeï¬ƒcientis\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\nmodels,dropoutisnotequivalenttoweightdecay.\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\napproachâ€™ssuccess.Itisjustameansofapproximating thesumoverallsub-\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 178,
      "type": "default"
    }
  },
  {
    "content": "convergencetimeduetothereducedstochasticityinthecomputationofthe\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\n(butalsomorecomputationally expensive)approximation totheaverageoverall\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\nproblems,buthasnotyetyieldedasigniï¬cantimprovementorbeenappliedtoa\nlargeproblem.\nJustasstochasticityisnotnecessarytoachievetheregularizingÂ eï¬€ect of",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 179,
      "type": "default"
    }
  },
  {
    "content": "dropout,itisalsonotsuï¬ƒcient.Todemonstratethis,Warde-Farley2014etal.()\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\nitsregularizingeï¬€ect.Dropoutboostingtrainstheentireensembletojointly\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\ndropoutisanalogoustobagging,Â this approachisanalogoustoboosting.As\nintended,experimentswithdropoutboostingshowalmostnoregularizationeï¬€ect",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 180,
      "type": "default"
    }
  },
  {
    "content": "comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\ndropoutasrobustnesstonoise.Theregularizationeï¬€ectofthebaggedensembleis\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\nperformwellindependently ofeachother.\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\nlargeensemblesofmodelsthatshareweights.Â DropConnectisaspecialcaseof",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 181,
      "type": "default"
    }
  },
  {
    "content": "dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiï¬€erent\nspatiallocationsofeachfeaturemap.Â Sofar,dropoutremainsthemostwidely\nusedimplicitensemblemethod.\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 182,
      "type": "default"
    }
  },
  {
    "content": "behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\nimplementsaformofbaggingwithparametersharing.Earlier,Â wedescribed\n2 6 6",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 183,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ndropoutasÂ bagginganensembleofmodelsformedbyincludingorÂ excluding\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\ninclusionandexclusion.Inprinciple,anykindofrandommodiï¬cationisadmissible.\nInpractice,wemustchoosemodiï¬cationfamiliesthatneuralnetworksareable\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\napproximateinferencerule.Wecanthinkofanyformofmodiï¬cationparametrized",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 184,
      "type": "default"
    }
  },
  {
    "content": "byavectorÂµastraininganensembleconsistingof p( y ,|xÂµ)forallpossible\nvaluesofÂµ.ThereisnorequirementthatÂµhaveaï¬nitenumberofvalues.For\nexample,Âµcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\nweightsbyÂµâˆ¼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\nE[Âµ] = 1thestandardnetworkautomatically implementsapproximate inference\nintheensemble,withoutneedinganyweightscaling.\nSofarwehavedescribeddropoutpurelyasameansofperformingeï¬ƒcient,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 185,
      "type": "default"
    }
  },
  {
    "content": "approximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\nswappinggenesbetweentwodiï¬€erentorganisms,createsevolutionarypressurefor",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 186,
      "type": "default"
    }
  },
  {
    "content": "genestobecomenotjustgood,buttobecomereadilyswappedbetweendiï¬€erent\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.Â Warde-\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\nconcludedthatdropoutoï¬€ersadditionalimprovementstogeneralization error",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 187,
      "type": "default"
    }
  },
  {
    "content": "beyondthoseobtainedbyensemblesofindependentmodels.\nItisimportanttounderstandthatalargeportionofthepowerofdropout\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyï¬ndingthenose,\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 188,
      "type": "default"
    }
  },
  {
    "content": "theimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\n2 6 7",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 189,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\ndistributionthatthemodelhasacquiredsofar.\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\nnoisewereadditivewithï¬xedscale,thenarectiï¬edlinearhiddenunit h iwith\naddednoise î€couldsimplylearntohave h ibecomeverylargeinordertomake\ntheaddednoise î€insigniï¬cantbycomparison.Multiplicativenoisedoesnotallow",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 190,
      "type": "default"
    }
  },
  {
    "content": "suchapathologicalsolutiontothenoiserobustnessproblem.\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\noptimization, butthenoisecanhavearegularizingeï¬€ect,andsometimesmakes\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\n7.13AdversarialTraining\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 191,
      "type": "default"
    }
  },
  {
    "content": "evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\nsearchforexamplesthatthemodelmisclassiï¬es. ()foundthat Szegedy etal.2014b\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\nproceduretosearchforaninputxî€°nearadatapointxsuchthatthemodel",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 192,
      "type": "default"
    }
  },
  {
    "content": "outputisverydiï¬€erentatxî€°.Inmanycases,xî€°canbesosimilartoxthata\nhumanobservercannottellthediï¬€erencebetweentheoriginalexampleandthe\nadversarialexample,butthenetworkcanmakehighlydiï¬€erentpredictions.See\nï¬gureforanexample.7.8\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\nthatarebeyondthescopeofthischapter.Â However,theyareinterestinginthe\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 193,
      "type": "default"
    }
  },
  {
    "content": "testsetviaadversarialtrainingâ€”trainingonadversariallyperturbedexamples\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\nadversarialÂ examplesisÂ excessiveÂ linearity.NeuralÂ networksÂ arebuiltÂ outÂ of\nprimarilylinearbuildingblocks.Â Insomeexperimentstheoverallfunctionthey\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\n2 6 8",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 194,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\n+ .007Ã— =\nx sign(âˆ‡ x J(Î¸x , , y))x+\nî€sign(âˆ‡ x J(Î¸x , , y))\ny=â€œpandaâ€ â€œnematodeâ€â€œgibbonâ€\nw/57.7%\nconï¬dencew/8.2%\nconï¬dencew/99.3%\nconï¬dence\nFigure7.8:Â AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\nrespecttotheinput,wecanchangeGoogLeNetâ€™sclassiï¬cationoftheimage.Reproduced",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 195,
      "type": "default"
    }
  },
  {
    "content": "withpermissionfrom (). Goodfellow e t a l .2014b\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\nifithasnumerousinputs.Ifwechangeeachinputby î€,thenalinearfunction\nwithweightswcanchangebyasmuchas î€||||w 1,whichcanbeaverylarge\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 196,
      "type": "default"
    }
  },
  {
    "content": "introducingalocalconstancypriorintosupervisedneuralnets.\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\nfromnearlylineartonearlylocallyconstantandthushavetheï¬‚exibilitytocapture\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 197,
      "type": "default"
    }
  },
  {
    "content": "Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\nmodelitselfassignssomelabel Ë† y.Themodelâ€™slabel Ë† ymaynotbethetruelabel,\nbutifthemodelishighquality,thenË† yhasahighprobabilityofprovidingthe\ntruelabel.Wecanseekanadversarialexamplexî€°thatcausestheclassiï¬erto\noutputalabel yî€°with yî€°î€¶=Ë† y.Adversarialexamplesgeneratedusingnotthetrue\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 198,
      "type": "default"
    }
  },
  {
    "content": "examples(Miyato2015etal.,).Theclassiï¬ermaythenbetrainedtoassignthe\nsamelabeltoxandxî€°.Thisencouragestheclassiï¬ertolearnafunctionthatis\n2 6 9",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 199,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\nlies.Theassumptionmotivatingthisapproachisthatdiï¬€erentclassesusuallylie\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\nfromoneclassmanifoldtoanotherclassmanifold.\n7.14TangentÂ Distance,Â TangentProp,andÂ Manifold\nTangentClassiï¬er\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 200,
      "type": "default"
    }
  },
  {
    "content": "section.5.11.3\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiï¬er\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 201,
      "type": "default"
    }
  },
  {
    "content": "onthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\nrespectivelybelong.Althoughthatmaybecomputationally diï¬ƒcult(itwould\nrequiresolvinganoptimization problem,toï¬ndthenearestpairofpointson M 1\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 202,
      "type": "default"
    }
  },
  {
    "content": "linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\nonetospecifythetangentvectors.\nInarelatedspirit,thetangentpropalgorithm( ,)(ï¬gure) Simardetal.19927.9\ntrainsaneuralnetclassiï¬erwithanextrapenaltytomakeeachoutput f(x)of\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\nsameclassconcentrate.Localinvarianceisachievedbyrequiring âˆ‡ x f(x)tobe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 203,
      "type": "default"
    }
  },
  {
    "content": "orthogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\nregularizationpenalty:â„¦\nâ„¦() = fî˜\niî€\n(âˆ‡ x f())xî€¾v( ) iî€‘2\n. (7.67)\n2 7 0",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 204,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\ntheeï¬€ectoftransformationssuchastranslation,rotation,andscalinginimages.\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 205,
      "type": "default"
    }
  },
  {
    "content": "butalsointhecontextofreinforcementlearning(,). Thrun1995\nTangentpropagation isÂ closelyrelatedÂ todatasetÂ augmentation.InÂ both\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\nnetwork.Thediï¬€erenceisthatinthecaseofdatasetaugmentation, thenetworkis\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\nmorethananinï¬nitesimalamountofthesetransformations.Tangentpropagation",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 206,
      "type": "default"
    }
  },
  {
    "content": "doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\ntheÂ speciï¬edÂ transformation.WhileÂ thisanalyticalÂ approac hÂ isintellectually\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\ninï¬nitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\nlargerperturbations.Second,theinï¬nitesimalapproachposesdiï¬ƒcultiesformodels",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 207,
      "type": "default"
    }
  },
  {
    "content": "basedonrectiï¬edlinearunits.Thesemodelscanonlyshrinktheirderivatives\nbyturningunitsoï¬€orshrinkingtheirweights.Theyarenotabletoshrinktheir\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\nunitscan.Datasetaugmentation workswellwithrectiï¬edlinearunitsbecause\ndiï¬€erentsubsetsofrectiï¬edunitscanactivatefordiï¬€erenttransformedversionsof\neachoriginalinput.\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 208,
      "type": "default"
    }
  },
  {
    "content": "1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\nï¬ndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\naugmentationusingmanuallyspeciï¬edtransformationsbothrequirethatthe\nmodelshouldbeinvarianttocertainspeciï¬eddirectionsofchangeintheinput.\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 209,
      "type": "default"
    }
  },
  {
    "content": "invarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\nasdatasetaugmentationisthenon-inï¬nitesimalversionoftangentpropagation,\nadversarialtrainingisthenon-inï¬nitesimalversionofdoublebackprop.\nThemanifoldtangentclassiï¬er(,),eliminatestheneedto Rifaietal.2011c\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\n2 7 1",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 210,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nx 1x 2N o r m a lT a ng e nt\nFigure7.9:Â Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\n1992 Rifai2011c )andmanifoldtangentclassiï¬er( e t a l .,),whichbothregularizethe\nclassiï¬eroutputfunction f(x).Eachcurverepresentsthemanifoldforadiï¬€erentclass,\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 211,
      "type": "default"
    }
  },
  {
    "content": "classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\ntangentdirectionsandmanynormaldirections.Weexpecttheclassiï¬cationfunctionto\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\nclassiï¬erregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 212,
      "type": "default"
    }
  },
  {
    "content": "propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\nmanifold)whilethemanifoldtangentclassiï¬erestimatesthemanifoldtangentdirections\nbytraininganautoencodertoï¬tthetrainingdata.Theuseofautoencoderstoestimate\nmanifoldswillbedescribedinchapter.14\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiï¬ermakesuse\nofthistechniquetoavoidneedinguser-speciï¬edtangentvectors.Â Asillustrated",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 213,
      "type": "default"
    }
  },
  {
    "content": "inï¬gure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\nandincludefactorsthatmustbelearnedbecausetheyareobject-speciï¬c(suchas\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiï¬er\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiï¬er\nasintangentprop(equation).7.67",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 214,
      "type": "default"
    }
  },
  {
    "content": "asintangentprop(equation).7.67\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\n2 7 2",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 215,
      "type": "default"
    }
  },
  {
    "content": "CHAPTER7.REGULARIZATIONFORDEEPLEARNING\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\nthemeofmachinelearningisoptimization, describednext.\n2 7 3",
    "metadata": {
      "source": "[12]part-2-chapter-7.pdf",
      "chunk_id": 216,
      "type": "default"
    }
  },
  {
    "content": "P a rt I I I\nDeepLearningResearch\n486",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 0,
      "type": "default"
    }
  },
  {
    "content": "This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes\nt o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y .\nIn t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d\nlearning problems â€” how t o learn t o map one v e c t or t o another, given e nough\ne x amples of t he mapping.\nN ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 1,
      "type": "default"
    }
  },
  {
    "content": "wis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle\nmis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples\nf r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial\napplications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d\ndata t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 2,
      "type": "default"
    }
  },
  {
    "content": "t he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary\nf or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of\nt as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or\ns e mi-s up e r v is e d learning.\nMan y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d\nlearning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 3,
      "type": "default"
    }
  },
  {
    "content": "deep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of\nt as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d\nlearning and s ome of t he p opular t hought ab out how w e c an make progres s in t his\nï¬ e ld.\nA c e ntral c aus e of t he diï¬ƒculties with uns upervis e d learning is t he high di-\nmens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 4,
      "type": "default"
    }
  },
  {
    "content": "c hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l\nc h a l l e ng e r e gards generalization: t he num b e r of c onï¬gurations we may wan t t o\ndis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and\nt his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 5,
      "type": "default"
    }
  },
  {
    "content": "ha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e\nas s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or\nlearning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit\nprobabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially\nwith t he n um b e r of dimens ion s .",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 6,
      "type": "default"
    }
  },
  {
    "content": "with t he n um b e r of dimens ion s .\nWith probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o\np e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on.\nâ€¢ I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19\nt he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 7,
      "type": "default"
    }
  },
  {
    "content": "v ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r\n4 8 7",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 8,
      "type": "default"
    }
  },
  {
    "content": "a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs\nt o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization\nc ons t an t whic h s ums o v e r t he v alues of a and c .\nâ€¢ I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition\nf unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 9,
      "type": "default"
    }
  },
  {
    "content": "bilit y f unctions c ome up in inference ( ab o v e ) as well as in learning.Â Man y\nprobabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately ,\nlearning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-\nr ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That\nc omputation is generally as intractable as c omputing t he partition f unction\nits e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 10,
      "type": "default"
    }
  },
  {
    "content": "t e n us e d t o deal with t he partition f unction ( c omputing it or its gradient).\nU nfortun ately , MCMC metho ds s uï¬€er when t he mo des of t he mo del dis t r ibu-\nt ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces\n( s e c t ion ) . 17.5\nOne wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,\nand many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 11,
      "type": "default"
    }
  },
  {
    "content": "b o ok. A nother interes t in g w ay ,Â als o dis c us s e d here,Â w ould b e t o av oid t hes e\nin t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire\ns uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n\nprop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary\napproac hes t o generativ e mo deling are dis c us s e d in c hapter . 20",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 12,
      "type": "default"
    }
  },
  {
    "content": "P art is t he mos t imp ortant f or a r e s e arc herâ€”s om e one who wan t s t o un- I I I\nders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he ï¬ e ld of deep\nlearning, and pus h t he ï¬ e ld f orward t ow ards t r ue artiï¬cial intelligence.\n4 8 8",
    "metadata": {
      "source": "[18]part-3-deep-learning-research.pdf",
      "chunk_id": 13,
      "type": "default"
    }
  }
]


============ ./process_document.py ============
# process_documents.py

import os
import io # For BytesIO
import logging # For logging
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any # Added Any, Tuple
from PIL import Image, ImageFilter
import pytesseract
import fitz # PyMuPDF
import pypdf # For PDF parsing
from docx import Document as DocxDocument
from pptx import Presentation
import PyPDF2 # For basic PDF metadata
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument

# --- Logger Setup ---
# Configure logging to output to console. You can customize this for file logging.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# --- NLTK Downloads (Run once if not already downloaded) ---
# Moved to a function for clarity, can be called at app startup
def download_nltk_resources():
    """Downloads necessary NLTK resources if not already present."""
    nltk_resources = {
        'punkt': 'tokenizers/punkt',
        'stopwords': 'corpora/stopwords',
        'wordnet': 'corpora/wordnet',
        'omw-1.4': 'corpora/omw-1.4' # WordNet 1.4 ontology
    }
    for res_name, res_path in nltk_resources.items():
        try:
            nltk.data.find(res_path)
            logger.info(f"NLTK resource '{res_name}' found.")
        except nltk.downloader.DownloadError:
            logger.info(f"NLTK resource '{res_name}' not found. Downloading...")
            nltk.download(res_name)
            logger.info(f"NLTK resource '{res_name}' downloaded.")

# --- SpaCy Model Download (Run once if not already downloaded) ---
def download_spacy_model(model_name: str = "en_core_web_sm"):
    """Downloads a spaCy model if not already present."""
    try:
        spacy.load(model_name)
        logger.info(f"SpaCy model '{model_name}' found.")
    except OSError:
        logger.warning(f"SpaCy model '{model_name}' not found. Attempting download...")
        try:
            os.system(f"python -m spacy download {model_name}")
            logger.info(f"SpaCy model '{model_name}' downloaded successfully. Please restart the application if it was just installed.")
            # Note: A restart might be needed for the new model to be picked up by the current Python process
            # For production, ensure models are pre-installed in the environment.
        except Exception as e:
            logger.error(f"Failed to download spaCy model '{model_name}': {e}")
            logger.error("Please install it manually: python -m spacy download en_core_web_sm")


# --- Perform Resource Downloads (Call once at application startup) ---
# In a real application, this would be part of an initialization script or a startup hook.
download_nltk_resources()
download_spacy_model()


# --- Global NLP Models (Load once) ---
logger.info("Loading global NLP models...")
try:
    nlp_core = spacy.load("en_core_web_sm")
    stop_words = set(stopwords.words('english'))
    porter_stemmer = PorterStemmer() # Not used in current clean_and_normalize_text, but kept for completeness
    wordnet_lemmatizer = WordNetLemmatizer()
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    logger.info("Global NLP models loaded successfully.")
except Exception as e:
    logger.error(f"Fatal error loading global NLP models: {e}")
    # In a real app, you might want to exit or prevent further operations if models can't load.
    raise  # Re-raise the exception to halt execution if critical models fail

# --- Import pdfplumber after other initializations ---
# pdfplumber can sometimes have import-time issues if dependencies are not perfectly met.
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
    logger.info("pdfplumber imported successfully.")
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("pdfplumber not found. Rich PDF content extraction (tables, layout) will be limited. Install with: pip install pdfplumber")


# --- File Parsing Functions ---

def parse_pdf_simple(file_path: str) -> Optional[str]:
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf:
        logger.error("pypdf library not found. PDF parsing will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        logger.info(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        logger.info(f"Extracted {len(text_content)} characters from PDF (pypdf).")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)} with pypdf: {e}")
        return None

def parse_docx(file_path: str) -> Optional[str]:
    """Extracts text content from a DOCX file."""
    if not DocxDocument:
        logger.error("python-docx library not found. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        logger.info(f"Extracted {len(text_content)} characters from DOCX: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"DOCX file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}")
        return None

def parse_txt(file_path: str) -> Optional[str]:
    """Reads text content from a TXT file (or similar plain text)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text_content = f.read()
        logger.info(f"Read {len(text_content)} characters from TXT file: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"TXT file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}")
        return None

# Conditional PPTX parsing
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path: str) -> Optional[str]:
        """Extracts text content from a PPTX file."""
        text_content = ""
        try:
            prs = Presentation(file_path)
            for slide_num, slide in enumerate(prs.slides):
                slide_text_parts = []
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            slide_text_parts.append(shape_text)
                if slide_text_parts:
                    text_content += "\n".join(slide_text_parts) + "\n\n" # Newline between shapes, double for new slide
            logger.info(f"Extracted {len(text_content)} characters from PPTX: {os.path.basename(file_path)}")
            return text_content.strip() if text_content.strip() else None
        except FileNotFoundError:
            logger.error(f"PPTX file not found: {file_path}")
            return None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}")
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped. Install with: pip install python-pptx")
    def parse_pptx(file_path: str) -> Optional[str]: # type: ignore
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path: str) -> Optional[str]:
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.info(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf_simple(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path)
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']:
        return parse_txt(file_path)
    elif ext == '.doc':
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

# --- Core Processing Functions ---

def extract_raw_content(file_path: str) -> Dict[str, Any]:
    """
    1. Extracts raw text, tables, images, and basic layout info from a document.
    Determines if the document (if PDF) is likely scanned.
    """
    logger.info(f"Starting raw content extraction for: {os.path.basename(file_path)}")
    text_content: str = ""
    tables: List[Any] = [] # Can be list of DataFrames or list of lists
    images: List[Image.Image] = []
    layout_info: List[Dict[str, Any]] = []
    is_scanned: bool = False
    file_extension: str = os.path.splitext(file_path)[1].lower()

    initial_text = parse_file(file_path) # Uses the simple parsers for a baseline
    if initial_text:
        text_content = initial_text
    else:
        logger.warning(f"Could not extract initial text from {file_path}. Proceeding with empty text.")
        # Still return structure, even if empty
        return {
            'text_content': "", 'tables': tables, 'images': images,
            'layout_info': layout_info, 'is_scanned': False, 'file_type': file_extension
        }

    # If it's a PDF, try to extract richer content
    if file_extension == '.pdf':
        if not PDFPLUMBER_AVAILABLE:
            logger.warning("pdfplumber is not available. Rich PDF extraction (tables, layout) will be skipped.")
        else:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_check_parts = []
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            pdfplumber_text_check_parts.append(page_text)
                    pdfplumber_text_check = "".join(pdfplumber_text_check_parts)

                    # Heuristic for scanned PDF: if pdfplumber extracts very little text
                    # compared to page count or absolute minimum.
                    # This assumes non-scanned PDFs have a reasonable amount of selectable text.
                    # Adjust thresholds as needed.
                    min_chars_per_page = 50 # Heuristic: expect at least this many chars per page if not scanned
                    absolute_min_chars = 200 # Heuristic: expect at least this many chars total if not scanned

                    if len(pdf.pages) > 0 and \
                       (len(pdfplumber_text_check) < min_chars_per_page * len(pdf.pages) and \
                        len(pdfplumber_text_check) < absolute_min_chars):
                        is_scanned = True
                        logger.info(f"PDF {os.path.basename(file_path)} detected as potentially scanned based on low text content from pdfplumber.")

                    full_pdfplumber_text = ""
                    for page_num, page in enumerate(pdf.pages):
                        page_text_with_layout = page.extract_text(x_tolerance=1, y_tolerance=1, layout=False) # layout=True can be slow
                        if page_text_with_layout:
                            full_pdfplumber_text += page_text_with_layout + "\n"

                        # Capture basic layout info (bounding boxes of text lines/words)
                        # This can be very verbose, enable if detailed layout is critical
                        # for char_obj in page.chars:
                        #     layout_info.append({
                        #         'char': char_obj['text'],
                        #         'bbox': (char_obj['x0'], char_obj['top'], char_obj['x1'], char_obj['bottom']),
                        #         'page': page_num
                        #     })

                        # Extract tables using pdfplumber
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data in page_tables_data:
                                if table_data and len(table_data) > 0: # Ensure there's data
                                    # Check if first row looks like a header (heuristic)
                                    if len(table_data) > 1 and all(isinstance(cell, str) for cell in table_data[0]):
                                        try:
                                            tables.append(pd.DataFrame(table_data[1:], columns=table_data[0]))
                                        except Exception as df_err:
                                            logger.warning(f"Could not convert table to DataFrame on page {page_num} (using header): {df_err}. Appending raw list.")
                                            tables.append(table_data)
                                    else: # No clear header or single row table
                                        tables.append(table_data) # Append raw list
                    
                    # Prioritize pdfplumber's text if it's richer
                    if len(full_pdfplumber_text.strip()) > len(text_content):
                        logger.info("Using text extracted by pdfplumber as it's more comprehensive.")
                        text_content = full_pdfplumber_text.strip()

            except Exception as e:
                logger.warning(f"Error during rich PDF content extraction with pdfplumber for {os.path.basename(file_path)}: {e}")
                # Fallback to initial_text if pdfplumber fails badly

        # Extract images using PyMuPDF (fitz) - generally more robust for images
        try:
            doc = fitz.open(file_path)
            for page_idx in range(len(doc)):
                for img_info in doc.get_page_images(page_idx):
                    xref = img_info[0] # XREF of the image
                    try:
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        pil_image = Image.open(io.BytesIO(image_bytes))
                        images.append(pil_image)
                    except Exception as img_err:
                        logger.warning(f"Could not open image xref {xref} from page {page_idx} of {os.path.basename(file_path)}: {img_err}")
            doc.close()
            logger.info(f"Extracted {len(images)} images using PyMuPDF from {os.path.basename(file_path)}.")
        except Exception as e:
            logger.warning(f"Error extracting images with PyMuPDF from {os.path.basename(file_path)}: {e}")

    logger.info(f"Raw content extraction complete for {os.path.basename(file_path)}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {
        'text_content': text_content,
        'tables': tables,
        'images': images,
        'layout_info': layout_info, # Often empty for non-PDFs or if detailed layout extraction is off
        'is_scanned': is_scanned,   # Primarily relevant for PDFs
        'file_type': file_extension
    }

def perform_ocr(image_data: List[Image.Image]) -> str:
    """
    2. Processes images to extract text using OCR (pytesseract).
    """
    if not image_data:
        logger.info("No images provided for OCR.")
        return ""

    logger.info(f"Performing OCR on {len(image_data)} image(s).")
    ocr_text_parts: List[str] = []
    for i, img in enumerate(image_data):
        try:
            # Basic preprocessing
            img_gray = img.convert('L')
            # img_binary = img_gray.point(lambda x: 0 if x < 150 else 255, '1') # Example threshold
            # img_filtered = img_binary.filter(ImageFilter.MedianFilter(size=3)) # Example filter

            # Tesseract works well with grayscale. Extensive preprocessing can sometimes harm.
            # Test various preprocessing steps for your specific image types.
            text = pytesseract.image_to_string(img_gray) # Using grayscale
            if text.strip():
                ocr_text_parts.append(text.strip())
            logger.debug(f"OCR successful for image {i+1}.")
        except pytesseract.TesseractNotFoundError:
            logger.error("Tesseract is not installed or not in your PATH. OCR will fail.")
            # This is a critical error, so perhaps re-raise or handle at a higher level
            raise
        except Exception as e:
            logger.error(f"Error during OCR for image {i+1}: {e}")
    
    full_ocr_text = "\n\n".join(ocr_text_parts)
    logger.info(f"OCR process completed. Extracted {len(full_ocr_text)} characters.")
    return full_ocr_text

def clean_and_normalize_text(text: str) -> str:
    """
    3. Cleans and normalizes text for NLP readiness.
    """
    if not text:
        logger.info("No text provided for cleaning and normalization.")
        return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")

    # 1. Noise Removal
    text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags more robustly
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # Remove URLs
    text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses
    # Keep essential punctuation for sentence structure, remove others
    text = re.sub(r'[^a-zA-Z0-9\s.,!?-]', '', text) # Keep alphanumeric, spaces, and basic punctuation
    text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

    # 2. Case Normalization
    text = text.lower()

    # 3. Tokenization (NLTK)
    try:
        tokens = nltk.word_tokenize(text)
    except Exception as e:
        logger.error(f"NLTK word_tokenize failed: {e}. Falling back to simple split.")
        tokens = text.split()


    # 4. Stop Word Removal
    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1] # Also remove single chars

    # 5. Lemmatization
    try:
        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in filtered_tokens]
    except Exception as e:
        logger.error(f"WordNet lemmatization failed: {e}. Using filtered tokens directly.")
        lemmatized_tokens = filtered_tokens

    cleaned_text = " ".join(lemmatized_tokens)
    logger.info(f"Text cleaning and normalization complete. Final length: {len(cleaned_text)}")
    return cleaned_text

def reconstruct_layout(original_text: str, layout_info: List[Dict[str, Any]], tables: List[Any], file_type: str) -> str:
    """
    4. Integrates tables into text. Advanced layout reconstruction is complex and
       currently simplified here. Focuses on table integration.
    """
    logger.info("Starting layout reconstruction (primarily table integration).")
    processed_text = original_text

    # De-hyphenation (simple regex-based approach)
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', processed_text)
    processed_text = re.sub(r'(\w+)-\s*(\w+)', r'\1\2', processed_text) # For hyphens not at line end

    # Integrate tables as Markdown strings
    table_text_representations: List[str] = []
    if tables:
        logger.info(f"Integrating {len(tables)} tables into the text.")
        for i, table_content in enumerate(tables):
            table_header = f"\n\n--- Table {i+1} Content ---\n"
            table_footer = "\n--- End of Table Content ---\n"
            table_md = ""
            if isinstance(table_content, pd.DataFrame):
                try:
                    table_md = table_content.to_markdown(index=False)
                except Exception as e:
                    logger.warning(f"Could not convert DataFrame table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            elif isinstance(table_content, list): # Raw list of lists from pdfplumber
                # Attempt to format as a simple markdown table
                try:
                    if table_content and isinstance(table_content[0], list):
                        # Assuming first row could be header
                        header = table_content[0]
                        data_rows = table_content[1:]
                        table_md = "| " + " | ".join(map(str, header)) + " |\n"
                        table_md += "| " + " | ".join(["---"] * len(header)) + " |\n"
                        for row in data_rows:
                            table_md += "| " + " | ".join(map(str, row)) + " |\n"
                    else: # Flat list or other structure
                        table_md = "\n".join(map(str,table_content))

                except Exception as e:
                    logger.warning(f"Could not format list-based table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            else:
                table_md = str(table_content)

            table_text_representations.append(table_header + table_md + table_footer)
        
        # Append all table representations at the end of the document text
        processed_text += "\n\n" + "\n\n".join(table_text_representations)
    
    logger.info("Layout reconstruction (table integration) complete.")
    return processed_text.strip()


def extract_metadata(file_path: str, processed_text: str, file_type_from_extraction: str) -> Dict[str, Any]:
    """
    5. Extracts standard and content-based metadata.
    """
    logger.info(f"Starting metadata extraction for: {os.path.basename(file_path)}")
    metadata: Dict[str, Any] = {}
    file_extension = os.path.splitext(file_path)[1].lower()
    if not file_extension: # If somehow extension is empty, use the one from raw content
        file_extension = file_type_from_extraction

    # Common metadata
    metadata['file_name'] = os.path.basename(file_path)
    metadata['file_path'] = file_path # Store full path for reference
    metadata['file_type'] = file_extension
    try:
        metadata['file_size_bytes'] = os.path.getsize(file_path)
        metadata['creation_time_epoch'] = os.path.getctime(file_path)
        metadata['modification_time_epoch'] = os.path.getmtime(file_path)
    except FileNotFoundError:
        logger.error(f"File not found during metadata extraction (size/time): {file_path}")
    except Exception as e:
        logger.warning(f"Could not get file system metadata for {file_path}: {e}")

    # File-type specific metadata
    page_count_approx = 0
    if file_extension == '.pdf':
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                doc_info = reader.metadata
                if doc_info:
                    metadata['pdf_title'] = doc_info.get('/Title', '').strip()
                    metadata['pdf_author'] = doc_info.get('/Author', '').strip()
                    # Add more as needed from PyPDF2
                page_count_approx = len(reader.pages)
        except Exception as e:
            logger.warning(f"Error extracting PDF-specific metadata (PyPDF2): {e}")
    elif file_extension == '.docx':
        try:
            doc = DocxDocument(file_path)
            # python-docx doesn't easily give page count. Paragraph count is a rough proxy.
            page_count_approx = sum(1 for _ in doc.paragraphs) # Number of paragraphs
            # Core properties can be accessed if set
            # metadata['docx_author'] = doc.core_properties.author
            # metadata['docx_title'] = doc.core_properties.title
        except Exception as e:
            logger.warning(f"Error extracting DOCX-specific metadata: {e}")
    elif file_extension == '.pptx' and PPTX_SUPPORTED:
        try:
            prs = Presentation(file_path)
            page_count_approx = len(prs.slides) # Number of slides
        except Exception as e:
            logger.warning(f"Error extracting PPTX-specific metadata: {e}")
    
    if not page_count_approx and processed_text: # Fallback for TXT or if above failed
        page_count_approx = processed_text.count('\n\n') + 1 # Approx by double newlines for "pages"

    metadata['approx_page_count'] = page_count_approx

    # Content-based metadata using spaCy NER
    if processed_text and nlp_core:
        logger.info("Extracting named entities using spaCy...")
        try:
            doc_nlp = nlp_core(processed_text[:1000000]) # Limit NER processing length for performance
            entities: Dict[str, List[str]] = {label: [] for label in nlp_core.pipe_labels.get("ner", [])}
            for ent in doc_nlp.ents:
                if ent.label_ not in entities: # Should not happen if initialized above
                    entities[ent.label_] = []
                entities[ent.label_].append(ent.text)
            # Filter out empty entity types
            metadata['named_entities'] = {k: v for k, v in entities.items() if v}
            logger.info(f"Extracted {sum(len(v) for v in metadata['named_entities'].values())} named entities.")
        except Exception as e:
            logger.error(f"Error during spaCy NER processing: {e}")
            metadata['named_entities'] = {}
    else:
        metadata['named_entities'] = {}
        logger.info("Skipping NER (no text or nlp_core not available).")

    logger.info(f"Metadata extraction complete for {os.path.basename(file_path)}.")
    return metadata

def chunk_text(text_to_chunk: str, base_metadata: Dict[str, Any], chunk_size: int = 512, chunk_overlap: int = 64) -> List[Dict[str, Any]]:
    """
    6. Divides text into chunks with associated metadata using RecursiveCharacterTextSplitter.
    """
    if not text_to_chunk or not isinstance(text_to_chunk, str):
        logger.warning(f"Invalid or empty text input for chunking (file: {base_metadata.get('file_name', 'unknown')}). Skipping chunking.")
        return []

    logger.info(f"Starting text chunking for {base_metadata.get('file_name', 'unknown')}. Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], # More robust separators
        keep_separator=True # Keep separators to maintain context
    )

    # Langchain's create_documents expects a list of texts, or can take metadata per document
    # Here, we process one large text derived from the whole document.
    try:
        langchain_documents: List[LangchainDocument] = text_splitter.create_documents([text_to_chunk])
    except Exception as e:
        logger.error(f"Error creating documents with RecursiveCharacterTextSplitter for {base_metadata.get('file_name', 'unknown')}: {e}")
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    file_name_prefix = os.path.splitext(base_metadata.get('file_name', 'doc'))[0]

    for i, doc_chunk in enumerate(langchain_documents):
        chunk_metadata = base_metadata.copy() # Start with base document metadata
        chunk_metadata['chunk_id'] = f"{file_name_prefix}_chunk_{i:04d}" # Padded for sorting
        chunk_metadata['chunk_index'] = i
        chunk_metadata['chunk_char_count'] = len(doc_chunk.page_content)
        
        # Add any metadata specific to this chunk if LangchainDocument provides it
        # (e.g., if splitting by pages from a PDF loader, it might have 'page_number')
        for key, value in doc_chunk.metadata.items():
            if key not in chunk_metadata:
                chunk_metadata[key] = value

        output_chunks.append({
            'id': chunk_metadata['chunk_id'],
            'text_content': doc_chunk.page_content,
            'metadata': chunk_metadata # All merged metadata
        })
    
    logger.info(f"Split '{base_metadata.get('file_name', 'unknown')}' into {len(output_chunks)} chunks.")
    return output_chunks

def generate_embeddings(chunks_to_embed: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    7. Generates vector embeddings for each text chunk.
    """
    if not chunks_to_embed:
        logger.info("No chunks provided for embedding generation.")
        return []

    logger.info(f"Starting embedding generation for {len(chunks_to_embed)} chunks.")
    texts_to_embed = [chunk['text_content'] for chunk in chunks_to_embed if chunk.get('text_content')]
    
    if not texts_to_embed:
        logger.warning("No text content found in chunks to embed.")
        return chunks_to_embed # Return original chunks, possibly without embeddings

    try:
        embeddings = embedding_model.encode(texts_to_embed, show_progress_bar=False) # Set to True for console progress
        
        # Assign embeddings back to the corresponding chunks
        # This assumes a 1:1 correspondence and order is maintained.
        # If some chunks had no text, the embedding list will be shorter.
        chunk_idx_with_text = 0
        for i, chunk in enumerate(chunks_to_embed):
            if chunk.get('text_content'):
                if chunk_idx_with_text < len(embeddings):
                    chunk['embedding_vector'] = embeddings[chunk_idx_with_text].tolist()
                    chunk_idx_with_text += 1
                else:
                    logger.warning(f"Mismatch in embedding count for chunk {chunk.get('id')}. No embedding assigned.")
                    chunk['embedding_vector'] = None # Or an empty list
            else:
                chunk['embedding_vector'] = None # Or an empty list for chunks with no text

        logger.info(f"Embeddings generated for {chunk_idx_with_text} chunks.")
        return chunks_to_embed
    except Exception as e:
        logger.error(f"Error during embedding generation: {e}")
        # Optionally, mark chunks as failed or return them without embeddings
        for chunk in chunks_to_embed:
            chunk['embedding_vector'] = None # Indicate failure
        return chunks_to_embed


# --- Main Orchestration Function ---
def process_uploaded_document(file_path: str, username: str) -> Dict[str, Any]:
    """
    Orchestrates the entire document processing pipeline for a single uploaded file.
    Takes username for logging/auditing purposes.
    Returns a dictionary with 'status' and 'data' or 'message'.
    """
    logger.info(f"User '{username}' initiated processing for file: {file_path}")

    if not os.path.exists(file_path):
        logger.error(f"File not found: {file_path}")
        return {"status": "error", "message": f"File not found: {file_path}", "file_path": file_path}

    try:
        # 1. Extract Raw Content
        logger.info("Step 1: Extracting Raw Content...")
        raw_content = extract_raw_content(file_path)
        logger.info(f"Step 1: Raw content extracted. Text length: {len(raw_content.get('text_content',''))}, Scanned: {raw_content.get('is_scanned')}")

        # 2. Perform OCR if necessary
        ocr_text_contribution = ""
        if raw_content['file_type'] == '.pdf' and raw_content['is_scanned']:
            if raw_content['images']:
                logger.info("Step 2: Detected scanned PDF with images, performing OCR...")
                ocr_text_contribution = perform_ocr(raw_content['images'])
                # Combine OCR text: prioritize if it's substantial, otherwise append.
                # This logic can be refined based on OCR quality.
                if len(ocr_text_contribution) > 0.5 * len(raw_content['text_content']): # If OCR text is significant
                     raw_content['text_content'] = ocr_text_contribution + "\n\n" + raw_content['text_content']
                else: # Append if OCR text is smaller or complementary
                     raw_content['text_content'] += "\n\n" + ocr_text_contribution
                logger.info(f"Step 2: OCR completed. Total text length after OCR: {len(raw_content['text_content'])}")
            else:
                logger.warning("Step 2: Detected scanned PDF but no images found for OCR. Proceeding with available text.")
        else:
            logger.info(f"Step 2: OCR not required or not applicable for file type: {raw_content['file_type']}")

        if not raw_content['text_content'] and not ocr_text_contribution: # Check after potential OCR
            logger.warning(f"No text content could be extracted from {os.path.basename(file_path)} after raw extraction and OCR.")
            # Depending on requirements, you might return an error or empty data
            # For now, we proceed, but chunking/embedding will yield empty results.

        # 3. Clean and Normalize Text
        logger.info("Step 3: Cleaning and Normalizing Text...")
        cleaned_text = clean_and_normalize_text(raw_content['text_content'])
        logger.info(f"Step 3: Text cleaned and normalized. Length: {len(cleaned_text)}")

        # 4. Reconstruct Layout and Integrate Tables
        logger.info("Step 4: Reconstructing Layout and Integrating Tables...")
        structurally_coherent_text = reconstruct_layout(
            cleaned_text,
            raw_content['layout_info'],
            raw_content['tables'],
            raw_content['file_type']
        )
        logger.info(f"Step 4: Layout reconstructed. Final text length for chunking: {len(structurally_coherent_text)}")

        # 5. Extract Metadata
        logger.info("Step 5: Extracting Metadata...")
        # Pass the most complete text and original file_type for robust metadata
        metadata = extract_metadata(file_path, structurally_coherent_text, raw_content['file_type'])
        metadata['processing_user'] = username # Add username to metadata
        logger.info("Step 5: Metadata extracted.")
        logger.debug(f"Extracted metadata keys: {list(metadata.keys())}")


        # 6. Chunk Text
        logger.info("Step 6: Chunking Text...")
        if not structurally_coherent_text:
             logger.warning("No text available for chunking. Skipping chunking and embedding.")
             chunks = []
        else:
            chunks = chunk_text(structurally_coherent_text, metadata, chunk_size=512, chunk_overlap=64)
        logger.info(f"Step 6: Text chunked into {len(chunks)} chunks.")

        # 7. Generate Embeddings
        logger.info("Step 7: Generating Embeddings...")
        final_chunks_with_embeddings = generate_embeddings(chunks)
        logger.info(f"Step 7: Embeddings generated for {sum(1 for c in final_chunks_with_embeddings if c.get('embedding_vector'))} chunks.")

        logger.info(f"Successfully finished processing for file: {file_path} by user '{username}'")
        return {"status": "success", "data": final_chunks_with_embeddings, "file_path": file_path}

    except pytesseract.TesseractNotFoundError:
        logger.critical("Tesseract is not installed or not found in PATH. OCR-dependent processing cannot continue.")
        return {"status": "error", "message": "Tesseract (OCR engine) not found. Please install it.", "file_path": file_path}
    except Exception as e:
        logger.error(f"Critical error during processing of {file_path} for user '{username}': {e}", exc_info=True)
        return {"status": "error", "message": f"An unexpected error occurred: {str(e)}", "file_path": file_path}


# --- Example Usage ---
# if __name__ == "__main__":
    logger.info("Starting example document processing...")

    # Create dummy files for demonstration (or use paths to your actual test files)
    # For robust testing, use real PDF, DOCX, PPTX files.
    DUMMY_FILES_DIR = "test_docs"
    os.makedirs(DUMMY_FILES_DIR, exist_ok=True)

    sample_txt_content = "This is a sample text document for testing the processing pipeline.\nIt contains multiple sentences and paragraphs.\nNatural language processing is fascinating. We will chunk this text and generate embeddings."
    sample_txt_path = os.path.join(DUMMY_FILES_DIR, "sample.txt")
    with open(sample_txt_path, "w", encoding="utf-8") as f:
        f.write(sample_txt_content)
    
    # A very basic PDF (for a real test, use a proper PDF with text, images, tables)
    sample_pdf_path = os.path.join(DUMMY_FILES_DIR, "sample.pdf")
    try:
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import letter
        c = canvas.Canvas(sample_pdf_path, pagesize=letter)
        c.drawString(72, 720, "Hello from a ReportLab PDF!")
        c.drawString(72, 700, "This is the first page with some text.")
        c.showPage()
        c.drawString(72, 720, "This is the second page.")
        # Add a dummy table-like text
        c.drawString(72, 650, "Header1,Header2")
        c.drawString(72, 635, "Data1A,Data1B")
        c.drawString(72, 620, "Data2A,Data2B")
        c.save()
        logger.info(f"Created dummy PDF: {sample_pdf_path}")
    except ImportError:
        logger.warning("ReportLab not found. Cannot create dummy PDF. Please install it (`pip install reportlab`) or provide your own PDF.")
        # Create a minimal text-based PDF if reportlab is not available
        if not os.path.exists(sample_pdf_path): # Only if reportlab failed and file doesn't exist
            with open(sample_pdf_path, "w", encoding="utf-8") as f:
                 f.write("%PDF-1.4\n1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n3 0 obj<</Type/Page/MediaBox[0 0 612 792]/Contents 4 0 R/Parent 2 0 R/Resources<<>>>>endobj\n4 0 obj<</Length 40>>stream\nBT /F1 24 Tf 100 700 Td (Minimal PDF) Tj ET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f\n0000000009 00000 n\n0000000058 00000 n\n0000000117 00000 n\n0000000200 00000 n\ntrailer<</Size 5/Root 1 0 R>>\nstartxref\n245\n%%EOF")
            logger.info(f"Created minimal fallback PDF: {sample_pdf_path}")


    # Example: Processing a TXT file
    username = "test_user"
    if os.path.exists(sample_txt_path):
        logger.info(f"\n--- Processing TXT file: {sample_txt_path} ---")
        result_txt = process_uploaded_document(sample_txt_path, username)
        if result_txt["status"] == "success":
            logger.info(f"Successfully processed {sample_txt_path}. Chunks: {len(result_txt['data'])}")
            if result_txt['data']:
                logger.info("--- Sample of TXT Output (First Chunk) ---")
                first_chunk = result_txt['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                # logger.info(f"Metadata: {first_chunk.get('metadata')}") # Can be verbose
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_txt_path}: {result_txt['message']}")
    else:
        logger.warning(f"Sample TXT file not found at {sample_txt_path}, skipping TXT test.")

    # Example: Processing a PDF file
    if os.path.exists(sample_pdf_path):
        logger.info(f"\n--- Processing PDF file: {sample_pdf_path} ---")
        result_pdf = process_uploaded_document(sample_pdf_path, username)
        if result_pdf["status"] == "success":
            logger.info(f"Successfully processed {sample_pdf_path}. Chunks: {len(result_pdf['data'])}")
            if result_pdf['data']:
                logger.info("--- Sample of PDF Output (First Chunk) ---")
                first_chunk = result_pdf['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                logger.info(f"File type from metadata: {first_chunk.get('metadata', {}).get('file_type')}")
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_pdf_path}: {result_pdf['message']}")
    else:
        logger.warning(f"Sample PDF file not found at {sample_pdf_path}, skipping PDF test.")

    # Clean up dummy files (optional)
    # logger.info(f"Cleaning up dummy files in {DUMMY_FILES_DIR}...")
    # for f_name in os.listdir(DUMMY_FILES_DIR):
    #     os.remove(os.path.join(DUMMY_FILES_DIR, f_name))
    # os.rmdir(DUMMY_FILES_DIR)
    # logger.info("Dummy files cleaned up.")


============ ./Readne.txt ============
conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app




============ ./requirements.txt ============
Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
re
typing
PIL # For image processing
pytesseract # OCR
pillow






============ ./__init__.py ============






rag_service/config.py

python
# server/config.py
import os
import logging

# â”€â”€â”€ Logging Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger(__name__)
LOGGING_LEVEL_NAME = os.getenv('LOGGING_LEVEL', 'INFO').upper()
LOGGING_LEVEL      = getattr(logging, LOGGING_LEVEL_NAME, logging.INFO)
LOGGING_FORMAT     = '%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s'

# === Base Directory ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
logger.info(f"[Config] Base Directory: {BASE_DIR}")



def setup_logging():
    """Configure logging across the app."""
    root_logger = logging.getLogger()
    if not root_logger.handlers:  # prevent duplicate handlers
        handler = logging.StreamHandler()
        formatter = logging.Formatter(LOGGING_FORMAT)
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(LOGGING_LEVEL)

    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("faiss.loader").setLevel(logging.WARNING)
    logging.getLogger(__name__).info(f"Logging initialized at {LOGGING_LEVEL_NAME}")

# === Embedding Model Configuration ===
DEFAULT_DOC_EMBED_MODEL = 'mixedbread-ai/mxbai-embed-large-v1'
DOCUMENT_EMBEDDING_MODEL_NAME = os.getenv('DOCUMENT_EMBEDDING_MODEL_NAME', DEFAULT_DOC_EMBED_MODEL)
MAX_TEXT_LENGTH_FOR_NER = int(os.getenv("MAX_TEXT_LENGTH_FOR_NER", 500000))
logger.info(f"[Config] Document Embedding Model: {DOCUMENT_EMBEDDING_MODEL_NAME}")

# Model dimension mapping
_MODEL_TO_DIM_MAPPING = {
    'mixedbread-ai/mxbai-embed-large-v1': 1024,
    'BAAI/bge-large-en-v1.5': 1024,
    'all-MiniLM-L6-v2': 384,
    'sentence-transformers/all-mpnet-base-v2': 768,
}
_FALLBACK_DIM = 768

DOCUMENT_VECTOR_DIMENSION = int(os.getenv(
    "DOCUMENT_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(DOCUMENT_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))
logger.info(f"[Config] Document Vector Dimension: {DOCUMENT_VECTOR_DIMENSION}")

# === AI Core Chunking Config ===
AI_CORE_CHUNK_SIZE = int(os.getenv("AI_CORE_CHUNK_SIZE", 512))
AI_CORE_CHUNK_OVERLAP = int(os.getenv("AI_CORE_CHUNK_OVERLAP", 100))
logger.info(f"[Config] Chunk Size: {AI_CORE_CHUNK_SIZE}, Overlap: {AI_CORE_CHUNK_OVERLAP}")

# === SpaCy Configuration ===
SPACY_MODEL_NAME = os.getenv('SPACY_MODEL_NAME', 'en_core_web_sm')
logger.info(f"[Config] SpaCy Model: {SPACY_MODEL_NAME}")

# === Qdrant Configuration ===
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
QDRANT_COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "my_qdrant_rag_collection")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
QDRANT_URL = os.getenv("QDRANT_URL", None)

QDRANT_COLLECTION_VECTOR_DIM = DOCUMENT_VECTOR_DIMENSION
logger.info(f"[Config] Qdrant Vector Dimension: {QDRANT_COLLECTION_VECTOR_DIM}")

# === Query Embedding Configuration ===
QUERY_EMBEDDING_MODEL_NAME = os.getenv("QUERY_EMBEDDING_MODEL_NAME", DOCUMENT_EMBEDDING_MODEL_NAME)
QUERY_VECTOR_DIMENSION = int(os.getenv(
    "QUERY_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(QUERY_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))

if QUERY_VECTOR_DIMENSION != QDRANT_COLLECTION_VECTOR_DIM:
    logger.info(f"[âš ï¸ Config Warning] Query vector dim ({QUERY_VECTOR_DIMENSION}) != Qdrant dim ({QDRANT_COLLECTION_VECTOR_DIM})")
    # Optionally enforce consistency
    # raise ValueError("Query and Document vector dimensions do not match!")
else:
    logger.info(f"[Config] Query Model: {QUERY_EMBEDDING_MODEL_NAME}")
    logger.info(f"[Config] Query Vector Dimension: {QUERY_VECTOR_DIMENSION}")

QDRANT_DEFAULT_SEARCH_K = int(os.getenv("QDRANT_DEFAULT_SEARCH_K", 5))
QDRANT_SEARCH_MIN_RELEVANCE_SCORE = float(os.getenv("QDRANT_SEARCH_MIN_RELEVANCE_SCORE", 0.1))

# === API Port Configuration ===
API_PORT = int(os.getenv('API_PORT', 5000))
logger.info(f"[Config] API Running Port: {API_PORT}")

# === Optional: Tesseract OCR Path (uncomment if used) ===
# TESSERACT_CMD = os.getenv('TESSERACT_CMD')
# if TESSERACT_CMD:
#     import pytesseract
#     pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD
#     logger.info(f"[Config] Tesseract Path: {TESSERACT_CMD}")


# â”€â”€â”€ Library Availability Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import pypdf
    PYPDF_AVAILABLE      = True
    PYPDF_PDFREADERROR   = pypdf.errors.PdfReadError
except ImportError:
    PYPDF_AVAILABLE      = False
    PYPDF_PDFREADERROR   = Exception

try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE       = True
except ImportError:
    DOCX_AVAILABLE       = False
    DocxDocument         = None

try:
    from pptx import Presentation
    PPTX_AVAILABLE       = True
except ImportError:
    PPTX_AVAILABLE       = False
    Presentation         = None

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    pdfplumber           = None

try:
    import pandas as pd
    PANDAS_AVAILABLE     = True
except ImportError:
    PANDAS_AVAILABLE     = False
    pd                   = None

try:
    from PIL import Image
    PIL_AVAILABLE        = True
except ImportError:
    PIL_AVAILABLE        = False
    Image                = None

try:
    import fitz
    FITZ_AVAILABLE       = True
except ImportError:
    FITZ_AVAILABLE       = False
    fitz                 = None

try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
    TESSERACT_ERROR       = pytesseract.TesseractNotFoundError
except ImportError:
    PYTESSERACT_AVAILABLE = False
    pytesseract           = None
    TESSERACT_ERROR       = Exception

try:
    import PyPDF2
    PYPDF2_AVAILABLE      = True
except ImportError:
    PYPDF2_AVAILABLE      = False
    PyPDF2                = None

# â”€â”€â”€ Optional: Preload SpaCy & Embedding Model â”€â”€â”€â”€â”€â”€â”€
try:
    import spacy
    SPACY_LIB_AVAILABLE = True
    nlp_spacy_core      = spacy.load(SPACY_MODEL_NAME)
    SPACY_MODEL_LOADED  = True
except Exception as e:
    SPACY_LIB_AVAILABLE = False
    nlp_spacy_core      = None
    SPACY_MODEL_LOADED  = False
    logger.warning(f"Failed to load SpaCy model '{SPACY_MODEL_NAME}': {e}")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = True
    document_embedding_model = SentenceTransformer(DOCUMENT_EMBEDDING_MODEL_NAME)
    EMBEDDING_MODEL_LOADED = True
except Exception as e:
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = False
    document_embedding_model = None
    EMBEDDING_MODEL_LOADED = False
    logger.warning(f"Failed to load Sentence transformers: {e}")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_SPLITTER_AVAILABLE = True
except ImportError:
    LANGCHAIN_SPLITTER_AVAILABLE = False
    RecursiveCharacterTextSplitter = None # Placeholder


rag_service/default.py

python
import os
import logging
import sys
import traceback

# --- Path Setup ---
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
project_root_dir = os.path.dirname(server_dir)
sys.path.insert(0, server_dir)
# --- End Path Setup ---

try:
    from rag_service import config
    from rag_service import faiss_handler
    from rag_service import file_parser
except ImportError as e:
     print("ImportError:", e)
     print("Failed to import modules. Ensure the script is run correctly relative to the project structure.")
     print("Current sys.path:", sys.path)
     exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


class DefaultVectorDBBuilder:
    def __init__(self):
        logger.info("Initializing embedding model...")
        try:
            # Use SentenceTransformer as configured in config.py
            self.embed_model = faiss_handler.get_embedding_model()
            if self.embed_model is None:
                 raise RuntimeError("Failed to initialize Sentence Transformer embedding model.")
        except Exception as e:
            logger.error(f"Fatal error initializing embedding model: {e}", exc_info=True)
            raise

        self.chunk_size = config.CHUNK_SIZE
        self.chunk_overlap = config.CHUNK_OVERLAP

        self.default_docs_dir = config.DEFAULT_ASSETS_DIR
        self.index_dir = config.FAISS_INDEX_DIR
        self.default_user_id = config.DEFAULT_INDEX_USER_ID

        self.default_index_user_path = faiss_handler.get_user_index_path(self.default_user_id)
        self.index_file_path = os.path.join(self.default_index_user_path, "index.faiss")
        self.pkl_file_path = os.path.join(self.default_index_user_path, "index.pkl")

        try:
            faiss_handler.ensure_faiss_dir()
            os.makedirs(self.default_index_user_path, exist_ok=True)
        except Exception as e:
             logger.error(f"Failed to create necessary directories: {e}")
             raise

        logger.info(f"Default assets directory: {self.default_docs_dir}")
        logger.info(f"Default index directory: {self.default_index_user_path}")


    def create_default_index(self, force_rebuild=True): # Keep force_rebuild flag
        """Scans default assets, parses files, creates embeddings, and saves the FAISS index."""
        logger.info("--- Starting Default Index Creation ---")

        # --- Force Rebuild Logic ---
        if force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
            logger.warning(f"force_rebuild=True. Deleting existing default index files in {self.default_index_user_path}.")
            try:
                if os.path.exists(self.index_file_path): os.remove(self.index_file_path)
                if os.path.exists(self.pkl_file_path): os.remove(self.pkl_file_path)
                # Clear from cache if loaded
                if self.default_user_id in faiss_handler.loaded_indices:
                    del faiss_handler.loaded_indices[self.default_user_id]
                logger.info("Removed existing default index files and cleared cache.")
            except OSError as e:
                logger.error(f"Error removing existing index files: {e}")
                return False # Stop if we can't remove old files
        elif not force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
             logger.info("Default index already exists and force_rebuild=False. Skipping creation.")
             # Try loading it to confirm validity
             try:
                 faiss_handler.load_or_create_index(self.default_user_id)
                 logger.info("Existing default index loaded successfully.")
                 return True
             except Exception as load_err:
                 logger.error(f"Failed to load existing default index: {load_err}. Consider running with force_rebuild=True.")
                 return False


        # --- Process Documents ---
        all_documents = []
        files_processed = 0
        files_skipped = 0

        logger.info(f"Scanning for processable files in: {self.default_docs_dir}")
        if not os.path.isdir(self.default_docs_dir):
            logger.error(f"Default assets directory not found: {self.default_docs_dir}")
            return False

        for root, _, files in os.walk(self.default_docs_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                # logger.debug(f"Found file: {filename}")
                try:
                    text_content = file_parser.parse_file(file_path)
                    if text_content and text_content.strip():
                        langchain_docs = file_parser.chunk_text(
                            text_content, filename, self.default_user_id
                        )
                        if langchain_docs:
                            all_documents.extend(langchain_docs)
                            files_processed += 1
                            logger.info(f"Parsed and chunked: {filename} ({len(langchain_docs)} chunks)")
                        else:
                            logger.warning(f"Skipped {filename}: No chunks generated.")
                            files_skipped += 1
                    else:
                        logger.warning(f"Skipped {filename}: No text content or unsupported type.")
                        files_skipped += 1
                except Exception as e:
                    logger.error(f"Error processing file {filename}: {e}")
                    traceback.print_exc()
                    files_skipped += 1

        if not all_documents:
            logger.error(f"No processable documents found or generated in {self.default_docs_dir}. Cannot create index.")
            # Still create an empty index structure if the directory was valid
            try:
                 logger.info("Creating an empty index structure as no documents were found.")
                 faiss_handler.load_or_create_index(self.default_user_id) # Creates empty index
                 logger.info("Empty default index created successfully.")
                 return True # Success, but empty
            except Exception as empty_create_err:
                 logger.error(f"Failed to create empty index structure: {empty_create_err}", exc_info=True)
                 return False


        logger.info(f"Total files processed: {files_processed}, skipped: {files_skipped}")
        logger.info(f"Creating embeddings and adding {len(all_documents)} total chunks to index...")

        try:
            # The load_or_create_index function will handle creating the empty structure
            # if it doesn't exist (or after deletion if force_rebuild=True)
            logger.info("Ensuring FAISS index structure exists...")
            index_instance = faiss_handler.load_or_create_index(self.default_user_id)

            logger.info(f"Adding {len(all_documents)} documents to the default index '{self.default_user_id}'...")
            # Use the updated handler function which now manages IDs correctly
            faiss_handler.add_documents_to_index(self.default_user_id, all_documents)

            # Verify save occurred
            if not os.path.exists(self.index_file_path) or not os.path.exists(self.pkl_file_path):
                 logger.error("Index files were not found after adding documents. Check permissions or disk space.")
                 return False

            logger.info(f"Successfully created/updated and saved default index ({self.default_user_id}) with {len(all_documents)} document chunks.")
            logger.info("--- Default Index Creation Finished ---")
            return True

        except Exception as e:
            logger.error(f"Failed during embedding or index creation: {e}", exc_info=True)
            logger.error("--- Default Index Creation Failed ---")
            return False

def main():
    print("--- Running Default Index Builder ---")
    try:
        builder = DefaultVectorDBBuilder()
    except Exception as init_err:
        print(f"FATAL: Failed to initialize builder: {init_err}")
        sys.exit(1)

    if not os.path.isdir(builder.default_docs_dir):
         logger.error(f"Default assets directory '{builder.default_docs_dir}' is missing.")
         sys.exit(1)

    # --- Always force rebuild as requested ---
    force = True
    logger.info(f"Starting index creation (force_rebuild={force})...")
    if not builder.create_default_index(force_rebuild=force):
        logger.error("Index creation process failed.")
        sys.exit(1)
    else:
        logger.info("Default index creation process completed successfully.")
        sys.exit(0)

if __name__ == "__main__":
    main()



rag_service/faiss_handler.py

python
# server/rag_service/faiss_handler.py

import os
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.embeddings import Embeddings as LangchainEmbeddings
from langchain_core.documents import Document as LangchainDocument
from langchain_community.docstore import InMemoryDocstore
from rag_service import config
import numpy as np
import time
import logging
import pickle
import uuid
import shutil # Import shutil for removing directories

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)

embedding_model: LangchainEmbeddings | None = None
loaded_indices = {}
_embedding_dimension = None # Cache the dimension

def get_embedding_dimension(embedder: LangchainEmbeddings) -> int:
    """Gets and caches the embedding dimension."""
    global _embedding_dimension
    if _embedding_dimension is None:
        try:
            logger.info("Determining embedding dimension...")
            dummy_embedding = embedder.embed_query("dimension_check")
            dimension = len(dummy_embedding)
            if not isinstance(dimension, int) or dimension <= 0:
                raise ValueError(f"Invalid embedding dimension obtained: {dimension}")
            _embedding_dimension = dimension
            logger.info(f"Detected embedding dimension: {_embedding_dimension}")
        except Exception as e:
            logger.error(f"CRITICAL ERROR determining embedding dimension: {e}", exc_info=True)
            raise RuntimeError(f"Failed to determine embedding dimension: {e}")
    return _embedding_dimension

def get_embedding_model():
    global embedding_model
    if embedding_model is None:
        if config.EMBEDDING_TYPE == 'sentence-transformer':
            logger.info(f"Initializing HuggingFace Embeddings for Sentence Transformer (Model: {config.EMBEDDING_MODEL_NAME})")
            try:
                # Try CUDA first, fallback to CPU
                try:
                    if faiss.get_num_gpus() > 0:
                        device = 'cuda'
                        logger.info("CUDA detected. Using GPU for embeddings.")
                    else:
                        raise RuntimeError("No GPU found") # Force fallback
                except Exception:
                    device = 'cpu'
                    logger.warning("CUDA not available or GPU check failed. Using CPU for embeddings. This might be slow.")

                embedding_model = HuggingFaceEmbeddings(
                    model_name=config.EMBEDDING_MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True} # Often recommended for cosine similarity / MIPS with FAISS
                )
                # Determine and cache dimension on successful load
                get_embedding_dimension(embedding_model)

                logger.info("Testing embedding function...")
                test_embedding_doc = embedding_model.embed_documents(["test document"])
                test_embedding_query = embedding_model.embed_query("test query")
                if not test_embedding_doc or not test_embedding_query:
                    raise ValueError("Embedding test failed, returned empty results.")
                logger.info(f"Embedding test successful.")

            except Exception as e:
                logger.error(f"Error loading HuggingFace Embeddings for '{config.EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
                embedding_model = None
                raise RuntimeError(f"Failed to load embedding model: {e}")
        else:
            raise ValueError(f"Unsupported embedding type in config: {config.EMBEDDING_TYPE}. Expected 'sentence-transformer'.")
    return embedding_model

def get_user_index_path(user_id):
    safe_user_id = str(user_id).replace('.', '_').replace('/', '_').replace('\\', '_')
    user_dir = os.path.join(config.FAISS_INDEX_DIR, f"user_{safe_user_id}")
    return user_dir

def _delete_index_files(index_path, user_id):
    """Safely deletes index files for a user."""
    logger.warning(f"Deleting potentially incompatible index files for user '{user_id}' at {index_path}")
    try:
        if os.path.isdir(index_path):
            shutil.rmtree(index_path)
            logger.info(f"Successfully deleted directory: {index_path}")
        # If only loose files exist (less likely with save_local)
        index_file = os.path.join(index_path, "index.faiss")
        pkl_file = os.path.join(index_path, "index.pkl")
        if os.path.exists(index_file): os.remove(index_file)
        if os.path.exists(pkl_file): os.remove(pkl_file)
    except OSError as e:
        logger.error(f"Error deleting index files/directory for user '{user_id}' at {index_path}: {e}", exc_info=True)
        # Don't raise here, allow fallback to creating new index if possible

def load_or_create_index(user_id):
    global loaded_indices
    if user_id in loaded_indices:
        # **Even if cached, re-verify dimension on subsequent loads in case model changed**
        index = loaded_indices[user_id]
        embedder = get_embedding_model() # Ensure model is loaded
        current_dim = get_embedding_dimension(embedder)
        if hasattr(index, 'index') and index.index is not None and index.index.d != current_dim:
            logger.warning(f"Cached index for user '{user_id}' has dimension {index.index.d}, but current model has dimension {current_dim}. Discarding cache and forcing reload/recreate.")
            del loaded_indices[user_id] # Remove from cache
            # Fall through to load/create logic below
        else:
            logger.debug(f"Returning cached index for user '{user_id}'.")
            return index # Return cached and verified index

    index_path = get_user_index_path(user_id)
    index_file = os.path.join(index_path, "index.faiss")
    pkl_file = os.path.join(index_path, "index.pkl")

    embedder = get_embedding_model()
    if embedder is None:
        raise RuntimeError("Embedding model is not available.")
    current_embedding_dim = get_embedding_dimension(embedder)

    force_recreate = False
    if os.path.exists(index_file) and os.path.exists(pkl_file):
        logger.info(f"Attempting to load existing FAISS index for user '{user_id}' from {index_path}")
        try:
            start_time = time.time()
            # Temporarily load to check dimension
            index = FAISS.load_local(
                folder_path=index_path,
                embeddings=embedder,
                allow_dangerous_deserialization=True # Use with caution if index source isn't trusted
            )
            end_time = time.time()

            # --- CRITICAL DIMENSION CHECK ---
            if not hasattr(index, 'index') or index.index is None:
                 logger.warning(f"Loaded index for user '{user_id}' has no 'index' attribute or it's None. Forcing recreation.")
                 force_recreate = True
            elif index.index.d != current_embedding_dim:
                logger.warning(f"DIMENSION MISMATCH! Index for user '{user_id}' has dimension {index.index.d}, but current embedding model has dimension {current_embedding_dim}. Index is incompatible and will be recreated.")
                force_recreate = True
            elif index.index.ntotal == 0:
                logger.info(f"Loaded index for user '{user_id}' is empty (0 vectors). Will use it but note it's empty.")
                 # Not forcing recreate, just noting it's empty
            # --- END DIMENSION CHECK ---

            if force_recreate:
                _delete_index_files(index_path, user_id)
                # Don't return the incompatible index, fall through to create new one
            else:
                # If dimensions match and index is valid
                logger.info(f"Index for user '{user_id}' loaded successfully in {end_time - start_time:.2f} seconds. Dimension ({index.index.d}) matches. Contains {index.index.ntotal} vectors.")
                loaded_indices[user_id] = index
                return index

        except (pickle.UnpicklingError, EOFError, ModuleNotFoundError, AttributeError, ValueError) as load_err:
            logger.error(f"Error loading index for user '{user_id}' from {index_path}: {load_err}")
            logger.warning("Index files might be corrupted or incompatible. Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs
        except Exception as e:
            logger.error(f"Unexpected error loading index for user '{user_id}': {e}", exc_info=True)
            logger.warning("Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs

    # --- Create New Index Logic ---
    # This block runs if files didn't exist OR force_recreate is True
    logger.info(f"Creating new FAISS index structure for user '{user_id}' at {index_path} with dimension {current_embedding_dim}")
    try:
        # Ensure directory exists (it might have been deleted)
        os.makedirs(index_path, exist_ok=True)

        # Use the already determined dimension
        # Use IndexFlatIP if embeddings are normalized (recommended)
        faiss_index = faiss.IndexIDMap(faiss.IndexFlatIP(current_embedding_dim))
        # faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(current_embedding_dim)) # Use L2 if not normalized

        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}

        index = FAISS(
            embedding_function=embedder,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            normalize_L2=False # Set True if using IndexFlatIP and normalized embeddings (which we are with encode_kwargs)
        )

        logger.info(f"Initialized empty index structure for user '{user_id}'.")
        loaded_indices[user_id] = index # Add to cache immediately
        save_index(user_id) # Save the empty structure
        logger.info(f"New empty index for user '{user_id}' created and saved.")
        return index
    except Exception as e:
        logger.error(f"CRITICAL ERROR creating new index for user '{user_id}': {e}", exc_info=True)
        if user_id in loaded_indices:
            del loaded_indices[user_id] # Clean up cache on failure
        # Attempt to clean up directory if creation failed badly
        _delete_index_files(index_path, user_id)
        raise RuntimeError(f"Failed to initialize FAISS index for user '{user_id}'")


def add_documents_to_index(user_id, documents: list[LangchainDocument]):
    if not documents:
        logger.warning(f"No documents provided to add for user '{user_id}'.")
        return

    try:
        index = load_or_create_index(user_id) # This now handles dimension checks/recreation
        embedder = get_embedding_model() # Ensure model is loaded

        # --- VERIFY DIMENSIONS AGAIN before adding (paranoid check) ---
        current_dim = get_embedding_dimension(embedder)
        if not hasattr(index, 'index') or index.index is None:
             logger.error(f"Index object for user '{user_id}' is invalid after load/create. Cannot add documents.")
             raise RuntimeError("Failed to get valid index structure.")
        if index.index.d != current_dim:
             logger.error(f"FATAL: Dimension mismatch just before adding documents for user '{user_id}'. Index: {index.index.d}, Model: {current_dim}. This shouldn't happen if load_or_create_index worked.")
             # Attempt recovery by deleting and trying again? Risky loop potential.
             _delete_index_files(get_user_index_path(user_id), user_id)
             if user_id in loaded_indices: del loaded_indices[user_id]
             raise RuntimeError(f"Inconsistent index dimension detected for user '{user_id}'. Please retry.")
        # --- END VERIFY ---

        logger.info(f"Adding {len(documents)} documents to index for user '{user_id}' (Index dim: {index.index.d})...")
        start_time = time.time()

        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        # Generate embeddings using the current model
        embeddings = embedder.embed_documents(texts)
        if not embeddings or len(embeddings) != len(texts):
             logger.error(f"Embedding generation failed or returned unexpected number of vectors for user '{user_id}'.")
             raise ValueError("Embedding generation failed.")
        if len(embeddings[0]) != current_dim:
             logger.error(f"Generated embeddings have incorrect dimension ({len(embeddings[0])}) for user '{user_id}', expected {current_dim}.")
             raise ValueError("Generated embedding dimension mismatch.")

        embeddings_np = np.array(embeddings, dtype=np.float32)

        # Generate unique IDs for FAISS
        ids = [str(uuid.uuid4()) for _ in texts]
        ids_np = np.array([uuid.UUID(id_).int & (2**63 - 1) for id_ in ids], dtype=np.int64)


        # Add embeddings and their corresponding IDs to the FAISS index
        index.index.add_with_ids(embeddings_np, ids_np)

        # Add the original documents and their metadata to the Langchain Docstore,
        # using the generated string UUIDs as keys.
        # Map the FAISS integer ID back to the string UUID used in the docstore.
        docstore_additions = {doc_id: doc for doc_id, doc in zip(ids, documents)}
        index.docstore.add(docstore_additions)
        for i, faiss_id in enumerate(ids_np):
            index.index_to_docstore_id[int(faiss_id)] = ids[i] # Map FAISS int ID -> string UUID

        end_time = time.time()
        logger.info(f"Successfully added {len(documents)} vectors/documents for user '{user_id}' in {end_time - start_time:.2f} seconds. Total vectors: {index.index.ntotal}")
        save_index(user_id)
    except Exception as e:
        logger.error(f"Error adding documents for user '{user_id}': {e}", exc_info=True)
        # Don't re-raise here if app.py handles it, but ensure logging is clear
        raise # Re-raise the exception so app.py can catch it and return 500

def query_index(user_id, query_text, k=3):
    all_results_with_scores = []
    embedder = get_embedding_model()

    if embedder is None:
        logger.error("Embedding model is not available for query.")
        raise ConnectionError("Embedding model is not available for query.")

    try:
        start_time = time.time()
        user_index = None # Initialize to None
        default_index = None # Initialize to None

        # Query User Index
        try:
            user_index = load_or_create_index(user_id) # Assign to user_index
            if hasattr(user_index, 'index') and user_index.index is not None and user_index.index.ntotal > 0:
                logger.info(f"Querying index for user: '{user_id}' (Dim: {user_index.index.d}, Vectors: {user_index.index.ntotal}) with k={k}")
                user_results = user_index.similarity_search_with_score(query_text, k=k)
                logger.info(f"User index '{user_id}' query returned {len(user_results)} results.")
                all_results_with_scores.extend(user_results)
            else:
                logger.info(f"Skipping query for user '{user_id}': Index is empty or invalid.")
        except FileNotFoundError:
            logger.warning(f"User index files for '{user_id}' not found on disk (might be first time). Skipping query for this index.")
        except RuntimeError as e:
            logger.error(f"Could not load or create user index for '{user_id}': {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error querying user index for '{user_id}': {e}", exc_info=True)


        # Query Default Index (if different from user_id)
        if user_id != config.DEFAULT_INDEX_USER_ID:
            try:
                default_index = load_or_create_index(config.DEFAULT_INDEX_USER_ID) # Assign to default_index
                if hasattr(default_index, 'index') and default_index.index is not None and default_index.index.ntotal > 0:
                    logger.info(f"Querying default index '{config.DEFAULT_INDEX_USER_ID}' (Dim: {default_index.index.d}, Vectors: {default_index.index.ntotal}) with k={k}")
                    default_results = default_index.similarity_search_with_score(query_text, k=k)
                    logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' query returned {len(default_results)} results.")
                    all_results_with_scores.extend(default_results)
                else:
                    logger.info(f"Skipping query for default index '{config.DEFAULT_INDEX_USER_ID}': Index is empty or invalid.")
            except FileNotFoundError:
                 logger.warning(f"Default index '{config.DEFAULT_INDEX_USER_ID}' not found on disk (run default.py?). Skipping query.")
            except RuntimeError as e:
                logger.error(f"Could not load or create default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)
            except Exception as e:
                logger.error(f"Unexpected error querying default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)

        query_time = time.time()
        logger.info(f"Completed all index queries in {query_time - start_time:.2f} seconds. Found {len(all_results_with_scores)} raw results.")

        # --- Deduplication and Sorting ---
        unique_results = {}
        for doc, score in all_results_with_scores:
            if not doc or not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                logger.warning(f"Skipping invalid document object in results: {doc}")
                continue

            # Use the fallback content-based key
            content_key = f"{doc.metadata.get('documentName', 'Unknown')}_{doc.page_content[:200]}"
            unique_key = content_key # Use the content key directly

            # Add or update if the new score is better (lower for L2 distance / IP distance if normalized)
            if unique_key not in unique_results or score < unique_results[unique_key][1]:
                unique_results[unique_key] = (doc, score)

        # Sort by score (ascending for L2 distance / IP distance)
        sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
        final_results = sorted_results[:k] # Get top k unique results

        logger.info(f"Returning {len(final_results)} unique results after filtering and sorting.")
        return final_results
    except Exception as e:
        logger.error(f"Error during query processing for user '{user_id}': {e}", exc_info=True)
        return [] # Return empty list on error


def save_index(user_id):
    global loaded_indices
    if user_id not in loaded_indices:
        logger.warning(f"Index for user '{user_id}' not found in cache, cannot save.")
        return

    index = loaded_indices[user_id]
    index_path = get_user_index_path(user_id)

    if not isinstance(index, FAISS) or not hasattr(index, 'index') or not hasattr(index, 'docstore') or not hasattr(index, 'index_to_docstore_id'):
        logger.error(f"Cannot save index for user '{user_id}': Invalid index object in cache.")
        return

    # Ensure the target directory exists before saving
    try:
        os.makedirs(index_path, exist_ok=True)
        logger.info(f"Saving FAISS index for user '{user_id}' to {index_path} (Vectors: {index.index.ntotal if hasattr(index.index, 'ntotal') else 'N/A'})...")
        start_time = time.time()
        # This saves index.faiss and index.pkl
        index.save_local(folder_path=index_path)
        end_time = time.time()
        logger.info(f"Index for user '{user_id}' saved successfully in {end_time - start_time:.2f} seconds.")
    except Exception as e:
        logger.error(f"Error saving FAISS index for user '{user_id}' to {index_path}: {e}", exc_info=True)

# --- ADD THIS FUNCTION DEFINITION BACK ---
def ensure_faiss_dir():
    """Ensures the base FAISS index directory exists."""
    try:
        os.makedirs(config.FAISS_INDEX_DIR, exist_ok=True)
        logger.info(f"Ensured FAISS base directory exists: {config.FAISS_INDEX_DIR}")
    except OSError as e:
        logger.error(f"Could not create FAISS base directory {config.FAISS_INDEX_DIR}: {e}")
        raise # Raise the error to prevent startup if dir creation fails
# --- END OF ADDED FUNCTION ---



rag_service/file_parser.py

python
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error



rag_service/Readne.txt


conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app




rag_service/requirements.txt


Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
re
typing
PIL # For image processing
pytesseract # OCR
pillow
qdrant-client






rag_service/vector_db_service.py

python
import uuid
import logging
from typing import List, Dict, Tuple, Optional, Any

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

# Assuming vector_db_service.py and config.py are in the same package directory (e.g., rag_service/)
# and you run your application as a module (e.g., python -m rag_service.main_app)
# or have otherwise correctly set up the Python path.
import config # Changed to relative import

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Document: # For search result formatting
    def __init__(self, page_content: str, metadata: dict):
        self.page_content = page_content
        self.metadata = metadata

    def to_dict(self):
        return {"page_content": self.page_content, "metadata": self.metadata}

class VectorDBService:
    def __init__(self):
        logger.info("Initializing VectorDBService...")
        logger.info(f"  Qdrant Host: {config.QDRANT_HOST}, Port: {config.QDRANT_PORT}, URL: {config.QDRANT_URL}")
        logger.info(f"  Collection: {config.QDRANT_COLLECTION_NAME}")
        logger.info(f"  Query Embedding Model: {config.QUERY_EMBEDDING_MODEL_NAME}")
        
        # The vector dimension for the Qdrant collection is defined by the DOCUMENT embedding model
        # This is set in config.QDRANT_COLLECTION_VECTOR_DIM
        self.vector_dim = config.QDRANT_COLLECTION_VECTOR_DIM
        logger.info(f"  Service expects Vector Dim for Qdrant collection: {self.vector_dim} (from document model config)")

        if config.QDRANT_URL:
            self.client = QdrantClient(
                url=config.QDRANT_URL,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )
        else:
            self.client = QdrantClient(
                host=config.QDRANT_HOST,
                port=config.QDRANT_PORT,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )

        try:
            # This model is for encoding search queries.
            # Its output dimension MUST match self.vector_dim (QDRANT_COLLECTION_VECTOR_DIM).
            logger.info(f"  Loading query embedding model: '{config.QUERY_EMBEDDING_MODEL_NAME}'")
            self.model = SentenceTransformer(config.QUERY_EMBEDDING_MODEL_NAME)
            model_embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"  Query model loaded. Output dimension: {model_embedding_dim}")

            if model_embedding_dim != self.vector_dim:
                error_msg = (
                    f"CRITICAL DIMENSION MISMATCH: Query model '{config.QUERY_EMBEDDING_MODEL_NAME}' "
                    f"outputs embeddings of dimension {model_embedding_dim}, but the Qdrant collection "
                    f"is configured for dimension {self.vector_dim} (derived from document model: "
                    f"'{config.DOCUMENT_EMBEDDING_MODEL_NAME}'). Search functionality will fail. "
                    "Ensure query and document models produce compatible embedding dimensions, "
                    "or environment variables for dimensions are correctly set."
                )
                logger.error(error_msg)
                raise ValueError(error_msg) # Critical error, stop initialization
            else:
                logger.info(f"  Query model output dimension ({model_embedding_dim}) matches "
                            f"Qdrant collection dimension ({self.vector_dim}).")

        except Exception as e:
            logger.error(f"Error initializing SentenceTransformer model '{config.QUERY_EMBEDDING_MODEL_NAME}' for query encoding: {e}", exc_info=True)
            raise # Re-raise to prevent service startup with a non-functional query encoder

        self.collection_name = config.QDRANT_COLLECTION_NAME
        # No ThreadPoolExecutor needed here if document encoding is external

    def _recreate_qdrant_collection(self):
        logger.info(f"Attempting to (re)create collection '{self.collection_name}' with vector size {self.vector_dim}.")
        try:
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=self.vector_dim,
                    distance=models.Distance.COSINE,
                ),
            )
            logger.info(f"Collection '{self.collection_name}' (re)created successfully.")
        except Exception as e_recreate:
            logger.error(f"Failed to (re)create collection '{self.collection_name}': {e_recreate}", exc_info=True)
            raise

    def setup_collection(self):
        try:
            collection_info = self.client.get_collection(collection_name=self.collection_name)
            logger.info(f"Collection '{self.collection_name}' already exists.")
            
            # Handle different Qdrant client versions for accessing vector config
            current_vectors_config = None
            if hasattr(collection_info.config.params, 'vectors'): # For simple vector config
                if isinstance(collection_info.config.params.vectors, models.VectorParams):
                     current_vectors_config = collection_info.config.params.vectors
                elif isinstance(collection_info.config.params.vectors, dict): # For named vectors
                    # Assuming default unnamed vector or first one if named
                    default_vector_name = '' # Common for single vector setup
                    if default_vector_name in collection_info.config.params.vectors:
                        current_vectors_config = collection_info.config.params.vectors[default_vector_name]
                    elif collection_info.config.params.vectors: # Get first one if default not found
                        current_vectors_config = next(iter(collection_info.config.params.vectors.values()))

            if not current_vectors_config:
                 logger.error(f"Could not determine vector configuration for existing collection '{self.collection_name}'. Recreating.")
                 self._recreate_qdrant_collection()
            elif current_vectors_config.size != self.vector_dim:
                logger.warning(f"Collection '{self.collection_name}' vector size {current_vectors_config.size} "
                               f"differs from service's expected {self.vector_dim}. Recreating.")
                self._recreate_qdrant_collection()
            elif current_vectors_config.distance != models.Distance.COSINE: # Ensure distance is also checked
                logger.warning(f"Collection '{self.collection_name}' distance {current_vectors_config.distance} "
                               f"differs from expected {models.Distance.COSINE}. Recreating.")
                self._recreate_qdrant_collection()
            else:
                logger.info(f"Collection '{self.collection_name}' configuration is compatible (Size: {current_vectors_config.size}, Distance: {current_vectors_config.distance}).")

        except Exception as e: # Broad exception for Qdrant client errors
            # More specific check for "Not found" type errors
            if "not found" in str(e).lower() or \
               (hasattr(e, 'status_code') and e.status_code == 404) or \
               " à¦­à¦¾à¦—à§à¦¯à¦¬à¦¾à¦¨" in str(e).lower(): # "Lucky" in Bengali, seems to be part of an error message you encountered
                 logger.info(f"Collection '{self.collection_name}' not found. Attempting to create...")
            else:
                 logger.warning(f"Error checking collection '{self.collection_name}': {type(e).__name__} - {e}. Attempting to (re)create anyway...")
            self._recreate_qdrant_collection()

    def add_processed_chunks(self, processed_chunks: List[Dict[str, Any]]) -> int:
        if not processed_chunks:
            logger.warning("add_processed_chunks received an empty list. No points to upsert.")
            return 0

        points_to_upsert = []
        doc_name_for_logging = "Unknown Document"

        for chunk_data in processed_chunks:
            point_id = chunk_data.get('id', str(uuid.uuid4()))
            vector = chunk_data.get('embedding')
            
            payload = chunk_data.get('metadata', {}).copy()
            payload['chunk_text_content'] = chunk_data.get('text_content', '')

            if not doc_name_for_logging or doc_name_for_logging == "Unknown Document":
                doc_name_for_logging = payload.get('original_name', payload.get('document_name', "Unknown Document"))

            if not vector:
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' is missing 'embedding'. Skipping.")
                continue
            if not isinstance(vector, list) or not all(isinstance(x, (float, int)) for x in vector): # Allow int too, SentenceTransformer can return float32 which might be int-like in lists
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has an invalid 'embedding' format. Skipping.")
                continue
            if len(vector) != self.vector_dim:
                logger.error(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has embedding dimension {len(vector)}, "
                             f"but collection expects {self.vector_dim}. Skipping. "
                             f"Ensure ai_core's document embedding model ('{config.DOCUMENT_EMBEDDING_MODEL_NAME}') "
                             f"output dimension matches configuration.")
                continue

            points_to_upsert.append(models.PointStruct(
                id=point_id,
                vector=[float(v) for v in vector], # Ensure all are floats for Qdrant
                payload=payload
            ))

        if not points_to_upsert:
            logger.warning(f"No valid points constructed from processed_chunks for document: {doc_name_for_logging}.")
            return 0

        try:
            self.client.upsert(collection_name=self.collection_name, points=points_to_upsert, wait=True) # wait=True can be useful for debugging
            logger.info(f"Successfully upserted {len(points_to_upsert)} chunks for document: {doc_name_for_logging} into Qdrant.")
            return len(points_to_upsert)
        except Exception as e:
            logger.error(f"Error upserting processed chunks to Qdrant for document: {doc_name_for_logging}: {e}", exc_info=True)
            raise

    def search_documents(self, query: str, k: int = -1, filter_conditions: Optional[models.Filter] = None) -> Tuple[List[Document], str, Dict]:
        # Use default k from config if not provided or invalid
        if k <= 0:
            k_to_use = config.QDRANT_DEFAULT_SEARCH_K
        else:
            k_to_use = k

        context_docs = []
        formatted_context_text = "No relevant context was found in the available documents."
        context_docs_map = {}

        logger.info(f"Searching with query (first 50 chars): '{query[:50]}...', k: {k_to_use}")
        if filter_conditions:
            try: filter_dict = filter_conditions.dict()
            except AttributeError: # For older Pydantic versions
                try: filter_dict = filter_conditions.model_dump()
                except AttributeError: filter_dict = str(filter_conditions) # Fallback
            logger.info(f"Applying filter: {filter_dict}")
        else:
            logger.info("No filter applied for search.")

        try:
            query_embedding = self.model.encode(query).tolist()
            logger.debug(f"Generated query_embedding (length: {len(query_embedding)}, first 5 dims: {query_embedding[:5]})")

            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                query_filter=filter_conditions,
                limit=k_to_use,
                with_payload=True,
                score_threshold=config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE # Apply score threshold directly in search
            )
            logger.info(f"Qdrant client.search returned {len(search_results)} results (after score threshold).")

            if not search_results:
                return context_docs, formatted_context_text, context_docs_map

            for idx, point in enumerate(search_results):
                # Score threshold is already applied by Qdrant if score_threshold parameter is used.
                # If not using score_threshold in client.search, uncomment this:
                # if point.score < config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE:
                #     logger.debug(f"Skipping point ID {point.id} with score {point.score:.4f} (below threshold {config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE})")
                #     continue

                payload = point.payload
                content = payload.get("chunk_text_content", payload.get("text_content", payload.get("chunk_text", "")))

                retrieved_metadata = payload.copy()
                retrieved_metadata["qdrant_id"] = point.id
                retrieved_metadata["score"] = point.score

                doc = Document(page_content=content, metadata=retrieved_metadata)
                context_docs.append(doc)

            # Format context and citations
            formatted_context_parts = []
            for i, doc_obj in enumerate(context_docs):
                citation_index = i + 1
                doc_meta = doc_obj.metadata
                # Use more robust fetching of metadata keys
                display_subject = doc_meta.get("title", doc_meta.get("subject", "Unknown Subject")) # Prefer title for subject
                doc_name = doc_meta.get("original_name", doc_meta.get("file_name", "N/A"))
                page_num_info = f" (Page: {doc_meta.get('page_number', 'N/A')})" if doc_meta.get('page_number') else "" # Add page number if available
                
                content_preview = doc_obj.page_content[:200] + "..." if len(doc_obj.page_content) > 200 else doc_obj.page_content

                formatted = (f"[{citation_index}] Score: {doc_meta.get('score', 0.0):.4f} | "
                             f"Source: {doc_name}{page_num_info} | Subject: {display_subject}\n"
                             f"Content: {content_preview}") # Show content preview
                formatted_context_parts.append(formatted)

                context_docs_map[str(citation_index)] = {
                    "subject": display_subject,
                    "document_name": doc_name,
                    "page_number": doc_meta.get("page_number"),
                    "content_preview": content_preview, # Store preview
                    "full_content": doc_obj.page_content, # Store full content for potential later use
                    "score": doc_meta.get("score", 0.0),
                    "qdrant_id": doc_meta.get("qdrant_id"),
                    "original_metadata": doc_meta # Store all original metadata from payload
                }
            if formatted_context_parts:
                formatted_context_text = "\n\n---\n\n".join(formatted_context_parts)
            else:
                formatted_context_text = "No sufficiently relevant context was found after filtering."

        except Exception as e:
            logger.error(f"Qdrant search/RAG error: {e}", exc_info=True)
            formatted_context_text = "Error retrieving context due to an internal server error."

        return context_docs, formatted_context_text, context_docs_map

    def close(self):
        logger.info("VectorDBService close called.")
        # No specific resources like ThreadPoolExecutor to release in this version.
        # QdrantClient does not have an explicit close() method in recent versions.


rag_service/__init__.py

python



routes/analysis.js

javascript
console.log("Hello World");



routes/auth.js

javascript
// server/routes/auth.js
const express = require('express');
const { v4: uuidv4 } = require('uuid'); // For generating session IDs
const User = require('../models/User'); // Mongoose User model
require('dotenv').config();

const router = express.Router();

// --- @route   POST /api/auth/signup ---
// --- @desc    Register a new user ---
// --- @access  Public ---
router.post('/signup', async (req, res) => {
  const { username, password } = req.body;

  // Basic validation
  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }
  if (password.length < 6) {
     return res.status(400).json({ message: 'Password must be at least 6 characters long' });
  }

  try {
    // Check if user already exists
    const existingUser = await User.findOne({ username });
    if (existingUser) {
      return res.status(400).json({ message: 'Username already exists' });
    }

    // Create new user (password hashing is handled by pre-save middleware in User model)
    const newUser = new User({ username, password });
    await newUser.save();

    // Generate a new session ID for the first login
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Note: Mongoose excludes 'select: false' fields by default after save() too
    res.status(201).json({
      _id: newUser._id, // Send user ID
      username: newUser.username,
      sessionId: sessionId, // Send session ID on successful signup/login
      message: 'User registered successfully',
    });

  } catch (error) {
    console.error('Signup Error:', error);
    // Handle potential duplicate key errors more gracefully if needed
    if (error.code === 11000) {
        return res.status(400).json({ message: 'Username already exists.' });
    }
    res.status(500).json({ message: 'Server error during signup' });
  }
});

// --- @route   POST /api/auth/signin ---
// --- @desc    Authenticate user (using custom static method) ---
// --- @access  Public ---
router.post('/signin', async (req, res) => {
  const { username, password } = req.body;

  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }

  try {
    // *** CHANGE HERE: Use the static method from User model ***
    // This method finds the user AND selects the password field AND compares the password
    const user = await User.findByCredentials(username, password);

    // Check if the method returned a user (means credentials were valid)
    if (!user) {
      // findByCredentials returns null if user not found OR password doesn't match
      return res.status(401).json({ message: 'Invalid credentials' }); // Use generic message
    }

    // User authenticated successfully if we reached here

    // Generate a NEW session ID for this login session
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Even though 'user' has the password field selected from findByCredentials,
    // Mongoose's .toJSON() or spreading might still exclude it if schema default is select:false.
    // Explicitly create the response object.
    res.status(200).json({
      _id: user._id, // Send user ID
      username: user.username,
      sessionId: sessionId, // Send a *new* session ID on each successful login
      message: 'Login successful',
    });

  } catch (error) {
    // Log the specific error for debugging
    console.error('Signin Error:', error);
    // Check if the error came from the comparePassword method (e.g., bcrypt issue)
    if (error.message === "Password field not available for comparison.") {
        // This shouldn't happen if findByCredentials is used correctly, but good to check
        console.error("Developer Error: Password field was not selected before comparison attempt.");
        return res.status(500).json({ message: 'Internal server configuration error during signin.' });
    }
    res.status(500).json({ message: 'Server error during signin' });
  }
});


module.exports = router;



routes/chat.js

javascript
// server/routes/chat.js
const express = require('express');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const ChatHistory = require('../models/ChatHistory');
const { v4: uuidv4 } = require('uuid');
const { generateContentWithHistory } = require('../services/geminiService');

const router = express.Router();

// --- Helper to call Python RAG Query Endpoint ---
async function queryPythonRagService(userId, query, k = 5, filter = null) { // Added filter parameter
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL;
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL is not set in environment. Cannot query RAG service.");
        throw new Error("RAG service configuration error."); // Throw error to be caught by caller
    }
    const searchUrl = `${pythonServiceUrl}/query`; // <<<--- CORRECTED TO /query ---

    console.log(`Querying Python RAG service for User ${userId} at ${searchUrl} with query (first 50): "${query.substring(0,50)}...", k=${k}`);
    
    const payload = {
        query: query,
        k: k
    };

    if (filter && typeof filter === 'object' && Object.keys(filter).length > 0) {
        payload.filter = filter;
        console.log(`  Applying filter to Python RAG search:`, filter);
    } else {
        console.log(`  No filter applied to Python RAG search.`);
    }

    try {
        const response = await axios.post(searchUrl, payload, { 
            headers: { 'Content-Type': 'application/json' },
            timeout: 30000 
        });

        // Python /search returns: { ..., retrieved_documents_list: [{ page_content: "...", metadata: {...} }], ... }
        if (response.data && Array.isArray(response.data.retrieved_documents_list)) {
            console.log(`Python RAG service /search returned ${response.data.results_count} results.`);
            
            // Adapt to the structure expected by the Node.js /api/chat/rag and /api/chat/message routes
            const adaptedDocs = response.data.retrieved_documents_list.map(doc => {
                const metadata = doc.metadata || {};
                return {
                    // The /api/chat/message route expects 'documentName' and 'content'
                    documentName: metadata.original_name || metadata.file_name || metadata.title || 'Unknown Document',
                    content: doc.page_content || "", 
                    score: metadata.score, // Pass along the score
                    // Include any other metadata if needed by the Gemini prompt construction
                    // e.g., page_chunk_info: metadata.page_chunk_info || metadata.chunk_index,
                    // e.g., qdrant_id: metadata.qdrant_id 
                };
            });
            
            console.log(`  Adapted ${adaptedDocs.length} documents for Node.js service.`);
            return adaptedDocs; 

        } else {
             console.warn(`Python RAG service /search returned unexpected data structure:`, response.data);
             // Throw an error or return empty if the structure is critical
             throw new Error("Received unexpected data structure from RAG search service.");
        }
    } catch (error) {
        const errorStatus = error.response?.status;
        const errorData = error.response?.data;
        let errorMsg = "Unknown RAG search error";

        if (errorData) {
            if (typeof errorData === 'string' && errorData.toLowerCase().includes("<!doctype html>")) {
                errorMsg = `HTML error page received from Python RAG service (Status: ${errorStatus}). URL (${searchUrl}) might be incorrect or Python service has an issue.`;
                console.error(`Error querying Python RAG service: Received HTML error page. URL: ${searchUrl}, Status: ${errorStatus}`);
            } else {
                errorMsg = errorData?.error || error.message || "Error response from RAG service had no specific message.";
                console.error(`Error querying Python RAG service at ${searchUrl}. Status: ${errorStatus}, Python Error: ${errorMsg}`, errorData);
            }
        } else if (error.request) {
            errorMsg = `No response received from Python RAG service at ${searchUrl}. It might be down or unreachable.`;
            console.error(`Error querying Python RAG service at ${searchUrl}: No response received. ${error.message}`);
        } else {
            errorMsg = error.message;
            console.error(`Error setting up or sending request to Python RAG service at ${searchUrl}: ${error.message}`);
        }
        throw new Error(`RAG Search Failed: ${errorMsg}`); // Propagate error
    }
}

// --- @route   POST /api/chat/rag ---
router.post('/rag', tempAuth, async (req, res) => {
    const { message, filter } = req.body; 
    const userId = req.user._id.toString(); 

    if (!message || typeof message !== 'string' || message.trim() === '') {
        return res.status(400).json({ message: 'Query message text required.' });
    }

    console.log(`>>> POST /api/chat/rag: User=${userId}. Query: "${message.substring(0,50)}..."`);

    try {
        const kValue = parseInt(process.env.RAG_DEFAULT_K) || 5; 
        const clientFilter = filter && typeof filter === 'object' ? filter : null; 
        
        const relevantDocs = await queryPythonRagService(userId, message.trim(), kValue, clientFilter); 
        
        console.log(`<<< POST /api/chat/rag successful for User ${userId}. Found ${relevantDocs.length} docs.`);
        res.status(200).json({ relevantDocs }); // `relevantDocs` is now an array of { documentName, content, score }

    } catch (error) { 
        console.error(`!!! Error processing RAG query for User ${userId}:`, error.message);
        res.status(500).json({ message: error.message || "Failed to retrieve relevant documents." });
    }
});

// --- @route   POST /api/chat/message ---
// (This route should now work correctly with the adapted `relevantDocs` structure)
router.post('/message', tempAuth, async (req, res) => {
    const { message, history, sessionId, systemPrompt, isRagEnabled, relevantDocs } = req.body;
    const userId = req.user._id.toString(); 

    if (!message || typeof message !== 'string' || message.trim() === '') return res.status(400).json({ message: 'Message text required.' });
    if (!sessionId || typeof sessionId !== 'string') return res.status(400).json({ message: 'Session ID required.' });
    if (!Array.isArray(history)) return res.status(400).json({ message: 'Invalid history format.'});
    const useRAG = !!isRagEnabled; 

    console.log(`>>> POST /api/chat/message: User=${userId}, Session=${sessionId}, RAG=${useRAG}. Query: "${message.substring(0,50)}..."`);

    let contextString = "";
    let citationHints = []; 

    try {
        // `relevantDocs` is now an array of {documentName, content, score} from queryPythonRagService
        if (useRAG && Array.isArray(relevantDocs) && relevantDocs.length > 0) {
            console.log(`   RAG Enabled: Processing ${relevantDocs.length} relevant documents provided by client.`);
            contextString = "Answer the user's question based primarily on the following context documents.\nIf the context documents do not contain the necessary information to answer the question fully, clearly state what information is missing from the context *before* potentially providing an answer based on your general knowledge.\n\n--- Context Documents ---\n";
            
            relevantDocs.forEach((doc, index) => {
                if (!doc || typeof doc.documentName !== 'string' || typeof doc.content !== 'string') {
                    console.warn("   Skipping invalid/incomplete document in relevantDocs (missing 'documentName' or 'content'):", doc);
                    return; 
                }
                const docName = doc.documentName;
                const scoreDisplay = doc.score !== undefined ? `(Rel. Score: ${doc.score.toFixed(4)})` : ''; 
                const fullContent = doc.content; 

                contextString += `\n[${index + 1}] Source: ${docName} ${scoreDisplay}\nContent:\n${fullContent}\n---\n`;
                citationHints.push(`[${index + 1}] ${docName}`);
            });
            contextString += "\n--- End of Context ---\n\n";
            console.log(`   Constructed context string. ${citationHints.length} valid docs used.`);
        } else {
            console.log(`   RAG Disabled or no relevant documents provided by client.`);
        }

        const historyForGeminiAPI = history.map(msg => ({
             role: msg.role,
             parts: msg.parts.map(part => ({ text: part.text || '' }))
        })).filter(msg => msg && msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string');

        let finalUserQueryText = "";
        if (contextString) { 
            const citationInstruction = `When referencing information ONLY from the context documents provided above, please cite the source using the format [Number] Document Name (e.g., ${citationHints.slice(0, Math.min(3, citationHints.length)).join(', ')}).`;
            finalUserQueryText = `CONTEXT:\n${contextString}\nINSTRUCTIONS: ${citationInstruction}\n\nUSER QUESTION: ${message.trim()}`;
        } else {
            finalUserQueryText = message.trim();
        }

        const finalHistoryForGemini = [
            ...historyForGeminiAPI,
            { role: "user", parts: [{ text: finalUserQueryText }] }
        ];

        console.log(`   Calling Gemini API. History length for Gemini: ${finalHistoryForGemini.length}. System Prompt: ${!!systemPrompt}`);

        const geminiResponseText = await generateContentWithHistory(finalHistoryForGemini, systemPrompt);

        const modelResponseMessage = {
            role: 'model',
            parts: [{ text: geminiResponseText }],
            timestamp: new Date()
        };

        console.log(`<<< POST /api/chat/message successful for session ${sessionId}.`);
        res.status(200).json({ reply: modelResponseMessage });

    } catch (error) {
        console.error(`!!! Error processing chat message for session ${sessionId}:`, error);
        let statusCode = error.status || error.response?.status || 500;
        let clientMessage = error.message || error.response?.data?.message || "Failed to get response from AI service.";
        
        if (statusCode === 500 && !error.response?.data?.message) { 
            clientMessage = "An internal server error occurred while processing the AI response.";
        }
        res.status(statusCode).json({ message: clientMessage });
    }
});

// --- @route POST /api/chat/history --- (Keep your existing implementation)
router.post('/history', tempAuth, async (req, res) => {
    const { sessionId, messages } = req.body;
    const userId = req.user._id; 
    if (!sessionId) return res.status(400).json({ message: 'Session ID required to save history.' });
    if (!Array.isArray(messages)) return res.status(400).json({ message: 'Invalid messages format.' });

    console.log(`>>> POST /api/chat/history: User=${userId}, Session=${sessionId}, Messages=${messages.length}`);

    try {
        const validMessages = messages.filter(m =>
            m && typeof m.role === 'string' &&
            Array.isArray(m.parts) && m.parts.length > 0 &&
            typeof m.parts[0].text === 'string' &&
            m.timestamp 
        ).map(m => ({ 
            role: m.role,
            parts: [{ text: m.parts[0].text }], 
            timestamp: new Date(m.timestamp) 
        }));

        if (validMessages.length !== messages.length) {
             console.warn(`Session ${sessionId}: Filtered out ${messages.length - validMessages.length} invalid messages during save attempt.`);
        }
        if (validMessages.length === 0 && messages.length > 0) { 
            console.warn(`Session ${sessionId}: All ${messages.length} messages were invalid. No history saved.`);
            const newSessionIdForClient = uuidv4();
            return res.status(200).json({
                message: 'No valid messages to save. Chat not saved. New session ID provided.',
                savedSessionId: null,
                newSessionId: newSessionIdForClient
            });
        }
        if (validMessages.length === 0 && messages.length === 0) { 
             console.log(`Session ${sessionId}: No messages provided to save. Generating new session ID for client.`);
             const newSessionIdForClient = uuidv4();
             return res.status(200).json({
                 message: 'No history provided to save. New session ID for client.',
                 savedSessionId: null,
                 newSessionId: newSessionIdForClient
             });
        }

        const savedHistory = await ChatHistory.findOneAndUpdate(
            { sessionId: sessionId, userId: userId },
            { $set: { userId: userId, sessionId: sessionId, messages: validMessages, updatedAt: Date.now() } },
            { new: true, upsert: true, setDefaultsOnInsert: true }
        );
        const newClientSessionId = uuidv4(); 
        console.log(`<<< POST /api/chat/history: History saved for session ${savedHistory.sessionId}. New client session ID: ${newClientSessionId}`);
        res.status(200).json({
            message: 'Chat history saved successfully.',
            savedSessionId: savedHistory.sessionId,
            newSessionId: newClientSessionId 
        });
    } catch (error) {
        console.error(`!!! Error saving chat history for session ${sessionId}:`, error);
        if (error.name === 'ValidationError') return res.status(400).json({ message: "Validation Error saving history: " + error.message });
        if (error.code === 11000) return res.status(409).json({ message: "Conflict: Session ID might already exist unexpectedly." });
        res.status(500).json({ message: 'Failed to save chat history due to a server error.' });
    }
});

// --- @route GET /api/chat/sessions --- (Keep your existing implementation)
router.get('/sessions', tempAuth, async (req, res) => {
    const userId = req.user._id;
    console.log(`>>> GET /api/chat/sessions: User=${userId}`);
    try {
        const sessions = await ChatHistory.find({ userId: userId })
            .sort({ updatedAt: -1 }) 
            .select('sessionId createdAt updatedAt messages') 
            .lean(); 

        const sessionSummaries = sessions.map(session => {
             const firstUserMessage = session.messages?.find(m => m.role === 'user');
             let preview = 'Chat Session'; 
             if (firstUserMessage?.parts?.[0]?.text) {
                 preview = firstUserMessage.parts[0].text.substring(0, 75);
                 if (firstUserMessage.parts[0].text.length > 75) {
                     preview += '...';
                 }
             }
             return {
                 sessionId: session.sessionId,
                 createdAt: session.createdAt,
                 updatedAt: session.updatedAt,
                 messageCount: session.messages?.length || 0,
                 preview: preview
             };
        });
        console.log(`<<< GET /api/chat/sessions: Found ${sessionSummaries.length} sessions for User ${userId}.`);
        res.status(200).json(sessionSummaries);
    } catch (error) {
        console.error(`!!! Error fetching chat sessions for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat sessions.' });
    }
});

// --- @route GET /api/chat/session/:sessionId --- (Keep your existing implementation)
router.get('/session/:sessionId', tempAuth, async (req, res) => {
    const userId = req.user._id;
    const { sessionId } = req.params;
    console.log(`>>> GET /api/chat/session/${sessionId}: User=${userId}`);
    if (!sessionId) return res.status(400).json({ message: 'Session ID parameter is required.' });
    try {
        const session = await ChatHistory.findOne({ sessionId: sessionId, userId: userId }).lean();
        if (!session) {
            console.log(`--- GET /api/chat/session/${sessionId}: Session not found for User ${userId}.`);
            return res.status(404).json({ message: 'Chat session not found or access denied.' });
        }
        console.log(`<<< GET /api/chat/session/${sessionId}: Session found for User ${userId}.`);
        res.status(200).json(session);
    } catch (error) {
        console.error(`!!! Error fetching chat session ${sessionId} for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat session details.' });
    }
});

module.exports = router;


routes/files.js

javascript
// server/routes/files.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const { tempAuth } = require('../middleware/authMiddleware');

const router = express.Router();

const ASSETS_DIR = path.join(__dirname, '..', 'assets');
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets');

// --- Helper functions ---
const sanitizeUsernameForDir = (username) => {
    if (!username) return '';
    return username.replace(/[^a-zA-Z0-9_-]/g, '_');
};
const parseServerFilename = (filename) => {
    const match = filename.match(/^(\d+)-(.*?)(\.\w+)$/);
    if (match && match.length === 4) {
        return { timestamp: match[1], originalName: `${match[2]}${match[3]}`, extension: match[3] };
    }
     // Handle cases where the original name might not have an extension or parsing fails
    const ext = path.extname(filename);
    const base = filename.substring(0, filename.length - ext.length);
    const tsMatch = base.match(/^(\d+)-(.*)$/);
    if (tsMatch) {
        return { timestamp: tsMatch[1], originalName: `${tsMatch[2]}${ext}`, extension: ext };
    }
    // Fallback if no timestamp prefix found (less ideal)
    return { timestamp: null, originalName: filename, extension: path.extname(filename) };
};
const ensureDirExists = async (dirPath) => {
    try { await fs.mkdir(dirPath, { recursive: true }); }
    catch (error) { if (error.code !== 'EEXIST') { console.error(`Error creating dir ${dirPath}:`, error); throw error; } }
};
// --- End Helper Functions ---


// --- @route   GET /api/files ---
// Use tempAuth middleware
router.get('/', tempAuth, async (req, res) => {
    // req.user is guaranteed to exist here because of tempAuth middleware
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username);
    if (!sanitizedUsername) {
        console.warn("GET /api/files: Invalid user identifier after sanitization.");
        return res.status(400).json({ message: 'Invalid user identifier.' });
    }

    const userAssetsDir = path.join(ASSETS_DIR, sanitizedUsername);
    const fileTypes = ['docs', 'images', 'code', 'others'];
    const userFiles = [];

    try {
        // Check if user directory exists
        try { await fs.access(userAssetsDir); }
        catch (e) {
             if (e.code === 'ENOENT') { return res.status(200).json([]); } // No dir, no files
             throw e; // Other error
        }

        // Scan subdirectories
        for (const type of fileTypes) {
            const typeDir = path.join(userAssetsDir, type);
            try {
                const filesInDir = await fs.readdir(typeDir);
                for (const filename of filesInDir) {
                    const filePath = path.join(typeDir, filename);
                    try {
                        const stats = await fs.stat(filePath);
                        if (stats.isFile()) {
                            const parsed = parseServerFilename(filename);
                            userFiles.push({
                                serverFilename: filename, originalName: parsed.originalName, type: type,
                                relativePath: path.join(type, filename).replace(/\\/g, '/'),
                                size: stats.size, lastModified: stats.mtime,
                            });
                        }
                    } catch (statError) { console.warn(`GET /api/files: Stat failed for ${filePath}:`, statError.message); }
                }
            } catch (err) { if (err.code !== 'ENOENT') { console.warn(`GET /api/files: Read failed for ${typeDir}:`, err.message); } }
        }

        userFiles.sort((a, b) => a.originalName.localeCompare(b.originalName));
        res.status(200).json(userFiles);

    } catch (error) {
        console.error(`!!! Error in GET /api/files for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to retrieve file list.' });
    }
});

// --- @route   PATCH /api/files/:serverFilename ---
// Use tempAuth middleware
router.patch('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const { newOriginalName } = req.body;
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username); // req.user set by tempAuth

    // Validations
    if (!sanitizedUsername) return res.status(400).json({ message: 'Invalid user identifier.' });
    if (!serverFilename) return res.status(400).json({ message: 'Server filename parameter is required.' });
    if (!newOriginalName || typeof newOriginalName !== 'string' || newOriginalName.trim() === '') return res.status(400).json({ message: 'New file name is required.' });
    if (newOriginalName.includes('/') || newOriginalName.includes('\\') || newOriginalName.includes('..')) return res.status(400).json({ message: 'New file name contains invalid characters.' });

    try {
        const parsedOld = parseServerFilename(serverFilename);
        if (!parsedOld.timestamp) return res.status(400).json({ message: 'Invalid server filename format (missing timestamp prefix).' });

        // Find current file path
        let currentPath = null; let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsername, type, serverFilename);
            try { await fs.access(potentialPath); currentPath = potentialPath; fileType = type; break; }
            catch (e) { if (e.code !== 'ENOENT') throw e; }
        }
        if (!currentPath) return res.status(404).json({ message: 'File not found or access denied.' });

        // Construct new path
        const newExt = path.extname(newOriginalName) || parsedOld.extension; // Preserve original ext if new one is missing
        const newBaseName = path.basename(newOriginalName, path.extname(newOriginalName)); // Get base name without extension
        const sanitizedNewBase = newBaseName.replace(/[^a-zA-Z0-9._-]/g, '_'); // Sanitize only the base name
        const finalNewOriginalName = `${sanitizedNewBase}${newExt}`; // Reconstruct original name
        const newServerFilename = `${parsedOld.timestamp}-${finalNewOriginalName}`; // Keep timestamp, use sanitized original name
        const newPath = path.join(ASSETS_DIR, sanitizedUsername, fileType, newServerFilename);

        // Perform rename
        await fs.rename(currentPath, newPath);

        res.status(200).json({
            message: 'File renamed successfully!', oldFilename: serverFilename,
            newFilename: newServerFilename, newOriginalName: finalNewOriginalName,
        });

    } catch (error) {
        console.error(`!!! Error in PATCH /api/files/${serverFilename} for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to rename the file.' });
    }
});


// --- @route   DELETE /api/files/:serverFilename ---
// Use tempAuth middleware
router.delete('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username); // req.user set by tempAuth

    // Validations
    if (!sanitizedUsername) return res.status(400).json({ message: 'Invalid user identifier.' });
    if (!serverFilename) return res.status(400).json({ message: 'Server filename parameter is required.' });

    try {
        // Find current path
        let currentPath = null; let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsername, type, serverFilename);
            try { await fs.access(potentialPath); currentPath = potentialPath; fileType = type; break; }
            catch (e) { if (e.code !== 'ENOENT') throw e; }
        }
        if (!currentPath) return res.status(404).json({ message: 'File not found or access denied.' });

        // Determine backup path
        const backupUserDir = path.join(BACKUP_DIR, sanitizedUsername, fileType);
        await ensureDirExists(backupUserDir);
        const backupPath = path.join(backupUserDir, serverFilename);

        // Perform move
        await fs.rename(currentPath, backupPath);

        res.status(200).json({ message: 'File deleted successfully (moved to backup).', filename: serverFilename });

    } catch (error) {
        console.error(`!!! Error in DELETE /api/files/${serverFilename} for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to delete the file.' });
    }
});

module.exports = router;



routes/network.js

javascript
const express = require('express');
const router = express.Router();
const os = require('os');

function getAllIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost by default

    for (const [name, netInterface] of Object.entries(interfaces)) {
        // Skip loopback and potentially virtual interfaces if desired
        if (name.includes('lo') || name.toLowerCase().includes('virtual') || name.toLowerCase().includes('vmnet')) continue;

        for (const addr of netInterface) {
            // Focus on IPv4, non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

router.get('/ip', (req, res) => {
    res.json({
        ips: getAllIPs(),
        // req.ip might be less reliable behind proxies, but can be included
        // currentRequestIp: req.ip
    });
});

module.exports = router;



routes/syllabus.js

javascript
// server/routes/syllabus.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const { tempAuth } = require('../middleware/authMiddleware'); // Protect the route

const router = express.Router();
const SYLLABI_DIR = path.join(__dirname, '..', 'syllabi');

// --- @route   GET /api/syllabus/:subjectId ---
// --- @desc    Get syllabus content for a specific subject ---
// --- @access  Private (requires auth) ---
router.get('/:subjectId', tempAuth, async (req, res) => {
    const { subjectId } = req.params;

    // Basic sanitization: Allow only alphanumeric and underscores
    // Prevents directory traversal (e.g., ../../etc/passwd)
    const sanitizedSubjectId = subjectId.replace(/[^a-zA-Z0-9_]/g, '');

    if (!sanitizedSubjectId || sanitizedSubjectId !== subjectId) {
        console.warn(`Syllabus request rejected due to invalid characters: ${subjectId}`);
        return res.status(400).json({ message: 'Invalid subject identifier format.' });
    }

    const filePath = path.join(SYLLABI_DIR, `${sanitizedSubjectId}.md`);

    try {
        // Check if file exists first (more specific error)
        await fs.access(filePath);

        // Read the file content
        const content = await fs.readFile(filePath, 'utf-8');

        res.status(200).json({ syllabus: content });

    } catch (error) {
        if (error.code === 'ENOENT') {
            console.warn(`Syllabus file not found: ${filePath}`);
            return res.status(404).json({ message: `Syllabus for '${subjectId}' not found.` });
        } else {
            console.error(`Error reading syllabus file ${filePath}:`, error);
            return res.status(500).json({ message: 'Server error retrieving syllabus.' });
        }
    }
});

module.exports = router;



routes/upload.js

javascript
// server/routes/upload.js
const express = require('express');
const multer = require('multer');
const path = require('path');
const fs = require('fs');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const User = require('../models/User'); // Import the User model
const { ANALYSIS_PROMPTS } = require('../config/promptTemplates'); 
const geminiService = require('../services/geminiService');

const router = express.Router();

// --- Constants ---
const UPLOAD_DIR = path.join(__dirname, '..', 'assets');
const MAX_FILE_SIZE = 20 * 1024 * 1024; // 20 MB

// Define allowed types by mimetype and extension (lowercase)
// Mapping mimetype to subfolder name
const allowedMimeTypes = {
    // Documents -> 'docs'
    'application/pdf': 'docs',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docs', // .docx
    'application/msword': 'docs', // .doc (Might be less reliable mimetype)
    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'docs', // .pptx
    'application/vnd.ms-powerpoint': 'docs', // .ppt (Might be less reliable mimetype)
    'text/plain': 'docs', // .txt
    // Code -> 'code'
    'text/x-python': 'code', // .py
    'application/javascript': 'code', // .js
    'text/javascript': 'code', // .js (alternative)
    'text/markdown': 'code', // .md
    'text/html': 'code', // .html
    'application/xml': 'code', // .xml
    'text/xml': 'code', // .xml
    'application/json': 'code', // .json
    'text/csv': 'code', // .csv
    // Images -> 'images'
    'image/jpeg': 'images',
    'image/png': 'images',
    'image/bmp': 'images',
    'image/gif': 'images',
    // Add more specific types if needed, otherwise they fall into 'others'
};
// Define allowed extensions (lowercase) - This is a secondary check
const allowedExtensions = [
    '.pdf', '.docx', '.doc', '.pptx', '.ppt', '.txt',
    '.py', '.js', '.md', '.html', '.xml', '.json', '.csv', '.log', // Added .log
    '.jpg', '.jpeg', '.png', '.bmp', '.gif'
];

// --- Multer Config ---
const storage = multer.diskStorage({
    destination: (req, file, cb) => {
        // tempAuth middleware ensures req.user exists here
        if (!req.user || !req.user.username) {
            // This should ideally not happen if tempAuth works correctly
            console.error("Multer Destination Error: User context missing after auth middleware.");
            return cb(new Error("Authentication error: User context not found."));
        }
        const sanitizedUsername = req.user.username.replace(/[^a-zA-Z0-9_-]/g, '_');
        const fileMimeType = file.mimetype.toLowerCase();

        // Determine subfolder based on mimetype, default to 'others'
        const fileTypeSubfolder = allowedMimeTypes[fileMimeType] || 'others';
        const destinationPath = path.join(UPLOAD_DIR, sanitizedUsername, fileTypeSubfolder);

        // Ensure the destination directory exists (use async for safety)
        fs.mkdir(destinationPath, { recursive: true }, (err) => {
             if (err) {
                 console.error(`Error creating destination path ${destinationPath}:`, err);
                 cb(err);
             } else {
                 cb(null, destinationPath);
             }
         });
    },
    filename: (req, file, cb) => {
        const timestamp = Date.now();
        const fileExt = path.extname(file.originalname).toLowerCase();
        // Sanitize base name: remove extension, replace invalid chars, limit length
        const sanitizedBaseName = path.basename(file.originalname, fileExt)
                                      .replace(/[^a-zA-Z0-9._-]/g, '_') // Allow letters, numbers, dot, underscore, hyphen
                                      .substring(0, 100); // Limit base name length
        const uniqueFilename = `${timestamp}-${sanitizedBaseName}${fileExt}`;
        cb(null, uniqueFilename);
    }
});

const fileFilter = (req, file, cb) => {
    // tempAuth middleware should run before this, ensuring req.user exists
    if (!req.user) {
         console.warn(`Upload Rejected (File Filter): User context missing.`);
         const error = new multer.MulterError('UNAUTHENTICATED'); // Custom code?
         error.message = `User not authenticated.`;
         return cb(error, false);
    }

    const fileExt = path.extname(file.originalname).toLowerCase();
    const mimeType = file.mimetype.toLowerCase();

    // Primary check: Mimetype must be in our known list OR extension must be allowed
    // Secondary check: Extension must be in the allowed list
    const isMimeTypeKnown = !!allowedMimeTypes[mimeType];
    const isExtensionAllowed = allowedExtensions.includes(fileExt);

    // Allow if (MIME type is known OR extension is explicitly allowed) AND extension is in the allowed list
    // This allows known MIME types even if extension isn't listed, and listed extensions even if MIME isn't known (e.g. text/plain for .log)
    // But we always require the extension itself to be in the allowed list for safety.
    // if ((isMimeTypeKnown || isExtensionAllowed) && isExtensionAllowed) {

    // Stricter: Allow only if BOTH mimetype is known AND extension is allowed
    if (isMimeTypeKnown && isExtensionAllowed) {
        cb(null, true); // Accept file
    } else {
        console.warn(`Upload Rejected (File Filter): User='${req.user.username}', File='${file.originalname}', MIME='${mimeType}', Ext='${fileExt}'. MimeKnown=${isMimeTypeKnown}, ExtAllowed=${isExtensionAllowed}`);
        const error = new multer.MulterError('LIMIT_UNEXPECTED_FILE');
        error.message = `Invalid file type or extension. Allowed extensions: ${allowedExtensions.join(', ')}`;
        cb(error, false); // Reject file
    }
};

const upload = multer({
    storage: storage,
    fileFilter: fileFilter,
    limits: { fileSize: MAX_FILE_SIZE }
});
// --- End Multer Config ---


// --- Function to call Python RAG service ---
async function triggerPythonRagProcessing(userId, filePath, originalName) {
    // Read URL from environment variable set during startup
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL;
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL is not set in environment. Cannot trigger processing.");
        // Optionally: Delete the uploaded file if processing can't be triggered?
        // await fs.promises.unlink(filePath).catch(e => console.error(`Failed to delete unprocessed file ${filePath}: ${e}`));
        return { success: false, message: "RAG service URL not configured." }; // Indicate failure
    }
    const addDocumentUrl = `${pythonServiceUrl}/add_document`;
    console.log(`Triggering Python RAG processing for ${originalName} (User: ${userId}) at ${addDocumentUrl}`);
    try {
        // Send absolute path
        const response = await axios.post(addDocumentUrl, {
            user_id: userId,
            file_path: filePath, // Send the absolute path
            original_name: originalName
        }, { timeout: 300000 }); // 5 minute timeout for processing

        console.log(`Python RAG service response for ${originalName}:`, response.data);

        const text = response.data?.raw_text_for_analysis || "";

        // --- DATABASE UPDATE | Filename & text ---
        if (response.data?.status === "added" && originalName && userId) {
            try {
                const newDocumentEntry = {
                    filename: originalName,
                    text: text,
                };

                const updatedUser = await User.findByIdAndUpdate(
                    userId,
                    { $push: { uploadedDocuments: newDocumentEntry } },
                    { new: true, runValidators: true }
                );

                if (!updatedUser) {
                    console.error(`Failed to find user with ID ${userId} to save document info for ${originalName}.`);
                    return {
                        success: false,
                        message: `User not found for saving document metadata. Status: ${response.data?.status}`,
                        Text: text, 
                        Status: response.data?.status
                    };
                }
                console.log(`Successfully saved document info ('${originalName}') and raw text to user ${userId}.`);

            } catch (dbError) {
                console.error(`Database error saving document info for ${originalName} (User: ${userId}):`, dbError);
                return {
                    success: false,
                    message: `DB error saving document metadata: ${dbError.message}. Python processing status: ${response.data?.status}`,
                    Text: text, // Still return text if Python provided it
                    Status: response.data?.status
                };
            }
        } 
        else if (originalName && userId) { // If not saving, log why
            console.warn(`Skipping DB update for ${originalName} (User: ${userId}). HTTP Status: ${response.status}, Python Custom Status: ${response.data?.status}.`);
        } 
        else {
            console.warn(`Skipping DB update due to missing originalName or userId. Python Custom Status: ${response.data?.status}`);
        }

        // --- END DATABASE UPDATE ---


        // Check response.data.status ('added' or 'skipped')
        if (response.data?.status === 'skipped') {
             console.warn(`Python RAG service skipped processing ${originalName}: ${response.data.message}`);
             return { success: true, status: 'skipped', message: response.data.message, text: text};
        } else if (response.data?.status === 'added') {
             return { success: true, status: 'added', message: response.data.message, text: text };
        } else {
             console.warn(`Unexpected response status from Python RAG service for ${originalName}: ${response.data?.status}`);
             return { success: false, message: `Unexpected RAG status: ${response.data?.status}` };
        }

    } catch (error) {
        const errorMsg = error.response?.data?.error || error.message || "Unknown RAG service error";
        console.error(`Error calling Python RAG service for ${originalName}:`, errorMsg);
        // Maybe delete the file if the call fails? Depends on retry logic.
        // await fs.promises.unlink(filePath).catch(e => console.error(`Failed to delete file ${filePath} after RAG call error: ${e}`));
        return { success: false, message: `RAG service call failed: ${errorMsg}` }; // Indicate failure
    }
}
// --- End Function ---


// --- Function to call Generate Analysis
async function triggerAnalysisGeneration(userId, originalName, textForAnalysis) {
    console.log(`Starting analysis generation for document '${originalName}', User ID: ${userId}. Text length: ${textForAnalysis.length}`);

    let allAnalysesSuccessful = true; // Assume success initially
    const analysisResults = {
        faq: null,
        topics: null,
        mindmap: null
    };
    const logCtx = { userId, originalName }; // Context for logging within generateSingleAnalysis

    // Inner helper function to generate a single type of analysis
    async function generateSingleAnalysis(type, promptContent, context) {
        try {
            console.log(`Attempting to generate ${type} for '${context.originalName}' (User: ${context.userId}).`);

            // Prepare history for geminiService.generateContentWithHistory
            // The 'promptContent' (which is the system prompt) will be passed as the second argument.
            const historyForGemini = [
                { role: 'user', parts: [{ text: "Please perform the requested analysis based on the system instruction provided." }] }
            ];

            const generatedText = await geminiService.generateContentWithHistory(
                historyForGemini,
                promptContent // This is passed as systemPromptText to generateContentWithHistory
            );

            if (!generatedText || typeof generatedText !== 'string' || generatedText.trim() === "") {
                console.warn(`Gemini returned empty or invalid content for ${type} for '${context.originalName}'.`);
                allAnalysesSuccessful = false; // Update the outer scope variable
                return `Notice: No content was generated by the AI for ${type}. The input text might have been unsuitable or the AI returned an empty response.`;
            }

            console.log(`${type} generation successful for '${context.originalName}'. Length: ${generatedText.length}`);
            return generatedText.trim();

        } catch (error) {
            console.error(`Error during ${type} generation for '${context.originalName}' (User: ${context.userId}): ${error.message}`);
            allAnalysesSuccessful = false; // Update the outer scope variable
            // Return a user-friendly error message, or a snippet of the technical error
            const errorMessage = error.message || "Unknown error during AI generation.";
            return `Error generating ${type}: ${errorMessage.split('\n')[0].substring(0, 250)}`; // First line of error, truncated
        }
    }

    // 1. Generate FAQs
    console.log(`[Analysis Step 1/3] Preparing FAQ generation for '${originalName}'.`);
    const faqPrompt = ANALYSIS_PROMPTS.faq.getPrompt(textForAnalysis);
    analysisResults.faq = await generateSingleAnalysis('FAQ', faqPrompt, logCtx);
    if (!allAnalysesSuccessful) {
        console.warn(`FAQ generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
        // We continue even if one fails, allAnalysesSuccessful flag will reflect the overall status.
    }

    // 2. Generate Topics
    console.log(`[Analysis Step 2/3] Preparing Topics generation for '${originalName}'.`);
    const topicsPrompt = ANALYSIS_PROMPTS.topics.getPrompt(textForAnalysis);
    analysisResults.topics = await generateSingleAnalysis('Topics', topicsPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.topics.startsWith("Error generating Topics:")) { // Check if this specific step failed
        console.warn(`Topics generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
    }


    // 3. Generate Mindmap
    console.log(`[Analysis Step 3/3] Preparing Mindmap generation for '${originalName}'.`);
    const mindmapPrompt = ANALYSIS_PROMPTS.mindmap.getPrompt(textForAnalysis);
    analysisResults.mindmap = await generateSingleAnalysis('Mindmap', mindmapPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.mindmap.startsWith("Error generating Mindmap:")) { // Check if this specific step failed
        console.warn(`Mindmap generation failed or produced no content for '${originalName}'.`);
    }

    // Log final outcome of the analysis generation process
    if (allAnalysesSuccessful) {
        console.log(`All analyses (FAQ, Topics, Mindmap) appear to have been generated successfully for '${originalName}'.`);
    } else {
        console.warn(`One or more analyses failed or produced no content for '${originalName}'. Review individual results for details.`);
        // Log the specific results for easier debugging
        console.warn(`FAQ Result for '${originalName}': ${analysisResults.faq.substring(0,100)}...`);
        console.warn(`Topics Result for '${originalName}': ${analysisResults.topics.substring(0,100)}...`);
        console.warn(`Mindmap Result for '${originalName}': ${analysisResults.mindmap.substring(0,100)}...`);
    }

    return {
        success: allAnalysesSuccessful,
        results: analysisResults
    };
}
// --- End Analysis Generation Function ---


// --- Modified Upload Route ---
router.post('/', tempAuth, (req, res) => {
    const uploader = upload.single('file');

    uploader(req, res, async function (err) { // <<<< ASYNC HERE IS KEY
        if (!req.user) {
             console.error("Upload handler: User context missing.");
             return res.status(401).json({ message: "Authentication error." });
        }
        const userId = req.user._id.toString();

        if (err) {
            // ... (your multer error handling - this part is fine)
            console.error(`Multer error for user ${req.user.username}:`, err.message);
            if (err instanceof multer.MulterError) { /* ... */ return res.status(400).json({ message: err.message || "File upload failed."}); }
            return res.status(500).json({ message: "Server error during upload prep." });
        }
        if (!req.file) {
            console.warn(`No file received for user ${req.user.username}.`);
            return res.status(400).json({ message: "No file received or file type rejected." });
        }

        const { path: filePath, originalname: originalName, filename: serverFilename } = req.file;
        const absoluteFilePath = path.resolve(filePath);
        console.log(`Upload received: User '${req.user.username}', File: ${serverFilename}, Original: ${originalName}`);

        // --- Main Try-Catch for the entire RAG + Analysis process ---
        try {
            // ----- STAGE 1: MongoDB Pre-check for existing originalName -----
            const userForPreCheck = await User.findById(userId).select('uploadedDocuments');
            if (!userForPreCheck) {
                console.error(`Upload Aborted: User ${userId} not found (pre-check). Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (user not found): ${e}`));
                return res.status(404).json({ message: "User not found, cannot process upload." });
            }
            const existingDocument = userForPreCheck.uploadedDocuments.find(doc => doc.filename === originalName);
            if (existingDocument) {
                console.log(`Upload Halted: '${originalName}' already exists for user ${userId}. Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (duplicate): ${e}`));
                return res.status(409).json({
                    message: `File '${originalName}' already exists. No new processing initiated.`,
                    filename: serverFilename, originalname: originalName,
                });
            }
            console.log(`Pre-check passed for '${originalName}'. Proceeding to RAG.`);
            // ----- END STAGE 1 -----


            // ----- STAGE 2: RAG Processing -----
            const ragResult = await triggerPythonRagProcessing(userId, absoluteFilePath, originalName);

            if (!ragResult.success || !ragResult.text || ragResult.text.trim() === "") {
                let message = `RAG processing failed or returned no text for '${originalName}'.`;
                if (ragResult.message) message += ` Details: ${ragResult.message}`;
                console.error(message + ` (User: ${userId})`);

                // If RAG failed, the file text wasn't added to DB by triggerPythonRagProcessing
                // (assuming triggerPythonRagProcessing only adds to DB on its own success).
                // So, delete the physical file.
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (RAG fail/no text) for ${absoluteFilePath}: ${e}`));

                return res.status(500).json({ // Or 422 if it's a content issue from RAG
                    message: message,
                    filename: serverFilename, originalname: originalName
                });
            }
            console.log(`RAG processing completed with text for '${originalName}'. Proceeding to analysis.`);
            // At this point, triggerPythonRagProcessing should have added the document with text to MongoDB.
            // If it didn't, the logic in triggerPythonRagProcessing needs adjustment.


            // ----- STAGE 3: Analysis Generation -----
            const analysisOutcome = await triggerAnalysisGeneration(userId, originalName, ragResult.text);


            // ----- STAGE 4: Handle Analysis Outcome & DB Update -----
            if (analysisOutcome.success) {
                console.log(`All analyses generated successfully for '${originalName}'. Storing in DB.`);
                await User.updateOne(
                    { _id: userId, "uploadedDocuments.filename": originalName },
                    {
                        $set: {
                            "uploadedDocuments.$.analysis.faq": analysisOutcome.results.faq,
                            "uploadedDocuments.$.analysis.topics": analysisOutcome.results.topics,
                            "uploadedDocuments.$.analysis.mindmap": analysisOutcome.results.mindmap,
                        }
                    }
                );
                console.log(`Successfully stored all analyses for '${originalName}' in MongoDB.`);
                return res.status(200).json({
                    message: "File uploaded and all analyses completed successfully.",
                    filename: serverFilename, originalname: originalName,
                    // analysis: analysisOutcome.results // Optionally send analysis back
                });
            } else {
                console.warn(`One or more analyses failed for '${originalName}'. No analysis data from this attempt will be stored in DB.`);
                // The RAG text is already in the DB. This analysis attempt failed.
                // Optionally update a status for this document in DB to indicate analysis failure
                return res.status(422).json({
                    message: "File uploaded and RAG processing complete, but content analysis generation failed. The document text is saved. You may not need to re-upload but can try to trigger analysis again later if such a feature exists, or contact support.",
                    filename: serverFilename, originalname: originalName,
                    // failedAnalysisDetails: analysisOutcome.results // For client-side debugging if needed
                });
            }
        } catch (processError) {
            // Catch any unhandled errors from awaited promises or synchronous code
            console.error(`!!! Overall processing error for ${originalName} (User: ${userId}):`, processError);
            // Try to clean up the uploaded file if it exists and an error occurred.
            // This is a bit tricky as RAG might have already added to DB.
            // If processError is from RAG or before, file deletion is safer.
            // If it's after RAG success but during analysis or DB update of analysis,
            // the RAG text is already in DB.
            if (absoluteFilePath) {
                 // Consider if an error after RAG success should still delete the physical file.
                 // Usually, if the RAG text is in DB, the physical file on disk is secondary.
                 // But if the entire transaction failed, deleting is fine.
                 await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (overall fail) for ${absoluteFilePath}: ${e}`));
            }
            return res.status(500).json({
                message: `Server error during file processing: ${processError.message}`,
                filename: serverFilename, originalname: originalName
            });
        }
    });
});


module.exports = router;



server.js

javascript
// server/server.js
const express = require('express');
const dotenv = require('dotenv'); // Removed dotenv
const cors = require('cors');
const path = require('path');
const { getLocalIPs } = require('./utils/networkUtils');
const fs = require('fs');
const axios = require('axios');
const os = require('os');
const mongoose = require('mongoose'); // Import mongoose for closing connection
const readline = require('readline').createInterface({ // For prompting
  input: process.stdin,
  output: process.stdout,
});

// --- Custom Modules ---
const connectDB = require('./config/db');
const { performAssetCleanup } = require('./utils/assetCleanup');

// --- Configuration Loading ---
dotenv.config(); // Removed dotenv

// --- Configuration Defaults & Variables ---
const DEFAULT_PORT = 5001;
const DEFAULT_MONGO_URI = 'mongodb://localhost:27017/chatbotGeminiDB'; // Default DB URI
const DEFAULT_PYTHON_RAG_URL = 'http://localhost:5002'; // Default RAG service URL

let port = process.env.PORT || DEFAULT_PORT; // Use environment variable PORT if set, otherwise default
let mongoUri = process.env.MONGO_URI || ''; // Use environment variable if set
let pythonRagUrl = process.env.PYTHON_RAG_SERVICE_URL || ''; // Use environment variable if set
let geminiApiKey = process.env.GEMINI_API_KEY || ''; // MUST be set via environment

// --- Express Application Setup ---
const app = express();

// --- Core Middleware ---
app.use(cors()); // Allows requests from frontend (potentially on different IPs in LAN)
app.use(express.json());

// --- Basic Root Route ---
app.get('/', (req, res) => res.send('Chatbot Backend API is running...'));

// --- API Route Mounting ---
app.use('/api/network', require('./routes/network')); // For IP info
app.use('/api/auth', require('./routes/auth'));
app.use('/api/chat', require('./routes/chat'));
app.use('/api/upload', require('./routes/upload'));
app.use('/api/files', require('./routes/files'));
app.use('/api/syllabus', require('./routes/syllabus')); // <-- ADD THIS LINE
// app.use('/api/analysis', require('./routes/analysis'));
// app.use('/api/kg', require('./routes/kg')); // Knowledge Graph route



// --- Centralized Error Handling Middleware ---
app.use((err, req, res, next) => {
    console.error("Unhandled Error:", err.stack || err);
    const statusCode = err.status || 500;
    let message = err.message || 'An internal server error occurred.';
    // Sanitize potentially sensitive error details in production
    if (process.env.NODE_ENV === 'production' && statusCode === 500) {
        message = 'An internal server error occurred.';
    }
    // Ensure response is JSON for API routes
    if (req.originalUrl.startsWith('/api/')) {
         return res.status(statusCode).json({ message: message });
    }
    // Fallback for non-API routes if any
    res.status(statusCode).send(message);
});

// --- Server Instance Variable ---
let server;

// --- Graceful Shutdown Logic ---
const gracefulShutdown = async (signal) => {
    console.log(`\n${signal} received. Shutting down gracefully...`);
    readline.close(); // Close readline interface
    try {
        // Close HTTP server first to stop accepting new connections
        if (server) {
            server.close(async () => {
                console.log('HTTP server closed.');
                // Close MongoDB connection
                try {
                    await mongoose.connection.close();
                    console.log('MongoDB connection closed.');
                } catch (dbCloseError) {
                    console.error("Error closing MongoDB connection:", dbCloseError);
                }
                process.exit(0); // Exit after server and DB are closed
            });
        } else {
             // If server wasn't assigned, try closing DB and exit
             try {
                 await mongoose.connection.close();
                 console.log('MongoDB connection closed.');
             } catch (dbCloseError) {
                 console.error("Error closing MongoDB connection:", dbCloseError);
             }
            process.exit(0);
        }

        // Force exit after timeout if server.close callback doesn't finish
        setTimeout(() => {
            console.error('Graceful shutdown timed out, forcing exit.');
            process.exit(1);
        }, 10000); // 10 seconds

    } catch (shutdownError) {
        console.error("Error during graceful shutdown initiation:", shutdownError);
        process.exit(1);
    }
};

process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

// --- RAG Service Health Check ---
async function checkRagService(url) {
    console.log(`\nChecking RAG service health at ${url}...`);
    try {
        const response = await axios.get(`${url}/health`, { timeout: 7000 }); // 7 second timeout
        if (response.status === 200 && response.data?.status === 'ok') {
            console.log('âœ“ RAG service is available and healthy.');
            console.log(`  Embedding: ${response.data.embedding_model_type} (${response.data.embedding_model_name})`);
            console.log(`  Default Index Loaded: ${response.data.default_index_loaded}`);
            if (response.data.message && response.data.message.includes("Warning:")) {
                 console.warn(`  RAG Health Warning: ${response.data.message}`);
            }
            return true;
        } else {
             console.warn(`! RAG service responded but status is not OK: ${response.status} - ${JSON.stringify(response.data)}`);
             return false;
        }
    } catch (error) {
        console.warn('! RAG service is not reachable.');
        if (error.code === 'ECONNREFUSED') {
             console.warn(`  Connection refused at ${url}. Ensure the RAG service (server/rag_service/app.py) is running.`);
        } else if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
             console.warn(`  Connection timed out to ${url}. The RAG service might be slow to start or unresponsive.`);
        }
         else {
             console.warn(`  Error: ${error.message}`);
        }
        console.warn('  RAG features (document upload processing, context retrieval) will be unavailable.');
        return false;
    }
}

// --- Directory Structure Check (Simplified) ---
async function ensureServerDirectories() {
    const dirs = [
        path.join(__dirname, 'assets'),
        path.join(__dirname, 'backup_assets'),
        // Add other essential dirs if needed
    ];
    console.log("\nEnsuring server directories exist...");
    try {
        for (const dir of dirs) {
            // Check existence synchronously, create asynchronously
            if (!fs.existsSync(dir)) {
                await fs.promises.mkdir(dir, { recursive: true });
                console.log(`  Created directory: ${dir}`);
            } else {
                // console.log(`  Directory exists: ${dir}`); // Optional: be less verbose
            }
        }
        console.log("âœ“ Server directories checked/created.");
    } catch (error) {
        console.error('!!! Error creating essential server directories:', error);
        throw error; // Prevent server start if essential dirs fail
    }
}

// --- Prompt for Configuration ---
function askQuestion(query) {
    return new Promise(resolve => readline.question(query, resolve));
}

async function configureAndStart() {
    console.log("--- Starting Server Configuration ---");
    
    // 1. Gemini API Key Check
    if (!geminiApiKey) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! FATAL: GEMINI_API_KEY environment variable is not set. !!!");
        console.error("!!! Please set it before running the server:               !!!");
        console.error("!!! export GEMINI_API_KEY='YOUR_API_KEY'                   !!!");
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1);
    } else {
        console.log("âœ“ GEMINI_API_KEY found.");
    }

    // 2. MongoDB URI
    if (!
        mongoUri) {
        const answer = await askQuestion(`Enter MongoDB URI or press Enter for default (${DEFAULT_MONGO_URI}): `);
        mongoUri = answer.trim() || DEFAULT_MONGO_URI;
    }
    console.log(`Using MongoDB URI: ${mongoUri}`);

    // 3. Python RAG Service URL
    if (!pythonRagUrl) {
        const answer = await askQuestion(`Enter Python RAG Service URL or press Enter for default (${DEFAULT_PYTHON_RAG_URL}): `);
        pythonRagUrl = answer.trim() || DEFAULT_PYTHON_RAG_URL;
    }
    console.log(`Using Python RAG Service URL: ${pythonRagUrl}`);

    // 4. Port (Optional override via prompt, primarily uses ENV or default)
    // You could add a prompt here if needed, but ENV variable is common practice
    console.log(`Node.js server will listen on port: ${port}`);

    readline.close(); // Close the prompt interface

    // --- Pass configuration to other modules (if needed) ---
    // We'll make connectDB and services read directly or pass via function calls
    process.env.MONGO_URI = mongoUri; // Set for db.js
    process.env.PYTHON_RAG_SERVICE_URL = pythonRagUrl; // Set for chat.js, upload.js
    // GEMINI_API_KEY is already in process.env

    console.log("--- Configuration Complete ---");

    // --- Proceed with Server Startup ---
    await startServer();
}


// --- Asynchronous Server Startup Function ---
async function startServer() {
    console.log("\n--- Starting Server Initialization ---");
    try {
        await ensureServerDirectories(); // Check/create assets, backup_assets dirs
        await connectDB(mongoUri); // Connect to MongoDB - Pass URI explicitly
        await performAssetCleanup(); // Backup existing assets, create fresh user folders
        await checkRagService(pythonRagUrl); // Check Python RAG service status

        const PORT = port; // Use the configured port
        const availableIPs = getLocalIPs(); // Get all local IPs

        server = app.listen(PORT, '0.0.0.0', () => { // Listen on all interfaces
            console.log('\n=== Node.js Server Ready ===');
            console.log(`ðŸš€ Server listening on port ${PORT}`);
            console.log('   Access the application via these URLs (using common frontend ports):');
            const frontendPorts = [3000, 3001, 8080, 5173]; // Common React/Vite ports
            availableIPs.forEach(ip => {
                 frontendPorts.forEach(fp => {
                    console.log(`   - http://${ip}:${fp} (Frontend) -> Connects to Backend at http://${ip}:${PORT}`);
                 });
            });
            console.log('============================\n');
            console.log("ðŸ’¡ Hint: Client automatically detects backend IP based on how you access the frontend.");
            console.log(`   Ensure firewalls allow connections on port ${PORT} (Backend) and your frontend port.`);
            console.log("--- Server Initialization Complete ---");
        });

    } catch (error) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! Failed to start Node.js server:", error.message);
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1); // Exit if initialization fails
    }
}

// --- Execute Configuration and Server Start ---
configureAndStart();



services/geminiService.js

javascript
// server/services/geminiService.js
const { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } = require('@google/generative-ai');
// require('dotenv').config(); // Removed dotenv

// Read API Key directly from environment variables
const API_KEY = process.env.GEMINI_API_KEY;
const MODEL_NAME = "gemini-1.5-flash"; // Or read from env: process.env.GEMINI_MODEL_NAME || "gemini-1.5-flash";

if (!API_KEY) {
    // This check is now primarily done in server.js before starting
    // But keep a safeguard here.
    console.error("FATAL ERROR: GEMINI_API_KEY is not available in the environment. Server should have exited.");
    // Throw an error instead of exiting here, let the caller handle it
    throw new Error("GEMINI_API_KEY is missing.");
}

const genAI = new GoogleGenerativeAI(API_KEY);

const baseGenerationConfig = {
    temperature: 0.7, // Moderate temperature for creative but grounded responses
    maxOutputTokens: 4096, // Adjust as needed, Flash model limit might be higher
    // topP: 0.9, // Example: Could add nucleus sampling
    // topK: 40,  // Example: Could add top-k sampling
};

// Stricter safety settings - adjust as needed for your use case
const baseSafetySettings = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
];

const generateContentWithHistory = async (chatHistory, systemPromptText = null, relevantDocs = []) => {
    try {
        if (!Array.isArray(chatHistory) || chatHistory.length === 0) {
             throw new Error("Chat history must be a non-empty array.");
        }
        // Gemini API requires history to end with a 'user' message for sendMessage
        if (chatHistory[chatHistory.length - 1].role !== 'user') {
            console.error("History for Gemini API must end with a 'user' role message.");
            // Attempt to fix by removing trailing non-user messages if any? Risky.
            // Or just throw error.
            throw new Error("Internal error: Invalid chat history sequence for API call.");
        }

        // --- Prepare Model Options ---
        const modelOptions = {
            model: MODEL_NAME,
            generationConfig: baseGenerationConfig,
            safetySettings: baseSafetySettings,
            // Add system instruction if provided
            ...(systemPromptText && typeof systemPromptText === 'string' && systemPromptText.trim() !== '' && {
                systemInstruction: {
                    // Gemini expects system instruction parts as an array
                    parts: [{ text: systemPromptText.trim() }]
                }
             })
        };
        const model = genAI.getGenerativeModel(modelOptions);


        // --- Prepare History for startChat ---
        // History for startChat should NOT include the latest user message
        const historyForStartChat = chatHistory.slice(0, -1)
            .map(msg => ({ // Ensure correct format
                 role: msg.role,
                 parts: msg.parts.map(part => ({ text: part.text || '' }))
            }))
            .filter(msg => msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string'); // Basic validation

        // --- Start Chat Session ---
        const chat = model.startChat({
            history: historyForStartChat,
        });

        // --- Prepare the message to send ---
        // Get the text from the last user message in the original history
        let lastUserMessageText = chatHistory[chatHistory.length - 1].parts[0].text;

        // Optional: Add a subtle hint for citation if RAG was used (Gemini might pick it up)
        // if (relevantDocs.length > 0) {
        //     const citationHint = ` (Remember to cite sources like ${relevantDocs.map((doc, i) => `[${i+1}] ${doc.documentName}`).slice(0,2).join(', ')} if applicable)`;
        //     lastUserMessageText += citationHint;
        // }

        console.log(`Sending message to Gemini. History length sent to startChat: ${historyForStartChat.length}. System Prompt Used: ${!!modelOptions.systemInstruction}`);
        // console.log("Last User Message Text Sent:", lastUserMessageText.substring(0, 200) + "..."); // Log truncated message

        // --- Send Message ---
        const result = await chat.sendMessage(lastUserMessageText);

        // --- Process Response ---
        const response = result.response;
        const candidate = response?.candidates?.[0];

        // --- Validate Response ---
        if (!candidate || candidate.finishReason === 'STOP' || candidate.finishReason === 'MAX_TOKENS') {
            // Normal completion or max tokens reached
            const responseText = candidate?.content?.parts?.[0]?.text;
            if (typeof responseText === 'string') {
                return responseText; // Success
            } else {
                 console.warn("Gemini response finished normally but text content is missing or invalid.", { finishReason: candidate?.finishReason, content: candidate?.content });
                 throw new Error("Received an empty or invalid response from the AI service.");
            }
        } else {
             // Handle blocked responses or other issues
             const finishReason = candidate?.finishReason || 'Unknown';
             const safetyRatings = candidate?.safetyRatings;
             console.warn("Gemini response was potentially blocked or had issues.", { finishReason, safetyRatings });

             let blockMessage = `AI response generation failed or was blocked.`;
             if (finishReason) blockMessage += ` Reason: ${finishReason}.`;
             if (safetyRatings) {
                const blockedCategories = safetyRatings.filter(r => r.blocked).map(r => r.category).join(', ');
                if (blockedCategories) {
                    blockMessage += ` Blocked Categories: ${blockedCategories}.`;
                }
             }

             const error = new Error(blockMessage);
             error.status = 400; // Treat as a bad request or policy issue
             throw error;
        }

    } catch (error) {
        console.error("Gemini API Call Error:", error?.message || error);
        // Improve error message for client
        let clientMessage = "Failed to get response from AI service.";
        if (error.message?.includes("API key not valid")) {
            clientMessage = "AI Service Error: Invalid API Key.";
        } else if (error.message?.includes("blocked")) {
            clientMessage = error.message; // Use the specific block message
        } else if (error.status === 400) {
             clientMessage = `AI Service Error: ${error.message}`;
        }

        const enhancedError = new Error(clientMessage);
        enhancedError.status = error.status || 500; // Keep original status if available
        enhancedError.originalError = error; // Attach original error if needed for server logs
        throw enhancedError;
    }
};

module.exports = { generateContentWithHistory };



utils/assetCleanup.js

javascript
const fs = require('fs').promises; // Use fs.promises for async operations
const path = require('path');

// Define constants relative to this file's location (server/utils)
const ASSETS_DIR = path.join(__dirname, '..', 'assets'); // Go up one level to server/assets
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets'); // Go up one level to server/backup_assets
const FOLDER_TYPES = ['docs', 'images', 'code', 'others']; // Folders within each user's asset dir

/**
 * Moves existing user asset folders (docs, images, code, others) to a timestamped
 * backup location and recreates empty asset folders for each user on server startup.
 */
async function performAssetCleanup() {
    console.log("\n--- Starting Asset Cleanup ---");
    try {
        // Ensure backup base directory exists
        await fs.mkdir(BACKUP_DIR, { recursive: true });

        // List potential user directories in assets
        let userDirs = [];
        try {
            userDirs = await fs.readdir(ASSETS_DIR);
        } catch (err) {
            if (err.code === 'ENOENT') {
                console.log("Assets directory doesn't exist yet, creating it and skipping cleanup.");
                await fs.mkdir(ASSETS_DIR, { recursive: true }); // Ensure assets dir exists
                console.log("--- Finished Asset Cleanup (No existing assets found) ---");
                return; // Nothing to clean up
            }
            throw err; // Re-throw other errors accessing assets dir
        }

        if (userDirs.length === 0) {
             console.log("Assets directory is empty. Skipping backup/move operations.");
             console.log("--- Finished Asset Cleanup (No user assets found) ---");
             return;
        }

        const timestamp = new Date().toISOString().replace(/[:.]/g, '-'); // Create a safe timestamp string

        for (const userName of userDirs) {
            const userAssetPath = path.join(ASSETS_DIR, userName);
            const userBackupPathBase = path.join(BACKUP_DIR, userName);
            const userTimestampBackupPath = path.join(userBackupPathBase, `backup_${timestamp}`);

            try {
                // Check if the item in assets is actually a directory
                const stats = await fs.stat(userAssetPath);
                if (!stats.isDirectory()) {
                    console.log(`  Skipping non-directory item in assets: ${userName}`);
                    continue;
                }

                console.log(`  Processing assets for user: [${userName}]`);
                let backupDirCreated = false; // Track if backup dir was created for this user/run
                let movedSomething = false; // Track if anything was actually moved

                // Process each defined folder type (docs, images, etc.)
                for (const type of FOLDER_TYPES) {
                    const sourceTypePath = path.join(userAssetPath, type);
                    try {
                        // Check if the source type directory exists before trying to move
                        await fs.access(sourceTypePath);

                        // If source exists, ensure the timestamped backup directory is ready
                        if (!backupDirCreated) {
                            await fs.mkdir(userTimestampBackupPath, { recursive: true });
                            backupDirCreated = true;
                            // console.log(`    Created backup directory: ${userTimestampBackupPath}`);
                        }

                        // Define the destination path in the backup folder
                        const backupTypePath = path.join(userTimestampBackupPath, type);
                        // console.log(`    Moving ${sourceTypePath} to ${backupTypePath}`);
                        // Move the existing type folder to the backup location
                        await fs.rename(sourceTypePath, backupTypePath);
                        movedSomething = true;

                    } catch (accessErr) {
                        // Ignore error if the source directory doesn't exist (ENOENT)
                        if (accessErr.code !== 'ENOENT') {
                            console.error(`    Error accessing source folder ${sourceTypePath}:`, accessErr.message);
                        }
                        // If ENOENT, the folder doesn't exist, nothing to move.
                    }

                    // Always ensure the empty type directory exists in the main assets folder
                    try {
                        // console.log(`    Ensuring empty directory: ${sourceTypePath}`);
                        await fs.mkdir(sourceTypePath, { recursive: true });
                    } catch (mkdirErr) {
                         console.error(`    Failed to recreate directory ${sourceTypePath}:`, mkdirErr.message);
                    }
                } // End loop through FOLDER_TYPES

                 if (movedSomething) {
                     console.log(`  Finished backup for user [${userName}] to backup_${timestamp}`);
                 } else {
                     console.log(`  No existing asset types found to backup for user [${userName}]`);
                 }


            } catch (userDirStatErr) {
                 // Error checking if the item in assets is a directory
                 console.error(`Error processing potential user asset directory ${userAssetPath}:`, userDirStatErr.message);
            }
        } // End loop through userDirs

        console.log("--- Finished Asset Cleanup ---");

    } catch (error) {
        // Catch errors related to backup dir creation or reading the main assets dir
        console.error("!!! Critical Error during Asset Cleanup process:", error);
    }
}

// Export the function to be used elsewhere
module.exports = { performAssetCleanup };



utils/networkUtils.js

javascript
const os = require('os');

function getLocalIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost

    for (const iface of Object.values(interfaces)) {
        for (const addr of iface) {
            // Include IPv4 non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

function getPreferredLocalIP() {
    const ips = getLocalIPs();
    // Prioritize non-localhost, non-link-local (169.254) IPs
    // Often 192.168.* or 10.* or 172.16-31.* are common private ranges
    return ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost' && (ip.startsWith('192.168.') || ip.startsWith('10.') || ip.match(/^172\.(1[6-9]|2[0-9]|3[0-1])\./))) ||
           ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost') || // Any other non-link-local
           'localhost'; // Fallback
}

module.exports = { getLocalIPs, getPreferredLocalIP };



