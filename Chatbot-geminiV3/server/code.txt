.env


PORT=5001 # Port for the backend (make sure it's free)
MONGO_URI = "mongodb://localhost:27017/chatbot_gemini" # Your MongoDB connection string
JWT_SECRET = "your_super_strong_and_secret_jwt_key_12345" # A strong, random secret key for JWT
GEMINI_API_KEY = "AIzaSyBiCYeCICXAuEEqVvwwaiOpO9gTg6mJLzw" # Your actual Gemini API Key



code.txt





config/db.js

javascript
const mongoose = require('mongoose');
// const dotenv = require('dotenv'); // Removed dotenv

// dotenv.config(); // Removed dotenv

// Modified connectDB to accept the URI as an argument
const connectDB = async (mongoUri) => {
  if (!mongoUri) {
      console.error('MongoDB Connection Error: URI is missing.');
      process.exit(1);
  }
  try {
    // console.log(`Attempting MongoDB connection to: ${mongoUri}`); // Debug: Careful logging URI
    const conn = await mongoose.connect(mongoUri, {
      // Mongoose 6+ uses these defaults, so they are not needed
      // useNewUrlParser: true,
      // useUnifiedTopology: true,
      // serverSelectionTimeoutMS: 5000 // Example: Optional: Timeout faster
    });

    console.log(`âœ“ MongoDB Connected Successfully`); // Simpler success message
    return conn; // Return connection object if needed elsewhere
  } catch (error) {
    console.error('MongoDB Connection Error:', error.message);
    // Exit process with failure
    process.exit(1);
  }
};

module.exports = connectDB;



config/promptTemplates.js

javascript
// server/config/promptTemplates.js

const ANALYSIS_THINKING_PREFIX_TEMPLATE = `**STEP 1: THINKING PROCESS (Recommended):**
*   Before generating the analysis, briefly outline your plan in \`<thinking>\` tags. Example: \`<thinking>Analyzing for FAQs. Will scan for key questions and answers presented in the text.</thinking>\`
*   If you include thinking, place the final analysis *after* the \`</thinking>\` tag.

**STEP 2: ANALYSIS OUTPUT:**
*   Generate the requested analysis based **strictly** on the text provided below.
*   Follow the specific OUTPUT FORMAT instructions carefully.

--- START DOCUMENT TEXT ---
{doc_text_for_llm}
--- END DOCUMENT TEXT ---
`; // Note: Escaped backticks if your template string itself uses them inside.

const ANALYSIS_PROMPTS = {
    faq: {
        // No need for PromptTemplate class here, just the string parts
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate 5-7 Frequently Asked Questions (FAQs) with concise answers based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first FAQ (after thinking, if used). Do **NOT** include preamble.
*   Format each FAQ as:
    Q: [Question derived ONLY from the text]
    A: [Answer derived ONLY from the text, concise]
*   If the text doesn't support an answer, don't invent one. Use Markdown for formatting if appropriate (e.g., lists within an answer).

**BEGIN OUTPUT (Start with 'Q:' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    topics: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Identify the 5-8 most important topics discussed. Provide a 1-2 sentence explanation per topic based ONLY on the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the first topic (after thinking, if used). Do **NOT** include preamble.
*   Format as a Markdown bulleted list:
    *   **Topic Name:** Brief explanation derived ONLY from the text content (1-2 sentences max).

**BEGIN OUTPUT (Start with '*   **' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    },
    mindmap: {
        getPrompt: (docTextForLlm) => {
            let baseTemplate = ANALYSIS_THINKING_PREFIX_TEMPLATE.replace('{doc_text_for_llm}', docTextForLlm);
            baseTemplate += `
**TASK:** Generate a mind map outline in Markdown list format representing key concepts and hierarchy ONLY from the text.

**OUTPUT FORMAT (Strict):**
*   Start directly with the main topic as the top-level item (using '-') (after thinking, if used). Do **NOT** include preamble.
*   Use nested Markdown lists ('-' or '*') with indentation (2 or 4 spaces) for hierarchy.
*   Focus **strictly** on concepts and relationships mentioned in the text. Be concise.

**BEGIN OUTPUT (Start with e.g., '- Main Topic' or \`<thinking>\`):**
`;
            return baseTemplate;
        }
    }
};

module.exports = { ANALYSIS_PROMPTS };


faiss_indices/sample.txt





middleware/authMiddleware.js

javascript
// server/middleware/authMiddleware.js
const User = require('../models/User');

// TEMPORARY Authentication Middleware (INSECURE - for debugging only)
// Checks for 'X-User-ID' header and attaches user to req.user
const tempAuth = async (req, res, next) => {
    const userId = req.headers['x-user-id']; // Read custom header (lowercase)

    // console.log("TempAuth Middleware: Checking for X-User-ID:", userId); // Debug log

    if (!userId) {
        console.warn("TempAuth Middleware: Missing X-User-ID header.");
        // Send 401 immediately if header is missing
        return res.status(401).json({ message: 'Unauthorized: Missing User ID header' });
    }

    try {
        // Find user by the ID provided in the header
        // Ensure Mongoose is connected before this runs (handled by server.js)
        const user = await User.findById(userId).select('-password'); // Exclude password

        if (!user) {
            console.warn(`TempAuth Middleware: User not found for ID: ${userId}`);
            // Send 401 if user ID is provided but not found in DB
            return res.status(401).json({ message: 'Unauthorized: User not found' });
        }

        // Attach user object to the request
        req.user = user;
        // console.log("TempAuth Middleware: User attached:", req.user.username); // Debug log
        next(); // Proceed to the next middleware or route handler

    } catch (error) {
        console.error('TempAuth Middleware: Error fetching user:', error);
        // Handle potential invalid ObjectId format errors
        if (error.name === 'CastError' && error.kind === 'ObjectId') {
             return res.status(400).json({ message: 'Bad Request: Invalid User ID format' });
        }
        // Send 500 for other unexpected errors during auth check
        res.status(500).json({ message: 'Server error during temporary authentication' });
    }
};

// Export the temporary middleware
module.exports = { tempAuth };



models/ChatHistory.js

javascript
const mongoose = require('mongoose');

const MessageSchema = new mongoose.Schema({
    role: {
        type: String,
        enum: ['user', 'model'], // Gemini roles
        required: true
    },
    parts: [{
        text: {
            type: String,
            required: true
        }
        // _id: false // Mongoose adds _id by default, can disable if truly not needed per part
    }],
    timestamp: {
        type: Date,
        default: Date.now
    }
}, { _id: false }); // Don't create separate _id for each message object in the array

const ChatHistorySchema = new mongoose.Schema({
    userId: {
        type: mongoose.Schema.Types.ObjectId,
        ref: 'User',
        required: true,
        index: true,
    },
    sessionId: {
        type: String,
        required: true,
        unique: true,
        index: true,
    },
    messages: [MessageSchema], // Array of message objects
    createdAt: {
        type: Date,
        default: Date.now,
    },
    updatedAt: {
        type: Date,
        default: Date.now,
    }
});

// Update `updatedAt` timestamp before saving any changes
ChatHistorySchema.pre('save', function (next) {
    if (this.isModified()) { // Only update if document changed
      this.updatedAt = Date.now();
    }
    next();
});

// Also update `updatedAt` on findOneAndUpdate operations if messages are modified
ChatHistorySchema.pre('findOneAndUpdate', function(next) {
  this.set({ updatedAt: new Date() });
  next();
});


const ChatHistory = mongoose.model('ChatHistory', ChatHistorySchema);

module.exports = ChatHistory;



models/User.js

javascript
const mongoose = require('mongoose');
const bcrypt = require('bcryptjs');

const UserSchema = new mongoose.Schema({
  username: {
    type: String,
    required: [true, 'Please provide a username'],
    unique: true,
    trim: true,
  },
  password: {
    type: String,
    required: [true, 'Please provide a password'],
    minlength: 6,
    select: false, // Explicitly prevent password from being returned by default
  },
  uploadedDocuments: [
    {
      filename: {
        type: String,
      },
      text: {
        type: String,
        default: "",
      },
      analysis: {
        faq: {
          type: String,
          default: "",
        },
        topics: {
          type: String,
          default: "",
        },
        mindmap: {
          type: String,
          default: "",
        },
      },
    },
  ],
  createdAt: {
    type: Date,
    default: Date.now,
  },
});

// Password hashing middleware before saving
UserSchema.pre('save', async function (next) {
  // Only hash the password if it has been modified (or is new)
  if (!this.isModified('password')) {
    return next();
  }
  try {
    const salt = await bcrypt.genSalt(10);
    this.password = await bcrypt.hash(this.password, salt);
    next();
  } catch (err) {
    next(err);
  }
});

// Method to compare entered password with hashed password
// Ensure we fetch the password field when needed for comparison
UserSchema.methods.comparePassword = async function (candidatePassword) {
  // 'this.password' might be undefined due to 'select: false'
  // Fetch the user again including the password if needed, or ensure the calling context selects it
  // However, bcrypt.compare handles the comparison securely.
  // We assume 'this.password' is available in the context where comparePassword is called.
  if (!this.password) {
      // This scenario should be handled by the calling code (e.g., findOne().select('+password'))
      // Or by using a static method like findByCredentials
      console.error("Attempted to compare password, but password field was not loaded on the User object."); // Added more specific log
      throw new Error("Password field not available for comparison.");
  }
  // Use bcryptjs's compare function
  return await bcrypt.compare(candidatePassword, this.password);
};

// Ensure password is selected when finding user for login comparison
UserSchema.statics.findByCredentials = async function(username, password) {
    // Find user by username AND explicitly select the password field
    const user = await this.findOne({ username }).select('+password');
    if (!user) {
        console.log(`findByCredentials: User not found for username: ${username}`); // Debug log
        return null; // User not found
    }
    // Now 'user' object has the password field, safe to call comparePassword
    const isMatch = await user.comparePassword(password);
    if (!isMatch) {
        console.log(`findByCredentials: Password mismatch for username: ${username}`); // Debug log
        return null; // Password doesn't match
    }
    console.log(`findByCredentials: Credentials match for username: ${username}`); // Debug log
    // Return user object (password will still be selected here, but won't be sent in JSON response usually)
    return user;
};


const User = mongoose.model('User', UserSchema);

module.exports = User;



rag_service/ai_core.py

python
# ./ai_core.py

# Standard Library Imports
import logging
import os
import io
import re
import copy
import uuid
from typing import Any, Callable, Dict, List, Optional


# --- Global Initializations ---
logger = logging.getLogger(__name__)

# --- Configuration Import ---
# Assumes 'server/config.py' is the actual config file.
# `import config` will work if 'server/' directory is in sys.path.
try:
    import config # This should import server/config.py
except ImportError as e:
    logger.info(f"CRITICAL: Failed to import 'config' (expected server/config.py): {e}. ")



# Local aliases for config flags, models, constants, and classes

# Availability Flags
PYPDF_AVAILABLE = config.PYPDF_AVAILABLE
PDFPLUMBER_AVAILABLE = config.PDFPLUMBER_AVAILABLE
PANDAS_AVAILABLE = config.PANDAS_AVAILABLE
DOCX_AVAILABLE = config.DOCX_AVAILABLE
PIL_AVAILABLE = config.PIL_AVAILABLE
FITZ_AVAILABLE = config.FITZ_AVAILABLE
PYTESSERACT_AVAILABLE = config.PYTESSERACT_AVAILABLE
SPACY_MODEL_LOADED = config.SPACY_MODEL_LOADED
PYPDF2_AVAILABLE = config.PYPDF2_AVAILABLE
EMBEDDING_MODEL_LOADED = config.EMBEDDING_MODEL_LOADED
MAX_TEXT_LENGTH_FOR_NER  = config.MAX_TEXT_LENGTH_FOR_NER
LANGCHAIN_SPLITTER_AVAILABLE = config.LANGCHAIN_SPLITTER_AVAILABLE

# Error Strings
PYPDF_PDFREADERROR = config.PYPDF_PDFREADERROR
TESSERACT_ERROR = config.TESSERACT_ERROR

# Libraries and Models
pypdf = config.pypdf
PyPDF2 = config.PyPDF2
pdfplumber = config.pdfplumber
pd = config.pd
DocxDocument = config.DocxDocument
Image = config.Image
fitz = config.fitz
pytesseract = config.pytesseract
nlp_spacy_core = config.nlp_spacy_core
document_embedding_model = config.document_embedding_model
RecursiveCharacterTextSplitter = config.RecursiveCharacterTextSplitter

# Constants
AI_CORE_CHUNK_SIZE = config.AI_CORE_CHUNK_SIZE
AI_CORE_CHUNK_OVERLAP = config.AI_CORE_CHUNK_OVERLAP
DOCUMENT_EMBEDDING_MODEL_NAME = config.DOCUMENT_EMBEDDING_MODEL_NAME


# --- Stage 1: File Parsing and Raw Content Extraction --- (Functions as previously corrected)
def _parse_pdf_content(file_path: str) -> Optional[str]:
    if not PYPDF_AVAILABLE or not pypdf:
        logger.error("pypdf library not available. PDF parsing with pypdf will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text: text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"pypdf: Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pypdf: File not found: {file_path}"); return None
    except PYPDF_PDFREADERROR as pdf_err: 
        logger.error(f"pypdf: Error reading PDF {os.path.basename(file_path)}: {pdf_err}"); return None
    except Exception as e:
        logger.error(f"pypdf: Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_docx_content(file_path: str) -> Optional[str]:
    if not DOCX_AVAILABLE or not DocxDocument:
        logger.error("python-docx library not available. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"docx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"docx: Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_txt_content(file_path: str) -> Optional[str]:
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: text_content = f.read()
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"txt: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"txt: Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _parse_pptx_content(file_path: str) -> Optional[str]:
    if not PPTX_AVAILABLE or not Presentation:
        logger.warning(f"python-pptx not available. PPTX parsing for {os.path.basename(file_path)} skipped.")
        return None
    text_content = ""
    try:
        prs = Presentation(file_path)
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip(): text_content += shape.text.strip() + "\n\n"
        return text_content.strip() or None
    except FileNotFoundError:
        logger.error(f"pptx: File not found: {file_path}"); return None
    except Exception as e:
        logger.error(f"pptx: Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True); return None

def _get_parser_for_file(file_path: str) -> Optional[Callable]:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf': return _parse_pdf_content
    if ext == '.docx': return _parse_docx_content
    if ext == '.pptx': return _parse_pptx_content
    if ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: return _parse_txt_content
    logger.warning(f"Unsupported file extension for basic parsing: {ext} ({os.path.basename(file_path)})")
    return None

def extract_raw_content_from_file(file_path: str) -> Dict[str, Any]:
    file_base_name = os.path.basename(file_path)
    logger.info(f"Starting raw content extraction for: {file_base_name}")
    text_content, tables, images, is_scanned = "", [], [], False
    file_extension = os.path.splitext(file_path)[1].lower()

    parser_func = _get_parser_for_file(file_path)
    if parser_func:
        initial_text = parser_func(file_path)
        if initial_text: text_content = initial_text

    if file_extension == '.pdf':
        if PDFPLUMBER_AVAILABLE and pdfplumber:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_parts = [p.extract_text(x_tolerance=1, y_tolerance=1) or "" for p in pdf.pages]
                    pdfplumber_text = "\n".join(pdfplumber_text_parts)
                    clean_pdfplumber_text_len = len(pdfplumber_text.replace("\n", ""))

                    if len(pdf.pages) > 0 and (clean_pdfplumber_text_len < 50 * len(pdf.pages) and clean_pdfplumber_text_len < 200):
                        is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (low text from pdfplumber).")
                    
                    if len(pdfplumber_text.strip()) > len(text_content.strip()): text_content = pdfplumber_text.strip()
                    elif not text_content.strip() and pdfplumber_text.strip(): text_content = pdfplumber_text.strip()
                    
                    for page_num, page in enumerate(pdf.pages): 
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data_list in page_tables_data:
                                if table_data_list and PANDAS_AVAILABLE and pd:
                                    try:
                                        if len(table_data_list) > 1 and all(isinstance(c, str) or c is None for c in table_data_list[0]):
                                            tables.append(pd.DataFrame(table_data_list[1:], columns=table_data_list[0]))
                                        else: tables.append(table_data_list)
                                    except Exception as df_err: logger.warning(f"pdfplumber: DataFrame conversion error for table on page {page_num} of {file_base_name}: {df_err}"); tables.append(table_data_list)
                                elif table_data_list: 
                                    tables.append(table_data_list)
                    if tables: logger.info(f"pdfplumber: Extracted {len(tables)} tables from {file_base_name}.")
            except Exception as e: logger.warning(f"pdfplumber: Error during rich PDF processing for {file_base_name}: {e}", exc_info=True)
        
        elif not text_content.strip() and PYPDF_AVAILABLE and pypdf: 
            try:
                if len(pypdf.PdfReader(file_path).pages) > 0: is_scanned = True; logger.info(f"PDF {file_base_name} potentially scanned (no text from pypdf and pdfplumber not used/failed).")
            except: pass

        if FITZ_AVAILABLE and fitz and PIL_AVAILABLE and Image: 
            try:
                doc = fitz.open(file_path)
                for page_idx in range(len(doc)):
                    for img_info in doc.get_page_images(page_idx):
                        try: images.append(Image.open(io.BytesIO(doc.extract_image(img_info[0])["image"])))
                        except Exception as img_err: logger.warning(f"fitz: Could not open image xref {img_info[0]} from page {page_idx} of {file_base_name}: {img_err}")
                if images: logger.info(f"fitz: Extracted {len(images)} images from {file_base_name}.")
                doc.close()
            except Exception as e: logger.warning(f"fitz: Error extracting images from PDF {file_base_name}: {e}", exc_info=True)

    if not text_content.strip() and file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif']:
        is_scanned = True
        if PIL_AVAILABLE and Image:
            try: images.append(Image.open(file_path))
            except Exception as e: logger.error(f"Could not open image file {file_base_name}: {e}", exc_info=True)

    logger.info(f"Raw content extraction complete for {file_base_name}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {'text_content': text_content.strip(), 'tables': tables, 'images': images, 'is_scanned': is_scanned, 'file_type': file_extension}

# --- Stage 2: OCR --- (Function as previously corrected)
def perform_ocr_on_images(image_objects: List[Any]) -> str:
    if not image_objects: return ""
    if not PYTESSERACT_AVAILABLE or not pytesseract:
        logger.error("Pytesseract is not available. OCR cannot be performed.")
        return ""

    logger.info(f"Performing OCR on {len(image_objects)} image(s).")
    ocr_text_parts = []
    images_ocrd = 0
    for i, img in enumerate(image_objects):
        try:
            if not (PIL_AVAILABLE and Image and isinstance(img, Image.Image)):
                logger.warning(f"Skipping non-PIL Image object at index {i} for OCR.")
                continue
            text = pytesseract.image_to_string(img.convert('L'))
            if text and text.strip(): 
                ocr_text_parts.append(text.strip())
                images_ocrd += 1
        except Exception as e:
            if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
                logger.critical("Tesseract executable not found in PATH. OCR will fail for subsequent images too.")
                raise 
            logger.error(f"Error during OCR for image {i+1}/{len(image_objects)}: {e}", exc_info=True)
    
    full_ocr_text = "\n\n--- OCR Text from Image ---\n\n".join(ocr_text_parts)
    logger.info(f"OCR: Extracted {len(full_ocr_text)} chars from {images_ocrd} image(s) (out of {len(image_objects)} provided).")
    return full_ocr_text

# --- Stage 3: Text Cleaning and Normalization --- (Function as previously corrected)
def clean_and_normalize_text_content(text: str) -> str:
    if not text or not text.strip(): return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")
    text = re.sub(r'<script[^>]*>.*?</script>|<style[^>]*>.*?</style>|<[^>]+>', ' ', text, flags=re.I | re.S)
    text = re.sub(r'http\S+|www\S+|https\S+|\S*@\S*\s?', '', text, flags=re.MULTILINE)
    text = re.sub(r'[\n\t\r]+', ' ', text) 
    text = re.sub(r'\s+', ' ', text).strip() 
    text = re.sub(r'[^\w\s.,!?-]', '', text) 
    text_lower = text.lower()

    if not SPACY_MODEL_LOADED or not nlp_spacy_core:
        logger.warning("SpaCy model not loaded. Skipping tokenization/lemmatization. Returning regex-cleaned text.")
        return text_lower
    try:
        doc = nlp_spacy_core(text_lower, disable=['parser', 'ner']) 
        lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 1]
        final_cleaned_text = " ".join(lemmatized_tokens)
        logger.info(f"SpaCy-based text cleaning and normalization complete. Final length: {len(final_cleaned_text)}")
        if final_cleaned_text: logger.info(f"Final cleaned text (first 200 chars): {final_cleaned_text[:200]}...")
        return final_cleaned_text
    except Exception as e:
        logger.error(f"SpaCy processing failed: {e}. Returning pre-SpaCy cleaned text.", exc_info=True)
        return text_lower

# --- Stage 4: Layout Reconstruction & Table Integration --- (Function as previously corrected)
def reconstruct_document_layout(text_content: str, tables_data: List[Any], file_type: str) -> str:
    if not text_content and not tables_data: return ""
    logger.info(f"Starting layout reconstruction for file type '{file_type}'. Initial text length: {len(text_content)}, Tables: {len(tables_data)}.")
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text_content) 
    processed_text = re.sub(r'(\w+)-(\w+)', r'\1\2', processed_text)       

    if tables_data:
        table_md_parts = []
        for i, table_obj in enumerate(tables_data):
            header_md = f"\n\n[START OF TABLE {i+1}]\n"
            footer_md = f"\n[END OF TABLE {i+1}]\n"
            md_content = ""
            try:
                if PANDAS_AVAILABLE and pd and isinstance(table_obj, pd.DataFrame): 
                    md_content = table_obj.to_markdown(index=False)
                elif isinstance(table_obj, list) and table_obj and all(isinstance(r, list) for r in table_obj):
                    if table_obj and table_obj[0]: 
                        md_content = "| " + " | ".join(map(str, table_obj[0])) + " |\n"
                        md_content += "| " + " | ".join(["---"] * len(table_obj[0])) + " |\n"
                        for row_idx, row_data in enumerate(table_obj[1:]):
                            if len(row_data) == len(table_obj[0]): 
                                md_content += "| " + " | ".join(map(str, row_data)) + " |\n"
                            else:
                                logger.warning(f"Table {i+1}, row {row_idx+1} length mismatch with header. Skipping row.")
                    else: md_content = "[Empty Table Data]"
                else: md_content = str(table_obj)
            except Exception as e: 
                logger.warning(f"Table {i+1} to markdown conversion error: {e}. Using string representation."); 
                md_content = str(table_obj)
            if md_content: table_md_parts.append(header_md + md_content + footer_md)
        if table_md_parts: processed_text += "\n\n" + "\n\n".join(table_md_parts)
    
    processed_text = re.sub(r'\s+', ' ', processed_text).strip()
    logger.info(f"Layout reconstruction complete. Final text length: {len(processed_text)}")
    return processed_text

# --- Stage 5: Metadata Extraction --- (Function as previously corrected, uses original_file_name for logging)
def extract_document_metadata_info(file_path: str, processed_text: str, file_type_from_extraction: str, original_file_name: str, user_id: str) -> Dict[str, Any]:
    logger.info(f"Starting metadata extraction for: {original_file_name} (User: {user_id})")
    doc_meta = {'file_name': original_file_name, 'file_path_on_server': file_path, 'original_file_type': file_type_from_extraction,
                'processing_user': user_id, 'title': original_file_name, 'author': "Unknown", 'creation_date': None,
                'modification_date': None, 'page_count': 0, 'char_count_processed_text': len(processed_text),
                'named_entities': {}, 'structural_elements': "Paragraphs, Tables (inferred)", 'is_scanned_pdf': False}
    try:
        doc_meta['file_size_bytes'] = os.path.getsize(file_path)
        if PANDAS_AVAILABLE and pd:
            doc_meta['creation_date_os'] = pd.Timestamp(os.path.getctime(file_path), unit='s').isoformat()
            doc_meta['modification_date_os'] = pd.Timestamp(os.path.getmtime(file_path), unit='s').isoformat()
    except Exception as e: logger.warning(f"Metadata: OS metadata error for {original_file_name}: {e}")

    if file_type_from_extraction == '.pdf' and PYPDF2_AVAILABLE and PyPDF2:
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                info = reader.metadata
                if info:
                    if hasattr(info, 'title') and info.title: doc_meta['title'] = str(info.title).strip()
                    if hasattr(info, 'author') and info.author: doc_meta['author'] = str(info.author).strip()
                    if hasattr(info, 'creation_date') and info.creation_date and PANDAS_AVAILABLE and pd: 
                        doc_meta['creation_date'] = pd.Timestamp(info.creation_date).isoformat()
                doc_meta['page_count'] = len(reader.pages)
        except Exception as e: logger.warning(f"Metadata: PyPDF2 error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.docx' and DOCX_AVAILABLE and DocxDocument:
        try:
            doc = DocxDocument(file_path)
            props = doc.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = sum(1 for p in doc.paragraphs if p.text.strip()) or 1
        except Exception as e: logger.warning(f"Metadata: DOCX error for {original_file_name}: {e}", exc_info=True)
    elif file_type_from_extraction == '.pptx' and PPTX_AVAILABLE and Presentation:
        try:
            prs = Presentation(file_path)
            props = prs.core_properties
            if props.title: doc_meta['title'] = props.title
            if props.author: doc_meta['author'] = props.author
            if props.created and PANDAS_AVAILABLE and pd: doc_meta['creation_date'] = pd.Timestamp(props.created).isoformat()
            doc_meta['page_count'] = len(prs.slides)
        except Exception as e: logger.warning(f"Metadata: PPTX error for {original_file_name}: {e}", exc_info=True)

    if doc_meta['page_count'] == 0 and processed_text: doc_meta['page_count'] = processed_text.count('\n\n') + 1

    if processed_text and SPACY_MODEL_LOADED and nlp_spacy_core:
        logger.info(f"Extracting named entities using SpaCy for {original_file_name}...")
        try:
            max_len = getattr(config, 'MAX_TEXT_LENGTH_FOR_NER', 500000) 
            text_for_ner = processed_text[:max_len]
            spacy_doc = nlp_spacy_core(text_for_ner) 
            ner_labels = nlp_spacy_core.pipe_labels.get("ner", [])
            entities = {label: [] for label in ner_labels}
            for ent in spacy_doc.ents:
                if ent.label_ in entities: entities[ent.label_].append(ent.text)
            doc_meta['named_entities'] = {k: list(set(v)) for k, v in entities.items() if v} 
            num_entities = sum(len(v_list) for v_list in doc_meta['named_entities'].values())
            logger.info(f"Extracted {num_entities} named entities for {original_file_name}.")
        except Exception as e: logger.error(f"Metadata: NER error for {original_file_name}: {e}", exc_info=True); doc_meta['named_entities'] = {}
    else: logger.info(f"Skipping NER for {original_file_name} (no text or SpaCy model not available/loaded)."); doc_meta['named_entities'] = {}
    
    logger.info(f"Metadata extraction complete for {original_file_name}.")
    return doc_meta

# --- Stage 6: Text Chunking ---
def chunk_document_into_segments(
    text_to_chunk: str,
    document_level_metadata: Dict[str, Any]
) -> List[Dict[str, Any]]:
    if not text_to_chunk or not text_to_chunk.strip():
        logger.warning(f"Chunking: No text for {document_level_metadata.get('file_name', 'unknown')}.")
        return []

    if not LANGCHAIN_SPLITTER_AVAILABLE or not RecursiveCharacterTextSplitter:
        logger.error("RecursiveCharacterTextSplitter not available. Cannot chunk text.")
        return []
        
    # CORRECTED: Use AI_CORE_CHUNK_SIZE and AI_CORE_CHUNK_OVERLAP from server/config.py
    chunk_s = AI_CORE_CHUNK_SIZE
    chunk_o = AI_CORE_CHUNK_OVERLAP

    original_doc_name_for_log = document_level_metadata.get('file_name', 'unknown')
    logger.info(f"Starting text chunking for {original_doc_name_for_log}. "
                f"Using config: CHUNK_SIZE={chunk_s}, CHUNK_OVERLAP={chunk_o}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_s,
        chunk_overlap=chunk_o,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], 
        keep_separator=True 
    )

    try: raw_text_segments: List[str] = text_splitter.split_text(text_to_chunk)
    except Exception as e: 
        logger.error(f"Chunking: Error splitting text for {original_doc_name_for_log}: {e}", exc_info=True)
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    base_file_name_for_ref = os.path.splitext(original_doc_name_for_log)[0] 

    for i, segment_content in enumerate(raw_text_segments):
        if not segment_content.strip(): 
            logger.debug(f"Skipping empty chunk at index {i} for {original_doc_name_for_log}.")
            continue

        chunk_specific_metadata = copy.deepcopy(document_level_metadata)
        
        qdrant_point_id = str(uuid.uuid4()) # Generate UUID for Qdrant

        chunk_specific_metadata['chunk_id'] = qdrant_point_id 
        chunk_specific_metadata['chunk_reference_name'] = f"{base_file_name_for_ref}_chunk_{i:04d}"
        chunk_specific_metadata['chunk_index'] = i
        chunk_specific_metadata['chunk_char_count'] = len(segment_content)
        
        output_chunks.append({
            'id': qdrant_point_id, 
            'text_content': segment_content,
            'metadata': chunk_specific_metadata 
        })
    
    logger.info(f"Chunking: Split '{original_doc_name_for_log}' into {len(output_chunks)} non-empty chunks with UUID IDs.")
    return output_chunks

# --- Stage 7: Embedding Generation --- (Function as previously corrected)
def generate_segment_embeddings(document_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not document_chunks: return []
    if not EMBEDDING_MODEL_LOADED or not document_embedding_model: # Check if model is loaded
        logger.error("Embedding model not loaded. Cannot generate embeddings.")
        for chunk_dict in document_chunks: chunk_dict['embedding'] = None # Ensure key exists
        return document_chunks

    # CORRECTED: Get model name from DOCUMENT_EMBEDDING_MODEL_NAME
    model_name_for_logging = getattr(config, 'DOCUMENT_EMBEDDING_MODEL_NAME', "Unknown Model")
    logger.info(f"Embedding: Starting embedding generation for {len(document_chunks)} chunks "
                f"using model: {model_name_for_logging}.")
    
    texts_to_embed: List[str] = []
    valid_chunk_indices: List[int] = []

    for i, chunk_dict in enumerate(document_chunks):
        text_content = chunk_dict.get('text_content')
        if text_content and text_content.strip():
            texts_to_embed.append(text_content)
            valid_chunk_indices.append(i)
        else:
            chunk_dict['embedding'] = None
            logger.debug(f"Embedding: Chunk {chunk_dict.get('id', i)} has no text content, skipping embedding.")

    if not texts_to_embed:
        logger.warning("Embedding: No actual text content found in any provided chunks to generate embeddings.")
        return document_chunks

    try:
        logger.info(f"Embedding: Generating embeddings for {len(texts_to_embed)} non-empty text segments...")
        embeddings_np_array = document_embedding_model.encode(texts_to_embed, show_progress_bar=False)
        
        for i, original_chunk_index in enumerate(valid_chunk_indices):
            if i < len(embeddings_np_array):
                document_chunks[original_chunk_index]['embedding'] = embeddings_np_array[i].tolist()
            else:
                logger.error(f"Embedding: Mismatch in embedding count for chunk at original index {original_chunk_index}. Assigning None.")
                document_chunks[original_chunk_index]['embedding'] = None
        
        logger.info(f"Embedding: Embeddings generated and assigned to {len(valid_chunk_indices)} chunks.")
        
    except Exception as e:
        logger.error(f"Embedding: Error during embedding generation with model {model_name_for_logging}: {e}", exc_info=True)
        for original_chunk_index in valid_chunk_indices:
            document_chunks[original_chunk_index]['embedding'] = None
        
    return document_chunks


# --- Main Orchestration Function (to be called by app.py) ---
def process_document_for_qdrant(file_path: str, original_name: str, user_id: str) -> List[Dict[str, Any]]:
    logger.info(f"ai_core: Orchestrating document processing for {original_name}, user {user_id}")
    if not os.path.exists(file_path): 
        logger.error(f"File not found at ai_core entry point: {file_path}")
        raise FileNotFoundError(f"File not found: {file_path}")

    try:

        # Step 1: Extract Raw content
        raw_content = extract_raw_content_from_file(file_path)
        # Example of how to access: raw_content['text_content'], raw_content['images'], etc.
        # file_type will be important: raw_content['file_type']
        # is_scanned will be important: raw_content['is_scanned']
        initial_extracted_text = raw_content.get('text_content', "") # THIS IS WHAT YOU WANT TO RETURN for Node.js analysis


        # Step 2: Perform OCR if needed
        ocr_text_output = ""
        if raw_content.get('is_scanned') and raw_content.get('images'):
            if PYTESSERACT_AVAILABLE and pytesseract:
                 ocr_text_output = perform_ocr_on_images(raw_content['images'])
            else:
                logger.warning(f"OCR requested for {original_name} but Pytesseract is not available/configured. Skipping OCR.")

        combined_text = raw_content.get('text_content', "")
        if ocr_text_output:
            if raw_content.get('is_scanned') and len(ocr_text_output) > len(combined_text) / 2:
                 combined_text = ocr_text_output + "\n\n" + combined_text
            else:
                 combined_text += "\n\n" + ocr_text_output
        
        if not combined_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No text content for {original_name} after raw extraction/OCR, and no tables. Returning empty.")
            return []

        cleaned_text = clean_and_normalize_text_content(combined_text)
        if not cleaned_text.strip() and not raw_content.get('tables'):
            logger.warning(f"No meaningful text for {original_name} after cleaning, and no tables. Returning empty.")
            return []

        text_for_metadata_and_chunking = reconstruct_document_layout(
            cleaned_text,
            raw_content.get('tables', []),
            raw_content.get('file_type', '')
        )

        doc_metadata = extract_document_metadata_info(
            file_path,
            text_for_metadata_and_chunking, 
            raw_content.get('file_type', ''),
            original_name,
            user_id
        )
        doc_metadata['is_scanned_pdf'] = raw_content.get('is_scanned', False)

        # This now uses corrected config variable names and UUIDs for IDs
        chunks_with_metadata = chunk_document_into_segments( 
            text_for_metadata_and_chunking,
            doc_metadata
        )
        if not chunks_with_metadata:
            logger.warning(f"No chunks produced for {original_name}. Returning empty list.")
            return []

        # This now uses corrected config variable name for logging
        final_chunks_with_embeddings = generate_segment_embeddings(chunks_with_metadata)
        
        logger.info(f"ai_core: Successfully processed {original_name}. Generated {len(final_chunks_with_embeddings)} chunks.")
        return final_chunks_with_embeddings, initial_extracted_text

    except Exception as e: 
        if TESSERACT_ERROR and isinstance(e, TESSERACT_ERROR):
            logger.critical(f"ai_core: Tesseract (OCR engine) was not found during processing of {original_name}. OCR could not be performed.", exc_info=False)
            raise 
        
        logger.error(f"ai_core: Critical error during processing of {original_name}: {e}", exc_info=True)
        raise



rag_service/app.py

python
# server/main_qdrant_app.py

import os
import sys
import traceback # For detailed error logging
from flask import Flask, request, jsonify, current_app
import logging

# --- Add server directory to sys.path ---
# This allows us to import modules from the 'server' directory and sub-packages like 'rag_service'
# Assumes this file (main_qdrant_app.py) is in the 'server/' directory.
SERVER_DIR = os.path.dirname(os.path.abspath(__file__))
if SERVER_DIR not in sys.path:
    sys.path.insert(0, SERVER_DIR)

import config # Should pick up server/config.py
config.setup_logging()


# --- Import configurations and services ---
try:
    from vector_db_service import VectorDBService # From server/vector_db_service.py
    import ai_core # From server/rag_service/ai_core.py
except ImportError as e:
    print(f"CRITICAL IMPORT ERROR: {e}. Ensure all modules are correctly placed and server directory is in PYTHONPATH.")
    print("PYTHONPATH:", sys.path)
    sys.exit(1)

logger = logging.getLogger(__name__)
app = Flask(__name__)

# --- Initialize VectorDBService ---
vector_service = None
try:
    logger.info("Initializing VectorDBService for Qdrant...")
    vector_service = VectorDBService()
    vector_service.setup_collection() # Create/validate Qdrant collection on startup
    app.vector_service = vector_service
    logger.info("VectorDBService initialized and collection setup successfully.")
except Exception as e:
    logger.critical(f"Failed to initialize VectorDBService or setup Qdrant collection: {e}", exc_info=True)
    app.vector_service = None
    # Application might be non-functional, consider exiting or running in a degraded state
    # For now, vector_service will be None, and endpoints will fail gracefully.

# --- Helper for Error Responses ---
def create_error_response(message, status_code=500):
    current_app.logger.error(f"API Error ({status_code}): {message}")
    return jsonify({"error": message}), status_code

# === API Endpoints ===

@app.route('/health', methods=['GET'])
def health_check():
    current_app.logger.info("--- Health Check Request ---")
    status_details = {
        "status": "error",
        "qdrant_service": "not_initialized",
        "qdrant_collection_name": config.QDRANT_COLLECTION_NAME,
        "qdrant_collection_status": "unknown",
        "document_embedding_model": config.DOCUMENT_EMBEDDING_MODEL_NAME,
        "query_embedding_model": config.QUERY_EMBEDDING_MODEL_NAME,
        "expected_vector_dimension": config.QDRANT_COLLECTION_VECTOR_DIM,
    }
    http_status_code = 503

    if not vector_service:
        status_details["qdrant_service"] = "failed_to_initialize"
        return jsonify(status_details), http_status_code

    status_details["qdrant_service"] = "initialized"
    try:
        collection_info = vector_service.client.get_collection(collection_name=vector_service.collection_name)
        status_details["qdrant_collection_status"] = "exists"
        if hasattr(collection_info.config.params.vectors, 'size'): # Simple vector config
             actual_dim = collection_info.config.params.vectors.size
        elif isinstance(collection_info.config.params.vectors, dict): # Named vectors
            default_vector_conf = collection_info.config.params.vectors.get('')
            actual_dim = default_vector_conf.size if default_vector_conf else "multiple_named_not_checked"
        else:
            actual_dim = "unknown_format"

        status_details["actual_vector_dimension"] = actual_dim
        if actual_dim == config.QDRANT_COLLECTION_VECTOR_DIM:
            status_details["status"] = "ok"
            http_status_code = 200
            current_app.logger.info("Health check successful.")
        else:
            status_details["qdrant_collection_status"] = f"exists_with_dimension_mismatch (Expected {config.QDRANT_COLLECTION_VECTOR_DIM}, Got {actual_dim})"
            current_app.logger.warning(f"Health check: Qdrant dimension mismatch.")

    except Exception as e:
        status_details["qdrant_collection_status"] = f"error_accessing_collection: {str(e)}"
        current_app.logger.error(f"Health check: Error accessing Qdrant collection: {e}", exc_info=True)

    return jsonify(status_details), http_status_code


@app.route('/add_document', methods=['POST'])
def add_document_qdrant():
    current_app.logger.info("--- /add_document Request (Qdrant) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    user_id = data.get('user_id')
    # file_path is path ON THE SERVER where Flask can access it.
    # In a real system, this often comes from a file upload process that saves the file first.
    file_path = data.get('file_path')
    original_name = data.get('original_name') # Original filename from client

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    current_app.logger.info(f"Processing file: '{original_name}' (Path: '{file_path}') for user: '{user_id}'")

    if not os.path.exists(file_path):
        current_app.logger.error(f"File not found at server path: {file_path}")
        return create_error_response(f"File not found at server path: {file_path}", 404)

    try:
        # Assuming ai_core.process_document_for_embeddings is updated to return two values,
        # or you have a new function like ai_core.process_document_for_qdrant
        current_app.logger.info(f"Calling ai_core to process document: '{original_name}'")
        processed_chunks_with_embeddings, raw_text_for_node_analysis = ai_core.process_document_for_qdrant(
            file_path=file_path,
            original_name=original_name,
            user_id=user_id
        )
        # If you kept the old function name and just changed its return:
        # processed_chunks_with_embeddings, raw_text_for_node_analysis = ai_core.process_document_for_embeddings(...)

        num_chunks_added_to_qdrant = 0
        processing_status = "processed_no_content"

        # Check if chunks were generated before trying to add them
        if processed_chunks_with_embeddings:
            current_app.logger.info(f"ai_core generated {len(processed_chunks_with_embeddings)} chunks for '{original_name}'. Adding to Qdrant.")
            # Ensure vector_service is used correctly here
            num_chunks_added_to_qdrant = current_app.vector_service.add_processed_chunks(processed_chunks_with_embeddings)
            if num_chunks_added_to_qdrant > 0:
                processing_status = "added"
                current_app.logger.info(f"Successfully added {num_chunks_added_to_qdrant} chunks from '{original_name}' to Qdrant.")
            else:
                processing_status = "processed_chunks_not_added"
        elif raw_text_for_node_analysis: # If no chunks, but raw text was extracted
            current_app.logger.info(f"ai_core produced no processable RAG chunks for '{original_name}', but raw text was extracted.")
            processing_status = "processed_for_analysis_only" # No RAG chunks, but text for analysis
        else: # No chunks and no raw text (e.g., empty or unparseable file)
            current_app.logger.warning(f"ai_core produced no RAG chunks and no raw text for '{original_name}'.")
            # Return a specific response indicating nothing useful was extracted
            return jsonify({
                "message": f"Processed '{original_name}' but no content was extracted for RAG or analysis.",
                "status": "no_content_extracted",
                "filename": original_name,
                "user_id": user_id,
                "num_chunks_added_to_qdrant": 0,
                "raw_text_for_analysis": "" # Empty string
            }), 200 # 200 OK as the processing attempt itself didn't fail

        # Construct the success response for Node.js
        response_payload = {
            "message": f"Successfully processed '{original_name}'. Embeddings operation completed.",
            "status": processing_status,
            "filename": original_name,
            "user_id": user_id,
            "num_chunks_added_to_qdrvecorant": num_chunks_added_to_qdrant,
            "raw_text_for_analysis": raw_text_for_node_analysis if raw_text_for_node_analysis is not None else "" # Ensure it's always a string
        }
        current_app.logger.info(f"Successfully processed '{original_name}'. Returning raw text and Qdrant status.")
        return jsonify(response_payload), 201 # 201 Created if resources (chunks) were made

    except FileNotFoundError as e:
        current_app.logger.error(f"Add Document Error for '{original_name}' - FileNotFoundError: {e}", exc_info=True)
        return create_error_response(f"File not found during processing: {str(e)}", 404)
    except config.TESSERACT_ERROR: # Make sure config is imported and TESSERACT_ERROR is defined
        current_app.logger.critical(f"Add Document Error for '{original_name}' - Tesseract (OCR) not found.")
        return create_error_response("OCR engine (Tesseract) not found or not configured correctly on the server.", 500)
    except ValueError as e: # e.g., vector dimension mismatch from add_processed_chunks
        current_app.logger.error(f"Add Document Error for '{original_name}' - ValueError: {e}", exc_info=True)
        return create_error_response(f"Configuration or data error: {str(e)}", 400)
    except Exception as e:
        current_app.logger.error(f"Add Document Error for '{original_name}' - Unexpected Exception: {e}\n{traceback.format_exc()}", exc_info=True) # Ensure traceback is imported
        return create_error_response(f"Failed to process document '{original_name}' due to an internal error.", 500)


@app.route('/query', methods=['POST']) # Changed from /query to /search for Qdrant context
def search_qdrant_documents():
    current_app.logger.info("--- /search Request (Qdrant) ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    if not vector_service:
        return create_error_response("VectorDBService (Qdrant) is not available.", 503)

    data = request.get_json()
    query_text = data.get('query')
    k = data.get('k', config.QDRANT_DEFAULT_SEARCH_K) # Use default from config
    
    # Optional: Allow passing filter conditions in the request body
    # Example filter: {"user_id": "some_user", "original_name": "some_doc.pdf"}
    filter_payload = data.get('filter') 
    qdrant_filters = None

    if filter_payload and isinstance(filter_payload, dict):
        from qdrant_client import models as qdrant_models # Import here to keep it local
        conditions = []
        for key, value in filter_payload.items():
            # Ensure key is a valid payload key you store (e.g., user_id, original_name, page_number)
            # This simple example assumes exact match for string values.
            # For numerical ranges or other conditions, qdrant_models.Range, etc. would be used.
            conditions.append(qdrant_models.FieldCondition(key=key, match=qdrant_models.MatchValue(value=value)))
        
        if conditions:
            qdrant_filters = qdrant_models.Filter(must=conditions)
            current_app.logger.info(f"Applying Qdrant filter: {filter_payload}")


    if not query_text:
        return create_error_response("Missing 'query' field in request body", 400)

    try:
        k = int(k)
    except ValueError:
        return create_error_response("'k' must be an integer", 400)

    current_app.logger.info(f"Performing Qdrant search for query (first 100 chars): '{query_text[:100]}...' with k={k}")

    try:
        # `search_documents` returns: docs_list, formatted_context_string, context_map
        docs, formatted_context, docs_map = vector_service.search_documents(
            query=query_text,
            k=k,
            filter_conditions=qdrant_filters
        )

        # The response from your original /query endpoint was `{"relevantDocs": [...]}`
        # Let's adapt to return something similar, but with more Qdrant-style info.
        # `docs` is a list of `Document` objects from `vector_db_service`.
        
        response_payload = {
            "query": query_text,
            "k_requested": k,
            "filter_applied": filter_payload, # Show what filter was used
            "results_count": len(docs),
            "formatted_context_snippet": formatted_context, # The RAG-style formatted string
            "retrieved_documents_map": docs_map, # The map with citation index as key
            "retrieved_documents_list": [doc.to_dict() for doc in docs] # List of Document objects as dicts
        }
        current_app.logger.info(f"Qdrant search successful. Returning {len(docs)} results.")
        return jsonify(response_payload), 200

    except Exception as e:
        current_app.logger.error(f"Qdrant search failed: {e}\n{traceback.format_exc()}")
        return create_error_response(f"Error during Qdrant search: {str(e)}", 500)


if __name__ == '__main__':
    logger.info(f"--- Starting Qdrant RAG API Service on port {config.API_PORT} ---")
    logger.info(f"Document Embedding Model (ai_core): {config.DOCUMENT_EMBEDDING_MODEL_NAME} (Dim: {config.DOCUMENT_VECTOR_DIMENSION})")
    logger.info(f"Query Embedding Model (vector_db_service): {config.QUERY_EMBEDDING_MODEL_NAME} (Dim: {config.QUERY_VECTOR_DIMENSION})")
    logger.info(f"Qdrant Collection: {config.QDRANT_COLLECTION_NAME} (Expected Dim: {config.QDRANT_COLLECTION_VECTOR_DIM})")
    
    # For production, use a proper WSGI server like gunicorn or waitress
    # Example: gunicorn --workers 4 --bind 0.0.0.0:5000 main_qdrant_app:app
    app.run(host='0.0.0.0', port=config.API_PORT, debug=True) # debug=True for development


rag_service/code.txt


============ ./ai_core.py ============
import os
process_pdf_for_embeddings
from PIL import Image # For image processing
import pytesseract # OCR
import fitz # PyMuPDF for PDF parsing
import pdfplumber # For tables and layout
import PyPDF2 # For basic metadata
import re # Regular expressions for cleaning



============ ./app.py ============
# server/rag_service/app.py

import os
import sys
from flask import Flask, request, jsonify
from process_document import process_uploaded_document
import ai_core

# Add server directory to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
sys.path.insert(0, server_dir) # Ensure rag_service can be imported

# Now import local modules AFTER adjusting sys.path
from rag_service import config
import rag_service.file_parser as file_parser
import rag_service.faiss_handler as faiss_handler
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

def create_error_response(message, status_code=500):
    logger.error(f"API Error Response ({status_code}): {message}")
    return jsonify({"error": message}), status_code

@app.route('/health', methods=['GET'])
def health_check():
    logger.info("\n--- Received request at /health ---")
    status_details = {
        "status": "error",
        "embedding_model_type": config.EMBEDDING_TYPE,
        "embedding_model_name": config.EMBEDDING_MODEL_NAME,
        "embedding_dimension": None,
        "sentence_transformer_load": None,
        "default_index_loaded": False,
        "default_index_vectors": 0,
        "default_index_dim": None,
        "message": ""
    }
    http_status_code = 503

    try:
        # Check Embedding Model
        model = faiss_handler.embedding_model
        if model is None:
            status_details["message"] = "Embedding model could not be initialized during startup."
            status_details["sentence_transformer_load"] = "Failed"
            raise RuntimeError(status_details["message"])
        else:
            status_details["sentence_transformer_load"] = "OK"
            try:
                 status_details["embedding_dimension"] = faiss_handler.get_embedding_dimension(model)
            except Exception as dim_err:
                 status_details["embedding_dimension"] = f"Error: {dim_err}"


        # Check Default Index
        if config.DEFAULT_INDEX_USER_ID in faiss_handler.loaded_indices:
            status_details["default_index_loaded"] = True
            default_index = faiss_handler.loaded_indices[config.DEFAULT_INDEX_USER_ID]
            if hasattr(default_index, 'index') and default_index.index:
                status_details["default_index_vectors"] = default_index.index.ntotal
                status_details["default_index_dim"] = default_index.index.d
            logger.info("Default index found in cache.")
        else:
            logger.info("Attempting to load default index for health check...")
            try:
                default_index = faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID)
                status_details["default_index_loaded"] = True
                if hasattr(default_index, 'index') and default_index.index:
                    status_details["default_index_vectors"] = default_index.index.ntotal
                    status_details["default_index_dim"] = default_index.index.d
                logger.info("Default index loaded successfully during health check.")
            except Exception as index_load_err:
                logger.error(f"Health check failed to load default index: {index_load_err}", exc_info=True)
                status_details["message"] = f"Failed to load default index: {index_load_err}"
                status_details["default_index_loaded"] = False
                raise # Re-raise to indicate failure

        # Final Status
        status_details["status"] = "ok"
        status_details["message"] = "RAG service is running, embedding model accessible, default index loaded."
        http_status_code = 200
        logger.info("Health check successful.")

    except Exception as e:
        logger.error(f"--- Health Check Error ---", exc_info=True)
        if not status_details["message"]: # Avoid overwriting specific error messages
            status_details["message"] = f"Health check failed: {str(e)}"
        # Ensure status is error if exception occurred
        status_details["status"] = "error"
        http_status_code = 503 # Service unavailable if health check fails critically

    return jsonify(status_details), http_status_code


@app.route('/add_document', methods=['POST'])
def add_document():
    logger.info("\n--- Received request at /add_document ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    file_path = data.get('file_path')
    original_name = data.get('original_name')

    if not all([user_id, file_path, original_name]):
        return create_error_response("Missing required fields: user_id, file_path, original_name", 400)

    logger.info(f"Processing file: {original_name} for user: {user_id}")
    logger.info(f"File path: {file_path}")

    if not os.path.exists(file_path):
        return create_error_response(f"File not found at path: {file_path}", 404)

    try:
        logger.info(f"Initialising [ process_file function ] for : {original_name} for user: {user_id}")

        final_chunks_with_embeddings = ai_core.process_pdf_for_embeddings(file_path, original_name, user_id)

        if not final_chunks_with_embeddings:
            logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
            return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200
        
        logger.info(f"Final Chunks with Embeddings: {final_chunks_with_embeddings}")

    except Exception as e:
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)

        
    # try:
    #     # 1. Parse File
    #     username = "abcd"
    #     text_content = process_uploaded_document(file_path, username=username)
    #     if text_content is None:
    #         logger.warning(f"Skipping embedding for {original_name}: File type not supported or parsing failed.")
    #         return jsonify({"message": f"File type of '{original_name}' not supported for RAG or parsing failed.", "filename": original_name, "status": "skipped"}), 200

    #     logger.info("Text_Content : {text_content}")
    #     # if not text_content.strip():
    #     #     logger.warning(f"Skipping embedding for {original_name}: No text content found after parsing.")
    #     #     return jsonify({"message": f"No text content extracted from '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 2. Chunk Text
    #     # documents = file_parser.chunk_text(text_content, original_name, user_id)
    #     # if not documents:
    #     #     logger.warning(f"No chunks created for {original_name}. Skipping add.")
    #     #     return jsonify({"message": f"No text chunks generated for '{original_name}'.", "filename": original_name, "status": "skipped"}), 200

    #     # # 3. Add to Index (faiss_handler now handles dimension checks/recreation)
    #     # faiss_handler.add_documents_to_index(user_id, documents)

    #     # logger.info(f"Successfully processed and added document: {original_name} for user: {user_id}")
    #     # return jsonify({
    #     #     "message": f"Document '{original_name}' processed and added to index.",
    #     #     "filename": original_name,
    #     #     "chunks_added": len(documents),
    #     #     "status": "added"
    #     # }), 200
    # except Exception as e:
        # Log the specific error from faiss_handler if it raised one
        logger.error(f"--- Add Document Error for file '{original_name}' ---", exc_info=True)
        return create_error_response(f"Failed to process document '{original_name}': {str(e)}", 500)


@app.route('/query', methods=['POST'])
def query_index_route():
    print("Called")
    logger.info("\n--- Received request at /query ---")
    if not request.is_json:
        return create_error_response("Request must be JSON", 400)

    data = request.get_json()
    user_id = data.get('user_id')
    query = data.get('query')
    k = data.get('k', 5) # Default to k=5 now

    if not user_id or not query:
        return create_error_response("Missing required fields: user_id, query", 400)

    logger.info(f"Querying for user: {user_id} with k={k}")
    # Avoid logging potentially sensitive query text in production
    logger.debug(f"Query text: '{query[:100]}...'")

    try:
        results = faiss_handler.query_index(user_id, query, k=k)

        formatted_results = []
        for doc, score in results:
            # --- SEND FULL CONTENT ---
            content = doc.page_content
            # --- (No snippet generation needed) ---

            formatted_results.append({
                "documentName": doc.metadata.get("documentName", "Unknown"),
                "score": float(score),
                "content": content, # Send the full content
                # Removed "content_snippet"
            })

        logger.info(f"Query successful for user {user_id}. Returning {len(formatted_results)} results.")
        return jsonify({"relevantDocs": formatted_results}), 200
    except Exception as e:
        logger.error(f"--- Query Error ---", exc_info=True)
        return create_error_response(f"Failed to query index: {str(e)}", 500)

if __name__ == '__main__':
    # Ensure base FAISS directory exists on startup
    try:
        faiss_handler.ensure_faiss_dir()
    except Exception as e:
        logger.critical(f"CRITICAL: Could not create FAISS base directory '{config.FAISS_INDEX_DIR}'. Exiting. Error: {e}", exc_info=True)
        sys.exit(1) # Exit if base dir cannot be created

    # Attempt to initialize embedding model on startup
    try:
        faiss_handler.get_embedding_model() # This also determines the dimension
        logger.info("Embedding model initialized successfully on startup.")
    except Exception as e:
        logger.error(f"CRITICAL: Embedding model failed to initialize on startup: {e}", exc_info=True)
        logger.error("Endpoints requiring embeddings (/add_document, /query) will fail.")
        # Decide if you want to exit or run in a degraded state
        sys.exit(1) # Exit if embedding model fails - essential service

    # Attempt to load/check the default index on startup
    try:
        faiss_handler.load_or_create_index(config.DEFAULT_INDEX_USER_ID) # This checks/creates/validates dimension
        logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' loaded/checked/created on startup.")
    except Exception as e:
        logger.warning(f"Warning: Could not load/create default index '{config.DEFAULT_INDEX_USER_ID}' on startup: {e}", exc_info=True)
        # Don't necessarily exit, but log clearly. Queries might only use user indices.

    # Start Flask App
    port = config.RAG_SERVICE_PORT
    logger.info(f"--- Starting RAG service ---")
    logger.info(f"Listening on: http://0.0.0.0:{port}")
    logger.info(f"Using Embedding: {config.EMBEDDING_TYPE} ({config.EMBEDDING_MODEL_NAME})")
    try:
        logger.info(f"Embedding Dimension: {faiss_handler.get_embedding_dimension(faiss_handler.embedding_model)}")
    except: pass # Dimension already logged or failed earlier
    logger.info(f"FAISS Index Path: {config.FAISS_INDEX_DIR}")
    logger.info("-----------------------------")
    # Use waitress or gunicorn for production instead of Flask's development server
    app.run(host='0.0.0.0', port=port, debug=os.getenv('FLASK_DEBUG') == '1')



============ ./code.txt ============



============ ./config.py ============
# server/rag_service/config.py
import os
import sys

# --- Determine Server Directory ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
SERVER_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '..')) # Path to the 'server' directory
print(f"[rag_service/config.py] Determined SERVER_DIR: {SERVER_DIR}")

# --- Embedding Model Configuration ---
# Switch to Sentence Transformer
EMBEDDING_TYPE = 'sentence-transformer'
# Specify the model name (make sure you have internet access for download on first run)
# Or choose another model compatible with sentence-transformers library
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'BAAI/bge-large-en-v1.5')
EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'mixedbread-ai/mxbai-embed-large-v1')
# EMBEDDING_MODEL_NAME_ST = os.getenv('SENTENCE_TRANSFORMER_MODEL', 'e5-large-v2')
EMBEDDING_MODEL_NAME = EMBEDDING_MODEL_NAME_ST
print(f"Using Sentence Transformer model: {EMBEDDING_MODEL_NAME}")

# --- FAISS Configuration ---
FAISS_INDEX_DIR = os.path.join(SERVER_DIR, 'faiss_indices')
DEFAULT_ASSETS_DIR = os.path.join(SERVER_DIR, 'default_assets', 'engineering')
DEFAULT_INDEX_USER_ID = '__DEFAULT__'

# --- Text Splitting Configuration ---
CHUNK_SIZE = 512#1000
CHUNK_OVERLAP = 100#150

# --- API Configuration ---
RAG_SERVICE_PORT = int(os.getenv('RAG_SERVICE_PORT', 5002))

# --- Print effective configuration ---
print(f"FAISS Index Directory: {FAISS_INDEX_DIR}")
print(f"Default Assets Directory (for default.py): {DEFAULT_ASSETS_DIR}")
print(f"RAG Service Port: {RAG_SERVICE_PORT}")
print(f"Default Index User ID: {DEFAULT_INDEX_USER_ID}")
print(f"Chunk Size: {CHUNK_SIZE}, Chunk Overlap: {CHUNK_OVERLAP}")





============ ./default.py ============
import os
import logging
import sys
import traceback

# --- Path Setup ---
current_dir = os.path.dirname(os.path.abspath(__file__))
server_dir = os.path.dirname(current_dir)
project_root_dir = os.path.dirname(server_dir)
sys.path.insert(0, server_dir)
# --- End Path Setup ---

try:
    from rag_service import config
    from rag_service import faiss_handler
    from rag_service import file_parser
except ImportError as e:
     print("ImportError:", e)
     print("Failed to import modules. Ensure the script is run correctly relative to the project structure.")
     print("Current sys.path:", sys.path)
     exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


class DefaultVectorDBBuilder:
    def __init__(self):
        logger.info("Initializing embedding model...")
        try:
            # Use SentenceTransformer as configured in config.py
            self.embed_model = faiss_handler.get_embedding_model()
            if self.embed_model is None:
                 raise RuntimeError("Failed to initialize Sentence Transformer embedding model.")
        except Exception as e:
            logger.error(f"Fatal error initializing embedding model: {e}", exc_info=True)
            raise

        self.chunk_size = config.CHUNK_SIZE
        self.chunk_overlap = config.CHUNK_OVERLAP

        self.default_docs_dir = config.DEFAULT_ASSETS_DIR
        self.index_dir = config.FAISS_INDEX_DIR
        self.default_user_id = config.DEFAULT_INDEX_USER_ID

        self.default_index_user_path = faiss_handler.get_user_index_path(self.default_user_id)
        self.index_file_path = os.path.join(self.default_index_user_path, "index.faiss")
        self.pkl_file_path = os.path.join(self.default_index_user_path, "index.pkl")

        try:
            faiss_handler.ensure_faiss_dir()
            os.makedirs(self.default_index_user_path, exist_ok=True)
        except Exception as e:
             logger.error(f"Failed to create necessary directories: {e}")
             raise

        logger.info(f"Default assets directory: {self.default_docs_dir}")
        logger.info(f"Default index directory: {self.default_index_user_path}")


    def create_default_index(self, force_rebuild=True): # Keep force_rebuild flag
        """Scans default assets, parses files, creates embeddings, and saves the FAISS index."""
        logger.info("--- Starting Default Index Creation ---")

        # --- Force Rebuild Logic ---
        if force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
            logger.warning(f"force_rebuild=True. Deleting existing default index files in {self.default_index_user_path}.")
            try:
                if os.path.exists(self.index_file_path): os.remove(self.index_file_path)
                if os.path.exists(self.pkl_file_path): os.remove(self.pkl_file_path)
                # Clear from cache if loaded
                if self.default_user_id in faiss_handler.loaded_indices:
                    del faiss_handler.loaded_indices[self.default_user_id]
                logger.info("Removed existing default index files and cleared cache.")
            except OSError as e:
                logger.error(f"Error removing existing index files: {e}")
                return False # Stop if we can't remove old files
        elif not force_rebuild and (os.path.exists(self.index_file_path) or os.path.exists(self.pkl_file_path)):
             logger.info("Default index already exists and force_rebuild=False. Skipping creation.")
             # Try loading it to confirm validity
             try:
                 faiss_handler.load_or_create_index(self.default_user_id)
                 logger.info("Existing default index loaded successfully.")
                 return True
             except Exception as load_err:
                 logger.error(f"Failed to load existing default index: {load_err}. Consider running with force_rebuild=True.")
                 return False


        # --- Process Documents ---
        all_documents = []
        files_processed = 0
        files_skipped = 0

        logger.info(f"Scanning for processable files in: {self.default_docs_dir}")
        if not os.path.isdir(self.default_docs_dir):
            logger.error(f"Default assets directory not found: {self.default_docs_dir}")
            return False

        for root, _, files in os.walk(self.default_docs_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                # logger.debug(f"Found file: {filename}")
                try:
                    text_content = file_parser.parse_file(file_path)
                    if text_content and text_content.strip():
                        langchain_docs = file_parser.chunk_text(
                            text_content, filename, self.default_user_id
                        )
                        if langchain_docs:
                            all_documents.extend(langchain_docs)
                            files_processed += 1
                            logger.info(f"Parsed and chunked: {filename} ({len(langchain_docs)} chunks)")
                        else:
                            logger.warning(f"Skipped {filename}: No chunks generated.")
                            files_skipped += 1
                    else:
                        logger.warning(f"Skipped {filename}: No text content or unsupported type.")
                        files_skipped += 1
                except Exception as e:
                    logger.error(f"Error processing file {filename}: {e}")
                    traceback.print_exc()
                    files_skipped += 1

        if not all_documents:
            logger.error(f"No processable documents found or generated in {self.default_docs_dir}. Cannot create index.")
            # Still create an empty index structure if the directory was valid
            try:
                 logger.info("Creating an empty index structure as no documents were found.")
                 faiss_handler.load_or_create_index(self.default_user_id) # Creates empty index
                 logger.info("Empty default index created successfully.")
                 return True # Success, but empty
            except Exception as empty_create_err:
                 logger.error(f"Failed to create empty index structure: {empty_create_err}", exc_info=True)
                 return False


        logger.info(f"Total files processed: {files_processed}, skipped: {files_skipped}")
        logger.info(f"Creating embeddings and adding {len(all_documents)} total chunks to index...")

        try:
            # The load_or_create_index function will handle creating the empty structure
            # if it doesn't exist (or after deletion if force_rebuild=True)
            logger.info("Ensuring FAISS index structure exists...")
            index_instance = faiss_handler.load_or_create_index(self.default_user_id)

            logger.info(f"Adding {len(all_documents)} documents to the default index '{self.default_user_id}'...")
            # Use the updated handler function which now manages IDs correctly
            faiss_handler.add_documents_to_index(self.default_user_id, all_documents)

            # Verify save occurred
            if not os.path.exists(self.index_file_path) or not os.path.exists(self.pkl_file_path):
                 logger.error("Index files were not found after adding documents. Check permissions or disk space.")
                 return False

            logger.info(f"Successfully created/updated and saved default index ({self.default_user_id}) with {len(all_documents)} document chunks.")
            logger.info("--- Default Index Creation Finished ---")
            return True

        except Exception as e:
            logger.error(f"Failed during embedding or index creation: {e}", exc_info=True)
            logger.error("--- Default Index Creation Failed ---")
            return False

def main():
    print("--- Running Default Index Builder ---")
    try:
        builder = DefaultVectorDBBuilder()
    except Exception as init_err:
        print(f"FATAL: Failed to initialize builder: {init_err}")
        sys.exit(1)

    if not os.path.isdir(builder.default_docs_dir):
         logger.error(f"Default assets directory '{builder.default_docs_dir}' is missing.")
         sys.exit(1)

    # --- Always force rebuild as requested ---
    force = True
    logger.info(f"Starting index creation (force_rebuild={force})...")
    if not builder.create_default_index(force_rebuild=force):
        logger.error("Index creation process failed.")
        sys.exit(1)
    else:
        logger.info("Default index creation process completed successfully.")
        sys.exit(0)

if __name__ == "__main__":
    main()



============ ./faiss_handler.py ============
# server/rag_service/faiss_handler.py

import os
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.embeddings import Embeddings as LangchainEmbeddings
from langchain_core.documents import Document as LangchainDocument
from langchain_community.docstore import InMemoryDocstore
from rag_service import config
import numpy as np
import time
import logging
import pickle
import uuid
import shutil # Import shutil for removing directories

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)

embedding_model: LangchainEmbeddings | None = None
loaded_indices = {}
_embedding_dimension = None # Cache the dimension

def get_embedding_dimension(embedder: LangchainEmbeddings) -> int:
    """Gets and caches the embedding dimension."""
    global _embedding_dimension
    if _embedding_dimension is None:
        try:
            logger.info("Determining embedding dimension...")
            dummy_embedding = embedder.embed_query("dimension_check")
            dimension = len(dummy_embedding)
            if not isinstance(dimension, int) or dimension <= 0:
                raise ValueError(f"Invalid embedding dimension obtained: {dimension}")
            _embedding_dimension = dimension
            logger.info(f"Detected embedding dimension: {_embedding_dimension}")
        except Exception as e:
            logger.error(f"CRITICAL ERROR determining embedding dimension: {e}", exc_info=True)
            raise RuntimeError(f"Failed to determine embedding dimension: {e}")
    return _embedding_dimension

def get_embedding_model():
    global embedding_model
    if embedding_model is None:
        if config.EMBEDDING_TYPE == 'sentence-transformer':
            logger.info(f"Initializing HuggingFace Embeddings for Sentence Transformer (Model: {config.EMBEDDING_MODEL_NAME})")
            try:
                # Try CUDA first, fallback to CPU
                try:
                    if faiss.get_num_gpus() > 0:
                        device = 'cuda'
                        logger.info("CUDA detected. Using GPU for embeddings.")
                    else:
                        raise RuntimeError("No GPU found") # Force fallback
                except Exception:
                    device = 'cpu'
                    logger.warning("CUDA not available or GPU check failed. Using CPU for embeddings. This might be slow.")

                embedding_model = HuggingFaceEmbeddings(
                    model_name=config.EMBEDDING_MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True} # Often recommended for cosine similarity / MIPS with FAISS
                )
                # Determine and cache dimension on successful load
                get_embedding_dimension(embedding_model)

                logger.info("Testing embedding function...")
                test_embedding_doc = embedding_model.embed_documents(["test document"])
                test_embedding_query = embedding_model.embed_query("test query")
                if not test_embedding_doc or not test_embedding_query:
                    raise ValueError("Embedding test failed, returned empty results.")
                logger.info(f"Embedding test successful.")

            except Exception as e:
                logger.error(f"Error loading HuggingFace Embeddings for '{config.EMBEDDING_MODEL_NAME}': {e}", exc_info=True)
                embedding_model = None
                raise RuntimeError(f"Failed to load embedding model: {e}")
        else:
            raise ValueError(f"Unsupported embedding type in config: {config.EMBEDDING_TYPE}. Expected 'sentence-transformer'.")
    return embedding_model

def get_user_index_path(user_id):
    safe_user_id = str(user_id).replace('.', '_').replace('/', '_').replace('\\', '_')
    user_dir = os.path.join(config.FAISS_INDEX_DIR, f"user_{safe_user_id}")
    return user_dir

def _delete_index_files(index_path, user_id):
    """Safely deletes index files for a user."""
    logger.warning(f"Deleting potentially incompatible index files for user '{user_id}' at {index_path}")
    try:
        if os.path.isdir(index_path):
            shutil.rmtree(index_path)
            logger.info(f"Successfully deleted directory: {index_path}")
        # If only loose files exist (less likely with save_local)
        index_file = os.path.join(index_path, "index.faiss")
        pkl_file = os.path.join(index_path, "index.pkl")
        if os.path.exists(index_file): os.remove(index_file)
        if os.path.exists(pkl_file): os.remove(pkl_file)
    except OSError as e:
        logger.error(f"Error deleting index files/directory for user '{user_id}' at {index_path}: {e}", exc_info=True)
        # Don't raise here, allow fallback to creating new index if possible

def load_or_create_index(user_id):
    global loaded_indices
    if user_id in loaded_indices:
        # **Even if cached, re-verify dimension on subsequent loads in case model changed**
        index = loaded_indices[user_id]
        embedder = get_embedding_model() # Ensure model is loaded
        current_dim = get_embedding_dimension(embedder)
        if hasattr(index, 'index') and index.index is not None and index.index.d != current_dim:
            logger.warning(f"Cached index for user '{user_id}' has dimension {index.index.d}, but current model has dimension {current_dim}. Discarding cache and forcing reload/recreate.")
            del loaded_indices[user_id] # Remove from cache
            # Fall through to load/create logic below
        else:
            logger.debug(f"Returning cached index for user '{user_id}'.")
            return index # Return cached and verified index

    index_path = get_user_index_path(user_id)
    index_file = os.path.join(index_path, "index.faiss")
    pkl_file = os.path.join(index_path, "index.pkl")

    embedder = get_embedding_model()
    if embedder is None:
        raise RuntimeError("Embedding model is not available.")
    current_embedding_dim = get_embedding_dimension(embedder)

    force_recreate = False
    if os.path.exists(index_file) and os.path.exists(pkl_file):
        logger.info(f"Attempting to load existing FAISS index for user '{user_id}' from {index_path}")
        try:
            start_time = time.time()
            # Temporarily load to check dimension
            index = FAISS.load_local(
                folder_path=index_path,
                embeddings=embedder,
                allow_dangerous_deserialization=True # Use with caution if index source isn't trusted
            )
            end_time = time.time()

            # --- CRITICAL DIMENSION CHECK ---
            if not hasattr(index, 'index') or index.index is None:
                 logger.warning(f"Loaded index for user '{user_id}' has no 'index' attribute or it's None. Forcing recreation.")
                 force_recreate = True
            elif index.index.d != current_embedding_dim:
                logger.warning(f"DIMENSION MISMATCH! Index for user '{user_id}' has dimension {index.index.d}, but current embedding model has dimension {current_embedding_dim}. Index is incompatible and will be recreated.")
                force_recreate = True
            elif index.index.ntotal == 0:
                logger.info(f"Loaded index for user '{user_id}' is empty (0 vectors). Will use it but note it's empty.")
                 # Not forcing recreate, just noting it's empty
            # --- END DIMENSION CHECK ---

            if force_recreate:
                _delete_index_files(index_path, user_id)
                # Don't return the incompatible index, fall through to create new one
            else:
                # If dimensions match and index is valid
                logger.info(f"Index for user '{user_id}' loaded successfully in {end_time - start_time:.2f} seconds. Dimension ({index.index.d}) matches. Contains {index.index.ntotal} vectors.")
                loaded_indices[user_id] = index
                return index

        except (pickle.UnpicklingError, EOFError, ModuleNotFoundError, AttributeError, ValueError) as load_err:
            logger.error(f"Error loading index for user '{user_id}' from {index_path}: {load_err}")
            logger.warning("Index files might be corrupted or incompatible. Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs
        except Exception as e:
            logger.error(f"Unexpected error loading index for user '{user_id}': {e}", exc_info=True)
            logger.warning("Attempting to delete and create a new index instead.")
            _delete_index_files(index_path, user_id)
            force_recreate = True # Ensure recreation logic runs

    # --- Create New Index Logic ---
    # This block runs if files didn't exist OR force_recreate is True
    logger.info(f"Creating new FAISS index structure for user '{user_id}' at {index_path} with dimension {current_embedding_dim}")
    try:
        # Ensure directory exists (it might have been deleted)
        os.makedirs(index_path, exist_ok=True)

        # Use the already determined dimension
        # Use IndexFlatIP if embeddings are normalized (recommended)
        faiss_index = faiss.IndexIDMap(faiss.IndexFlatIP(current_embedding_dim))
        # faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(current_embedding_dim)) # Use L2 if not normalized

        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}

        index = FAISS(
            embedding_function=embedder,
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            normalize_L2=False # Set True if using IndexFlatIP and normalized embeddings (which we are with encode_kwargs)
        )

        logger.info(f"Initialized empty index structure for user '{user_id}'.")
        loaded_indices[user_id] = index # Add to cache immediately
        save_index(user_id) # Save the empty structure
        logger.info(f"New empty index for user '{user_id}' created and saved.")
        return index
    except Exception as e:
        logger.error(f"CRITICAL ERROR creating new index for user '{user_id}': {e}", exc_info=True)
        if user_id in loaded_indices:
            del loaded_indices[user_id] # Clean up cache on failure
        # Attempt to clean up directory if creation failed badly
        _delete_index_files(index_path, user_id)
        raise RuntimeError(f"Failed to initialize FAISS index for user '{user_id}'")


def add_documents_to_index(user_id, documents: list[LangchainDocument]):
    if not documents:
        logger.warning(f"No documents provided to add for user '{user_id}'.")
        return

    try:
        index = load_or_create_index(user_id) # This now handles dimension checks/recreation
        embedder = get_embedding_model() # Ensure model is loaded

        # --- VERIFY DIMENSIONS AGAIN before adding (paranoid check) ---
        current_dim = get_embedding_dimension(embedder)
        if not hasattr(index, 'index') or index.index is None:
             logger.error(f"Index object for user '{user_id}' is invalid after load/create. Cannot add documents.")
             raise RuntimeError("Failed to get valid index structure.")
        if index.index.d != current_dim:
             logger.error(f"FATAL: Dimension mismatch just before adding documents for user '{user_id}'. Index: {index.index.d}, Model: {current_dim}. This shouldn't happen if load_or_create_index worked.")
             # Attempt recovery by deleting and trying again? Risky loop potential.
             _delete_index_files(get_user_index_path(user_id), user_id)
             if user_id in loaded_indices: del loaded_indices[user_id]
             raise RuntimeError(f"Inconsistent index dimension detected for user '{user_id}'. Please retry.")
        # --- END VERIFY ---

        logger.info(f"Adding {len(documents)} documents to index for user '{user_id}' (Index dim: {index.index.d})...")
        start_time = time.time()

        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        # Generate embeddings using the current model
        embeddings = embedder.embed_documents(texts)
        if not embeddings or len(embeddings) != len(texts):
             logger.error(f"Embedding generation failed or returned unexpected number of vectors for user '{user_id}'.")
             raise ValueError("Embedding generation failed.")
        if len(embeddings[0]) != current_dim:
             logger.error(f"Generated embeddings have incorrect dimension ({len(embeddings[0])}) for user '{user_id}', expected {current_dim}.")
             raise ValueError("Generated embedding dimension mismatch.")

        embeddings_np = np.array(embeddings, dtype=np.float32)

        # Generate unique IDs for FAISS
        ids = [str(uuid.uuid4()) for _ in texts]
        ids_np = np.array([uuid.UUID(id_).int & (2**63 - 1) for id_ in ids], dtype=np.int64)


        # Add embeddings and their corresponding IDs to the FAISS index
        index.index.add_with_ids(embeddings_np, ids_np)

        # Add the original documents and their metadata to the Langchain Docstore,
        # using the generated string UUIDs as keys.
        # Map the FAISS integer ID back to the string UUID used in the docstore.
        docstore_additions = {doc_id: doc for doc_id, doc in zip(ids, documents)}
        index.docstore.add(docstore_additions)
        for i, faiss_id in enumerate(ids_np):
            index.index_to_docstore_id[int(faiss_id)] = ids[i] # Map FAISS int ID -> string UUID

        end_time = time.time()
        logger.info(f"Successfully added {len(documents)} vectors/documents for user '{user_id}' in {end_time - start_time:.2f} seconds. Total vectors: {index.index.ntotal}")
        save_index(user_id)
    except Exception as e:
        logger.error(f"Error adding documents for user '{user_id}': {e}", exc_info=True)
        # Don't re-raise here if app.py handles it, but ensure logging is clear
        raise # Re-raise the exception so app.py can catch it and return 500

def query_index(user_id, query_text, k=3):
    all_results_with_scores = []
    embedder = get_embedding_model()

    if embedder is None:
        logger.error("Embedding model is not available for query.")
        raise ConnectionError("Embedding model is not available for query.")

    try:
        start_time = time.time()
        user_index = None # Initialize to None
        default_index = None # Initialize to None

        # Query User Index
        try:
            user_index = load_or_create_index(user_id) # Assign to user_index
            if hasattr(user_index, 'index') and user_index.index is not None and user_index.index.ntotal > 0:
                logger.info(f"Querying index for user: '{user_id}' (Dim: {user_index.index.d}, Vectors: {user_index.index.ntotal}) with k={k}")
                user_results = user_index.similarity_search_with_score(query_text, k=k)
                logger.info(f"User index '{user_id}' query returned {len(user_results)} results.")
                all_results_with_scores.extend(user_results)
            else:
                logger.info(f"Skipping query for user '{user_id}': Index is empty or invalid.")
        except FileNotFoundError:
            logger.warning(f"User index files for '{user_id}' not found on disk (might be first time). Skipping query for this index.")
        except RuntimeError as e:
            logger.error(f"Could not load or create user index for '{user_id}': {e}", exc_info=True)
        except Exception as e:
            logger.error(f"Unexpected error querying user index for '{user_id}': {e}", exc_info=True)


        # Query Default Index (if different from user_id)
        if user_id != config.DEFAULT_INDEX_USER_ID:
            try:
                default_index = load_or_create_index(config.DEFAULT_INDEX_USER_ID) # Assign to default_index
                if hasattr(default_index, 'index') and default_index.index is not None and default_index.index.ntotal > 0:
                    logger.info(f"Querying default index '{config.DEFAULT_INDEX_USER_ID}' (Dim: {default_index.index.d}, Vectors: {default_index.index.ntotal}) with k={k}")
                    default_results = default_index.similarity_search_with_score(query_text, k=k)
                    logger.info(f"Default index '{config.DEFAULT_INDEX_USER_ID}' query returned {len(default_results)} results.")
                    all_results_with_scores.extend(default_results)
                else:
                    logger.info(f"Skipping query for default index '{config.DEFAULT_INDEX_USER_ID}': Index is empty or invalid.")
            except FileNotFoundError:
                 logger.warning(f"Default index '{config.DEFAULT_INDEX_USER_ID}' not found on disk (run default.py?). Skipping query.")
            except RuntimeError as e:
                logger.error(f"Could not load or create default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)
            except Exception as e:
                logger.error(f"Unexpected error querying default index '{config.DEFAULT_INDEX_USER_ID}': {e}", exc_info=True)

        query_time = time.time()
        logger.info(f"Completed all index queries in {query_time - start_time:.2f} seconds. Found {len(all_results_with_scores)} raw results.")

        # --- Deduplication and Sorting ---
        unique_results = {}
        for doc, score in all_results_with_scores:
            if not doc or not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                logger.warning(f"Skipping invalid document object in results: {doc}")
                continue

            # Use the fallback content-based key
            content_key = f"{doc.metadata.get('documentName', 'Unknown')}_{doc.page_content[:200]}"
            unique_key = content_key # Use the content key directly

            # Add or update if the new score is better (lower for L2 distance / IP distance if normalized)
            if unique_key not in unique_results or score < unique_results[unique_key][1]:
                unique_results[unique_key] = (doc, score)

        # Sort by score (ascending for L2 distance / IP distance)
        sorted_results = sorted(unique_results.values(), key=lambda item: item[1])
        final_results = sorted_results[:k] # Get top k unique results

        logger.info(f"Returning {len(final_results)} unique results after filtering and sorting.")
        return final_results
    except Exception as e:
        logger.error(f"Error during query processing for user '{user_id}': {e}", exc_info=True)
        return [] # Return empty list on error


def save_index(user_id):
    global loaded_indices
    if user_id not in loaded_indices:
        logger.warning(f"Index for user '{user_id}' not found in cache, cannot save.")
        return

    index = loaded_indices[user_id]
    index_path = get_user_index_path(user_id)

    if not isinstance(index, FAISS) or not hasattr(index, 'index') or not hasattr(index, 'docstore') or not hasattr(index, 'index_to_docstore_id'):
        logger.error(f"Cannot save index for user '{user_id}': Invalid index object in cache.")
        return

    # Ensure the target directory exists before saving
    try:
        os.makedirs(index_path, exist_ok=True)
        logger.info(f"Saving FAISS index for user '{user_id}' to {index_path} (Vectors: {index.index.ntotal if hasattr(index.index, 'ntotal') else 'N/A'})...")
        start_time = time.time()
        # This saves index.faiss and index.pkl
        index.save_local(folder_path=index_path)
        end_time = time.time()
        logger.info(f"Index for user '{user_id}' saved successfully in {end_time - start_time:.2f} seconds.")
    except Exception as e:
        logger.error(f"Error saving FAISS index for user '{user_id}' to {index_path}: {e}", exc_info=True)

# --- ADD THIS FUNCTION DEFINITION BACK ---
def ensure_faiss_dir():
    """Ensures the base FAISS index directory exists."""
    try:
        os.makedirs(config.FAISS_INDEX_DIR, exist_ok=True)
        logger.info(f"Ensured FAISS base directory exists: {config.FAISS_INDEX_DIR}")
    except OSError as e:
        logger.error(f"Could not create FAISS base directory {config.FAISS_INDEX_DIR}: {e}")
        raise # Raise the error to prevent startup if dir creation fails
# --- END OF ADDED FUNCTION ---



============ ./file_parser.py ============
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error



============ ./generate_code.sh ============
#!/bin/bash

# Output file
OUTPUT_FILE="code.txt"

# Clear previous output
> "$OUTPUT_FILE"

# Traverse files and directories, excluding __pycache__ and default.faiss
find . -type f \
    ! -path "*/__pycache__/*" \
    ! -name "*.pyc" \
    ! -name "default.faiss" \
    | while read file; do
        echo "============ $file ============" >> "$OUTPUT_FILE"
        cat "$file" >> "$OUTPUT_FILE"
        echo -e "\n\n" >> "$OUTPUT_FILE"
    done

echo "âœ… Code has been saved to $OUTPUT_FILE"






============ ./process_document.py ============
# process_documents.py

import os
import io # For BytesIO
import logging # For logging
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any # Added Any, Tuple
from PIL import Image, ImageFilter
import pytesseract
import fitz # PyMuPDF
import pypdf # For PDF parsing
from docx import Document as DocxDocument
from pptx import Presentation
import PyPDF2 # For basic PDF metadata
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument

# --- Logger Setup ---
# Configure logging to output to console. You can customize this for file logging.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# --- NLTK Downloads (Run once if not already downloaded) ---
# Moved to a function for clarity, can be called at app startup
def download_nltk_resources():
    """Downloads necessary NLTK resources if not already present."""
    nltk_resources = {
        'punkt': 'tokenizers/punkt',
        'stopwords': 'corpora/stopwords',
        'wordnet': 'corpora/wordnet',
        'omw-1.4': 'corpora/omw-1.4' # WordNet 1.4 ontology
    }
    for res_name, res_path in nltk_resources.items():
        try:
            nltk.data.find(res_path)
            logger.info(f"NLTK resource '{res_name}' found.")
        except nltk.downloader.DownloadError:
            logger.info(f"NLTK resource '{res_name}' not found. Downloading...")
            nltk.download(res_name)
            logger.info(f"NLTK resource '{res_name}' downloaded.")

# --- SpaCy Model Download (Run once if not already downloaded) ---
def download_spacy_model(model_name: str = "en_core_web_sm"):
    """Downloads a spaCy model if not already present."""
    try:
        spacy.load(model_name)
        logger.info(f"SpaCy model '{model_name}' found.")
    except OSError:
        logger.warning(f"SpaCy model '{model_name}' not found. Attempting download...")
        try:
            os.system(f"python -m spacy download {model_name}")
            logger.info(f"SpaCy model '{model_name}' downloaded successfully. Please restart the application if it was just installed.")
            # Note: A restart might be needed for the new model to be picked up by the current Python process
            # For production, ensure models are pre-installed in the environment.
        except Exception as e:
            logger.error(f"Failed to download spaCy model '{model_name}': {e}")
            logger.error("Please install it manually: python -m spacy download en_core_web_sm")


# --- Perform Resource Downloads (Call once at application startup) ---
# In a real application, this would be part of an initialization script or a startup hook.
download_nltk_resources()
download_spacy_model()


# --- Global NLP Models (Load once) ---
logger.info("Loading global NLP models...")
try:
    nlp_core = spacy.load("en_core_web_sm")
    stop_words = set(stopwords.words('english'))
    porter_stemmer = PorterStemmer() # Not used in current clean_and_normalize_text, but kept for completeness
    wordnet_lemmatizer = WordNetLemmatizer()
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    logger.info("Global NLP models loaded successfully.")
except Exception as e:
    logger.error(f"Fatal error loading global NLP models: {e}")
    # In a real app, you might want to exit or prevent further operations if models can't load.
    raise  # Re-raise the exception to halt execution if critical models fail

# --- Import pdfplumber after other initializations ---
# pdfplumber can sometimes have import-time issues if dependencies are not perfectly met.
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
    logger.info("pdfplumber imported successfully.")
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("pdfplumber not found. Rich PDF content extraction (tables, layout) will be limited. Install with: pip install pdfplumber")


# --- File Parsing Functions ---

def parse_pdf_simple(file_path: str) -> Optional[str]:
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf:
        logger.error("pypdf library not found. PDF parsing will fail.")
        return None
    text_content = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        logger.info(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            except Exception as page_err:
                logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        logger.info(f"Extracted {len(text_content)} characters from PDF (pypdf).")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)} with pypdf: {e}")
        return None

def parse_docx(file_path: str) -> Optional[str]:
    """Extracts text content from a DOCX file."""
    if not DocxDocument:
        logger.error("python-docx library not found. DOCX parsing will fail.")
        return None
    try:
        doc = DocxDocument(file_path)
        text_content = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        logger.info(f"Extracted {len(text_content)} characters from DOCX: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"DOCX file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}")
        return None

def parse_txt(file_path: str) -> Optional[str]:
    """Reads text content from a TXT file (or similar plain text)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text_content = f.read()
        logger.info(f"Read {len(text_content)} characters from TXT file: {os.path.basename(file_path)}")
        return text_content.strip() if text_content.strip() else None
    except FileNotFoundError:
        logger.error(f"TXT file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}")
        return None

# Conditional PPTX parsing
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path: str) -> Optional[str]:
        """Extracts text content from a PPTX file."""
        text_content = ""
        try:
            prs = Presentation(file_path)
            for slide_num, slide in enumerate(prs.slides):
                slide_text_parts = []
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            slide_text_parts.append(shape_text)
                if slide_text_parts:
                    text_content += "\n".join(slide_text_parts) + "\n\n" # Newline between shapes, double for new slide
            logger.info(f"Extracted {len(text_content)} characters from PPTX: {os.path.basename(file_path)}")
            return text_content.strip() if text_content.strip() else None
        except FileNotFoundError:
            logger.error(f"PPTX file not found: {file_path}")
            return None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}")
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped. Install with: pip install python-pptx")
    def parse_pptx(file_path: str) -> Optional[str]: # type: ignore
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path: str) -> Optional[str]:
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.info(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf_simple(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path)
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']:
        return parse_txt(file_path)
    elif ext == '.doc':
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

# --- Core Processing Functions ---

def extract_raw_content(file_path: str) -> Dict[str, Any]:
    """
    1. Extracts raw text, tables, images, and basic layout info from a document.
    Determines if the document (if PDF) is likely scanned.
    """
    logger.info(f"Starting raw content extraction for: {os.path.basename(file_path)}")
    text_content: str = ""
    tables: List[Any] = [] # Can be list of DataFrames or list of lists
    images: List[Image.Image] = []
    layout_info: List[Dict[str, Any]] = []
    is_scanned: bool = False
    file_extension: str = os.path.splitext(file_path)[1].lower()

    initial_text = parse_file(file_path) # Uses the simple parsers for a baseline
    if initial_text:
        text_content = initial_text
    else:
        logger.warning(f"Could not extract initial text from {file_path}. Proceeding with empty text.")
        # Still return structure, even if empty
        return {
            'text_content': "", 'tables': tables, 'images': images,
            'layout_info': layout_info, 'is_scanned': False, 'file_type': file_extension
        }

    # If it's a PDF, try to extract richer content
    if file_extension == '.pdf':
        if not PDFPLUMBER_AVAILABLE:
            logger.warning("pdfplumber is not available. Rich PDF extraction (tables, layout) will be skipped.")
        else:
            try:
                with pdfplumber.open(file_path) as pdf:
                    pdfplumber_text_check_parts = []
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            pdfplumber_text_check_parts.append(page_text)
                    pdfplumber_text_check = "".join(pdfplumber_text_check_parts)

                    # Heuristic for scanned PDF: if pdfplumber extracts very little text
                    # compared to page count or absolute minimum.
                    # This assumes non-scanned PDFs have a reasonable amount of selectable text.
                    # Adjust thresholds as needed.
                    min_chars_per_page = 50 # Heuristic: expect at least this many chars per page if not scanned
                    absolute_min_chars = 200 # Heuristic: expect at least this many chars total if not scanned

                    if len(pdf.pages) > 0 and \
                       (len(pdfplumber_text_check) < min_chars_per_page * len(pdf.pages) and \
                        len(pdfplumber_text_check) < absolute_min_chars):
                        is_scanned = True
                        logger.info(f"PDF {os.path.basename(file_path)} detected as potentially scanned based on low text content from pdfplumber.")

                    full_pdfplumber_text = ""
                    for page_num, page in enumerate(pdf.pages):
                        page_text_with_layout = page.extract_text(x_tolerance=1, y_tolerance=1, layout=False) # layout=True can be slow
                        if page_text_with_layout:
                            full_pdfplumber_text += page_text_with_layout + "\n"

                        # Capture basic layout info (bounding boxes of text lines/words)
                        # This can be very verbose, enable if detailed layout is critical
                        # for char_obj in page.chars:
                        #     layout_info.append({
                        #         'char': char_obj['text'],
                        #         'bbox': (char_obj['x0'], char_obj['top'], char_obj['x1'], char_obj['bottom']),
                        #         'page': page_num
                        #     })

                        # Extract tables using pdfplumber
                        page_tables_data = page.extract_tables()
                        if page_tables_data:
                            for table_data in page_tables_data:
                                if table_data and len(table_data) > 0: # Ensure there's data
                                    # Check if first row looks like a header (heuristic)
                                    if len(table_data) > 1 and all(isinstance(cell, str) for cell in table_data[0]):
                                        try:
                                            tables.append(pd.DataFrame(table_data[1:], columns=table_data[0]))
                                        except Exception as df_err:
                                            logger.warning(f"Could not convert table to DataFrame on page {page_num} (using header): {df_err}. Appending raw list.")
                                            tables.append(table_data)
                                    else: # No clear header or single row table
                                        tables.append(table_data) # Append raw list
                    
                    # Prioritize pdfplumber's text if it's richer
                    if len(full_pdfplumber_text.strip()) > len(text_content):
                        logger.info("Using text extracted by pdfplumber as it's more comprehensive.")
                        text_content = full_pdfplumber_text.strip()

            except Exception as e:
                logger.warning(f"Error during rich PDF content extraction with pdfplumber for {os.path.basename(file_path)}: {e}")
                # Fallback to initial_text if pdfplumber fails badly

        # Extract images using PyMuPDF (fitz) - generally more robust for images
        try:
            doc = fitz.open(file_path)
            for page_idx in range(len(doc)):
                for img_info in doc.get_page_images(page_idx):
                    xref = img_info[0] # XREF of the image
                    try:
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        pil_image = Image.open(io.BytesIO(image_bytes))
                        images.append(pil_image)
                    except Exception as img_err:
                        logger.warning(f"Could not open image xref {xref} from page {page_idx} of {os.path.basename(file_path)}: {img_err}")
            doc.close()
            logger.info(f"Extracted {len(images)} images using PyMuPDF from {os.path.basename(file_path)}.")
        except Exception as e:
            logger.warning(f"Error extracting images with PyMuPDF from {os.path.basename(file_path)}: {e}")

    logger.info(f"Raw content extraction complete for {os.path.basename(file_path)}. Text length: {len(text_content)}, Tables: {len(tables)}, Images: {len(images)}, Scanned: {is_scanned}")
    return {
        'text_content': text_content,
        'tables': tables,
        'images': images,
        'layout_info': layout_info, # Often empty for non-PDFs or if detailed layout extraction is off
        'is_scanned': is_scanned,   # Primarily relevant for PDFs
        'file_type': file_extension
    }

def perform_ocr(image_data: List[Image.Image]) -> str:
    """
    2. Processes images to extract text using OCR (pytesseract).
    """
    if not image_data:
        logger.info("No images provided for OCR.")
        return ""

    logger.info(f"Performing OCR on {len(image_data)} image(s).")
    ocr_text_parts: List[str] = []
    for i, img in enumerate(image_data):
        try:
            # Basic preprocessing
            img_gray = img.convert('L')
            # img_binary = img_gray.point(lambda x: 0 if x < 150 else 255, '1') # Example threshold
            # img_filtered = img_binary.filter(ImageFilter.MedianFilter(size=3)) # Example filter

            # Tesseract works well with grayscale. Extensive preprocessing can sometimes harm.
            # Test various preprocessing steps for your specific image types.
            text = pytesseract.image_to_string(img_gray) # Using grayscale
            if text.strip():
                ocr_text_parts.append(text.strip())
            logger.debug(f"OCR successful for image {i+1}.")
        except pytesseract.TesseractNotFoundError:
            logger.error("Tesseract is not installed or not in your PATH. OCR will fail.")
            # This is a critical error, so perhaps re-raise or handle at a higher level
            raise
        except Exception as e:
            logger.error(f"Error during OCR for image {i+1}: {e}")
    
    full_ocr_text = "\n\n".join(ocr_text_parts)
    logger.info(f"OCR process completed. Extracted {len(full_ocr_text)} characters.")
    return full_ocr_text

def clean_and_normalize_text(text: str) -> str:
    """
    3. Cleans and normalizes text for NLP readiness.
    """
    if not text:
        logger.info("No text provided for cleaning and normalization.")
        return ""
    logger.info(f"Starting text cleaning and normalization. Initial length: {len(text)}")

    # 1. Noise Removal
    text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags more robustly
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # Remove URLs
    text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses
    # Keep essential punctuation for sentence structure, remove others
    text = re.sub(r'[^a-zA-Z0-9\s.,!?-]', '', text) # Keep alphanumeric, spaces, and basic punctuation
    text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

    # 2. Case Normalization
    text = text.lower()

    # 3. Tokenization (NLTK)
    try:
        tokens = nltk.word_tokenize(text)
    except Exception as e:
        logger.error(f"NLTK word_tokenize failed: {e}. Falling back to simple split.")
        tokens = text.split()


    # 4. Stop Word Removal
    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1] # Also remove single chars

    # 5. Lemmatization
    try:
        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in filtered_tokens]
    except Exception as e:
        logger.error(f"WordNet lemmatization failed: {e}. Using filtered tokens directly.")
        lemmatized_tokens = filtered_tokens

    cleaned_text = " ".join(lemmatized_tokens)
    logger.info(f"Text cleaning and normalization complete. Final length: {len(cleaned_text)}")
    return cleaned_text

def reconstruct_layout(original_text: str, layout_info: List[Dict[str, Any]], tables: List[Any], file_type: str) -> str:
    """
    4. Integrates tables into text. Advanced layout reconstruction is complex and
       currently simplified here. Focuses on table integration.
    """
    logger.info("Starting layout reconstruction (primarily table integration).")
    processed_text = original_text

    # De-hyphenation (simple regex-based approach)
    processed_text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', processed_text)
    processed_text = re.sub(r'(\w+)-\s*(\w+)', r'\1\2', processed_text) # For hyphens not at line end

    # Integrate tables as Markdown strings
    table_text_representations: List[str] = []
    if tables:
        logger.info(f"Integrating {len(tables)} tables into the text.")
        for i, table_content in enumerate(tables):
            table_header = f"\n\n--- Table {i+1} Content ---\n"
            table_footer = "\n--- End of Table Content ---\n"
            table_md = ""
            if isinstance(table_content, pd.DataFrame):
                try:
                    table_md = table_content.to_markdown(index=False)
                except Exception as e:
                    logger.warning(f"Could not convert DataFrame table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            elif isinstance(table_content, list): # Raw list of lists from pdfplumber
                # Attempt to format as a simple markdown table
                try:
                    if table_content and isinstance(table_content[0], list):
                        # Assuming first row could be header
                        header = table_content[0]
                        data_rows = table_content[1:]
                        table_md = "| " + " | ".join(map(str, header)) + " |\n"
                        table_md += "| " + " | ".join(["---"] * len(header)) + " |\n"
                        for row in data_rows:
                            table_md += "| " + " | ".join(map(str, row)) + " |\n"
                    else: # Flat list or other structure
                        table_md = "\n".join(map(str,table_content))

                except Exception as e:
                    logger.warning(f"Could not format list-based table {i+1} to markdown: {e}. Using string representation.")
                    table_md = str(table_content)
            else:
                table_md = str(table_content)

            table_text_representations.append(table_header + table_md + table_footer)
        
        # Append all table representations at the end of the document text
        processed_text += "\n\n" + "\n\n".join(table_text_representations)
    
    logger.info("Layout reconstruction (table integration) complete.")
    return processed_text.strip()


def extract_metadata(file_path: str, processed_text: str, file_type_from_extraction: str) -> Dict[str, Any]:
    """
    5. Extracts standard and content-based metadata.
    """
    logger.info(f"Starting metadata extraction for: {os.path.basename(file_path)}")
    metadata: Dict[str, Any] = {}
    file_extension = os.path.splitext(file_path)[1].lower()
    if not file_extension: # If somehow extension is empty, use the one from raw content
        file_extension = file_type_from_extraction

    # Common metadata
    metadata['file_name'] = os.path.basename(file_path)
    metadata['file_path'] = file_path # Store full path for reference
    metadata['file_type'] = file_extension
    try:
        metadata['file_size_bytes'] = os.path.getsize(file_path)
        metadata['creation_time_epoch'] = os.path.getctime(file_path)
        metadata['modification_time_epoch'] = os.path.getmtime(file_path)
    except FileNotFoundError:
        logger.error(f"File not found during metadata extraction (size/time): {file_path}")
    except Exception as e:
        logger.warning(f"Could not get file system metadata for {file_path}: {e}")

    # File-type specific metadata
    page_count_approx = 0
    if file_extension == '.pdf':
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                doc_info = reader.metadata
                if doc_info:
                    metadata['pdf_title'] = doc_info.get('/Title', '').strip()
                    metadata['pdf_author'] = doc_info.get('/Author', '').strip()
                    # Add more as needed from PyPDF2
                page_count_approx = len(reader.pages)
        except Exception as e:
            logger.warning(f"Error extracting PDF-specific metadata (PyPDF2): {e}")
    elif file_extension == '.docx':
        try:
            doc = DocxDocument(file_path)
            # python-docx doesn't easily give page count. Paragraph count is a rough proxy.
            page_count_approx = sum(1 for _ in doc.paragraphs) # Number of paragraphs
            # Core properties can be accessed if set
            # metadata['docx_author'] = doc.core_properties.author
            # metadata['docx_title'] = doc.core_properties.title
        except Exception as e:
            logger.warning(f"Error extracting DOCX-specific metadata: {e}")
    elif file_extension == '.pptx' and PPTX_SUPPORTED:
        try:
            prs = Presentation(file_path)
            page_count_approx = len(prs.slides) # Number of slides
        except Exception as e:
            logger.warning(f"Error extracting PPTX-specific metadata: {e}")
    
    if not page_count_approx and processed_text: # Fallback for TXT or if above failed
        page_count_approx = processed_text.count('\n\n') + 1 # Approx by double newlines for "pages"

    metadata['approx_page_count'] = page_count_approx

    # Content-based metadata using spaCy NER
    if processed_text and nlp_core:
        logger.info("Extracting named entities using spaCy...")
        try:
            doc_nlp = nlp_core(processed_text[:1000000]) # Limit NER processing length for performance
            entities: Dict[str, List[str]] = {label: [] for label in nlp_core.pipe_labels.get("ner", [])}
            for ent in doc_nlp.ents:
                if ent.label_ not in entities: # Should not happen if initialized above
                    entities[ent.label_] = []
                entities[ent.label_].append(ent.text)
            # Filter out empty entity types
            metadata['named_entities'] = {k: v for k, v in entities.items() if v}
            logger.info(f"Extracted {sum(len(v) for v in metadata['named_entities'].values())} named entities.")
        except Exception as e:
            logger.error(f"Error during spaCy NER processing: {e}")
            metadata['named_entities'] = {}
    else:
        metadata['named_entities'] = {}
        logger.info("Skipping NER (no text or nlp_core not available).")

    logger.info(f"Metadata extraction complete for {os.path.basename(file_path)}.")
    return metadata

def chunk_text(text_to_chunk: str, base_metadata: Dict[str, Any], chunk_size: int = 512, chunk_overlap: int = 64) -> List[Dict[str, Any]]:
    """
    6. Divides text into chunks with associated metadata using RecursiveCharacterTextSplitter.
    """
    if not text_to_chunk or not isinstance(text_to_chunk, str):
        logger.warning(f"Invalid or empty text input for chunking (file: {base_metadata.get('file_name', 'unknown')}). Skipping chunking.")
        return []

    logger.info(f"Starting text chunking for {base_metadata.get('file_name', 'unknown')}. Chunk size: {chunk_size}, Overlap: {chunk_overlap}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""], # More robust separators
        keep_separator=True # Keep separators to maintain context
    )

    # Langchain's create_documents expects a list of texts, or can take metadata per document
    # Here, we process one large text derived from the whole document.
    try:
        langchain_documents: List[LangchainDocument] = text_splitter.create_documents([text_to_chunk])
    except Exception as e:
        logger.error(f"Error creating documents with RecursiveCharacterTextSplitter for {base_metadata.get('file_name', 'unknown')}: {e}")
        return []
        
    output_chunks: List[Dict[str, Any]] = []
    file_name_prefix = os.path.splitext(base_metadata.get('file_name', 'doc'))[0]

    for i, doc_chunk in enumerate(langchain_documents):
        chunk_metadata = base_metadata.copy() # Start with base document metadata
        chunk_metadata['chunk_id'] = f"{file_name_prefix}_chunk_{i:04d}" # Padded for sorting
        chunk_metadata['chunk_index'] = i
        chunk_metadata['chunk_char_count'] = len(doc_chunk.page_content)
        
        # Add any metadata specific to this chunk if LangchainDocument provides it
        # (e.g., if splitting by pages from a PDF loader, it might have 'page_number')
        for key, value in doc_chunk.metadata.items():
            if key not in chunk_metadata:
                chunk_metadata[key] = value

        output_chunks.append({
            'id': chunk_metadata['chunk_id'],
            'text_content': doc_chunk.page_content,
            'metadata': chunk_metadata # All merged metadata
        })
    
    logger.info(f"Split '{base_metadata.get('file_name', 'unknown')}' into {len(output_chunks)} chunks.")
    return output_chunks

def generate_embeddings(chunks_to_embed: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    7. Generates vector embeddings for each text chunk.
    """
    if not chunks_to_embed:
        logger.info("No chunks provided for embedding generation.")
        return []

    logger.info(f"Starting embedding generation for {len(chunks_to_embed)} chunks.")
    texts_to_embed = [chunk['text_content'] for chunk in chunks_to_embed if chunk.get('text_content')]
    
    if not texts_to_embed:
        logger.warning("No text content found in chunks to embed.")
        return chunks_to_embed # Return original chunks, possibly without embeddings

    try:
        embeddings = embedding_model.encode(texts_to_embed, show_progress_bar=False) # Set to True for console progress
        
        # Assign embeddings back to the corresponding chunks
        # This assumes a 1:1 correspondence and order is maintained.
        # If some chunks had no text, the embedding list will be shorter.
        chunk_idx_with_text = 0
        for i, chunk in enumerate(chunks_to_embed):
            if chunk.get('text_content'):
                if chunk_idx_with_text < len(embeddings):
                    chunk['embedding_vector'] = embeddings[chunk_idx_with_text].tolist()
                    chunk_idx_with_text += 1
                else:
                    logger.warning(f"Mismatch in embedding count for chunk {chunk.get('id')}. No embedding assigned.")
                    chunk['embedding_vector'] = None # Or an empty list
            else:
                chunk['embedding_vector'] = None # Or an empty list for chunks with no text

        logger.info(f"Embeddings generated for {chunk_idx_with_text} chunks.")
        return chunks_to_embed
    except Exception as e:
        logger.error(f"Error during embedding generation: {e}")
        # Optionally, mark chunks as failed or return them without embeddings
        for chunk in chunks_to_embed:
            chunk['embedding_vector'] = None # Indicate failure
        return chunks_to_embed


# --- Main Orchestration Function ---
def process_uploaded_document(file_path: str, username: str) -> Dict[str, Any]:
    """
    Orchestrates the entire document processing pipeline for a single uploaded file.
    Takes username for logging/auditing purposes.
    Returns a dictionary with 'status' and 'data' or 'message'.
    """
    logger.info(f"User '{username}' initiated processing for file: {file_path}")

    if not os.path.exists(file_path):
        logger.error(f"File not found: {file_path}")
        return {"status": "error", "message": f"File not found: {file_path}", "file_path": file_path}

    try:
        # 1. Extract Raw Content
        logger.info("Step 1: Extracting Raw Content...")
        raw_content = extract_raw_content(file_path)
        logger.info(f"Step 1: Raw content extracted. Text length: {len(raw_content.get('text_content',''))}, Scanned: {raw_content.get('is_scanned')}")

        # 2. Perform OCR if necessary
        ocr_text_contribution = ""
        if raw_content['file_type'] == '.pdf' and raw_content['is_scanned']:
            if raw_content['images']:
                logger.info("Step 2: Detected scanned PDF with images, performing OCR...")
                ocr_text_contribution = perform_ocr(raw_content['images'])
                # Combine OCR text: prioritize if it's substantial, otherwise append.
                # This logic can be refined based on OCR quality.
                if len(ocr_text_contribution) > 0.5 * len(raw_content['text_content']): # If OCR text is significant
                     raw_content['text_content'] = ocr_text_contribution + "\n\n" + raw_content['text_content']
                else: # Append if OCR text is smaller or complementary
                     raw_content['text_content'] += "\n\n" + ocr_text_contribution
                logger.info(f"Step 2: OCR completed. Total text length after OCR: {len(raw_content['text_content'])}")
            else:
                logger.warning("Step 2: Detected scanned PDF but no images found for OCR. Proceeding with available text.")
        else:
            logger.info(f"Step 2: OCR not required or not applicable for file type: {raw_content['file_type']}")

        if not raw_content['text_content'] and not ocr_text_contribution: # Check after potential OCR
            logger.warning(f"No text content could be extracted from {os.path.basename(file_path)} after raw extraction and OCR.")
            # Depending on requirements, you might return an error or empty data
            # For now, we proceed, but chunking/embedding will yield empty results.

        # 3. Clean and Normalize Text
        logger.info("Step 3: Cleaning and Normalizing Text...")
        cleaned_text = clean_and_normalize_text(raw_content['text_content'])
        logger.info(f"Step 3: Text cleaned and normalized. Length: {len(cleaned_text)}")

        # 4. Reconstruct Layout and Integrate Tables
        logger.info("Step 4: Reconstructing Layout and Integrating Tables...")
        structurally_coherent_text = reconstruct_layout(
            cleaned_text,
            raw_content['layout_info'],
            raw_content['tables'],
            raw_content['file_type']
        )
        logger.info(f"Step 4: Layout reconstructed. Final text length for chunking: {len(structurally_coherent_text)}")

        # 5. Extract Metadata
        logger.info("Step 5: Extracting Metadata...")
        # Pass the most complete text and original file_type for robust metadata
        metadata = extract_metadata(file_path, structurally_coherent_text, raw_content['file_type'])
        metadata['processing_user'] = username # Add username to metadata
        logger.info("Step 5: Metadata extracted.")
        logger.debug(f"Extracted metadata keys: {list(metadata.keys())}")


        # 6. Chunk Text
        logger.info("Step 6: Chunking Text...")
        if not structurally_coherent_text:
             logger.warning("No text available for chunking. Skipping chunking and embedding.")
             chunks = []
        else:
            chunks = chunk_text(structurally_coherent_text, metadata, chunk_size=512, chunk_overlap=64)
        logger.info(f"Step 6: Text chunked into {len(chunks)} chunks.")

        # 7. Generate Embeddings
        logger.info("Step 7: Generating Embeddings...")
        final_chunks_with_embeddings = generate_embeddings(chunks)
        logger.info(f"Step 7: Embeddings generated for {sum(1 for c in final_chunks_with_embeddings if c.get('embedding_vector'))} chunks.")

        logger.info(f"Successfully finished processing for file: {file_path} by user '{username}'")
        return {"status": "success", "data": final_chunks_with_embeddings, "file_path": file_path}

    except pytesseract.TesseractNotFoundError:
        logger.critical("Tesseract is not installed or not found in PATH. OCR-dependent processing cannot continue.")
        return {"status": "error", "message": "Tesseract (OCR engine) not found. Please install it.", "file_path": file_path}
    except Exception as e:
        logger.error(f"Critical error during processing of {file_path} for user '{username}': {e}", exc_info=True)
        return {"status": "error", "message": f"An unexpected error occurred: {str(e)}", "file_path": file_path}


# --- Example Usage ---
# if __name__ == "__main__":
    logger.info("Starting example document processing...")

    # Create dummy files for demonstration (or use paths to your actual test files)
    # For robust testing, use real PDF, DOCX, PPTX files.
    DUMMY_FILES_DIR = "test_docs"
    os.makedirs(DUMMY_FILES_DIR, exist_ok=True)

    sample_txt_content = "This is a sample text document for testing the processing pipeline.\nIt contains multiple sentences and paragraphs.\nNatural language processing is fascinating. We will chunk this text and generate embeddings."
    sample_txt_path = os.path.join(DUMMY_FILES_DIR, "sample.txt")
    with open(sample_txt_path, "w", encoding="utf-8") as f:
        f.write(sample_txt_content)
    
    # A very basic PDF (for a real test, use a proper PDF with text, images, tables)
    sample_pdf_path = os.path.join(DUMMY_FILES_DIR, "sample.pdf")
    try:
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import letter
        c = canvas.Canvas(sample_pdf_path, pagesize=letter)
        c.drawString(72, 720, "Hello from a ReportLab PDF!")
        c.drawString(72, 700, "This is the first page with some text.")
        c.showPage()
        c.drawString(72, 720, "This is the second page.")
        # Add a dummy table-like text
        c.drawString(72, 650, "Header1,Header2")
        c.drawString(72, 635, "Data1A,Data1B")
        c.drawString(72, 620, "Data2A,Data2B")
        c.save()
        logger.info(f"Created dummy PDF: {sample_pdf_path}")
    except ImportError:
        logger.warning("ReportLab not found. Cannot create dummy PDF. Please install it (`pip install reportlab`) or provide your own PDF.")
        # Create a minimal text-based PDF if reportlab is not available
        if not os.path.exists(sample_pdf_path): # Only if reportlab failed and file doesn't exist
            with open(sample_pdf_path, "w", encoding="utf-8") as f:
                 f.write("%PDF-1.4\n1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n3 0 obj<</Type/Page/MediaBox[0 0 612 792]/Contents 4 0 R/Parent 2 0 R/Resources<<>>>>endobj\n4 0 obj<</Length 40>>stream\nBT /F1 24 Tf 100 700 Td (Minimal PDF) Tj ET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f\n0000000009 00000 n\n0000000058 00000 n\n0000000117 00000 n\n0000000200 00000 n\ntrailer<</Size 5/Root 1 0 R>>\nstartxref\n245\n%%EOF")
            logger.info(f"Created minimal fallback PDF: {sample_pdf_path}")


    # Example: Processing a TXT file
    username = "test_user"
    if os.path.exists(sample_txt_path):
        logger.info(f"\n--- Processing TXT file: {sample_txt_path} ---")
        result_txt = process_uploaded_document(sample_txt_path, username)
        if result_txt["status"] == "success":
            logger.info(f"Successfully processed {sample_txt_path}. Chunks: {len(result_txt['data'])}")
            if result_txt['data']:
                logger.info("--- Sample of TXT Output (First Chunk) ---")
                first_chunk = result_txt['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                # logger.info(f"Metadata: {first_chunk.get('metadata')}") # Can be verbose
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_txt_path}: {result_txt['message']}")
    else:
        logger.warning(f"Sample TXT file not found at {sample_txt_path}, skipping TXT test.")

    # Example: Processing a PDF file
    if os.path.exists(sample_pdf_path):
        logger.info(f"\n--- Processing PDF file: {sample_pdf_path} ---")
        result_pdf = process_uploaded_document(sample_pdf_path, username)
        if result_pdf["status"] == "success":
            logger.info(f"Successfully processed {sample_pdf_path}. Chunks: {len(result_pdf['data'])}")
            if result_pdf['data']:
                logger.info("--- Sample of PDF Output (First Chunk) ---")
                first_chunk = result_pdf['data'][0]
                logger.info(f"Chunk ID: {first_chunk.get('id')}")
                logger.info(f"Text Content (first 100 chars): {first_chunk.get('text_content', '')[:100]}...")
                logger.info(f"File type from metadata: {first_chunk.get('metadata', {}).get('file_type')}")
                logger.info(f"Embedding Vector (present): {bool(first_chunk.get('embedding_vector'))}")
        else:
            logger.error(f"Failed to process {sample_pdf_path}: {result_pdf['message']}")
    else:
        logger.warning(f"Sample PDF file not found at {sample_pdf_path}, skipping PDF test.")

    # Clean up dummy files (optional)
    # logger.info(f"Cleaning up dummy files in {DUMMY_FILES_DIR}...")
    # for f_name in os.listdir(DUMMY_FILES_DIR):
    #     os.remove(os.path.join(DUMMY_FILES_DIR, f_name))
    # os.rmdir(DUMMY_FILES_DIR)
    # logger.info("Dummy files cleaned up.")


============ ./Readne.txt ============
conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app




============ ./requirements.txt ============
Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
re
typing
PIL # For image processing
pytesseract # OCR
pillow






============ ./__init__.py ============






rag_service/config.py

python
# server/config.py
import os
import logging

# â”€â”€â”€ Logging Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger(__name__)
LOGGING_LEVEL_NAME = os.getenv('LOGGING_LEVEL', 'INFO').upper()
LOGGING_LEVEL      = getattr(logging, LOGGING_LEVEL_NAME, logging.INFO)
LOGGING_FORMAT     = '%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s'

# === Base Directory ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
logger.info(f"[Config] Base Directory: {BASE_DIR}")



def setup_logging():
    """Configure logging across the app."""
    root_logger = logging.getLogger()
    if not root_logger.handlers:  # prevent duplicate handlers
        handler = logging.StreamHandler()
        formatter = logging.Formatter(LOGGING_FORMAT)
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(LOGGING_LEVEL)

    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("faiss.loader").setLevel(logging.WARNING)
    logging.getLogger(__name__).info(f"Logging initialized at {LOGGING_LEVEL_NAME}")

# === Embedding Model Configuration ===
DEFAULT_DOC_EMBED_MODEL = 'mixedbread-ai/mxbai-embed-large-v1'
DOCUMENT_EMBEDDING_MODEL_NAME = os.getenv('DOCUMENT_EMBEDDING_MODEL_NAME', DEFAULT_DOC_EMBED_MODEL)
MAX_TEXT_LENGTH_FOR_NER = int(os.getenv("MAX_TEXT_LENGTH_FOR_NER", 500000))
logger.info(f"[Config] Document Embedding Model: {DOCUMENT_EMBEDDING_MODEL_NAME}")

# Model dimension mapping
_MODEL_TO_DIM_MAPPING = {
    'mixedbread-ai/mxbai-embed-large-v1': 1024,
    'BAAI/bge-large-en-v1.5': 1024,
    'all-MiniLM-L6-v2': 384,
    'sentence-transformers/all-mpnet-base-v2': 768,
}
_FALLBACK_DIM = 768

DOCUMENT_VECTOR_DIMENSION = int(os.getenv(
    "DOCUMENT_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(DOCUMENT_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))
logger.info(f"[Config] Document Vector Dimension: {DOCUMENT_VECTOR_DIMENSION}")

# === AI Core Chunking Config ===
AI_CORE_CHUNK_SIZE = int(os.getenv("AI_CORE_CHUNK_SIZE", 512))
AI_CORE_CHUNK_OVERLAP = int(os.getenv("AI_CORE_CHUNK_OVERLAP", 100))
logger.info(f"[Config] Chunk Size: {AI_CORE_CHUNK_SIZE}, Overlap: {AI_CORE_CHUNK_OVERLAP}")

# === SpaCy Configuration ===
SPACY_MODEL_NAME = os.getenv('SPACY_MODEL_NAME', 'en_core_web_sm')
logger.info(f"[Config] SpaCy Model: {SPACY_MODEL_NAME}")

# === Qdrant Configuration ===
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
QDRANT_COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME", "my_qdrant_rag_collection")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
QDRANT_URL = os.getenv("QDRANT_URL", None)

QDRANT_COLLECTION_VECTOR_DIM = DOCUMENT_VECTOR_DIMENSION
logger.info(f"[Config] Qdrant Vector Dimension: {QDRANT_COLLECTION_VECTOR_DIM}")

# === Query Embedding Configuration ===
QUERY_EMBEDDING_MODEL_NAME = os.getenv("QUERY_EMBEDDING_MODEL_NAME", DOCUMENT_EMBEDDING_MODEL_NAME)
QUERY_VECTOR_DIMENSION = int(os.getenv(
    "QUERY_VECTOR_DIMENSION",
    _MODEL_TO_DIM_MAPPING.get(QUERY_EMBEDDING_MODEL_NAME, _FALLBACK_DIM)
))

if QUERY_VECTOR_DIMENSION != QDRANT_COLLECTION_VECTOR_DIM:
    logger.info(f"[âš ï¸ Config Warning] Query vector dim ({QUERY_VECTOR_DIMENSION}) != Qdrant dim ({QDRANT_COLLECTION_VECTOR_DIM})")
    # Optionally enforce consistency
    # raise ValueError("Query and Document vector dimensions do not match!")
else:
    logger.info(f"[Config] Query Model: {QUERY_EMBEDDING_MODEL_NAME}")
    logger.info(f"[Config] Query Vector Dimension: {QUERY_VECTOR_DIMENSION}")

QDRANT_DEFAULT_SEARCH_K = int(os.getenv("QDRANT_DEFAULT_SEARCH_K", 5))
QDRANT_SEARCH_MIN_RELEVANCE_SCORE = float(os.getenv("QDRANT_SEARCH_MIN_RELEVANCE_SCORE", 0.1))

# === API Port Configuration ===
API_PORT = int(os.getenv('API_PORT', 5000))
logger.info(f"[Config] API Running Port: {API_PORT}")

# === Optional: Tesseract OCR Path (uncomment if used) ===
# TESSERACT_CMD = os.getenv('TESSERACT_CMD')
# if TESSERACT_CMD:
#     import pytesseract
#     pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD
#     logger.info(f"[Config] Tesseract Path: {TESSERACT_CMD}")


# â”€â”€â”€ Library Availability Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import pypdf
    PYPDF_AVAILABLE      = True
    PYPDF_PDFREADERROR   = pypdf.errors.PdfReadError
except ImportError:
    PYPDF_AVAILABLE      = False
    PYPDF_PDFREADERROR   = Exception

try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE       = True
except ImportError:
    DOCX_AVAILABLE       = False
    DocxDocument         = None

try:
    from pptx import Presentation
    PPTX_AVAILABLE       = True
except ImportError:
    PPTX_AVAILABLE       = False
    Presentation         = None

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    pdfplumber           = None

try:
    import pandas as pd
    PANDAS_AVAILABLE     = True
except ImportError:
    PANDAS_AVAILABLE     = False
    pd                   = None

try:
    from PIL import Image
    PIL_AVAILABLE        = True
except ImportError:
    PIL_AVAILABLE        = False
    Image                = None

try:
    import fitz
    FITZ_AVAILABLE       = True
except ImportError:
    FITZ_AVAILABLE       = False
    fitz                 = None

try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
    TESSERACT_ERROR       = pytesseract.TesseractNotFoundError
except ImportError:
    PYTESSERACT_AVAILABLE = False
    pytesseract           = None
    TESSERACT_ERROR       = Exception

try:
    import PyPDF2
    PYPDF2_AVAILABLE      = True
except ImportError:
    PYPDF2_AVAILABLE      = False
    PyPDF2                = None

# â”€â”€â”€ Optional: Preload SpaCy & Embedding Model â”€â”€â”€â”€â”€â”€â”€
try:
    import spacy
    SPACY_LIB_AVAILABLE = True
    nlp_spacy_core      = spacy.load(SPACY_MODEL_NAME)
    SPACY_MODEL_LOADED  = True
except Exception as e:
    SPACY_LIB_AVAILABLE = False
    nlp_spacy_core      = None
    SPACY_MODEL_LOADED  = False
    logger.warning(f"Failed to load SpaCy model '{SPACY_MODEL_NAME}': {e}")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = True
    document_embedding_model = SentenceTransformer(DOCUMENT_EMBEDDING_MODEL_NAME)
    EMBEDDING_MODEL_LOADED = True
except Exception as e:
    SENTENCE_TRANSFORMERS_LIB_AVAILABLE = False
    document_embedding_model = None
    EMBEDDING_MODEL_LOADED = False
    logger.warning(f"Failed to load Sentence transformers: {e}")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_SPLITTER_AVAILABLE = True
except ImportError:
    LANGCHAIN_SPLITTER_AVAILABLE = False
    RecursiveCharacterTextSplitter = None # Placeholder


rag_service/file_parser.py

python
# server/rag_service/file_parser.py
import os
try:
    import pypdf
except ImportError:
    print("pypdf not found, PDF parsing will fail. Install with: pip install pypdf")
    pypdf = None # Set to None if not installed

try:
    from docx import Document as DocxDocument
except ImportError:
    print("python-docx not found, DOCX parsing will fail. Install with: pip install python-docx")
    DocxDocument = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from rag_service import config # Import from package
import logging

# Configure logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Or DEBUG for more details
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.hasHandlers():
    logger.addHandler(handler)


def parse_pdf(file_path):
    """Extracts text content from a PDF file using pypdf."""
    if not pypdf: return None # Check if library loaded
    text = ""
    try:
        reader = pypdf.PdfReader(file_path)
        num_pages = len(reader.pages)
        # logger.debug(f"Reading {num_pages} pages from PDF: {os.path.basename(file_path)}")
        for i, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" # Add newline between pages
            except Exception as page_err:
                 logger.warning(f"Error extracting text from page {i+1} of {os.path.basename(file_path)}: {page_err}")
        # logger.debug(f"Extracted {len(text)} characters from PDF.")
        return text.strip() if text.strip() else None # Return None if empty after stripping
    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except pypdf.errors.PdfReadError as pdf_err:
        logger.error(f"Error reading PDF {os.path.basename(file_path)} (possibly corrupted or encrypted): {pdf_err}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error parsing PDF {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_docx(file_path):
    """Extracts text content from a DOCX file."""
    if not DocxDocument: return None # Check if library loaded
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        # logger.debug(f"Extracted {len(text)} characters from DOCX.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing DOCX {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

def parse_txt(file_path):
    """Reads text content from a TXT file (or similar plain text like .py, .js)."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        # logger.debug(f"Read {len(text)} characters from TXT file.")
        return text.strip() if text.strip() else None
    except Exception as e:
        logger.error(f"Error parsing TXT {os.path.basename(file_path)}: {e}", exc_info=True)
        return None

# Add PPTX parsing (requires python-pptx)
try:
    from pptx import Presentation
    PPTX_SUPPORTED = True
    def parse_pptx(file_path):
        """Extracts text content from a PPTX file."""
        text = ""
        try:
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        shape_text = shape.text.strip()
                        if shape_text:
                            text += shape_text + "\n" # Add newline between shape texts
            # logger.debug(f"Extracted {len(text)} characters from PPTX.")
            return text.strip() if text.strip() else None
        except Exception as e:
            logger.error(f"Error parsing PPTX {os.path.basename(file_path)}: {e}", exc_info=True)
            return None
except ImportError:
    PPTX_SUPPORTED = False
    logger.warning("python-pptx not installed. PPTX parsing will be skipped.")
    def parse_pptx(file_path):
        logger.warning(f"Skipping PPTX file {os.path.basename(file_path)} as python-pptx is not installed.")
        return None


def parse_file(file_path):
    """Parses a file based on its extension, returning text content or None."""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()
    logger.debug(f"Attempting to parse file: {os.path.basename(file_path)} (Extension: {ext})")

    if ext == '.pdf':
        return parse_pdf(file_path)
    elif ext == '.docx':
        return parse_docx(file_path)
    elif ext == '.pptx':
        return parse_pptx(file_path) # Use the conditional function
    elif ext in ['.txt', '.py', '.js', '.md', '.log', '.csv', '.html', '.xml', '.json']: # Expand text-like types
        return parse_txt(file_path)
    # Add other parsers here if needed (e.g., for .doc, .xls)
    elif ext == '.doc':
        # Requires antiword or similar external tool, more complex
        logger.warning(f"Parsing for legacy .doc files is not implemented: {os.path.basename(file_path)}")
        return None
    else:
        logger.warning(f"Unsupported file extension for parsing: {ext} ({os.path.basename(file_path)})")
        return None

def chunk_text(text, file_name, user_id):
    """Chunks text and creates Langchain Documents with metadata."""
    if not text or not isinstance(text, str):
        logger.warning(f"Invalid text input for chunking (file: {file_name}). Skipping.")
        return []

    # Use splitter configured in config.py
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.CHUNK_SIZE,
        chunk_overlap=config.CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False, # Use default separators
        # separators=["\n\n", "\n", " ", ""] # Default separators
    )

    try:
        chunks = text_splitter.split_text(text)
        if not chunks:
             logger.warning(f"Text splitting resulted in zero chunks for file: {file_name}")
             return []

        documents = []
        for i, chunk in enumerate(chunks):
             # Ensure chunk is not just whitespace before creating Document
             if chunk and chunk.strip():
                 documents.append(
                     LangchainDocument(
                         page_content=chunk,
                         metadata={
                             'userId': user_id, # Store user ID
                             'documentName': file_name, # Store original filename
                             'chunkIndex': i # Store chunk index for reference
                         }
                     )
                 )
        if documents:
            logger.info(f"Split '{file_name}' into {len(documents)} non-empty chunks.")
        else:
            logger.warning(f"No non-empty chunks created for file: {file_name} after splitting.")
        return documents
    except Exception as e:
        logger.error(f"Error during text splitting for file {file_name}: {e}", exc_info=True)
        return [] # Return empty list on error



rag_service/Readne.txt


conda activate RAG
python server/rag_service/app.py
OR
python -m server.rag_service.app

For testing
curl -X POST -H "Content-Type: application/json" -d '{"user_id": "__DEFAULT__", "query": "machine learning"}' http://localhost:5002/query

for production
pip install gunicorn
gunicorn --bind 0.0.0.0:5002 server.rag_service.app:app




rag_service/requirements.txt


Flask
requests
sentence-transformers
faiss-cpu # or faiss-gpu
langchain
langchain-huggingface
pypdf
PyPDF2
python-docx
python-dotenv
ollama # Keep if using Ollama embeddings
python-pptx # Added for PPTX parsing
uuid
langchain-community
pdfplumber
fitz # PyMuPDF for PDF parsing
pytesseract
nltk
spacy
spacy-layout
pandas
numpy
re
typing
PIL # For image processing
pytesseract # OCR
pillow
qdrant-client






rag_service/vector_db_service.py

python
import uuid
import logging
from typing import List, Dict, Tuple, Optional, Any

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

# Assuming vector_db_service.py and config.py are in the same package directory (e.g., rag_service/)
# and you run your application as a module (e.g., python -m rag_service.main_app)
# or have otherwise correctly set up the Python path.
import config # Changed to relative import

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Document: # For search result formatting
    def __init__(self, page_content: str, metadata: dict):
        self.page_content = page_content
        self.metadata = metadata

    def to_dict(self):
        return {"page_content": self.page_content, "metadata": self.metadata}

class VectorDBService:
    def __init__(self):
        logger.info("Initializing VectorDBService...")
        logger.info(f"  Qdrant Host: {config.QDRANT_HOST}, Port: {config.QDRANT_PORT}, URL: {config.QDRANT_URL}")
        logger.info(f"  Collection: {config.QDRANT_COLLECTION_NAME}")
        logger.info(f"  Query Embedding Model: {config.QUERY_EMBEDDING_MODEL_NAME}")
        
        # The vector dimension for the Qdrant collection is defined by the DOCUMENT embedding model
        # This is set in config.QDRANT_COLLECTION_VECTOR_DIM
        self.vector_dim = config.QDRANT_COLLECTION_VECTOR_DIM
        logger.info(f"  Service expects Vector Dim for Qdrant collection: {self.vector_dim} (from document model config)")

        if config.QDRANT_URL:
            self.client = QdrantClient(
                url=config.QDRANT_URL,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )
        else:
            self.client = QdrantClient(
                host=config.QDRANT_HOST,
                port=config.QDRANT_PORT,
                api_key=config.QDRANT_API_KEY,
                timeout=30
            )

        try:
            # This model is for encoding search queries.
            # Its output dimension MUST match self.vector_dim (QDRANT_COLLECTION_VECTOR_DIM).
            logger.info(f"  Loading query embedding model: '{config.QUERY_EMBEDDING_MODEL_NAME}'")
            self.model = SentenceTransformer(config.QUERY_EMBEDDING_MODEL_NAME)
            model_embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"  Query model loaded. Output dimension: {model_embedding_dim}")

            if model_embedding_dim != self.vector_dim:
                error_msg = (
                    f"CRITICAL DIMENSION MISMATCH: Query model '{config.QUERY_EMBEDDING_MODEL_NAME}' "
                    f"outputs embeddings of dimension {model_embedding_dim}, but the Qdrant collection "
                    f"is configured for dimension {self.vector_dim} (derived from document model: "
                    f"'{config.DOCUMENT_EMBEDDING_MODEL_NAME}'). Search functionality will fail. "
                    "Ensure query and document models produce compatible embedding dimensions, "
                    "or environment variables for dimensions are correctly set."
                )
                logger.error(error_msg)
                raise ValueError(error_msg) # Critical error, stop initialization
            else:
                logger.info(f"  Query model output dimension ({model_embedding_dim}) matches "
                            f"Qdrant collection dimension ({self.vector_dim}).")

        except Exception as e:
            logger.error(f"Error initializing SentenceTransformer model '{config.QUERY_EMBEDDING_MODEL_NAME}' for query encoding: {e}", exc_info=True)
            raise # Re-raise to prevent service startup with a non-functional query encoder

        self.collection_name = config.QDRANT_COLLECTION_NAME
        # No ThreadPoolExecutor needed here if document encoding is external

    def _recreate_qdrant_collection(self):
        logger.info(f"Attempting to (re)create collection '{self.collection_name}' with vector size {self.vector_dim}.")
        try:
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=self.vector_dim,
                    distance=models.Distance.COSINE,
                ),
            )
            logger.info(f"Collection '{self.collection_name}' (re)created successfully.")
        except Exception as e_recreate:
            logger.error(f"Failed to (re)create collection '{self.collection_name}': {e_recreate}", exc_info=True)
            raise

    def setup_collection(self):
        try:
            collection_info = self.client.get_collection(collection_name=self.collection_name)
            logger.info(f"Collection '{self.collection_name}' already exists.")
            
            # Handle different Qdrant client versions for accessing vector config
            current_vectors_config = None
            if hasattr(collection_info.config.params, 'vectors'): # For simple vector config
                if isinstance(collection_info.config.params.vectors, models.VectorParams):
                     current_vectors_config = collection_info.config.params.vectors
                elif isinstance(collection_info.config.params.vectors, dict): # For named vectors
                    # Assuming default unnamed vector or first one if named
                    default_vector_name = '' # Common for single vector setup
                    if default_vector_name in collection_info.config.params.vectors:
                        current_vectors_config = collection_info.config.params.vectors[default_vector_name]
                    elif collection_info.config.params.vectors: # Get first one if default not found
                        current_vectors_config = next(iter(collection_info.config.params.vectors.values()))

            if not current_vectors_config:
                 logger.error(f"Could not determine vector configuration for existing collection '{self.collection_name}'. Recreating.")
                 self._recreate_qdrant_collection()
            elif current_vectors_config.size != self.vector_dim:
                logger.warning(f"Collection '{self.collection_name}' vector size {current_vectors_config.size} "
                               f"differs from service's expected {self.vector_dim}. Recreating.")
                self._recreate_qdrant_collection()
            elif current_vectors_config.distance != models.Distance.COSINE: # Ensure distance is also checked
                logger.warning(f"Collection '{self.collection_name}' distance {current_vectors_config.distance} "
                               f"differs from expected {models.Distance.COSINE}. Recreating.")
                self._recreate_qdrant_collection()
            else:
                logger.info(f"Collection '{self.collection_name}' configuration is compatible (Size: {current_vectors_config.size}, Distance: {current_vectors_config.distance}).")

        except Exception as e: # Broad exception for Qdrant client errors
            # More specific check for "Not found" type errors
            if "not found" in str(e).lower() or \
               (hasattr(e, 'status_code') and e.status_code == 404) or \
               " à¦­à¦¾à¦—à§à¦¯à¦¬à¦¾à¦¨" in str(e).lower(): # "Lucky" in Bengali, seems to be part of an error message you encountered
                 logger.info(f"Collection '{self.collection_name}' not found. Attempting to create...")
            else:
                 logger.warning(f"Error checking collection '{self.collection_name}': {type(e).__name__} - {e}. Attempting to (re)create anyway...")
            self._recreate_qdrant_collection()

    def add_processed_chunks(self, processed_chunks: List[Dict[str, Any]]) -> int:
        if not processed_chunks:
            logger.warning("add_processed_chunks received an empty list. No points to upsert.")
            return 0

        points_to_upsert = []
        doc_name_for_logging = "Unknown Document"

        for chunk_data in processed_chunks:
            point_id = chunk_data.get('id', str(uuid.uuid4()))
            vector = chunk_data.get('embedding')
            
            payload = chunk_data.get('metadata', {}).copy()
            payload['chunk_text_content'] = chunk_data.get('text_content', '')

            if not doc_name_for_logging or doc_name_for_logging == "Unknown Document":
                doc_name_for_logging = payload.get('original_name', payload.get('document_name', "Unknown Document"))

            if not vector:
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' is missing 'embedding'. Skipping.")
                continue
            if not isinstance(vector, list) or not all(isinstance(x, (float, int)) for x in vector): # Allow int too, SentenceTransformer can return float32 which might be int-like in lists
                logger.warning(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has an invalid 'embedding' format. Skipping.")
                continue
            if len(vector) != self.vector_dim:
                logger.error(f"Chunk with ID '{point_id}' from '{doc_name_for_logging}' has embedding dimension {len(vector)}, "
                             f"but collection expects {self.vector_dim}. Skipping. "
                             f"Ensure ai_core's document embedding model ('{config.DOCUMENT_EMBEDDING_MODEL_NAME}') "
                             f"output dimension matches configuration.")
                continue

            points_to_upsert.append(models.PointStruct(
                id=point_id,
                vector=[float(v) for v in vector], # Ensure all are floats for Qdrant
                payload=payload
            ))

        if not points_to_upsert:
            logger.warning(f"No valid points constructed from processed_chunks for document: {doc_name_for_logging}.")
            return 0

        try:
            self.client.upsert(collection_name=self.collection_name, points=points_to_upsert, wait=True) # wait=True can be useful for debugging
            logger.info(f"Successfully upserted {len(points_to_upsert)} chunks for document: {doc_name_for_logging} into Qdrant.")
            return len(points_to_upsert)
        except Exception as e:
            logger.error(f"Error upserting processed chunks to Qdrant for document: {doc_name_for_logging}: {e}", exc_info=True)
            raise

    def search_documents(self, query: str, k: int = -1, filter_conditions: Optional[models.Filter] = None) -> Tuple[List[Document], str, Dict]:
        # Use default k from config if not provided or invalid
        if k <= 0:
            k_to_use = config.QDRANT_DEFAULT_SEARCH_K
        else:
            k_to_use = k

        context_docs = []
        formatted_context_text = "No relevant context was found in the available documents."
        context_docs_map = {}

        logger.info(f"Searching with query (first 50 chars): '{query[:50]}...', k: {k_to_use}")
        if filter_conditions:
            try: filter_dict = filter_conditions.dict()
            except AttributeError: # For older Pydantic versions
                try: filter_dict = filter_conditions.model_dump()
                except AttributeError: filter_dict = str(filter_conditions) # Fallback
            logger.info(f"Applying filter: {filter_dict}")
        else:
            logger.info("No filter applied for search.")

        try:
            query_embedding = self.model.encode(query).tolist()
            logger.debug(f"Generated query_embedding (length: {len(query_embedding)}, first 5 dims: {query_embedding[:5]})")

            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                query_filter=filter_conditions,
                limit=k_to_use,
                with_payload=True,
                score_threshold=config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE # Apply score threshold directly in search
            )
            logger.info(f"Qdrant client.search returned {len(search_results)} results (after score threshold).")

            if not search_results:
                return context_docs, formatted_context_text, context_docs_map

            for idx, point in enumerate(search_results):
                # Score threshold is already applied by Qdrant if score_threshold parameter is used.
                # If not using score_threshold in client.search, uncomment this:
                # if point.score < config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE:
                #     logger.debug(f"Skipping point ID {point.id} with score {point.score:.4f} (below threshold {config.QDRANT_SEARCH_MIN_RELEVANCE_SCORE})")
                #     continue

                payload = point.payload
                content = payload.get("chunk_text_content", payload.get("text_content", payload.get("chunk_text", "")))

                retrieved_metadata = payload.copy()
                retrieved_metadata["qdrant_id"] = point.id
                retrieved_metadata["score"] = point.score

                doc = Document(page_content=content, metadata=retrieved_metadata)
                context_docs.append(doc)

            # Format context and citations
            formatted_context_parts = []
            for i, doc_obj in enumerate(context_docs):
                citation_index = i + 1
                doc_meta = doc_obj.metadata
                # Use more robust fetching of metadata keys
                display_subject = doc_meta.get("title", doc_meta.get("subject", "Unknown Subject")) # Prefer title for subject
                doc_name = doc_meta.get("original_name", doc_meta.get("file_name", "N/A"))
                page_num_info = f" (Page: {doc_meta.get('page_number', 'N/A')})" if doc_meta.get('page_number') else "" # Add page number if available
                
                content_preview = doc_obj.page_content[:200] + "..." if len(doc_obj.page_content) > 200 else doc_obj.page_content

                formatted = (f"[{citation_index}] Score: {doc_meta.get('score', 0.0):.4f} | "
                             f"Source: {doc_name}{page_num_info} | Subject: {display_subject}\n"
                             f"Content: {content_preview}") # Show content preview
                formatted_context_parts.append(formatted)

                context_docs_map[str(citation_index)] = {
                    "subject": display_subject,
                    "document_name": doc_name,
                    "page_number": doc_meta.get("page_number"),
                    "content_preview": content_preview, # Store preview
                    "full_content": doc_obj.page_content, # Store full content for potential later use
                    "score": doc_meta.get("score", 0.0),
                    "qdrant_id": doc_meta.get("qdrant_id"),
                    "original_metadata": doc_meta # Store all original metadata from payload
                }
            if formatted_context_parts:
                formatted_context_text = "\n\n---\n\n".join(formatted_context_parts)
            else:
                formatted_context_text = "No sufficiently relevant context was found after filtering."

        except Exception as e:
            logger.error(f"Qdrant search/RAG error: {e}", exc_info=True)
            formatted_context_text = "Error retrieving context due to an internal server error."

        return context_docs, formatted_context_text, context_docs_map

    def close(self):
        logger.info("VectorDBService close called.")
        # No specific resources like ThreadPoolExecutor to release in this version.
        # QdrantClient does not have an explicit close() method in recent versions.


rag_service/__init__.py

python



routes/analysis.js

javascript
console.log("Hello World");



routes/auth.js

javascript
// server/routes/auth.js
const express = require('express');
const { v4: uuidv4 } = require('uuid'); // For generating session IDs
const User = require('../models/User'); // Mongoose User model
require('dotenv').config();

const router = express.Router();

// --- @route   POST /api/auth/signup ---
// --- @desc    Register a new user ---
// --- @access  Public ---
router.post('/signup', async (req, res) => {
  const { username, password } = req.body;

  // Basic validation
  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }
  if (password.length < 6) {
     return res.status(400).json({ message: 'Password must be at least 6 characters long' });
  }

  try {
    // Check if user already exists
    const existingUser = await User.findOne({ username });
    if (existingUser) {
      return res.status(400).json({ message: 'Username already exists' });
    }

    // Create new user (password hashing is handled by pre-save middleware in User model)
    const newUser = new User({ username, password });
    await newUser.save();

    // Generate a new session ID for the first login
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Note: Mongoose excludes 'select: false' fields by default after save() too
    res.status(201).json({
      _id: newUser._id, // Send user ID
      username: newUser.username,
      sessionId: sessionId, // Send session ID on successful signup/login
      message: 'User registered successfully',
    });

  } catch (error) {
    console.error('Signup Error:', error);
    // Handle potential duplicate key errors more gracefully if needed
    if (error.code === 11000) {
        return res.status(400).json({ message: 'Username already exists.' });
    }
    res.status(500).json({ message: 'Server error during signup' });
  }
});

// --- @route   POST /api/auth/signin ---
// --- @desc    Authenticate user (using custom static method) ---
// --- @access  Public ---
router.post('/signin', async (req, res) => {
  const { username, password } = req.body;

  if (!username || !password) {
    return res.status(400).json({ message: 'Please provide username and password' });
  }

  try {
    // *** CHANGE HERE: Use the static method from User model ***
    // This method finds the user AND selects the password field AND compares the password
    const user = await User.findByCredentials(username, password);

    // Check if the method returned a user (means credentials were valid)
    if (!user) {
      // findByCredentials returns null if user not found OR password doesn't match
      return res.status(401).json({ message: 'Invalid credentials' }); // Use generic message
    }

    // User authenticated successfully if we reached here

    // Generate a NEW session ID for this login session
    const sessionId = uuidv4();

    // Respond with user info (excluding password), and session ID
    // Even though 'user' has the password field selected from findByCredentials,
    // Mongoose's .toJSON() or spreading might still exclude it if schema default is select:false.
    // Explicitly create the response object.
    res.status(200).json({
      _id: user._id, // Send user ID
      username: user.username,
      sessionId: sessionId, // Send a *new* session ID on each successful login
      message: 'Login successful',
    });

  } catch (error) {
    // Log the specific error for debugging
    console.error('Signin Error:', error);
    // Check if the error came from the comparePassword method (e.g., bcrypt issue)
    if (error.message === "Password field not available for comparison.") {
        // This shouldn't happen if findByCredentials is used correctly, but good to check
        console.error("Developer Error: Password field was not selected before comparison attempt.");
        return res.status(500).json({ message: 'Internal server configuration error during signin.' });
    }
    res.status(500).json({ message: 'Server error during signin' });
  }
});


module.exports = router;



routes/chat.js

javascript
// server/routes/chat.js
const express = require('express');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const ChatHistory = require('../models/ChatHistory');
const { v4: uuidv4 } = require('uuid');
const { generateContentWithHistory } = require('../services/geminiService');

const router = express.Router();

// --- Helper to call Python RAG Query Endpoint ---
async function queryPythonRagService(userId, query, k = 5, filter = null) { // Added filter parameter
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL;
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL is not set in environment. Cannot query RAG service.");
        throw new Error("RAG service configuration error."); // Throw error to be caught by caller
    }
    const searchUrl = `${pythonServiceUrl}/query`; // <<<--- CORRECTED TO /query ---

    console.log(`Querying Python RAG service for User ${userId} at ${searchUrl} with query (first 50): "${query.substring(0,50)}...", k=${k}`);
    
    const payload = {
        query: query,
        k: k
    };

    if (filter && typeof filter === 'object' && Object.keys(filter).length > 0) {
        payload.filter = filter;
        console.log(`  Applying filter to Python RAG search:`, filter);
    } else {
        console.log(`  No filter applied to Python RAG search.`);
    }

    try {
        const response = await axios.post(searchUrl, payload, { 
            headers: { 'Content-Type': 'application/json' },
            timeout: 30000 
        });

        // Python /search returns: { ..., retrieved_documents_list: [{ page_content: "...", metadata: {...} }], ... }
        if (response.data && Array.isArray(response.data.retrieved_documents_list)) {
            console.log(`Python RAG service /search returned ${response.data.results_count} results.`);
            
            // Adapt to the structure expected by the Node.js /api/chat/rag and /api/chat/message routes
            const adaptedDocs = response.data.retrieved_documents_list.map(doc => {
                const metadata = doc.metadata || {};
                return {
                    // The /api/chat/message route expects 'documentName' and 'content'
                    documentName: metadata.original_name || metadata.file_name || metadata.title || 'Unknown Document',
                    content: doc.page_content || "", 
                    score: metadata.score, // Pass along the score
                    // Include any other metadata if needed by the Gemini prompt construction
                    // e.g., page_chunk_info: metadata.page_chunk_info || metadata.chunk_index,
                    // e.g., qdrant_id: metadata.qdrant_id 
                };
            });
            
            console.log(`  Adapted ${adaptedDocs.length} documents for Node.js service.`);
            return adaptedDocs; 

        } else {
             console.warn(`Python RAG service /search returned unexpected data structure:`, response.data);
             // Throw an error or return empty if the structure is critical
             throw new Error("Received unexpected data structure from RAG search service.");
        }
    } catch (error) {
        const errorStatus = error.response?.status;
        const errorData = error.response?.data;
        let errorMsg = "Unknown RAG search error";

        if (errorData) {
            if (typeof errorData === 'string' && errorData.toLowerCase().includes("<!doctype html>")) {
                errorMsg = `HTML error page received from Python RAG service (Status: ${errorStatus}). URL (${searchUrl}) might be incorrect or Python service has an issue.`;
                console.error(`Error querying Python RAG service: Received HTML error page. URL: ${searchUrl}, Status: ${errorStatus}`);
            } else {
                errorMsg = errorData?.error || error.message || "Error response from RAG service had no specific message.";
                console.error(`Error querying Python RAG service at ${searchUrl}. Status: ${errorStatus}, Python Error: ${errorMsg}`, errorData);
            }
        } else if (error.request) {
            errorMsg = `No response received from Python RAG service at ${searchUrl}. It might be down or unreachable.`;
            console.error(`Error querying Python RAG service at ${searchUrl}: No response received. ${error.message}`);
        } else {
            errorMsg = error.message;
            console.error(`Error setting up or sending request to Python RAG service at ${searchUrl}: ${error.message}`);
        }
        throw new Error(`RAG Search Failed: ${errorMsg}`); // Propagate error
    }
}

// --- @route   POST /api/chat/rag ---
router.post('/rag', tempAuth, async (req, res) => {
    const { message, filter } = req.body; 
    const userId = req.user._id.toString(); 

    if (!message || typeof message !== 'string' || message.trim() === '') {
        return res.status(400).json({ message: 'Query message text required.' });
    }

    console.log(`>>> POST /api/chat/rag: User=${userId}. Query: "${message.substring(0,50)}..."`);

    try {
        const kValue = parseInt(process.env.RAG_DEFAULT_K) || 5; 
        const clientFilter = filter && typeof filter === 'object' ? filter : null; 
        
        const relevantDocs = await queryPythonRagService(userId, message.trim(), kValue, clientFilter); 
        
        console.log(`<<< POST /api/chat/rag successful for User ${userId}. Found ${relevantDocs.length} docs.`);
        res.status(200).json({ relevantDocs }); // `relevantDocs` is now an array of { documentName, content, score }

    } catch (error) { 
        console.error(`!!! Error processing RAG query for User ${userId}:`, error.message);
        res.status(500).json({ message: error.message || "Failed to retrieve relevant documents." });
    }
});

// --- @route   POST /api/chat/message ---
// (This route should now work correctly with the adapted `relevantDocs` structure)
router.post('/message', tempAuth, async (req, res) => {
    const { message, history, sessionId, systemPrompt, isRagEnabled, relevantDocs } = req.body;
    const userId = req.user._id.toString(); 

    if (!message || typeof message !== 'string' || message.trim() === '') return res.status(400).json({ message: 'Message text required.' });
    if (!sessionId || typeof sessionId !== 'string') return res.status(400).json({ message: 'Session ID required.' });
    if (!Array.isArray(history)) return res.status(400).json({ message: 'Invalid history format.'});
    const useRAG = !!isRagEnabled; 

    console.log(`>>> POST /api/chat/message: User=${userId}, Session=${sessionId}, RAG=${useRAG}. Query: "${message.substring(0,50)}..."`);

    let contextString = "";
    let citationHints = []; 

    try {
        // `relevantDocs` is now an array of {documentName, content, score} from queryPythonRagService
        if (useRAG && Array.isArray(relevantDocs) && relevantDocs.length > 0) {
            console.log(`   RAG Enabled: Processing ${relevantDocs.length} relevant documents provided by client.`);
            contextString = "Answer the user's question based primarily on the following context documents.\nIf the context documents do not contain the necessary information to answer the question fully, clearly state what information is missing from the context *before* potentially providing an answer based on your general knowledge.\n\n--- Context Documents ---\n";
            
            relevantDocs.forEach((doc, index) => {
                if (!doc || typeof doc.documentName !== 'string' || typeof doc.content !== 'string') {
                    console.warn("   Skipping invalid/incomplete document in relevantDocs (missing 'documentName' or 'content'):", doc);
                    return; 
                }
                const docName = doc.documentName;
                const scoreDisplay = doc.score !== undefined ? `(Rel. Score: ${doc.score.toFixed(4)})` : ''; 
                const fullContent = doc.content; 

                contextString += `\n[${index + 1}] Source: ${docName} ${scoreDisplay}\nContent:\n${fullContent}\n---\n`;
                citationHints.push(`[${index + 1}] ${docName}`);
            });
            contextString += "\n--- End of Context ---\n\n";
            console.log(`   Constructed context string. ${citationHints.length} valid docs used.`);
        } else {
            console.log(`   RAG Disabled or no relevant documents provided by client.`);
        }

        const historyForGeminiAPI = history.map(msg => ({
             role: msg.role,
             parts: msg.parts.map(part => ({ text: part.text || '' }))
        })).filter(msg => msg && msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string');

        let finalUserQueryText = "";
        if (contextString) { 
            const citationInstruction = `When referencing information ONLY from the context documents provided above, please cite the source using the format [Number] Document Name (e.g., ${citationHints.slice(0, Math.min(3, citationHints.length)).join(', ')}).`;
            finalUserQueryText = `CONTEXT:\n${contextString}\nINSTRUCTIONS: ${citationInstruction}\n\nUSER QUESTION: ${message.trim()}`;
        } else {
            finalUserQueryText = message.trim();
        }

        const finalHistoryForGemini = [
            ...historyForGeminiAPI,
            { role: "user", parts: [{ text: finalUserQueryText }] }
        ];

        console.log(`   Calling Gemini API. History length for Gemini: ${finalHistoryForGemini.length}. System Prompt: ${!!systemPrompt}`);

        const geminiResponseText = await generateContentWithHistory(finalHistoryForGemini, systemPrompt);

        const modelResponseMessage = {
            role: 'model',
            parts: [{ text: geminiResponseText }],
            timestamp: new Date()
        };

        console.log(`<<< POST /api/chat/message successful for session ${sessionId}.`);
        res.status(200).json({ reply: modelResponseMessage });

    } catch (error) {
        console.error(`!!! Error processing chat message for session ${sessionId}:`, error);
        let statusCode = error.status || error.response?.status || 500;
        let clientMessage = error.message || error.response?.data?.message || "Failed to get response from AI service.";
        
        if (statusCode === 500 && !error.response?.data?.message) { 
            clientMessage = "An internal server error occurred while processing the AI response.";
        }
        res.status(statusCode).json({ message: clientMessage });
    }
});

// --- @route POST /api/chat/history --- (Keep your existing implementation)
router.post('/history', tempAuth, async (req, res) => {
    const { sessionId, messages } = req.body;
    const userId = req.user._id; 
    if (!sessionId) return res.status(400).json({ message: 'Session ID required to save history.' });
    if (!Array.isArray(messages)) return res.status(400).json({ message: 'Invalid messages format.' });

    console.log(`>>> POST /api/chat/history: User=${userId}, Session=${sessionId}, Messages=${messages.length}`);

    try {
        const validMessages = messages.filter(m =>
            m && typeof m.role === 'string' &&
            Array.isArray(m.parts) && m.parts.length > 0 &&
            typeof m.parts[0].text === 'string' &&
            m.timestamp 
        ).map(m => ({ 
            role: m.role,
            parts: [{ text: m.parts[0].text }], 
            timestamp: new Date(m.timestamp) 
        }));

        if (validMessages.length !== messages.length) {
             console.warn(`Session ${sessionId}: Filtered out ${messages.length - validMessages.length} invalid messages during save attempt.`);
        }
        if (validMessages.length === 0 && messages.length > 0) { 
            console.warn(`Session ${sessionId}: All ${messages.length} messages were invalid. No history saved.`);
            const newSessionIdForClient = uuidv4();
            return res.status(200).json({
                message: 'No valid messages to save. Chat not saved. New session ID provided.',
                savedSessionId: null,
                newSessionId: newSessionIdForClient
            });
        }
        if (validMessages.length === 0 && messages.length === 0) { 
             console.log(`Session ${sessionId}: No messages provided to save. Generating new session ID for client.`);
             const newSessionIdForClient = uuidv4();
             return res.status(200).json({
                 message: 'No history provided to save. New session ID for client.',
                 savedSessionId: null,
                 newSessionId: newSessionIdForClient
             });
        }

        const savedHistory = await ChatHistory.findOneAndUpdate(
            { sessionId: sessionId, userId: userId },
            { $set: { userId: userId, sessionId: sessionId, messages: validMessages, updatedAt: Date.now() } },
            { new: true, upsert: true, setDefaultsOnInsert: true }
        );
        const newClientSessionId = uuidv4(); 
        console.log(`<<< POST /api/chat/history: History saved for session ${savedHistory.sessionId}. New client session ID: ${newClientSessionId}`);
        res.status(200).json({
            message: 'Chat history saved successfully.',
            savedSessionId: savedHistory.sessionId,
            newSessionId: newClientSessionId 
        });
    } catch (error) {
        console.error(`!!! Error saving chat history for session ${sessionId}:`, error);
        if (error.name === 'ValidationError') return res.status(400).json({ message: "Validation Error saving history: " + error.message });
        if (error.code === 11000) return res.status(409).json({ message: "Conflict: Session ID might already exist unexpectedly." });
        res.status(500).json({ message: 'Failed to save chat history due to a server error.' });
    }
});

// --- @route GET /api/chat/sessions --- (Keep your existing implementation)
router.get('/sessions', tempAuth, async (req, res) => {
    const userId = req.user._id;
    console.log(`>>> GET /api/chat/sessions: User=${userId}`);
    try {
        const sessions = await ChatHistory.find({ userId: userId })
            .sort({ updatedAt: -1 }) 
            .select('sessionId createdAt updatedAt messages') 
            .lean(); 

        const sessionSummaries = sessions.map(session => {
             const firstUserMessage = session.messages?.find(m => m.role === 'user');
             let preview = 'Chat Session'; 
             if (firstUserMessage?.parts?.[0]?.text) {
                 preview = firstUserMessage.parts[0].text.substring(0, 75);
                 if (firstUserMessage.parts[0].text.length > 75) {
                     preview += '...';
                 }
             }
             return {
                 sessionId: session.sessionId,
                 createdAt: session.createdAt,
                 updatedAt: session.updatedAt,
                 messageCount: session.messages?.length || 0,
                 preview: preview
             };
        });
        console.log(`<<< GET /api/chat/sessions: Found ${sessionSummaries.length} sessions for User ${userId}.`);
        res.status(200).json(sessionSummaries);
    } catch (error) {
        console.error(`!!! Error fetching chat sessions for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat sessions.' });
    }
});

// --- @route GET /api/chat/session/:sessionId --- (Keep your existing implementation)
router.get('/session/:sessionId', tempAuth, async (req, res) => {
    const userId = req.user._id;
    const { sessionId } = req.params;
    console.log(`>>> GET /api/chat/session/${sessionId}: User=${userId}`);
    if (!sessionId) return res.status(400).json({ message: 'Session ID parameter is required.' });
    try {
        const session = await ChatHistory.findOne({ sessionId: sessionId, userId: userId }).lean();
        if (!session) {
            console.log(`--- GET /api/chat/session/${sessionId}: Session not found for User ${userId}.`);
            return res.status(404).json({ message: 'Chat session not found or access denied.' });
        }
        console.log(`<<< GET /api/chat/session/${sessionId}: Session found for User ${userId}.`);
        res.status(200).json(session);
    } catch (error) {
        console.error(`!!! Error fetching chat session ${sessionId} for user ${userId}:`, error);
        res.status(500).json({ message: 'Failed to retrieve chat session details.' });
    }
});

module.exports = router;


routes/files.js

javascript
// server/routes/files.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const { tempAuth } = require('../middleware/authMiddleware');

const router = express.Router();

const ASSETS_DIR = path.join(__dirname, '..', 'assets');
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets');

// --- Helper functions ---
const sanitizeUsernameForDir = (username) => {
    if (!username) return '';
    return username.replace(/[^a-zA-Z0-9_-]/g, '_');
};
const parseServerFilename = (filename) => {
    const match = filename.match(/^(\d+)-(.*?)(\.\w+)$/);
    if (match && match.length === 4) {
        return { timestamp: match[1], originalName: `${match[2]}${match[3]}`, extension: match[3] };
    }
     // Handle cases where the original name might not have an extension or parsing fails
    const ext = path.extname(filename);
    const base = filename.substring(0, filename.length - ext.length);
    const tsMatch = base.match(/^(\d+)-(.*)$/);
    if (tsMatch) {
        return { timestamp: tsMatch[1], originalName: `${tsMatch[2]}${ext}`, extension: ext };
    }
    // Fallback if no timestamp prefix found (less ideal)
    return { timestamp: null, originalName: filename, extension: path.extname(filename) };
};
const ensureDirExists = async (dirPath) => {
    try { await fs.mkdir(dirPath, { recursive: true }); }
    catch (error) { if (error.code !== 'EEXIST') { console.error(`Error creating dir ${dirPath}:`, error); throw error; } }
};
// --- End Helper Functions ---


// --- @route   GET /api/files ---
// Use tempAuth middleware
router.get('/', tempAuth, async (req, res) => {
    // req.user is guaranteed to exist here because of tempAuth middleware
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username);
    if (!sanitizedUsername) {
        console.warn("GET /api/files: Invalid user identifier after sanitization.");
        return res.status(400).json({ message: 'Invalid user identifier.' });
    }

    const userAssetsDir = path.join(ASSETS_DIR, sanitizedUsername);
    const fileTypes = ['docs', 'images', 'code', 'others'];
    const userFiles = [];

    try {
        // Check if user directory exists
        try { await fs.access(userAssetsDir); }
        catch (e) {
             if (e.code === 'ENOENT') { return res.status(200).json([]); } // No dir, no files
             throw e; // Other error
        }

        // Scan subdirectories
        for (const type of fileTypes) {
            const typeDir = path.join(userAssetsDir, type);
            try {
                const filesInDir = await fs.readdir(typeDir);
                for (const filename of filesInDir) {
                    const filePath = path.join(typeDir, filename);
                    try {
                        const stats = await fs.stat(filePath);
                        if (stats.isFile()) {
                            const parsed = parseServerFilename(filename);
                            userFiles.push({
                                serverFilename: filename, originalName: parsed.originalName, type: type,
                                relativePath: path.join(type, filename).replace(/\\/g, '/'),
                                size: stats.size, lastModified: stats.mtime,
                            });
                        }
                    } catch (statError) { console.warn(`GET /api/files: Stat failed for ${filePath}:`, statError.message); }
                }
            } catch (err) { if (err.code !== 'ENOENT') { console.warn(`GET /api/files: Read failed for ${typeDir}:`, err.message); } }
        }

        userFiles.sort((a, b) => a.originalName.localeCompare(b.originalName));
        res.status(200).json(userFiles);

    } catch (error) {
        console.error(`!!! Error in GET /api/files for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to retrieve file list.' });
    }
});

// --- @route   PATCH /api/files/:serverFilename ---
// Use tempAuth middleware
router.patch('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const { newOriginalName } = req.body;
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username); // req.user set by tempAuth

    // Validations
    if (!sanitizedUsername) return res.status(400).json({ message: 'Invalid user identifier.' });
    if (!serverFilename) return res.status(400).json({ message: 'Server filename parameter is required.' });
    if (!newOriginalName || typeof newOriginalName !== 'string' || newOriginalName.trim() === '') return res.status(400).json({ message: 'New file name is required.' });
    if (newOriginalName.includes('/') || newOriginalName.includes('\\') || newOriginalName.includes('..')) return res.status(400).json({ message: 'New file name contains invalid characters.' });

    try {
        const parsedOld = parseServerFilename(serverFilename);
        if (!parsedOld.timestamp) return res.status(400).json({ message: 'Invalid server filename format (missing timestamp prefix).' });

        // Find current file path
        let currentPath = null; let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsername, type, serverFilename);
            try { await fs.access(potentialPath); currentPath = potentialPath; fileType = type; break; }
            catch (e) { if (e.code !== 'ENOENT') throw e; }
        }
        if (!currentPath) return res.status(404).json({ message: 'File not found or access denied.' });

        // Construct new path
        const newExt = path.extname(newOriginalName) || parsedOld.extension; // Preserve original ext if new one is missing
        const newBaseName = path.basename(newOriginalName, path.extname(newOriginalName)); // Get base name without extension
        const sanitizedNewBase = newBaseName.replace(/[^a-zA-Z0-9._-]/g, '_'); // Sanitize only the base name
        const finalNewOriginalName = `${sanitizedNewBase}${newExt}`; // Reconstruct original name
        const newServerFilename = `${parsedOld.timestamp}-${finalNewOriginalName}`; // Keep timestamp, use sanitized original name
        const newPath = path.join(ASSETS_DIR, sanitizedUsername, fileType, newServerFilename);

        // Perform rename
        await fs.rename(currentPath, newPath);

        res.status(200).json({
            message: 'File renamed successfully!', oldFilename: serverFilename,
            newFilename: newServerFilename, newOriginalName: finalNewOriginalName,
        });

    } catch (error) {
        console.error(`!!! Error in PATCH /api/files/${serverFilename} for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to rename the file.' });
    }
});


// --- @route   DELETE /api/files/:serverFilename ---
// Use tempAuth middleware
router.delete('/:serverFilename', tempAuth, async (req, res) => {
    const { serverFilename } = req.params;
    const sanitizedUsername = sanitizeUsernameForDir(req.user.username); // req.user set by tempAuth

    // Validations
    if (!sanitizedUsername) return res.status(400).json({ message: 'Invalid user identifier.' });
    if (!serverFilename) return res.status(400).json({ message: 'Server filename parameter is required.' });

    try {
        // Find current path
        let currentPath = null; let fileType = '';
        const fileTypesToSearch = ['docs', 'images', 'code', 'others'];
        for (const type of fileTypesToSearch) {
            const potentialPath = path.join(ASSETS_DIR, sanitizedUsername, type, serverFilename);
            try { await fs.access(potentialPath); currentPath = potentialPath; fileType = type; break; }
            catch (e) { if (e.code !== 'ENOENT') throw e; }
        }
        if (!currentPath) return res.status(404).json({ message: 'File not found or access denied.' });

        // Determine backup path
        const backupUserDir = path.join(BACKUP_DIR, sanitizedUsername, fileType);
        await ensureDirExists(backupUserDir);
        const backupPath = path.join(backupUserDir, serverFilename);

        // Perform move
        await fs.rename(currentPath, backupPath);

        res.status(200).json({ message: 'File deleted successfully (moved to backup).', filename: serverFilename });

    } catch (error) {
        console.error(`!!! Error in DELETE /api/files/${serverFilename} for user ${sanitizedUsername}:`, error);
        res.status(500).json({ message: 'Failed to delete the file.' });
    }
});

module.exports = router;



routes/network.js

javascript
const express = require('express');
const router = express.Router();
const os = require('os');

function getAllIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost by default

    for (const [name, netInterface] of Object.entries(interfaces)) {
        // Skip loopback and potentially virtual interfaces if desired
        if (name.includes('lo') || name.toLowerCase().includes('virtual') || name.toLowerCase().includes('vmnet')) continue;

        for (const addr of netInterface) {
            // Focus on IPv4, non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

router.get('/ip', (req, res) => {
    res.json({
        ips: getAllIPs(),
        // req.ip might be less reliable behind proxies, but can be included
        // currentRequestIp: req.ip
    });
});

module.exports = router;



routes/syllabus.js

javascript
// server/routes/syllabus.js
const express = require('express');
const fs = require('fs').promises;
const path = require('path');
const { tempAuth } = require('../middleware/authMiddleware'); // Protect the route

const router = express.Router();
const SYLLABI_DIR = path.join(__dirname, '..', 'syllabi');

// --- @route   GET /api/syllabus/:subjectId ---
// --- @desc    Get syllabus content for a specific subject ---
// --- @access  Private (requires auth) ---
router.get('/:subjectId', tempAuth, async (req, res) => {
    const { subjectId } = req.params;

    // Basic sanitization: Allow only alphanumeric and underscores
    // Prevents directory traversal (e.g., ../../etc/passwd)
    const sanitizedSubjectId = subjectId.replace(/[^a-zA-Z0-9_]/g, '');

    if (!sanitizedSubjectId || sanitizedSubjectId !== subjectId) {
        console.warn(`Syllabus request rejected due to invalid characters: ${subjectId}`);
        return res.status(400).json({ message: 'Invalid subject identifier format.' });
    }

    const filePath = path.join(SYLLABI_DIR, `${sanitizedSubjectId}.md`);

    try {
        // Check if file exists first (more specific error)
        await fs.access(filePath);

        // Read the file content
        const content = await fs.readFile(filePath, 'utf-8');

        res.status(200).json({ syllabus: content });

    } catch (error) {
        if (error.code === 'ENOENT') {
            console.warn(`Syllabus file not found: ${filePath}`);
            return res.status(404).json({ message: `Syllabus for '${subjectId}' not found.` });
        } else {
            console.error(`Error reading syllabus file ${filePath}:`, error);
            return res.status(500).json({ message: 'Server error retrieving syllabus.' });
        }
    }
});

module.exports = router;



routes/upload.js

javascript
// server/routes/upload.js
const express = require('express');
const multer = require('multer');
const path = require('path');
const fs = require('fs');
const axios = require('axios');
const { tempAuth } = require('../middleware/authMiddleware');
const User = require('../models/User'); // Import the User model
const { ANALYSIS_PROMPTS } = require('../config/promptTemplates'); 
const geminiService = require('../services/geminiService');

const router = express.Router();

// --- Constants ---
const UPLOAD_DIR = path.join(__dirname, '..', 'assets');
const MAX_FILE_SIZE = 20 * 1024 * 1024; // 20 MB

// Define allowed types by mimetype and extension (lowercase)
// Mapping mimetype to subfolder name
const allowedMimeTypes = {
    // Documents -> 'docs'
    'application/pdf': 'docs',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docs', // .docx
    'application/msword': 'docs', // .doc (Might be less reliable mimetype)
    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'docs', // .pptx
    'application/vnd.ms-powerpoint': 'docs', // .ppt (Might be less reliable mimetype)
    'text/plain': 'docs', // .txt
    // Code -> 'code'
    'text/x-python': 'code', // .py
    'application/javascript': 'code', // .js
    'text/javascript': 'code', // .js (alternative)
    'text/markdown': 'code', // .md
    'text/html': 'code', // .html
    'application/xml': 'code', // .xml
    'text/xml': 'code', // .xml
    'application/json': 'code', // .json
    'text/csv': 'code', // .csv
    // Images -> 'images'
    'image/jpeg': 'images',
    'image/png': 'images',
    'image/bmp': 'images',
    'image/gif': 'images',
    // Add more specific types if needed, otherwise they fall into 'others'
};
// Define allowed extensions (lowercase) - This is a secondary check
const allowedExtensions = [
    '.pdf', '.docx', '.doc', '.pptx', '.ppt', '.txt',
    '.py', '.js', '.md', '.html', '.xml', '.json', '.csv', '.log', // Added .log
    '.jpg', '.jpeg', '.png', '.bmp', '.gif'
];

// --- Multer Config ---
const storage = multer.diskStorage({
    destination: (req, file, cb) => {
        // tempAuth middleware ensures req.user exists here
        if (!req.user || !req.user.username) {
            // This should ideally not happen if tempAuth works correctly
            console.error("Multer Destination Error: User context missing after auth middleware.");
            return cb(new Error("Authentication error: User context not found."));
        }
        const sanitizedUsername = req.user.username.replace(/[^a-zA-Z0-9_-]/g, '_');
        const fileMimeType = file.mimetype.toLowerCase();

        // Determine subfolder based on mimetype, default to 'others'
        const fileTypeSubfolder = allowedMimeTypes[fileMimeType] || 'others';
        const destinationPath = path.join(UPLOAD_DIR, sanitizedUsername, fileTypeSubfolder);

        // Ensure the destination directory exists (use async for safety)
        fs.mkdir(destinationPath, { recursive: true }, (err) => {
             if (err) {
                 console.error(`Error creating destination path ${destinationPath}:`, err);
                 cb(err);
             } else {
                 cb(null, destinationPath);
             }
         });
    },
    filename: (req, file, cb) => {
        const timestamp = Date.now();
        const fileExt = path.extname(file.originalname).toLowerCase();
        // Sanitize base name: remove extension, replace invalid chars, limit length
        const sanitizedBaseName = path.basename(file.originalname, fileExt)
                                      .replace(/[^a-zA-Z0-9._-]/g, '_') // Allow letters, numbers, dot, underscore, hyphen
                                      .substring(0, 100); // Limit base name length
        const uniqueFilename = `${timestamp}-${sanitizedBaseName}${fileExt}`;
        cb(null, uniqueFilename);
    }
});

const fileFilter = (req, file, cb) => {
    // tempAuth middleware should run before this, ensuring req.user exists
    if (!req.user) {
         console.warn(`Upload Rejected (File Filter): User context missing.`);
         const error = new multer.MulterError('UNAUTHENTICATED'); // Custom code?
         error.message = `User not authenticated.`;
         return cb(error, false);
    }

    const fileExt = path.extname(file.originalname).toLowerCase();
    const mimeType = file.mimetype.toLowerCase();

    // Primary check: Mimetype must be in our known list OR extension must be allowed
    // Secondary check: Extension must be in the allowed list
    const isMimeTypeKnown = !!allowedMimeTypes[mimeType];
    const isExtensionAllowed = allowedExtensions.includes(fileExt);

    // Allow if (MIME type is known OR extension is explicitly allowed) AND extension is in the allowed list
    // This allows known MIME types even if extension isn't listed, and listed extensions even if MIME isn't known (e.g. text/plain for .log)
    // But we always require the extension itself to be in the allowed list for safety.
    // if ((isMimeTypeKnown || isExtensionAllowed) && isExtensionAllowed) {

    // Stricter: Allow only if BOTH mimetype is known AND extension is allowed
    if (isMimeTypeKnown && isExtensionAllowed) {
        cb(null, true); // Accept file
    } else {
        console.warn(`Upload Rejected (File Filter): User='${req.user.username}', File='${file.originalname}', MIME='${mimeType}', Ext='${fileExt}'. MimeKnown=${isMimeTypeKnown}, ExtAllowed=${isExtensionAllowed}`);
        const error = new multer.MulterError('LIMIT_UNEXPECTED_FILE');
        error.message = `Invalid file type or extension. Allowed extensions: ${allowedExtensions.join(', ')}`;
        cb(error, false); // Reject file
    }
};

const upload = multer({
    storage: storage,
    fileFilter: fileFilter,
    limits: { fileSize: MAX_FILE_SIZE }
});
// --- End Multer Config ---


// --- Function to call Python RAG service ---
async function triggerPythonRagProcessing(userId, filePath, originalName) {
    // Read URL from environment variable set during startup
    const pythonServiceUrl = process.env.PYTHON_RAG_SERVICE_URL;
    if (!pythonServiceUrl) {
        console.error("PYTHON_RAG_SERVICE_URL is not set in environment. Cannot trigger processing.");
        // Optionally: Delete the uploaded file if processing can't be triggered?
        // await fs.promises.unlink(filePath).catch(e => console.error(`Failed to delete unprocessed file ${filePath}: ${e}`));
        return { success: false, message: "RAG service URL not configured." }; // Indicate failure
    }
    const addDocumentUrl = `${pythonServiceUrl}/add_document`;
    console.log(`Triggering Python RAG processing for ${originalName} (User: ${userId}) at ${addDocumentUrl}`);
    try {
        // Send absolute path
        const response = await axios.post(addDocumentUrl, {
            user_id: userId,
            file_path: filePath, // Send the absolute path
            original_name: originalName
        }, { timeout: 300000 }); // 5 minute timeout for processing

        console.log(`Python RAG service response for ${originalName}:`, response.data);

        const text = response.data?.raw_text_for_analysis || "";

        // --- DATABASE UPDATE | Filename & text ---
        if (response.data?.status === "added" && originalName && userId) {
            try {
                const newDocumentEntry = {
                    filename: originalName,
                    text: text,
                };

                const updatedUser = await User.findByIdAndUpdate(
                    userId,
                    { $push: { uploadedDocuments: newDocumentEntry } },
                    { new: true, runValidators: true }
                );

                if (!updatedUser) {
                    console.error(`Failed to find user with ID ${userId} to save document info for ${originalName}.`);
                    return {
                        success: false,
                        message: `User not found for saving document metadata. Status: ${response.data?.status}`,
                        Text: text, 
                        Status: response.data?.status
                    };
                }
                console.log(`Successfully saved document info ('${originalName}') and raw text to user ${userId}.`);

            } catch (dbError) {
                console.error(`Database error saving document info for ${originalName} (User: ${userId}):`, dbError);
                return {
                    success: false,
                    message: `DB error saving document metadata: ${dbError.message}. Python processing status: ${response.data?.status}`,
                    Text: text, // Still return text if Python provided it
                    Status: response.data?.status
                };
            }
        } 
        else if (originalName && userId) { // If not saving, log why
            console.warn(`Skipping DB update for ${originalName} (User: ${userId}). HTTP Status: ${response.status}, Python Custom Status: ${response.data?.status}.`);
        } 
        else {
            console.warn(`Skipping DB update due to missing originalName or userId. Python Custom Status: ${response.data?.status}`);
        }

        // --- END DATABASE UPDATE ---


        // Check response.data.status ('added' or 'skipped')
        if (response.data?.status === 'skipped') {
             console.warn(`Python RAG service skipped processing ${originalName}: ${response.data.message}`);
             return { success: true, status: 'skipped', message: response.data.message, text: text};
        } else if (response.data?.status === 'added') {
             return { success: true, status: 'added', message: response.data.message, text: text };
        } else {
             console.warn(`Unexpected response status from Python RAG service for ${originalName}: ${response.data?.status}`);
             return { success: false, message: `Unexpected RAG status: ${response.data?.status}` };
        }

    } catch (error) {
        const errorMsg = error.response?.data?.error || error.message || "Unknown RAG service error";
        console.error(`Error calling Python RAG service for ${originalName}:`, errorMsg);
        // Maybe delete the file if the call fails? Depends on retry logic.
        // await fs.promises.unlink(filePath).catch(e => console.error(`Failed to delete file ${filePath} after RAG call error: ${e}`));
        return { success: false, message: `RAG service call failed: ${errorMsg}` }; // Indicate failure
    }
}
// --- End Function ---


// --- Function to call Generate Analysis
async function triggerAnalysisGeneration(userId, originalName, textForAnalysis) {
    console.log(`Starting analysis generation for document '${originalName}', User ID: ${userId}. Text length: ${textForAnalysis.length}`);

    let allAnalysesSuccessful = true; // Assume success initially
    const analysisResults = {
        faq: null,
        topics: null,
        mindmap: null
    };
    const logCtx = { userId, originalName }; // Context for logging within generateSingleAnalysis

    // Inner helper function to generate a single type of analysis
    async function generateSingleAnalysis(type, promptContent, context) {
        try {
            console.log(`Attempting to generate ${type} for '${context.originalName}' (User: ${context.userId}).`);

            // Prepare history for geminiService.generateContentWithHistory
            // The 'promptContent' (which is the system prompt) will be passed as the second argument.
            const historyForGemini = [
                { role: 'user', parts: [{ text: "Please perform the requested analysis based on the system instruction provided." }] }
            ];

            const generatedText = await geminiService.generateContentWithHistory(
                historyForGemini,
                promptContent // This is passed as systemPromptText to generateContentWithHistory
            );

            if (!generatedText || typeof generatedText !== 'string' || generatedText.trim() === "") {
                console.warn(`Gemini returned empty or invalid content for ${type} for '${context.originalName}'.`);
                allAnalysesSuccessful = false; // Update the outer scope variable
                return `Notice: No content was generated by the AI for ${type}. The input text might have been unsuitable or the AI returned an empty response.`;
            }

            console.log(`${type} generation successful for '${context.originalName}'. Length: ${generatedText.length}`);
            return generatedText.trim();

        } catch (error) {
            console.error(`Error during ${type} generation for '${context.originalName}' (User: ${context.userId}): ${error.message}`);
            allAnalysesSuccessful = false; // Update the outer scope variable
            // Return a user-friendly error message, or a snippet of the technical error
            const errorMessage = error.message || "Unknown error during AI generation.";
            return `Error generating ${type}: ${errorMessage.split('\n')[0].substring(0, 250)}`; // First line of error, truncated
        }
    }

    // 1. Generate FAQs
    console.log(`[Analysis Step 1/3] Preparing FAQ generation for '${originalName}'.`);
    const faqPrompt = ANALYSIS_PROMPTS.faq.getPrompt(textForAnalysis);
    analysisResults.faq = await generateSingleAnalysis('FAQ', faqPrompt, logCtx);
    if (!allAnalysesSuccessful) {
        console.warn(`FAQ generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
        // We continue even if one fails, allAnalysesSuccessful flag will reflect the overall status.
    }

    // 2. Generate Topics
    console.log(`[Analysis Step 2/3] Preparing Topics generation for '${originalName}'.`);
    const topicsPrompt = ANALYSIS_PROMPTS.topics.getPrompt(textForAnalysis);
    analysisResults.topics = await generateSingleAnalysis('Topics', topicsPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.topics.startsWith("Error generating Topics:")) { // Check if this specific step failed
        console.warn(`Topics generation failed or produced no content for '${originalName}'. Continuing to next analysis type.`);
    }


    // 3. Generate Mindmap
    console.log(`[Analysis Step 3/3] Preparing Mindmap generation for '${originalName}'.`);
    const mindmapPrompt = ANALYSIS_PROMPTS.mindmap.getPrompt(textForAnalysis);
    analysisResults.mindmap = await generateSingleAnalysis('Mindmap', mindmapPrompt, logCtx);
    if (!allAnalysesSuccessful && analysisResults.mindmap.startsWith("Error generating Mindmap:")) { // Check if this specific step failed
        console.warn(`Mindmap generation failed or produced no content for '${originalName}'.`);
    }

    // Log final outcome of the analysis generation process
    if (allAnalysesSuccessful) {
        console.log(`All analyses (FAQ, Topics, Mindmap) appear to have been generated successfully for '${originalName}'.`);
    } else {
        console.warn(`One or more analyses failed or produced no content for '${originalName}'. Review individual results for details.`);
        // Log the specific results for easier debugging
        console.warn(`FAQ Result for '${originalName}': ${analysisResults.faq.substring(0,100)}...`);
        console.warn(`Topics Result for '${originalName}': ${analysisResults.topics.substring(0,100)}...`);
        console.warn(`Mindmap Result for '${originalName}': ${analysisResults.mindmap.substring(0,100)}...`);
    }

    return {
        success: allAnalysesSuccessful,
        results: analysisResults
    };
}
// --- End Analysis Generation Function ---


// --- Modified Upload Route ---
router.post('/', tempAuth, (req, res) => {
    const uploader = upload.single('file');

    uploader(req, res, async function (err) { // <<<< ASYNC HERE IS KEY
        if (!req.user) {
             console.error("Upload handler: User context missing.");
             return res.status(401).json({ message: "Authentication error." });
        }
        const userId = req.user._id.toString();

        if (err) {
            // ... (your multer error handling - this part is fine)
            console.error(`Multer error for user ${req.user.username}:`, err.message);
            if (err instanceof multer.MulterError) { /* ... */ return res.status(400).json({ message: err.message || "File upload failed."}); }
            return res.status(500).json({ message: "Server error during upload prep." });
        }
        if (!req.file) {
            console.warn(`No file received for user ${req.user.username}.`);
            return res.status(400).json({ message: "No file received or file type rejected." });
        }

        const { path: filePath, originalname: originalName, filename: serverFilename } = req.file;
        const absoluteFilePath = path.resolve(filePath);
        console.log(`Upload received: User '${req.user.username}', File: ${serverFilename}, Original: ${originalName}`);

        // --- Main Try-Catch for the entire RAG + Analysis process ---
        try {
            // ----- STAGE 1: MongoDB Pre-check for existing originalName -----
            const userForPreCheck = await User.findById(userId).select('uploadedDocuments');
            if (!userForPreCheck) {
                console.error(`Upload Aborted: User ${userId} not found (pre-check). Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (user not found): ${e}`));
                return res.status(404).json({ message: "User not found, cannot process upload." });
            }
            const existingDocument = userForPreCheck.uploadedDocuments.find(doc => doc.filename === originalName);
            if (existingDocument) {
                console.log(`Upload Halted: '${originalName}' already exists for user ${userId}. Deleting ${absoluteFilePath}`);
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (duplicate): ${e}`));
                return res.status(409).json({
                    message: `File '${originalName}' already exists. No new processing initiated.`,
                    filename: serverFilename, originalname: originalName,
                });
            }
            console.log(`Pre-check passed for '${originalName}'. Proceeding to RAG.`);
            // ----- END STAGE 1 -----


            // ----- STAGE 2: RAG Processing -----
            const ragResult = await triggerPythonRagProcessing(userId, absoluteFilePath, originalName);

            if (!ragResult.success || !ragResult.text || ragResult.text.trim() === "") {
                let message = `RAG processing failed or returned no text for '${originalName}'.`;
                if (ragResult.message) message += ` Details: ${ragResult.message}`;
                console.error(message + ` (User: ${userId})`);

                // If RAG failed, the file text wasn't added to DB by triggerPythonRagProcessing
                // (assuming triggerPythonRagProcessing only adds to DB on its own success).
                // So, delete the physical file.
                await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (RAG fail/no text) for ${absoluteFilePath}: ${e}`));

                return res.status(500).json({ // Or 422 if it's a content issue from RAG
                    message: message,
                    filename: serverFilename, originalname: originalName
                });
            }
            console.log(`RAG processing completed with text for '${originalName}'. Proceeding to analysis.`);
            // At this point, triggerPythonRagProcessing should have added the document with text to MongoDB.
            // If it didn't, the logic in triggerPythonRagProcessing needs adjustment.


            // ----- STAGE 3: Analysis Generation -----
            const analysisOutcome = await triggerAnalysisGeneration(userId, originalName, ragResult.text);


            // ----- STAGE 4: Handle Analysis Outcome & DB Update -----
            if (analysisOutcome.success) {
                console.log(`All analyses generated successfully for '${originalName}'. Storing in DB.`);
                await User.updateOne(
                    { _id: userId, "uploadedDocuments.filename": originalName },
                    {
                        $set: {
                            "uploadedDocuments.$.analysis.faq": analysisOutcome.results.faq,
                            "uploadedDocuments.$.analysis.topics": analysisOutcome.results.topics,
                            "uploadedDocuments.$.analysis.mindmap": analysisOutcome.results.mindmap,
                        }
                    }
                );
                console.log(`Successfully stored all analyses for '${originalName}' in MongoDB.`);
                return res.status(200).json({
                    message: "File uploaded and all analyses completed successfully.",
                    filename: serverFilename, originalname: originalName,
                    // analysis: analysisOutcome.results // Optionally send analysis back
                });
            } else {
                console.warn(`One or more analyses failed for '${originalName}'. No analysis data from this attempt will be stored in DB.`);
                // The RAG text is already in the DB. This analysis attempt failed.
                // Optionally update a status for this document in DB to indicate analysis failure
                return res.status(422).json({
                    message: "File uploaded and RAG processing complete, but content analysis generation failed. The document text is saved. You may not need to re-upload but can try to trigger analysis again later if such a feature exists, or contact support.",
                    filename: serverFilename, originalname: originalName,
                    // failedAnalysisDetails: analysisOutcome.results // For client-side debugging if needed
                });
            }
        } catch (processError) {
            // Catch any unhandled errors from awaited promises or synchronous code
            console.error(`!!! Overall processing error for ${originalName} (User: ${userId}):`, processError);
            // Try to clean up the uploaded file if it exists and an error occurred.
            // This is a bit tricky as RAG might have already added to DB.
            // If processError is from RAG or before, file deletion is safer.
            // If it's after RAG success but during analysis or DB update of analysis,
            // the RAG text is already in DB.
            if (absoluteFilePath) {
                 // Consider if an error after RAG success should still delete the physical file.
                 // Usually, if the RAG text is in DB, the physical file on disk is secondary.
                 // But if the entire transaction failed, deleting is fine.
                 await fs.promises.unlink(absoluteFilePath).catch(e => console.error(`Cleanup error (overall fail) for ${absoluteFilePath}: ${e}`));
            }
            return res.status(500).json({
                message: `Server error during file processing: ${processError.message}`,
                filename: serverFilename, originalname: originalName
            });
        }
    });
});


module.exports = router;



server.js

javascript
// server/server.js
const express = require('express');
const dotenv = require('dotenv'); // Removed dotenv
const cors = require('cors');
const path = require('path');
const { getLocalIPs } = require('./utils/networkUtils');
const fs = require('fs');
const axios = require('axios');
const os = require('os');
const mongoose = require('mongoose'); // Import mongoose for closing connection
const readline = require('readline').createInterface({ // For prompting
  input: process.stdin,
  output: process.stdout,
});

// --- Custom Modules ---
const connectDB = require('./config/db');
const { performAssetCleanup } = require('./utils/assetCleanup');

// --- Configuration Loading ---
dotenv.config(); // Removed dotenv

// --- Configuration Defaults & Variables ---
const DEFAULT_PORT = 5001;
const DEFAULT_MONGO_URI = 'mongodb://localhost:27017/chatbotGeminiDB'; // Default DB URI
const DEFAULT_PYTHON_RAG_URL = 'http://localhost:5002'; // Default RAG service URL

let port = process.env.PORT || DEFAULT_PORT; // Use environment variable PORT if set, otherwise default
let mongoUri = process.env.MONGO_URI || ''; // Use environment variable if set
let pythonRagUrl = process.env.PYTHON_RAG_SERVICE_URL || ''; // Use environment variable if set
let geminiApiKey = process.env.GEMINI_API_KEY || ''; // MUST be set via environment

// --- Express Application Setup ---
const app = express();

// --- Core Middleware ---
app.use(cors()); // Allows requests from frontend (potentially on different IPs in LAN)
app.use(express.json());

// --- Basic Root Route ---
app.get('/', (req, res) => res.send('Chatbot Backend API is running...'));

// --- API Route Mounting ---
app.use('/api/network', require('./routes/network')); // For IP info
app.use('/api/auth', require('./routes/auth'));
app.use('/api/chat', require('./routes/chat'));
app.use('/api/upload', require('./routes/upload'));
app.use('/api/files', require('./routes/files'));
app.use('/api/syllabus', require('./routes/syllabus')); // <-- ADD THIS LINE
// app.use('/api/analysis', require('./routes/analysis'));
// app.use('/api/kg', require('./routes/kg')); // Knowledge Graph route



// --- Centralized Error Handling Middleware ---
app.use((err, req, res, next) => {
    console.error("Unhandled Error:", err.stack || err);
    const statusCode = err.status || 500;
    let message = err.message || 'An internal server error occurred.';
    // Sanitize potentially sensitive error details in production
    if (process.env.NODE_ENV === 'production' && statusCode === 500) {
        message = 'An internal server error occurred.';
    }
    // Ensure response is JSON for API routes
    if (req.originalUrl.startsWith('/api/')) {
         return res.status(statusCode).json({ message: message });
    }
    // Fallback for non-API routes if any
    res.status(statusCode).send(message);
});

// --- Server Instance Variable ---
let server;

// --- Graceful Shutdown Logic ---
const gracefulShutdown = async (signal) => {
    console.log(`\n${signal} received. Shutting down gracefully...`);
    readline.close(); // Close readline interface
    try {
        // Close HTTP server first to stop accepting new connections
        if (server) {
            server.close(async () => {
                console.log('HTTP server closed.');
                // Close MongoDB connection
                try {
                    await mongoose.connection.close();
                    console.log('MongoDB connection closed.');
                } catch (dbCloseError) {
                    console.error("Error closing MongoDB connection:", dbCloseError);
                }
                process.exit(0); // Exit after server and DB are closed
            });
        } else {
             // If server wasn't assigned, try closing DB and exit
             try {
                 await mongoose.connection.close();
                 console.log('MongoDB connection closed.');
             } catch (dbCloseError) {
                 console.error("Error closing MongoDB connection:", dbCloseError);
             }
            process.exit(0);
        }

        // Force exit after timeout if server.close callback doesn't finish
        setTimeout(() => {
            console.error('Graceful shutdown timed out, forcing exit.');
            process.exit(1);
        }, 10000); // 10 seconds

    } catch (shutdownError) {
        console.error("Error during graceful shutdown initiation:", shutdownError);
        process.exit(1);
    }
};

process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

// --- RAG Service Health Check ---
async function checkRagService(url) {
    console.log(`\nChecking RAG service health at ${url}...`);
    try {
        const response = await axios.get(`${url}/health`, { timeout: 7000 }); // 7 second timeout
        if (response.status === 200 && response.data?.status === 'ok') {
            console.log('âœ“ RAG service is available and healthy.');
            console.log(`  Embedding: ${response.data.embedding_model_type} (${response.data.embedding_model_name})`);
            console.log(`  Default Index Loaded: ${response.data.default_index_loaded}`);
            if (response.data.message && response.data.message.includes("Warning:")) {
                 console.warn(`  RAG Health Warning: ${response.data.message}`);
            }
            return true;
        } else {
             console.warn(`! RAG service responded but status is not OK: ${response.status} - ${JSON.stringify(response.data)}`);
             return false;
        }
    } catch (error) {
        console.warn('! RAG service is not reachable.');
        if (error.code === 'ECONNREFUSED') {
             console.warn(`  Connection refused at ${url}. Ensure the RAG service (server/rag_service/app.py) is running.`);
        } else if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
             console.warn(`  Connection timed out to ${url}. The RAG service might be slow to start or unresponsive.`);
        }
         else {
             console.warn(`  Error: ${error.message}`);
        }
        console.warn('  RAG features (document upload processing, context retrieval) will be unavailable.');
        return false;
    }
}

// --- Directory Structure Check (Simplified) ---
async function ensureServerDirectories() {
    const dirs = [
        path.join(__dirname, 'assets'),
        path.join(__dirname, 'backup_assets'),
        // Add other essential dirs if needed
    ];
    console.log("\nEnsuring server directories exist...");
    try {
        for (const dir of dirs) {
            // Check existence synchronously, create asynchronously
            if (!fs.existsSync(dir)) {
                await fs.promises.mkdir(dir, { recursive: true });
                console.log(`  Created directory: ${dir}`);
            } else {
                // console.log(`  Directory exists: ${dir}`); // Optional: be less verbose
            }
        }
        console.log("âœ“ Server directories checked/created.");
    } catch (error) {
        console.error('!!! Error creating essential server directories:', error);
        throw error; // Prevent server start if essential dirs fail
    }
}

// --- Prompt for Configuration ---
function askQuestion(query) {
    return new Promise(resolve => readline.question(query, resolve));
}

async function configureAndStart() {
    console.log("--- Starting Server Configuration ---");
    
    // 1. Gemini API Key Check
    if (!geminiApiKey) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! FATAL: GEMINI_API_KEY environment variable is not set. !!!");
        console.error("!!! Please set it before running the server:               !!!");
        console.error("!!! export GEMINI_API_KEY='YOUR_API_KEY'                   !!!");
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1);
    } else {
        console.log("âœ“ GEMINI_API_KEY found.");
    }

    // 2. MongoDB URI
    if (!
        mongoUri) {
        const answer = await askQuestion(`Enter MongoDB URI or press Enter for default (${DEFAULT_MONGO_URI}): `);
        mongoUri = answer.trim() || DEFAULT_MONGO_URI;
    }
    console.log(`Using MongoDB URI: ${mongoUri}`);

    // 3. Python RAG Service URL
    if (!pythonRagUrl) {
        const answer = await askQuestion(`Enter Python RAG Service URL or press Enter for default (${DEFAULT_PYTHON_RAG_URL}): `);
        pythonRagUrl = answer.trim() || DEFAULT_PYTHON_RAG_URL;
    }
    console.log(`Using Python RAG Service URL: ${pythonRagUrl}`);

    // 4. Port (Optional override via prompt, primarily uses ENV or default)
    // You could add a prompt here if needed, but ENV variable is common practice
    console.log(`Node.js server will listen on port: ${port}`);

    readline.close(); // Close the prompt interface

    // --- Pass configuration to other modules (if needed) ---
    // We'll make connectDB and services read directly or pass via function calls
    process.env.MONGO_URI = mongoUri; // Set for db.js
    process.env.PYTHON_RAG_SERVICE_URL = pythonRagUrl; // Set for chat.js, upload.js
    // GEMINI_API_KEY is already in process.env

    console.log("--- Configuration Complete ---");

    // --- Proceed with Server Startup ---
    await startServer();
}


// --- Asynchronous Server Startup Function ---
async function startServer() {
    console.log("\n--- Starting Server Initialization ---");
    try {
        await ensureServerDirectories(); // Check/create assets, backup_assets dirs
        await connectDB(mongoUri); // Connect to MongoDB - Pass URI explicitly
        await performAssetCleanup(); // Backup existing assets, create fresh user folders
        await checkRagService(pythonRagUrl); // Check Python RAG service status

        const PORT = port; // Use the configured port
        const availableIPs = getLocalIPs(); // Get all local IPs

        server = app.listen(PORT, '0.0.0.0', () => { // Listen on all interfaces
            console.log('\n=== Node.js Server Ready ===');
            console.log(`ðŸš€ Server listening on port ${PORT}`);
            console.log('   Access the application via these URLs (using common frontend ports):');
            const frontendPorts = [3000, 3001, 8080, 5173]; // Common React/Vite ports
            availableIPs.forEach(ip => {
                 frontendPorts.forEach(fp => {
                    console.log(`   - http://${ip}:${fp} (Frontend) -> Connects to Backend at http://${ip}:${PORT}`);
                 });
            });
            console.log('============================\n');
            console.log("ðŸ’¡ Hint: Client automatically detects backend IP based on how you access the frontend.");
            console.log(`   Ensure firewalls allow connections on port ${PORT} (Backend) and your frontend port.`);
            console.log("--- Server Initialization Complete ---");
        });

    } catch (error) {
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        console.error("!!! Failed to start Node.js server:", error.message);
        console.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!");
        process.exit(1); // Exit if initialization fails
    }
}

// --- Execute Configuration and Server Start ---
configureAndStart();



services/geminiService.js

javascript
// server/services/geminiService.js
const { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } = require('@google/generative-ai');
// require('dotenv').config(); // Removed dotenv

// Read API Key directly from environment variables
const API_KEY = process.env.GEMINI_API_KEY;
const MODEL_NAME = "gemini-1.5-flash"; // Or read from env: process.env.GEMINI_MODEL_NAME || "gemini-1.5-flash";

if (!API_KEY) {
    // This check is now primarily done in server.js before starting
    // But keep a safeguard here.
    console.error("FATAL ERROR: GEMINI_API_KEY is not available in the environment. Server should have exited.");
    // Throw an error instead of exiting here, let the caller handle it
    throw new Error("GEMINI_API_KEY is missing.");
}

const genAI = new GoogleGenerativeAI(API_KEY);

const baseGenerationConfig = {
    temperature: 0.7, // Moderate temperature for creative but grounded responses
    maxOutputTokens: 4096, // Adjust as needed, Flash model limit might be higher
    // topP: 0.9, // Example: Could add nucleus sampling
    // topK: 40,  // Example: Could add top-k sampling
};

// Stricter safety settings - adjust as needed for your use case
const baseSafetySettings = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_ONLY_HIGH },
];

const generateContentWithHistory = async (chatHistory, systemPromptText = null, relevantDocs = []) => {
    try {
        if (!Array.isArray(chatHistory) || chatHistory.length === 0) {
             throw new Error("Chat history must be a non-empty array.");
        }
        // Gemini API requires history to end with a 'user' message for sendMessage
        if (chatHistory[chatHistory.length - 1].role !== 'user') {
            console.error("History for Gemini API must end with a 'user' role message.");
            // Attempt to fix by removing trailing non-user messages if any? Risky.
            // Or just throw error.
            throw new Error("Internal error: Invalid chat history sequence for API call.");
        }

        // --- Prepare Model Options ---
        const modelOptions = {
            model: MODEL_NAME,
            generationConfig: baseGenerationConfig,
            safetySettings: baseSafetySettings,
            // Add system instruction if provided
            ...(systemPromptText && typeof systemPromptText === 'string' && systemPromptText.trim() !== '' && {
                systemInstruction: {
                    // Gemini expects system instruction parts as an array
                    parts: [{ text: systemPromptText.trim() }]
                }
             })
        };
        const model = genAI.getGenerativeModel(modelOptions);


        // --- Prepare History for startChat ---
        // History for startChat should NOT include the latest user message
        const historyForStartChat = chatHistory.slice(0, -1)
            .map(msg => ({ // Ensure correct format
                 role: msg.role,
                 parts: msg.parts.map(part => ({ text: part.text || '' }))
            }))
            .filter(msg => msg.role && msg.parts && msg.parts.length > 0 && typeof msg.parts[0].text === 'string'); // Basic validation

        // --- Start Chat Session ---
        const chat = model.startChat({
            history: historyForStartChat,
        });

        // --- Prepare the message to send ---
        // Get the text from the last user message in the original history
        let lastUserMessageText = chatHistory[chatHistory.length - 1].parts[0].text;

        // Optional: Add a subtle hint for citation if RAG was used (Gemini might pick it up)
        // if (relevantDocs.length > 0) {
        //     const citationHint = ` (Remember to cite sources like ${relevantDocs.map((doc, i) => `[${i+1}] ${doc.documentName}`).slice(0,2).join(', ')} if applicable)`;
        //     lastUserMessageText += citationHint;
        // }

        console.log(`Sending message to Gemini. History length sent to startChat: ${historyForStartChat.length}. System Prompt Used: ${!!modelOptions.systemInstruction}`);
        // console.log("Last User Message Text Sent:", lastUserMessageText.substring(0, 200) + "..."); // Log truncated message

        // --- Send Message ---
        const result = await chat.sendMessage(lastUserMessageText);

        // --- Process Response ---
        const response = result.response;
        const candidate = response?.candidates?.[0];

        // --- Validate Response ---
        if (!candidate || candidate.finishReason === 'STOP' || candidate.finishReason === 'MAX_TOKENS') {
            // Normal completion or max tokens reached
            const responseText = candidate?.content?.parts?.[0]?.text;
            if (typeof responseText === 'string') {
                return responseText; // Success
            } else {
                 console.warn("Gemini response finished normally but text content is missing or invalid.", { finishReason: candidate?.finishReason, content: candidate?.content });
                 throw new Error("Received an empty or invalid response from the AI service.");
            }
        } else {
             // Handle blocked responses or other issues
             const finishReason = candidate?.finishReason || 'Unknown';
             const safetyRatings = candidate?.safetyRatings;
             console.warn("Gemini response was potentially blocked or had issues.", { finishReason, safetyRatings });

             let blockMessage = `AI response generation failed or was blocked.`;
             if (finishReason) blockMessage += ` Reason: ${finishReason}.`;
             if (safetyRatings) {
                const blockedCategories = safetyRatings.filter(r => r.blocked).map(r => r.category).join(', ');
                if (blockedCategories) {
                    blockMessage += ` Blocked Categories: ${blockedCategories}.`;
                }
             }

             const error = new Error(blockMessage);
             error.status = 400; // Treat as a bad request or policy issue
             throw error;
        }

    } catch (error) {
        console.error("Gemini API Call Error:", error?.message || error);
        // Improve error message for client
        let clientMessage = "Failed to get response from AI service.";
        if (error.message?.includes("API key not valid")) {
            clientMessage = "AI Service Error: Invalid API Key.";
        } else if (error.message?.includes("blocked")) {
            clientMessage = error.message; // Use the specific block message
        } else if (error.status === 400) {
             clientMessage = `AI Service Error: ${error.message}`;
        }

        const enhancedError = new Error(clientMessage);
        enhancedError.status = error.status || 500; // Keep original status if available
        enhancedError.originalError = error; // Attach original error if needed for server logs
        throw enhancedError;
    }
};

module.exports = { generateContentWithHistory };



utils/assetCleanup.js

javascript
const fs = require('fs').promises; // Use fs.promises for async operations
const path = require('path');

// Define constants relative to this file's location (server/utils)
const ASSETS_DIR = path.join(__dirname, '..', 'assets'); // Go up one level to server/assets
const BACKUP_DIR = path.join(__dirname, '..', 'backup_assets'); // Go up one level to server/backup_assets
const FOLDER_TYPES = ['docs', 'images', 'code', 'others']; // Folders within each user's asset dir

/**
 * Moves existing user asset folders (docs, images, code, others) to a timestamped
 * backup location and recreates empty asset folders for each user on server startup.
 */
async function performAssetCleanup() {
    console.log("\n--- Starting Asset Cleanup ---");
    try {
        // Ensure backup base directory exists
        await fs.mkdir(BACKUP_DIR, { recursive: true });

        // List potential user directories in assets
        let userDirs = [];
        try {
            userDirs = await fs.readdir(ASSETS_DIR);
        } catch (err) {
            if (err.code === 'ENOENT') {
                console.log("Assets directory doesn't exist yet, creating it and skipping cleanup.");
                await fs.mkdir(ASSETS_DIR, { recursive: true }); // Ensure assets dir exists
                console.log("--- Finished Asset Cleanup (No existing assets found) ---");
                return; // Nothing to clean up
            }
            throw err; // Re-throw other errors accessing assets dir
        }

        if (userDirs.length === 0) {
             console.log("Assets directory is empty. Skipping backup/move operations.");
             console.log("--- Finished Asset Cleanup (No user assets found) ---");
             return;
        }

        const timestamp = new Date().toISOString().replace(/[:.]/g, '-'); // Create a safe timestamp string

        for (const userName of userDirs) {
            const userAssetPath = path.join(ASSETS_DIR, userName);
            const userBackupPathBase = path.join(BACKUP_DIR, userName);
            const userTimestampBackupPath = path.join(userBackupPathBase, `backup_${timestamp}`);

            try {
                // Check if the item in assets is actually a directory
                const stats = await fs.stat(userAssetPath);
                if (!stats.isDirectory()) {
                    console.log(`  Skipping non-directory item in assets: ${userName}`);
                    continue;
                }

                console.log(`  Processing assets for user: [${userName}]`);
                let backupDirCreated = false; // Track if backup dir was created for this user/run
                let movedSomething = false; // Track if anything was actually moved

                // Process each defined folder type (docs, images, etc.)
                for (const type of FOLDER_TYPES) {
                    const sourceTypePath = path.join(userAssetPath, type);
                    try {
                        // Check if the source type directory exists before trying to move
                        await fs.access(sourceTypePath);

                        // If source exists, ensure the timestamped backup directory is ready
                        if (!backupDirCreated) {
                            await fs.mkdir(userTimestampBackupPath, { recursive: true });
                            backupDirCreated = true;
                            // console.log(`    Created backup directory: ${userTimestampBackupPath}`);
                        }

                        // Define the destination path in the backup folder
                        const backupTypePath = path.join(userTimestampBackupPath, type);
                        // console.log(`    Moving ${sourceTypePath} to ${backupTypePath}`);
                        // Move the existing type folder to the backup location
                        await fs.rename(sourceTypePath, backupTypePath);
                        movedSomething = true;

                    } catch (accessErr) {
                        // Ignore error if the source directory doesn't exist (ENOENT)
                        if (accessErr.code !== 'ENOENT') {
                            console.error(`    Error accessing source folder ${sourceTypePath}:`, accessErr.message);
                        }
                        // If ENOENT, the folder doesn't exist, nothing to move.
                    }

                    // Always ensure the empty type directory exists in the main assets folder
                    try {
                        // console.log(`    Ensuring empty directory: ${sourceTypePath}`);
                        await fs.mkdir(sourceTypePath, { recursive: true });
                    } catch (mkdirErr) {
                         console.error(`    Failed to recreate directory ${sourceTypePath}:`, mkdirErr.message);
                    }
                } // End loop through FOLDER_TYPES

                 if (movedSomething) {
                     console.log(`  Finished backup for user [${userName}] to backup_${timestamp}`);
                 } else {
                     console.log(`  No existing asset types found to backup for user [${userName}]`);
                 }


            } catch (userDirStatErr) {
                 // Error checking if the item in assets is a directory
                 console.error(`Error processing potential user asset directory ${userAssetPath}:`, userDirStatErr.message);
            }
        } // End loop through userDirs

        console.log("--- Finished Asset Cleanup ---");

    } catch (error) {
        // Catch errors related to backup dir creation or reading the main assets dir
        console.error("!!! Critical Error during Asset Cleanup process:", error);
    }
}

// Export the function to be used elsewhere
module.exports = { performAssetCleanup };



utils/networkUtils.js

javascript
const os = require('os');

function getLocalIPs() {
    const interfaces = os.networkInterfaces();
    const ips = new Set(['localhost']); // Include localhost

    for (const iface of Object.values(interfaces)) {
        for (const addr of iface) {
            // Include IPv4 non-internal addresses
            if (addr.family === 'IPv4' && !addr.internal) {
                ips.add(addr.address);
            }
        }
    }
    return Array.from(ips);
}

function getPreferredLocalIP() {
    const ips = getLocalIPs();
    // Prioritize non-localhost, non-link-local (169.254) IPs
    // Often 192.168.* or 10.* or 172.16-31.* are common private ranges
    return ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost' && (ip.startsWith('192.168.') || ip.startsWith('10.') || ip.match(/^172\.(1[6-9]|2[0-9]|3[0-1])\./))) ||
           ips.find(ip => !ip.startsWith('169.254.') && ip !== 'localhost') || // Any other non-link-local
           'localhost'; // Fallback
}

module.exports = { getLocalIPs, getPreferredLocalIP };



